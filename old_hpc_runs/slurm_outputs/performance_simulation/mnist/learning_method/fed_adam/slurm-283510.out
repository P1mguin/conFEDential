ctit088
2024-04-30 19:56:30.440068: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-30 19:56:36.495656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-30 19:57:01.188887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-30 19:59:46,723 | batch_run_simulation.py:80 | Loaded 108 configs with name MINST-LOGISTICREGRESSION-FEDADAM, running...
INFO flwr 2024-04-30 19:59:46,724 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 19:59:55,035 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-30 20:00:04,420	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:00:06,604	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:00:06,690	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:00:06,692	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 20:00:15,901 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 169007507661.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 76717503283.0}
INFO flwr 2024-04-30 20:00:15,901 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:00:15,901 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:00:15,917 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:00:15,918 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:00:15,919 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:00:15,919 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1527971)[0m 2024-04-30 20:00:21.076406: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1527966)[0m 2024-04-30 20:00:21.166330: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1527966)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-30 20:00:23,607 | server.py:94 | initial parameters (loss, other metrics): 2.3044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-04-30 20:00:23,607 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:00:23,608 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1527971)[0m 2024-04-30 20:00:23.771092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1527971)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1527971)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1527959)[0m 2024-04-30 20:00:21.129551: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1527959)[0m 2024-04-30 20:00:21.214155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1527959)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1527959)[0m 2024-04-30 20:00:23.777574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:00:46,784 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:00:47,976 | server.py:125 | fit progress: (1, 1.9233347177505493, {'accuracy': 0.5507, 'data_size': 10000}, 24.368599092005752)
INFO flwr 2024-04-30 20:00:47,976 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:00:47,977 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:00:56,329 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:00:57,510 | server.py:125 | fit progress: (2, 1.8917611837387085, {'accuracy': 0.5637, 'data_size': 10000}, 33.90288050001254)
INFO flwr 2024-04-30 20:00:57,511 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:00:57,511 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:01:05,251 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:01:06,435 | server.py:125 | fit progress: (3, 1.7344903945922852, {'accuracy': 0.7311, 'data_size': 10000}, 42.827996247971896)
INFO flwr 2024-04-30 20:01:06,436 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:01:06,436 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:01:14,161 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:01:15,327 | server.py:125 | fit progress: (4, 1.7112596035003662, {'accuracy': 0.7536, 'data_size': 10000}, 51.71906197100179)
INFO flwr 2024-04-30 20:01:15,327 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:01:15,327 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:01:22,734 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:01:23,700 | server.py:125 | fit progress: (5, 1.6438645124435425, {'accuracy': 0.8178, 'data_size': 10000}, 60.0921765199746)
INFO flwr 2024-04-30 20:01:23,700 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:01:23,700 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:01:31,603 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:01:32,558 | server.py:125 | fit progress: (6, 1.6928263902664185, {'accuracy': 0.7656, 'data_size': 10000}, 68.95013384299818)
INFO flwr 2024-04-30 20:01:32,558 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:01:32,558 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:01:39,843 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:01:41,039 | server.py:125 | fit progress: (7, 1.6809344291687012, {'accuracy': 0.7792, 'data_size': 10000}, 77.43112713197479)
INFO flwr 2024-04-30 20:01:41,039 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:01:41,039 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:01:48,587 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:01:49,735 | server.py:125 | fit progress: (8, 1.6366034746170044, {'accuracy': 0.8248, 'data_size': 10000}, 86.12730567000108)
INFO flwr 2024-04-30 20:01:49,735 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:01:49,735 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:01:57,053 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:01:58,243 | server.py:125 | fit progress: (9, 1.6302285194396973, {'accuracy': 0.8307, 'data_size': 10000}, 94.63512604596326)
INFO flwr 2024-04-30 20:01:58,243 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:01:58,243 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:02:05,735 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:02:06,885 | server.py:125 | fit progress: (10, 1.6349691152572632, {'accuracy': 0.8257, 'data_size': 10000}, 103.27758433000417)
INFO flwr 2024-04-30 20:02:06,885 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:02:06,885 | server.py:153 | FL finished in 103.27792486699764
INFO flwr 2024-04-30 20:02:06,886 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:02:06,886 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:02:06,886 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:02:06,886 | app.py:229 | app_fit: losses_centralized [(0, 2.3044753074645996), (1, 1.9233347177505493), (2, 1.8917611837387085), (3, 1.7344903945922852), (4, 1.7112596035003662), (5, 1.6438645124435425), (6, 1.6928263902664185), (7, 1.6809344291687012), (8, 1.6366034746170044), (9, 1.6302285194396973), (10, 1.6349691152572632)]
INFO flwr 2024-04-30 20:02:06,886 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0654), (1, 0.5507), (2, 0.5637), (3, 0.7311), (4, 0.7536), (5, 0.8178), (6, 0.7656), (7, 0.7792), (8, 0.8248), (9, 0.8307), (10, 0.8257)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8257
wandb:     loss 1.63497
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_195951-dvc5kzky
wandb: Find logs at: ./wandb/offline-run-20240430_195951-dvc5kzky/logs
INFO flwr 2024-04-30 20:02:10,320 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:02:11,062 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1527959)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1527959)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:02:15,697	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:02:15,807	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:02:15,878	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:02:15,880	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 20:02:25,353 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 167911035904.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 76247586816.0}
INFO flwr 2024-04-30 20:02:25,353 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:02:25,353 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:02:25,366 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:02:25,367 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:02:25,367 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:02:25,368 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:02:27,750 | server.py:94 | initial parameters (loss, other metrics): 2.3022866249084473, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-30 20:02:27,750 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:02:27,751 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1531095)[0m 2024-04-30 20:02:30.619050: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1531098)[0m 2024-04-30 20:02:30.719287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1531098)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1531095)[0m 2024-04-30 20:02:32.596275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1531095)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1531095)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1531096)[0m 2024-04-30 20:02:30.786909: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1531096)[0m 2024-04-30 20:02:30.868109: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1531096)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1531096)[0m 2024-04-30 20:02:32.738709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:02:45,628 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:02:46,613 | server.py:125 | fit progress: (1, 1.924864411354065, {'accuracy': 0.5605, 'data_size': 10000}, 18.862761144002434)
INFO flwr 2024-04-30 20:02:46,614 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:02:46,614 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:02:54,984 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:02:56,165 | server.py:125 | fit progress: (2, 1.6679507493972778, {'accuracy': 0.8177, 'data_size': 10000}, 28.413964531966485)
INFO flwr 2024-04-30 20:02:56,165 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:02:56,165 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:03:03,917 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:03:05,117 | server.py:125 | fit progress: (3, 1.691148281097412, {'accuracy': 0.7704, 'data_size': 10000}, 37.366078728984576)
INFO flwr 2024-04-30 20:03:05,117 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:03:05,117 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:03:12,716 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:03:13,926 | server.py:125 | fit progress: (4, 1.635772466659546, {'accuracy': 0.8297, 'data_size': 10000}, 46.175369240983855)
INFO flwr 2024-04-30 20:03:13,926 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:03:13,926 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:03:21,494 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:03:22,475 | server.py:125 | fit progress: (5, 1.6071323156356812, {'accuracy': 0.8577, 'data_size': 10000}, 54.72477104899008)
INFO flwr 2024-04-30 20:03:22,476 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:03:22,476 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:03:30,326 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:03:31,297 | server.py:125 | fit progress: (6, 1.5947948694229126, {'accuracy': 0.8679, 'data_size': 10000}, 63.54646489198785)
INFO flwr 2024-04-30 20:03:31,297 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:03:31,297 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:03:38,907 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:03:40,090 | server.py:125 | fit progress: (7, 1.5905455350875854, {'accuracy': 0.8723, 'data_size': 10000}, 72.33976432395866)
INFO flwr 2024-04-30 20:03:40,091 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:03:40,091 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:03:47,612 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:03:48,807 | server.py:125 | fit progress: (8, 1.5859745740890503, {'accuracy': 0.876, 'data_size': 10000}, 81.05665572499856)
INFO flwr 2024-04-30 20:03:48,807 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:03:48,808 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:03:56,433 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:03:57,593 | server.py:125 | fit progress: (9, 1.56709885597229, {'accuracy': 0.895, 'data_size': 10000}, 89.8426676359959)
INFO flwr 2024-04-30 20:03:57,593 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:03:57,594 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:04:05,225 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:04:06,427 | server.py:125 | fit progress: (10, 1.5632339715957642, {'accuracy': 0.8983, 'data_size': 10000}, 98.67653763096314)
INFO flwr 2024-04-30 20:04:06,427 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:04:06,427 | server.py:153 | FL finished in 98.67687588598346
INFO flwr 2024-04-30 20:04:06,428 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:04:06,428 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:04:06,428 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:04:06,428 | app.py:229 | app_fit: losses_centralized [(0, 2.3022866249084473), (1, 1.924864411354065), (2, 1.6679507493972778), (3, 1.691148281097412), (4, 1.635772466659546), (5, 1.6071323156356812), (6, 1.5947948694229126), (7, 1.5905455350875854), (8, 1.5859745740890503), (9, 1.56709885597229), (10, 1.5632339715957642)]
INFO flwr 2024-04-30 20:04:06,428 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.5605), (2, 0.8177), (3, 0.7704), (4, 0.8297), (5, 0.8577), (6, 0.8679), (7, 0.8723), (8, 0.876), (9, 0.895), (10, 0.8983)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8983
wandb:     loss 1.56323
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_200210-ztkymrcz
wandb: Find logs at: ./wandb/offline-run-20240430_200210-ztkymrcz/logs
INFO flwr 2024-04-30 20:04:09,867 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:04:10,533 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1531085)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1531085)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:04:15,150	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:04:15,244	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:04:15,357	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:04:15,359	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 20:04:24,843 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 167656523162.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 76138509926.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 20:04:24,843 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:04:24,844 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:04:24,859 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:04:24,860 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:04:24,860 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:04:24,860 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:04:27,054 | server.py:94 | initial parameters (loss, other metrics): 2.302816867828369, {'accuracy': 0.0734, 'data_size': 10000}
INFO flwr 2024-04-30 20:04:27,054 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:04:27,055 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1534757)[0m 2024-04-30 20:04:30.089188: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1534757)[0m 2024-04-30 20:04:30.194934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1534757)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1534757)[0m 2024-04-30 20:04:32.070014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1534769)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1534769)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1534767)[0m 2024-04-30 20:04:30.262467: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1534767)[0m 2024-04-30 20:04:30.357779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1534767)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1534767)[0m 2024-04-30 20:04:32.213024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:04:45,371 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:04:46,544 | server.py:125 | fit progress: (1, 1.838166356086731, {'accuracy': 0.6681, 'data_size': 10000}, 19.489041272026952)
INFO flwr 2024-04-30 20:04:46,544 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:04:46,544 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:04:54,820 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:04:55,986 | server.py:125 | fit progress: (2, 1.6658214330673218, {'accuracy': 0.814, 'data_size': 10000}, 28.93130844301777)
INFO flwr 2024-04-30 20:04:55,986 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:04:55,986 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:05:03,611 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:05:04,782 | server.py:125 | fit progress: (3, 1.6092690229415894, {'accuracy': 0.8595, 'data_size': 10000}, 37.726876391039696)
INFO flwr 2024-04-30 20:05:04,782 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:05:04,782 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:05:12,278 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:05:13,437 | server.py:125 | fit progress: (4, 1.590683102607727, {'accuracy': 0.8728, 'data_size': 10000}, 46.382629966014065)
INFO flwr 2024-04-30 20:05:13,438 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:05:13,438 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:05:21,131 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:05:22,304 | server.py:125 | fit progress: (5, 1.595191240310669, {'accuracy': 0.8688, 'data_size': 10000}, 55.24918108503334)
INFO flwr 2024-04-30 20:05:22,304 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:05:22,304 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:05:30,210 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:05:31,191 | server.py:125 | fit progress: (6, 1.587022066116333, {'accuracy': 0.8758, 'data_size': 10000}, 64.13668131304439)
INFO flwr 2024-04-30 20:05:31,192 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:05:31,192 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:05:38,616 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:05:39,792 | server.py:125 | fit progress: (7, 1.5707764625549316, {'accuracy': 0.891, 'data_size': 10000}, 72.7377051890362)
INFO flwr 2024-04-30 20:05:39,793 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:05:39,793 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:05:47,622 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:05:48,796 | server.py:125 | fit progress: (8, 1.5597193241119385, {'accuracy': 0.9028, 'data_size': 10000}, 81.74168189201737)
INFO flwr 2024-04-30 20:05:48,797 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:05:48,797 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:05:56,082 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:05:57,251 | server.py:125 | fit progress: (9, 1.5574078559875488, {'accuracy': 0.9047, 'data_size': 10000}, 90.1959457230405)
INFO flwr 2024-04-30 20:05:57,251 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:05:57,251 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:06:05,029 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:06:06,198 | server.py:125 | fit progress: (10, 1.5579251050949097, {'accuracy': 0.9035, 'data_size': 10000}, 99.1433871689951)
INFO flwr 2024-04-30 20:06:06,198 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:06:06,198 | server.py:153 | FL finished in 99.1437739750254
INFO flwr 2024-04-30 20:06:06,199 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:06:06,199 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:06:06,199 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:06:06,199 | app.py:229 | app_fit: losses_centralized [(0, 2.302816867828369), (1, 1.838166356086731), (2, 1.6658214330673218), (3, 1.6092690229415894), (4, 1.590683102607727), (5, 1.595191240310669), (6, 1.587022066116333), (7, 1.5707764625549316), (8, 1.5597193241119385), (9, 1.5574078559875488), (10, 1.5579251050949097)]
INFO flwr 2024-04-30 20:06:06,199 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0734), (1, 0.6681), (2, 0.814), (3, 0.8595), (4, 0.8728), (5, 0.8688), (6, 0.8758), (7, 0.891), (8, 0.9028), (9, 0.9047), (10, 0.9035)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9035
wandb:     loss 1.55793
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_200410-n0y603oi
wandb: Find logs at: ./wandb/offline-run-20240430_200410-n0y603oi/logs
INFO flwr 2024-04-30 20:06:09,632 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:06:10,238 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1534757)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1534757)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:06:14,818	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:06:14,902	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:06:15,003	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:06:15,005	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 20:06:24,551 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 75903014092.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 167107032884.0}
INFO flwr 2024-04-30 20:06:24,551 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:06:24,551 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:06:24,563 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:06:24,564 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:06:24,564 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:06:24,564 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:06:27,096 | server.py:94 | initial parameters (loss, other metrics): 2.304152250289917, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-30 20:06:27,097 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:06:27,097 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1537902)[0m 2024-04-30 20:06:29.989538: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1537902)[0m 2024-04-30 20:06:30.086077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1537902)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1537902)[0m 2024-04-30 20:06:32.054861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1537903)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1537903)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1537899)[0m 2024-04-30 20:06:30.084049: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1537899)[0m 2024-04-30 20:06:30.170302: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1537899)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1537901)[0m 2024-04-30 20:06:32.182479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:06:47,073 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:06:48,290 | server.py:125 | fit progress: (1, 1.9148750305175781, {'accuracy': 0.5761, 'data_size': 10000}, 21.192573936015833)
INFO flwr 2024-04-30 20:06:48,290 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:06:48,290 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:06:58,105 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:06:59,115 | server.py:125 | fit progress: (2, 1.7685304880142212, {'accuracy': 0.7087, 'data_size': 10000}, 32.01788162300363)
INFO flwr 2024-04-30 20:06:59,115 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:06:59,115 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:07:08,246 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:07:09,232 | server.py:125 | fit progress: (3, 1.7030112743377686, {'accuracy': 0.764, 'data_size': 10000}, 42.13512243999867)
INFO flwr 2024-04-30 20:07:09,232 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:07:09,233 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:07:18,376 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:07:19,588 | server.py:125 | fit progress: (4, 1.7375408411026, {'accuracy': 0.7256, 'data_size': 10000}, 52.49110023101093)
INFO flwr 2024-04-30 20:07:19,588 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:07:19,589 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:07:28,273 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:07:29,483 | server.py:125 | fit progress: (5, 1.7286404371261597, {'accuracy': 0.7299, 'data_size': 10000}, 62.38555915799225)
INFO flwr 2024-04-30 20:07:29,483 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:07:29,483 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:07:38,386 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:07:39,623 | server.py:125 | fit progress: (6, 1.6218650341033936, {'accuracy': 0.8422, 'data_size': 10000}, 72.52557249297388)
INFO flwr 2024-04-30 20:07:39,623 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:07:39,623 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:07:48,102 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:07:49,106 | server.py:125 | fit progress: (7, 1.6069883108139038, {'accuracy': 0.8554, 'data_size': 10000}, 82.00887141999556)
INFO flwr 2024-04-30 20:07:49,106 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:07:49,106 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:07:58,137 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:07:59,131 | server.py:125 | fit progress: (8, 1.597864031791687, {'accuracy': 0.8631, 'data_size': 10000}, 92.03389873995911)
INFO flwr 2024-04-30 20:07:59,131 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:07:59,131 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:08:07,994 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:08:08,988 | server.py:125 | fit progress: (9, 1.5863783359527588, {'accuracy': 0.8757, 'data_size': 10000}, 101.89088922098745)
INFO flwr 2024-04-30 20:08:08,988 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:08:08,988 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:08:17,757 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:08:18,948 | server.py:125 | fit progress: (10, 1.5866063833236694, {'accuracy': 0.874, 'data_size': 10000}, 111.85076388897141)
INFO flwr 2024-04-30 20:08:18,948 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:08:18,948 | server.py:153 | FL finished in 111.85111668898026
INFO flwr 2024-04-30 20:08:18,948 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:08:18,948 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:08:18,948 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:08:18,949 | app.py:229 | app_fit: losses_centralized [(0, 2.304152250289917), (1, 1.9148750305175781), (2, 1.7685304880142212), (3, 1.7030112743377686), (4, 1.7375408411026), (5, 1.7286404371261597), (6, 1.6218650341033936), (7, 1.6069883108139038), (8, 1.597864031791687), (9, 1.5863783359527588), (10, 1.5866063833236694)]
INFO flwr 2024-04-30 20:08:18,949 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.5761), (2, 0.7087), (3, 0.764), (4, 0.7256), (5, 0.7299), (6, 0.8422), (7, 0.8554), (8, 0.8631), (9, 0.8757), (10, 0.874)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.874
wandb:     loss 1.58661
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_200609-tyqre9k4
wandb: Find logs at: ./wandb/offline-run-20240430_200609-tyqre9k4/logs
INFO flwr 2024-04-30 20:08:22,403 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:08:23,107 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1537895)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1537895)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:08:27,582	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:08:27,701	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:08:27,813	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:08:27,814	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 20:08:37,358 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 166226753741.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 75525751603.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 20:08:37,358 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:08:37,358 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:08:37,371 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:08:37,373 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:08:37,373 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:08:37,373 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:08:40,068 | server.py:94 | initial parameters (loss, other metrics): 2.3035645484924316, {'accuracy': 0.0768, 'data_size': 10000}
INFO flwr 2024-04-30 20:08:40,069 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:08:40,069 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1541677)[0m 2024-04-30 20:08:42.771339: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1541677)[0m 2024-04-30 20:08:42.865175: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1541677)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1541677)[0m 2024-04-30 20:08:44.808749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1541677)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1541677)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1541676)[0m 2024-04-30 20:08:42.961627: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1541676)[0m 2024-04-30 20:08:43.053458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1541676)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1541676)[0m 2024-04-30 20:08:44.927301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:08:59,353 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:09:00,553 | server.py:125 | fit progress: (1, 1.8687968254089355, {'accuracy': 0.6571, 'data_size': 10000}, 20.483794316998683)
INFO flwr 2024-04-30 20:09:00,553 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:09:00,553 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:09:09,364 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:09:10,540 | server.py:125 | fit progress: (2, 1.6601011753082275, {'accuracy': 0.8201, 'data_size': 10000}, 30.470764981000684)
INFO flwr 2024-04-30 20:09:10,540 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:09:10,540 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:09:19,359 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:09:20,555 | server.py:125 | fit progress: (3, 1.6134036779403687, {'accuracy': 0.8581, 'data_size': 10000}, 40.48621631396236)
INFO flwr 2024-04-30 20:09:20,556 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:09:20,556 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:09:29,253 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:09:30,219 | server.py:125 | fit progress: (4, 1.599589228630066, {'accuracy': 0.867, 'data_size': 10000}, 50.1501712409663)
INFO flwr 2024-04-30 20:09:30,219 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:09:30,220 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:09:39,329 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:09:40,306 | server.py:125 | fit progress: (5, 1.5737855434417725, {'accuracy': 0.89, 'data_size': 10000}, 60.237063999986276)
INFO flwr 2024-04-30 20:09:40,306 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:09:40,307 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:09:49,064 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:09:50,019 | server.py:125 | fit progress: (6, 1.5698425769805908, {'accuracy': 0.894, 'data_size': 10000}, 69.95004375500139)
INFO flwr 2024-04-30 20:09:50,019 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:09:50,020 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:09:58,700 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:09:59,875 | server.py:125 | fit progress: (7, 1.5640062093734741, {'accuracy': 0.8997, 'data_size': 10000}, 79.8059746309882)
INFO flwr 2024-04-30 20:09:59,875 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:09:59,875 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:10:08,771 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:10:09,946 | server.py:125 | fit progress: (8, 1.5810495615005493, {'accuracy': 0.8821, 'data_size': 10000}, 89.87672569899587)
INFO flwr 2024-04-30 20:10:09,946 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:10:09,946 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:10:18,829 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:10:20,013 | server.py:125 | fit progress: (9, 1.5708763599395752, {'accuracy': 0.8913, 'data_size': 10000}, 99.94443014799617)
INFO flwr 2024-04-30 20:10:20,014 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:10:20,014 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:10:29,044 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:10:30,019 | server.py:125 | fit progress: (10, 1.5547094345092773, {'accuracy': 0.9077, 'data_size': 10000}, 109.94995747396024)
INFO flwr 2024-04-30 20:10:30,019 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:10:30,019 | server.py:153 | FL finished in 109.95029986498412
INFO flwr 2024-04-30 20:10:30,019 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:10:30,020 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:10:30,020 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:10:30,020 | app.py:229 | app_fit: losses_centralized [(0, 2.3035645484924316), (1, 1.8687968254089355), (2, 1.6601011753082275), (3, 1.6134036779403687), (4, 1.599589228630066), (5, 1.5737855434417725), (6, 1.5698425769805908), (7, 1.5640062093734741), (8, 1.5810495615005493), (9, 1.5708763599395752), (10, 1.5547094345092773)]
INFO flwr 2024-04-30 20:10:30,020 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0768), (1, 0.6571), (2, 0.8201), (3, 0.8581), (4, 0.867), (5, 0.89), (6, 0.894), (7, 0.8997), (8, 0.8821), (9, 0.8913), (10, 0.9077)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9077
wandb:     loss 1.55471
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_200822-laiikbwn
wandb: Find logs at: ./wandb/offline-run-20240430_200822-laiikbwn/logs
INFO flwr 2024-04-30 20:10:33,476 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:10:34,126 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1541676)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1541676)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:10:38,781	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:10:38,896	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:10:38,982	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:10:38,984	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 20:10:48,510 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 165360291636.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 75154410700.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 20:10:48,510 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:10:48,510 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:10:48,523 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:10:48,524 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:10:48,525 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:10:48,525 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:10:51,019 | server.py:94 | initial parameters (loss, other metrics): 2.304069757461548, {'accuracy': 0.0882, 'data_size': 10000}
INFO flwr 2024-04-30 20:10:51,020 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:10:51,020 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1545144)[0m 2024-04-30 20:10:53.969562: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1545144)[0m 2024-04-30 20:10:54.059242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1545144)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1545144)[0m 2024-04-30 20:10:55.997418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1545145)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1545145)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1545146)[0m 2024-04-30 20:10:54.141676: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1545146)[0m 2024-04-30 20:10:54.234628: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1545146)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1545141)[0m 2024-04-30 20:10:56.105579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:11:10,789 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:11:11,979 | server.py:125 | fit progress: (1, 1.9062554836273193, {'accuracy': 0.5796, 'data_size': 10000}, 20.959573176980484)
INFO flwr 2024-04-30 20:11:11,980 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:11:11,980 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:11:21,845 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:11:23,068 | server.py:125 | fit progress: (2, 1.721468210220337, {'accuracy': 0.7506, 'data_size': 10000}, 32.04792305501178)
INFO flwr 2024-04-30 20:11:23,068 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:11:23,068 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:11:32,379 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:11:33,575 | server.py:125 | fit progress: (3, 1.6177091598510742, {'accuracy': 0.8526, 'data_size': 10000}, 42.55572049599141)
INFO flwr 2024-04-30 20:11:33,576 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:11:33,576 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:11:42,289 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:11:43,289 | server.py:125 | fit progress: (4, 1.575588583946228, {'accuracy': 0.8905, 'data_size': 10000}, 52.26956157601671)
INFO flwr 2024-04-30 20:11:43,290 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:11:43,290 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:11:52,140 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:11:53,318 | server.py:125 | fit progress: (5, 1.5756608247756958, {'accuracy': 0.8879, 'data_size': 10000}, 62.298582794028334)
INFO flwr 2024-04-30 20:11:53,319 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:11:53,319 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:12:02,283 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:12:03,496 | server.py:125 | fit progress: (6, 1.5713107585906982, {'accuracy': 0.8926, 'data_size': 10000}, 72.4764902460156)
INFO flwr 2024-04-30 20:12:03,496 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:12:03,497 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:12:11,999 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:12:13,200 | server.py:125 | fit progress: (7, 1.577120065689087, {'accuracy': 0.8847, 'data_size': 10000}, 82.17980797501514)
INFO flwr 2024-04-30 20:12:13,200 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:12:13,200 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:12:21,802 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:12:23,006 | server.py:125 | fit progress: (8, 1.5714362859725952, {'accuracy': 0.8913, 'data_size': 10000}, 91.98668482498033)
INFO flwr 2024-04-30 20:12:23,007 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:12:23,007 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:12:32,496 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:12:33,678 | server.py:125 | fit progress: (9, 1.561288833618164, {'accuracy': 0.9015, 'data_size': 10000}, 102.65839635999873)
INFO flwr 2024-04-30 20:12:33,678 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:12:33,679 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:12:42,882 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:12:44,100 | server.py:125 | fit progress: (10, 1.5651284456253052, {'accuracy': 0.8957, 'data_size': 10000}, 113.07982276502298)
INFO flwr 2024-04-30 20:12:44,100 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:12:44,100 | server.py:153 | FL finished in 113.0802082780283
INFO flwr 2024-04-30 20:12:44,100 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:12:44,100 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:12:44,100 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:12:44,100 | app.py:229 | app_fit: losses_centralized [(0, 2.304069757461548), (1, 1.9062554836273193), (2, 1.721468210220337), (3, 1.6177091598510742), (4, 1.575588583946228), (5, 1.5756608247756958), (6, 1.5713107585906982), (7, 1.577120065689087), (8, 1.5714362859725952), (9, 1.561288833618164), (10, 1.5651284456253052)]
INFO flwr 2024-04-30 20:12:44,100 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0882), (1, 0.5796), (2, 0.7506), (3, 0.8526), (4, 0.8905), (5, 0.8879), (6, 0.8926), (7, 0.8847), (8, 0.8913), (9, 0.9015), (10, 0.8957)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8957
wandb:     loss 1.56513
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_201033-3uxjtdiq
wandb: Find logs at: ./wandb/offline-run-20240430_201033-3uxjtdiq/logs
INFO flwr 2024-04-30 20:12:47,552 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:12:48,244 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1545137)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1545137)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:12:52,843	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:12:52,965	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:12:53,072	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:12:53,074	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 20:13:02,573 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 74865463296.0, 'node:10.20.240.18': 1.0, 'memory': 164686081024.0}
INFO flwr 2024-04-30 20:13:02,574 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:13:02,574 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:13:02,586 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:13:02,588 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:13:02,588 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:13:02,589 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:13:04,842 | server.py:94 | initial parameters (loss, other metrics): 2.299149513244629, {'accuracy': 0.1618, 'data_size': 10000}
INFO flwr 2024-04-30 20:13:04,843 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:13:04,843 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1548912)[0m 2024-04-30 20:13:08.212627: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1548912)[0m 2024-04-30 20:13:08.304265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1548912)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1548902)[0m 2024-04-30 20:13:10.224603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1548912)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1548912)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1548915)[0m 2024-04-30 20:13:08.400072: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1548915)[0m 2024-04-30 20:13:08.490069: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1548915)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1548915)[0m 2024-04-30 20:13:10.441884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:13:29,271 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:13:30,273 | server.py:125 | fit progress: (1, 1.9214540719985962, {'accuracy': 0.5713, 'data_size': 10000}, 25.429407981981058)
INFO flwr 2024-04-30 20:13:30,273 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:13:30,273 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:13:42,175 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:13:43,377 | server.py:125 | fit progress: (2, 1.678863286972046, {'accuracy': 0.8012, 'data_size': 10000}, 38.5336622360046)
INFO flwr 2024-04-30 20:13:43,377 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:13:43,377 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:13:54,485 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:13:55,686 | server.py:125 | fit progress: (3, 1.633241057395935, {'accuracy': 0.8379, 'data_size': 10000}, 50.84322482801508)
INFO flwr 2024-04-30 20:13:55,687 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:13:55,687 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:14:07,240 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:14:08,481 | server.py:125 | fit progress: (4, 1.603211760520935, {'accuracy': 0.8645, 'data_size': 10000}, 63.63815698900726)
INFO flwr 2024-04-30 20:14:08,482 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:14:08,482 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:14:19,950 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:14:21,151 | server.py:125 | fit progress: (5, 1.5946719646453857, {'accuracy': 0.8708, 'data_size': 10000}, 76.30817356100306)
INFO flwr 2024-04-30 20:14:21,152 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:14:21,152 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:14:32,818 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:14:34,009 | server.py:125 | fit progress: (6, 1.579431176185608, {'accuracy': 0.8834, 'data_size': 10000}, 89.16575481096515)
INFO flwr 2024-04-30 20:14:34,009 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:14:34,009 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:14:45,620 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:14:46,599 | server.py:125 | fit progress: (7, 1.5667465925216675, {'accuracy': 0.8974, 'data_size': 10000}, 101.75544280698523)
INFO flwr 2024-04-30 20:14:46,599 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:14:46,599 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:14:57,566 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:14:58,739 | server.py:125 | fit progress: (8, 1.5646705627441406, {'accuracy': 0.8987, 'data_size': 10000}, 113.89556100801565)
INFO flwr 2024-04-30 20:14:58,739 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:14:58,739 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:15:10,460 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:15:11,675 | server.py:125 | fit progress: (9, 1.562882423400879, {'accuracy': 0.8995, 'data_size': 10000}, 126.83233925298555)
INFO flwr 2024-04-30 20:15:11,676 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:15:11,676 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:15:23,036 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:15:24,205 | server.py:125 | fit progress: (10, 1.5600124597549438, {'accuracy': 0.902, 'data_size': 10000}, 139.361729317985)
INFO flwr 2024-04-30 20:15:24,205 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:15:24,205 | server.py:153 | FL finished in 139.36209141497966
INFO flwr 2024-04-30 20:15:24,205 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:15:24,205 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:15:24,205 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:15:24,206 | app.py:229 | app_fit: losses_centralized [(0, 2.299149513244629), (1, 1.9214540719985962), (2, 1.678863286972046), (3, 1.633241057395935), (4, 1.603211760520935), (5, 1.5946719646453857), (6, 1.579431176185608), (7, 1.5667465925216675), (8, 1.5646705627441406), (9, 1.562882423400879), (10, 1.5600124597549438)]
INFO flwr 2024-04-30 20:15:24,206 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1618), (1, 0.5713), (2, 0.8012), (3, 0.8379), (4, 0.8645), (5, 0.8708), (6, 0.8834), (7, 0.8974), (8, 0.8987), (9, 0.8995), (10, 0.902)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.902
wandb:     loss 1.56001
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_201247-tm2k6g4c
wandb: Find logs at: ./wandb/offline-run-20240430_201247-tm2k6g4c/logs
INFO flwr 2024-04-30 20:15:27,763 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:15:28,600 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1548902)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1548902)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:15:33,384	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:15:33,519	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:15:33,610	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:15:33,612	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 20:15:43,208 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 163815378330.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 74492304998.0}
INFO flwr 2024-04-30 20:15:43,209 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:15:43,209 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:15:43,223 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:15:43,225 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:15:43,225 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:15:43,225 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:15:45,741 | server.py:94 | initial parameters (loss, other metrics): 2.3048603534698486, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-30 20:15:45,742 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:15:45,742 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1552137)[0m 2024-04-30 20:15:48.600466: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1552134)[0m 2024-04-30 20:15:48.688317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1552134)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1552137)[0m 2024-04-30 20:15:50.644756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1552137)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1552137)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1552144)[0m 2024-04-30 20:15:48.791971: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1552144)[0m 2024-04-30 20:15:48.885065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1552144)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1552144)[0m 2024-04-30 20:15:50.778543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:16:09,158 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:16:10,145 | server.py:125 | fit progress: (1, 1.905321717262268, {'accuracy': 0.6215, 'data_size': 10000}, 24.40297976700822)
INFO flwr 2024-04-30 20:16:10,145 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:16:10,145 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:16:22,133 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:16:23,121 | server.py:125 | fit progress: (2, 1.6604340076446533, {'accuracy': 0.8261, 'data_size': 10000}, 37.37955260602757)
INFO flwr 2024-04-30 20:16:23,122 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:16:23,122 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:16:34,291 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:16:35,269 | server.py:125 | fit progress: (3, 1.5933170318603516, {'accuracy': 0.8766, 'data_size': 10000}, 49.52734200103441)
INFO flwr 2024-04-30 20:16:35,269 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:16:35,270 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:16:46,723 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:16:47,947 | server.py:125 | fit progress: (4, 1.5922400951385498, {'accuracy': 0.8747, 'data_size': 10000}, 62.20505894400412)
INFO flwr 2024-04-30 20:16:47,947 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:16:47,947 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:16:58,442 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:16:59,638 | server.py:125 | fit progress: (5, 1.5689566135406494, {'accuracy': 0.896, 'data_size': 10000}, 73.89579803700326)
INFO flwr 2024-04-30 20:16:59,638 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:16:59,638 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:17:10,869 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:17:12,068 | server.py:125 | fit progress: (6, 1.5581861734390259, {'accuracy': 0.9069, 'data_size': 10000}, 86.3263569950359)
INFO flwr 2024-04-30 20:17:12,068 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:17:12,069 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:17:23,319 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:17:24,305 | server.py:125 | fit progress: (7, 1.5573030710220337, {'accuracy': 0.9066, 'data_size': 10000}, 98.56272857601289)
INFO flwr 2024-04-30 20:17:24,305 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:17:24,305 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:17:36,154 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:17:37,378 | server.py:125 | fit progress: (8, 1.5532742738723755, {'accuracy': 0.9086, 'data_size': 10000}, 111.63649031700334)
INFO flwr 2024-04-30 20:17:37,379 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:17:37,379 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:17:48,506 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:17:49,694 | server.py:125 | fit progress: (9, 1.549823522567749, {'accuracy': 0.9128, 'data_size': 10000}, 123.95229989098152)
INFO flwr 2024-04-30 20:17:49,694 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:17:49,695 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:18:01,150 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:18:02,425 | server.py:125 | fit progress: (10, 1.5478484630584717, {'accuracy': 0.9146, 'data_size': 10000}, 136.6829045090126)
INFO flwr 2024-04-30 20:18:02,425 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:18:02,425 | server.py:153 | FL finished in 136.6832392939832
INFO flwr 2024-04-30 20:18:02,425 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:18:02,425 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:18:02,425 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:18:02,426 | app.py:229 | app_fit: losses_centralized [(0, 2.3048603534698486), (1, 1.905321717262268), (2, 1.6604340076446533), (3, 1.5933170318603516), (4, 1.5922400951385498), (5, 1.5689566135406494), (6, 1.5581861734390259), (7, 1.5573030710220337), (8, 1.5532742738723755), (9, 1.549823522567749), (10, 1.5478484630584717)]
INFO flwr 2024-04-30 20:18:02,426 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.6215), (2, 0.8261), (3, 0.8766), (4, 0.8747), (5, 0.896), (6, 0.9069), (7, 0.9066), (8, 0.9086), (9, 0.9128), (10, 0.9146)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9146
wandb:     loss 1.54785
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_201528-kwdfacrv
wandb: Find logs at: ./wandb/offline-run-20240430_201528-kwdfacrv/logs
INFO flwr 2024-04-30 20:18:05,910 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:18:06,599 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1552134)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1552134)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:18:11,282	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:18:11,373	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:18:11,482	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c174086bd2f2719b.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:18:11,484	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c174086bd2f2719b.zip'.
INFO flwr 2024-04-30 20:18:21,075 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 162995278848.0, 'node:__internal_head__': 1.0, 'object_store_memory': 74140833792.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 20:18:21,076 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:18:21,076 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:18:21,088 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:18:21,089 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:18:21,089 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:18:21,089 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:18:22,957 | server.py:94 | initial parameters (loss, other metrics): 2.3016185760498047, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-30 20:18:22,958 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:18:22,958 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1555941)[0m 2024-04-30 20:18:26.567289: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1555933)[0m 2024-04-30 20:18:26.702470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1555933)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1555933)[0m 2024-04-30 20:18:28.614309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1555933)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1555933)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1555939)[0m 2024-04-30 20:18:26.759277: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1555939)[0m 2024-04-30 20:18:26.844876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1555939)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1555939)[0m 2024-04-30 20:18:28.679819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:18:46,932 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:18:47,922 | server.py:125 | fit progress: (1, 1.926649570465088, {'accuracy': 0.5914, 'data_size': 10000}, 24.96436317503685)
INFO flwr 2024-04-30 20:18:47,922 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:18:47,923 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:19:00,019 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:19:01,208 | server.py:125 | fit progress: (2, 1.6980258226394653, {'accuracy': 0.7779, 'data_size': 10000}, 38.2498677020194)
INFO flwr 2024-04-30 20:19:01,208 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:19:01,208 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:19:12,741 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:19:13,944 | server.py:125 | fit progress: (3, 1.6174116134643555, {'accuracy': 0.8496, 'data_size': 10000}, 50.98577808099799)
INFO flwr 2024-04-30 20:19:13,944 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:19:13,944 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:19:26,142 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:19:27,114 | server.py:125 | fit progress: (4, 1.6247037649154663, {'accuracy': 0.839, 'data_size': 10000}, 64.15608654404059)
INFO flwr 2024-04-30 20:19:27,114 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:19:27,114 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:19:38,549 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:19:39,786 | server.py:125 | fit progress: (5, 1.637133240699768, {'accuracy': 0.8237, 'data_size': 10000}, 76.82764751603827)
INFO flwr 2024-04-30 20:19:39,786 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:19:39,786 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:19:51,413 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:19:52,598 | server.py:125 | fit progress: (6, 1.5836519002914429, {'accuracy': 0.8803, 'data_size': 10000}, 89.64012925804127)
INFO flwr 2024-04-30 20:19:52,598 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:19:52,598 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:20:03,886 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:20:05,074 | server.py:125 | fit progress: (7, 1.563910961151123, {'accuracy': 0.8995, 'data_size': 10000}, 102.11581272201147)
INFO flwr 2024-04-30 20:20:05,074 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:20:05,074 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:20:16,193 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:20:17,406 | server.py:125 | fit progress: (8, 1.5629324913024902, {'accuracy': 0.8995, 'data_size': 10000}, 114.44852875400102)
INFO flwr 2024-04-30 20:20:17,407 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:20:17,407 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:20:28,438 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:20:29,426 | server.py:125 | fit progress: (9, 1.556062936782837, {'accuracy': 0.9073, 'data_size': 10000}, 126.46767079504207)
INFO flwr 2024-04-30 20:20:29,426 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:20:29,426 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:20:40,273 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:20:41,497 | server.py:125 | fit progress: (10, 1.5522372722625732, {'accuracy': 0.9094, 'data_size': 10000}, 138.5389089329983)
INFO flwr 2024-04-30 20:20:41,497 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:20:41,497 | server.py:153 | FL finished in 138.53927091800142
INFO flwr 2024-04-30 20:20:41,497 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:20:41,497 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:20:41,497 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:20:41,497 | app.py:229 | app_fit: losses_centralized [(0, 2.3016185760498047), (1, 1.926649570465088), (2, 1.6980258226394653), (3, 1.6174116134643555), (4, 1.6247037649154663), (5, 1.637133240699768), (6, 1.5836519002914429), (7, 1.563910961151123), (8, 1.5629324913024902), (9, 1.556062936782837), (10, 1.5522372722625732)]
INFO flwr 2024-04-30 20:20:41,498 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.5914), (2, 0.7779), (3, 0.8496), (4, 0.839), (5, 0.8237), (6, 0.8803), (7, 0.8995), (8, 0.8995), (9, 0.9073), (10, 0.9094)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9094
wandb:     loss 1.55224
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_201806-yrqhdsyb
wandb: Find logs at: ./wandb/offline-run-20240430_201806-yrqhdsyb/logs
INFO flwr 2024-04-30 20:20:45,007 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:20:45,734 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1555939)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1555939)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:20:50,422	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:20:50,550	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:20:50,669	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c174086bd2f2719b.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:20:50,671	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c174086bd2f2719b.zip'.
INFO flwr 2024-04-30 20:21:00,259 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 73686497280.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 161935160320.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-30 20:21:00,259 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:21:00,259 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:21:00,272 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:21:00,272 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:21:00,273 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:21:00,273 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:21:02,745 | server.py:94 | initial parameters (loss, other metrics): 2.3034725189208984, {'accuracy': 0.077, 'data_size': 10000}
INFO flwr 2024-04-30 20:21:02,745 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:21:02,745 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1559178)[0m 2024-04-30 20:21:05.752731: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1559178)[0m 2024-04-30 20:21:05.846343: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1559178)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1559169)[0m 2024-04-30 20:21:07.818544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1559169)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1559169)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1559171)[0m 2024-04-30 20:21:05.982470: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1559171)[0m 2024-04-30 20:21:06.073900: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1559171)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1559170)[0m 2024-04-30 20:21:07.879538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:21:33,658 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:21:34,904 | server.py:125 | fit progress: (1, 1.926903486251831, {'accuracy': 0.5531, 'data_size': 10000}, 32.15923215600196)
INFO flwr 2024-04-30 20:21:34,905 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:21:34,905 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:21:53,838 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:21:54,797 | server.py:125 | fit progress: (2, 1.760779857635498, {'accuracy': 0.714, 'data_size': 10000}, 52.05158369400306)
INFO flwr 2024-04-30 20:21:54,797 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:21:54,797 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:22:12,487 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:22:13,492 | server.py:125 | fit progress: (3, 1.747007966041565, {'accuracy': 0.7184, 'data_size': 10000}, 70.7466754540219)
INFO flwr 2024-04-30 20:22:13,492 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:22:13,492 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:22:30,723 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:22:31,705 | server.py:125 | fit progress: (4, 1.7261732816696167, {'accuracy': 0.7355, 'data_size': 10000}, 88.96008458902361)
INFO flwr 2024-04-30 20:22:31,706 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:22:31,706 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:22:49,137 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:22:50,359 | server.py:125 | fit progress: (5, 1.7184277772903442, {'accuracy': 0.7426, 'data_size': 10000}, 107.61404007003875)
INFO flwr 2024-04-30 20:22:50,360 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:22:50,360 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:23:09,004 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:23:10,192 | server.py:125 | fit progress: (6, 1.7259093523025513, {'accuracy': 0.7352, 'data_size': 10000}, 127.4468347330112)
INFO flwr 2024-04-30 20:23:10,192 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:23:10,193 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:23:28,963 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:23:30,162 | server.py:125 | fit progress: (7, 1.7240246534347534, {'accuracy': 0.7359, 'data_size': 10000}, 147.41648664302193)
INFO flwr 2024-04-30 20:23:30,162 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:23:30,162 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:23:50,109 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:23:51,309 | server.py:125 | fit progress: (8, 1.7117561101913452, {'accuracy': 0.7485, 'data_size': 10000}, 168.56348000100115)
INFO flwr 2024-04-30 20:23:51,309 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:23:51,309 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:24:08,388 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:24:09,398 | server.py:125 | fit progress: (9, 1.713254451751709, {'accuracy': 0.7471, 'data_size': 10000}, 186.6526285490254)
INFO flwr 2024-04-30 20:24:09,398 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:24:09,398 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:24:28,178 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:24:29,383 | server.py:125 | fit progress: (10, 1.7129530906677246, {'accuracy': 0.7468, 'data_size': 10000}, 206.63731417403324)
INFO flwr 2024-04-30 20:24:29,383 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:24:29,383 | server.py:153 | FL finished in 206.6376887470251
INFO flwr 2024-04-30 20:24:29,383 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:24:29,383 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:24:29,383 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:24:29,383 | app.py:229 | app_fit: losses_centralized [(0, 2.3034725189208984), (1, 1.926903486251831), (2, 1.760779857635498), (3, 1.747007966041565), (4, 1.7261732816696167), (5, 1.7184277772903442), (6, 1.7259093523025513), (7, 1.7240246534347534), (8, 1.7117561101913452), (9, 1.713254451751709), (10, 1.7129530906677246)]
INFO flwr 2024-04-30 20:24:29,384 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.077), (1, 0.5531), (2, 0.714), (3, 0.7184), (4, 0.7355), (5, 0.7426), (6, 0.7352), (7, 0.7359), (8, 0.7485), (9, 0.7471), (10, 0.7468)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7468
wandb:     loss 1.71295
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_202045-2csgqoci
wandb: Find logs at: ./wandb/offline-run-20240430_202045-2csgqoci/logs
INFO flwr 2024-04-30 20:24:32,863 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:24:33,556 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1559167)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1559167)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:24:38,288	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:24:38,371	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:24:38,469	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c174086bd2f2719b.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:24:38,470	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c174086bd2f2719b.zip'.
INFO flwr 2024-04-30 20:24:48,043 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 73165374259.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 160719206605.0}
INFO flwr 2024-04-30 20:24:48,043 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:24:48,044 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:24:48,056 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:24:48,058 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:24:48,058 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:24:48,058 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:24:50,394 | server.py:94 | initial parameters (loss, other metrics): 2.30568265914917, {'accuracy': 0.0528, 'data_size': 10000}
INFO flwr 2024-04-30 20:24:50,395 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:24:50,395 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1563042)[0m 2024-04-30 20:24:53.545097: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1563038)[0m 2024-04-30 20:24:53.679147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1563038)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1563038)[0m 2024-04-30 20:24:55.605628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1563042)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1563042)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1563041)[0m 2024-04-30 20:24:53.660722: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1563041)[0m 2024-04-30 20:24:53.749755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1563041)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1563041)[0m 2024-04-30 20:24:55.598994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:25:19,339 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:25:20,557 | server.py:125 | fit progress: (1, 1.9001359939575195, {'accuracy': 0.6033, 'data_size': 10000}, 30.161467238969635)
INFO flwr 2024-04-30 20:25:20,557 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:25:20,557 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:25:38,187 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:25:39,152 | server.py:125 | fit progress: (2, 1.6908107995986938, {'accuracy': 0.7856, 'data_size': 10000}, 48.75658077199478)
INFO flwr 2024-04-30 20:25:39,152 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:25:39,152 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:25:57,023 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:25:58,018 | server.py:125 | fit progress: (3, 1.5969444513320923, {'accuracy': 0.8755, 'data_size': 10000}, 67.62296057800995)
INFO flwr 2024-04-30 20:25:58,018 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:25:58,019 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:26:17,031 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:26:17,998 | server.py:125 | fit progress: (4, 1.5772069692611694, {'accuracy': 0.889, 'data_size': 10000}, 87.60309964400949)
INFO flwr 2024-04-30 20:26:17,998 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:26:17,999 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:26:37,569 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:26:38,792 | server.py:125 | fit progress: (5, 1.56890869140625, {'accuracy': 0.8947, 'data_size': 10000}, 108.39662386698183)
INFO flwr 2024-04-30 20:26:38,792 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:26:38,792 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:26:56,096 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:26:57,291 | server.py:125 | fit progress: (6, 1.56058931350708, {'accuracy': 0.9024, 'data_size': 10000}, 126.89551656297408)
INFO flwr 2024-04-30 20:26:57,291 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:26:57,291 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:27:15,339 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:27:16,548 | server.py:125 | fit progress: (7, 1.5733015537261963, {'accuracy': 0.8893, 'data_size': 10000}, 146.15248756902292)
INFO flwr 2024-04-30 20:27:16,548 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:27:16,548 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:27:35,740 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:27:36,945 | server.py:125 | fit progress: (8, 1.561210036277771, {'accuracy': 0.9018, 'data_size': 10000}, 166.54976160702063)
INFO flwr 2024-04-30 20:27:36,945 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:27:36,945 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:27:55,383 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:27:56,373 | server.py:125 | fit progress: (9, 1.5554358959197998, {'accuracy': 0.9067, 'data_size': 10000}, 185.97767741797725)
INFO flwr 2024-04-30 20:27:56,373 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:27:56,373 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:28:13,018 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:28:14,217 | server.py:125 | fit progress: (10, 1.552282691001892, {'accuracy': 0.9099, 'data_size': 10000}, 203.8217269339948)
INFO flwr 2024-04-30 20:28:14,217 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:28:14,217 | server.py:153 | FL finished in 203.8221197189996
INFO flwr 2024-04-30 20:28:14,217 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:28:14,217 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:28:14,218 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:28:14,218 | app.py:229 | app_fit: losses_centralized [(0, 2.30568265914917), (1, 1.9001359939575195), (2, 1.6908107995986938), (3, 1.5969444513320923), (4, 1.5772069692611694), (5, 1.56890869140625), (6, 1.56058931350708), (7, 1.5733015537261963), (8, 1.561210036277771), (9, 1.5554358959197998), (10, 1.552282691001892)]
INFO flwr 2024-04-30 20:28:14,218 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0528), (1, 0.6033), (2, 0.7856), (3, 0.8755), (4, 0.889), (5, 0.8947), (6, 0.9024), (7, 0.8893), (8, 0.9018), (9, 0.9067), (10, 0.9099)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9099
wandb:     loss 1.55228
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_202433-bnqdpd5x
wandb: Find logs at: ./wandb/offline-run-20240430_202433-bnqdpd5x/logs
INFO flwr 2024-04-30 20:28:17,708 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:28:18,389 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1563038)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1563038)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:28:23,071	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:28:23,165	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:28:23,242	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c174086bd2f2719b.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:28:23,244	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c174086bd2f2719b.zip'.
INFO flwr 2024-04-30 20:28:32,833 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 72653872742.0, 'memory': 159525703066.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 20:28:32,834 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:28:32,834 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:28:32,847 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:28:32,848 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:28:32,849 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:28:32,849 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:28:35,271 | server.py:94 | initial parameters (loss, other metrics): 2.3044581413269043, {'accuracy': 0.0625, 'data_size': 10000}
INFO flwr 2024-04-30 20:28:35,272 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:28:35,272 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1566876)[0m 2024-04-30 20:28:38.272781: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1566876)[0m 2024-04-30 20:28:38.367340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1566876)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1566884)[0m 2024-04-30 20:28:40.331949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1566884)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1566884)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1566886)[0m 2024-04-30 20:28:38.430191: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1566880)[0m 2024-04-30 20:28:38.430806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1566880)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1566883)[0m 2024-04-30 20:28:40.358975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:29:07,288 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:29:08,484 | server.py:125 | fit progress: (1, 1.8969132900238037, {'accuracy': 0.6366, 'data_size': 10000}, 33.21254161000252)
INFO flwr 2024-04-30 20:29:08,485 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:29:08,485 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:29:28,895 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:29:29,923 | server.py:125 | fit progress: (2, 1.6688446998596191, {'accuracy': 0.8253, 'data_size': 10000}, 54.651630552019924)
INFO flwr 2024-04-30 20:29:29,924 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:29:29,924 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:29:49,669 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:29:50,646 | server.py:125 | fit progress: (3, 1.6063131093978882, {'accuracy': 0.8679, 'data_size': 10000}, 75.37393343501026)
INFO flwr 2024-04-30 20:29:50,646 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:29:50,646 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:30:08,371 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:30:09,370 | server.py:125 | fit progress: (4, 1.5876268148422241, {'accuracy': 0.879, 'data_size': 10000}, 94.09809886402218)
INFO flwr 2024-04-30 20:30:09,370 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:30:09,370 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:30:26,403 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:30:27,608 | server.py:125 | fit progress: (5, 1.5732426643371582, {'accuracy': 0.8925, 'data_size': 10000}, 112.33596410002792)
INFO flwr 2024-04-30 20:30:27,608 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:30:27,608 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:30:46,512 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:30:47,716 | server.py:125 | fit progress: (6, 1.5615675449371338, {'accuracy': 0.9022, 'data_size': 10000}, 132.4444638430141)
INFO flwr 2024-04-30 20:30:47,717 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:30:47,717 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:31:06,686 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:31:07,895 | server.py:125 | fit progress: (7, 1.5571237802505493, {'accuracy': 0.9062, 'data_size': 10000}, 152.6228687610128)
INFO flwr 2024-04-30 20:31:07,895 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:31:07,895 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:31:26,856 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:31:28,047 | server.py:125 | fit progress: (8, 1.5515989065170288, {'accuracy': 0.9106, 'data_size': 10000}, 172.7750893330085)
INFO flwr 2024-04-30 20:31:28,047 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:31:28,047 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:31:49,191 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:31:50,187 | server.py:125 | fit progress: (9, 1.5485080480575562, {'accuracy': 0.9147, 'data_size': 10000}, 194.91557992302114)
INFO flwr 2024-04-30 20:31:50,188 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:31:50,188 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:32:09,175 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:32:10,372 | server.py:125 | fit progress: (10, 1.5507673025131226, {'accuracy': 0.9121, 'data_size': 10000}, 215.10006614704616)
INFO flwr 2024-04-30 20:32:10,372 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:32:10,372 | server.py:153 | FL finished in 215.10040807904443
INFO flwr 2024-04-30 20:32:10,372 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:32:10,372 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:32:10,372 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:32:10,373 | app.py:229 | app_fit: losses_centralized [(0, 2.3044581413269043), (1, 1.8969132900238037), (2, 1.6688446998596191), (3, 1.6063131093978882), (4, 1.5876268148422241), (5, 1.5732426643371582), (6, 1.5615675449371338), (7, 1.5571237802505493), (8, 1.5515989065170288), (9, 1.5485080480575562), (10, 1.5507673025131226)]
INFO flwr 2024-04-30 20:32:10,373 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0625), (1, 0.6366), (2, 0.8253), (3, 0.8679), (4, 0.879), (5, 0.8925), (6, 0.9022), (7, 0.9062), (8, 0.9106), (9, 0.9147), (10, 0.9121)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9121
wandb:     loss 1.55077
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_202817-1qudoof1
wandb: Find logs at: ./wandb/offline-run-20240430_202817-1qudoof1/logs
INFO flwr 2024-04-30 20:32:13,855 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:32:14,503 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1566876)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1566876)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:32:19,097	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:32:19,200	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:32:19,275	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c174086bd2f2719b.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:32:19,277	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c174086bd2f2719b.zip'.
INFO flwr 2024-04-30 20:32:29,405 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 158110499021.0, 'node:__internal_head__': 1.0, 'object_store_memory': 72047356723.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-30 20:32:29,406 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:32:29,406 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:32:29,418 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:32:29,419 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:32:29,419 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:32:29,419 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:32:31,703 | server.py:94 | initial parameters (loss, other metrics): 2.3048858642578125, {'accuracy': 0.0696, 'data_size': 10000}
INFO flwr 2024-04-30 20:32:31,703 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:32:31,704 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1570173)[0m 2024-04-30 20:32:34.885711: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1570173)[0m 2024-04-30 20:32:34.976239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1570173)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1570173)[0m 2024-04-30 20:32:36.940486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1570173)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1570173)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1570175)[0m 2024-04-30 20:32:35.222992: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1570175)[0m 2024-04-30 20:32:35.312719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1570175)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1570175)[0m 2024-04-30 20:32:37.116805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:33:18,909 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:33:20,152 | server.py:125 | fit progress: (1, 1.86466646194458, {'accuracy': 0.6728, 'data_size': 10000}, 48.44806441501714)
INFO flwr 2024-04-30 20:33:20,152 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:33:20,152 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:33:52,703 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:33:53,707 | server.py:125 | fit progress: (2, 1.6321086883544922, {'accuracy': 0.854, 'data_size': 10000}, 82.00342550803907)
INFO flwr 2024-04-30 20:33:53,707 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:33:53,707 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:34:29,190 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:34:30,188 | server.py:125 | fit progress: (3, 1.6301747560501099, {'accuracy': 0.8392, 'data_size': 10000}, 118.48435190704186)
INFO flwr 2024-04-30 20:34:30,188 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:34:30,188 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:35:01,482 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:35:02,472 | server.py:125 | fit progress: (4, 1.6068679094314575, {'accuracy': 0.8605, 'data_size': 10000}, 150.76787689502817)
INFO flwr 2024-04-30 20:35:02,472 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:35:02,472 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:35:33,118 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:35:34,340 | server.py:125 | fit progress: (5, 1.5700767040252686, {'accuracy': 0.8937, 'data_size': 10000}, 182.6360987770022)
INFO flwr 2024-04-30 20:35:34,340 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:35:34,340 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:36:06,358 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:36:07,566 | server.py:125 | fit progress: (6, 1.586313247680664, {'accuracy': 0.8752, 'data_size': 10000}, 215.86222347902367)
INFO flwr 2024-04-30 20:36:07,566 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:36:07,566 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:36:37,409 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:36:38,622 | server.py:125 | fit progress: (7, 1.5664525032043457, {'accuracy': 0.8967, 'data_size': 10000}, 246.91861704399344)
INFO flwr 2024-04-30 20:36:38,622 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:36:38,623 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:37:13,431 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:37:14,617 | server.py:125 | fit progress: (8, 1.5605647563934326, {'accuracy': 0.9016, 'data_size': 10000}, 282.91371437802445)
INFO flwr 2024-04-30 20:37:14,618 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:37:14,618 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:37:47,341 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:37:48,337 | server.py:125 | fit progress: (9, 1.5720211267471313, {'accuracy': 0.889, 'data_size': 10000}, 316.63380679499824)
INFO flwr 2024-04-30 20:37:48,338 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:37:48,338 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:38:21,163 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:38:22,360 | server.py:125 | fit progress: (10, 1.58176851272583, {'accuracy': 0.8789, 'data_size': 10000}, 350.6563658870291)
INFO flwr 2024-04-30 20:38:22,360 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:38:22,360 | server.py:153 | FL finished in 350.6567050490412
INFO flwr 2024-04-30 20:38:22,360 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:38:22,361 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:38:22,361 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:38:22,361 | app.py:229 | app_fit: losses_centralized [(0, 2.3048858642578125), (1, 1.86466646194458), (2, 1.6321086883544922), (3, 1.6301747560501099), (4, 1.6068679094314575), (5, 1.5700767040252686), (6, 1.586313247680664), (7, 1.5664525032043457), (8, 1.5605647563934326), (9, 1.5720211267471313), (10, 1.58176851272583)]
INFO flwr 2024-04-30 20:38:22,361 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0696), (1, 0.6728), (2, 0.854), (3, 0.8392), (4, 0.8605), (5, 0.8937), (6, 0.8752), (7, 0.8967), (8, 0.9016), (9, 0.889), (10, 0.8789)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8789
wandb:     loss 1.58177
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_203214-kro7km1w
wandb: Find logs at: ./wandb/offline-run-20240430_203214-kro7km1w/logs
INFO flwr 2024-04-30 20:38:25,835 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:38:26,632 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1570164)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1570164)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:38:31,349	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:38:31,440	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:38:31,533	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c174086bd2f2719b.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:38:31,535	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c174086bd2f2719b.zip'.
INFO flwr 2024-04-30 20:38:44,878 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 71151544320.0, 'node:10.20.240.18': 1.0, 'memory': 156020270080.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 20:38:44,878 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:38:44,878 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:38:44,891 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:38:44,892 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:38:44,892 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:38:44,892 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:38:47,892 | server.py:94 | initial parameters (loss, other metrics): 2.3002536296844482, {'accuracy': 0.1356, 'data_size': 10000}
INFO flwr 2024-04-30 20:38:47,892 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:38:47,892 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1574713)[0m 2024-04-30 20:38:50.310409: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1574713)[0m 2024-04-30 20:38:50.404819: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1574713)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1574713)[0m 2024-04-30 20:38:52.337923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1574713)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1574713)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1574702)[0m 2024-04-30 20:38:50.471165: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1574702)[0m 2024-04-30 20:38:50.560081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1574702)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1574702)[0m 2024-04-30 20:38:52.448201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:39:33,197 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:39:34,390 | server.py:125 | fit progress: (1, 1.8099123239517212, {'accuracy': 0.736, 'data_size': 10000}, 46.49771744699683)
INFO flwr 2024-04-30 20:39:34,390 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:39:34,390 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:40:05,766 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:40:06,785 | server.py:125 | fit progress: (2, 1.6845148801803589, {'accuracy': 0.7911, 'data_size': 10000}, 78.89295262697851)
INFO flwr 2024-04-30 20:40:06,785 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:40:06,786 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:40:42,573 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:40:43,554 | server.py:125 | fit progress: (3, 1.6237412691116333, {'accuracy': 0.8463, 'data_size': 10000}, 115.66186658898368)
INFO flwr 2024-04-30 20:40:43,554 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:40:43,554 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:41:16,117 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:41:17,093 | server.py:125 | fit progress: (4, 1.5796257257461548, {'accuracy': 0.8873, 'data_size': 10000}, 149.20124892599415)
INFO flwr 2024-04-30 20:41:17,094 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:41:17,094 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:41:49,810 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:41:51,035 | server.py:125 | fit progress: (5, 1.5612072944641113, {'accuracy': 0.9026, 'data_size': 10000}, 183.1430529599893)
INFO flwr 2024-04-30 20:41:51,035 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:41:51,036 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:42:24,495 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:42:25,694 | server.py:125 | fit progress: (6, 1.5575294494628906, {'accuracy': 0.9063, 'data_size': 10000}, 217.80139586899895)
INFO flwr 2024-04-30 20:42:25,694 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:42:25,694 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:42:57,324 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:42:58,523 | server.py:125 | fit progress: (7, 1.5550240278244019, {'accuracy': 0.9066, 'data_size': 10000}, 250.63105706195347)
INFO flwr 2024-04-30 20:42:58,523 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:42:58,524 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:43:31,938 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:43:33,157 | server.py:125 | fit progress: (8, 1.556351900100708, {'accuracy': 0.9066, 'data_size': 10000}, 285.26457258395385)
INFO flwr 2024-04-30 20:43:33,157 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:43:33,157 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:44:07,879 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:44:08,873 | server.py:125 | fit progress: (9, 1.5600411891937256, {'accuracy': 0.9025, 'data_size': 10000}, 320.9809625679627)
INFO flwr 2024-04-30 20:44:08,873 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:44:08,873 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:44:44,719 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:44:45,950 | server.py:125 | fit progress: (10, 1.5596413612365723, {'accuracy': 0.9027, 'data_size': 10000}, 358.05743823898956)
INFO flwr 2024-04-30 20:44:45,950 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:44:45,950 | server.py:153 | FL finished in 358.0577952169697
INFO flwr 2024-04-30 20:44:45,950 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:44:45,950 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:44:45,950 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:44:45,950 | app.py:229 | app_fit: losses_centralized [(0, 2.3002536296844482), (1, 1.8099123239517212), (2, 1.6845148801803589), (3, 1.6237412691116333), (4, 1.5796257257461548), (5, 1.5612072944641113), (6, 1.5575294494628906), (7, 1.5550240278244019), (8, 1.556351900100708), (9, 1.5600411891937256), (10, 1.5596413612365723)]
INFO flwr 2024-04-30 20:44:45,950 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1356), (1, 0.736), (2, 0.7911), (3, 0.8463), (4, 0.8873), (5, 0.9026), (6, 0.9063), (7, 0.9066), (8, 0.9066), (9, 0.9025), (10, 0.9027)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9027
wandb:     loss 1.55964
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_203826-lw6tuw25
wandb: Find logs at: ./wandb/offline-run-20240430_203826-lw6tuw25/logs
INFO flwr 2024-04-30 20:44:49,435 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:44:50,127 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1574711)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1574711)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:44:54,866	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:44:54,961	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:44:55,046	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c174086bd2f2719b.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:44:55,047	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c174086bd2f2719b.zip'.
INFO flwr 2024-04-30 20:45:04,615 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 153786446029.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 70194191155.0}
INFO flwr 2024-04-30 20:45:04,615 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:45:04,615 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:45:04,631 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:45:04,631 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:45:04,632 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:45:04,632 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:45:07,090 | server.py:94 | initial parameters (loss, other metrics): 2.3023762702941895, {'accuracy': 0.0925, 'data_size': 10000}
INFO flwr 2024-04-30 20:45:07,090 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:45:07,091 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1578967)[0m 2024-04-30 20:45:10.034826: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1578967)[0m 2024-04-30 20:45:10.127817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1578967)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1578967)[0m 2024-04-30 20:45:12.061673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1578967)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1578967)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1578975)[0m 2024-04-30 20:45:10.249533: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1578975)[0m 2024-04-30 20:45:10.338907: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1578975)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1578975)[0m 2024-04-30 20:45:12.233741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:45:54,002 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:45:55,207 | server.py:125 | fit progress: (1, 1.8686013221740723, {'accuracy': 0.7034, 'data_size': 10000}, 48.116468359017745)
INFO flwr 2024-04-30 20:45:55,207 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:45:55,208 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:46:23,768 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:46:24,751 | server.py:125 | fit progress: (2, 1.6976970434188843, {'accuracy': 0.7974, 'data_size': 10000}, 77.66080750798574)
INFO flwr 2024-04-30 20:46:24,752 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:46:24,752 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:46:55,970 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:46:56,951 | server.py:125 | fit progress: (3, 1.6223671436309814, {'accuracy': 0.8479, 'data_size': 10000}, 109.8601523739635)
INFO flwr 2024-04-30 20:46:56,951 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:46:56,951 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:47:31,773 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:47:32,782 | server.py:125 | fit progress: (4, 1.5936038494110107, {'accuracy': 0.8723, 'data_size': 10000}, 145.6916913089808)
INFO flwr 2024-04-30 20:47:32,783 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:47:32,783 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:48:08,173 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:48:09,378 | server.py:125 | fit progress: (5, 1.5754177570343018, {'accuracy': 0.8876, 'data_size': 10000}, 182.28710970899556)
INFO flwr 2024-04-30 20:48:09,378 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:48:09,378 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:48:39,742 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:48:40,960 | server.py:125 | fit progress: (6, 1.5593265295028687, {'accuracy': 0.9034, 'data_size': 10000}, 213.8691359249642)
INFO flwr 2024-04-30 20:48:40,960 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:48:40,960 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:49:16,858 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:49:18,059 | server.py:125 | fit progress: (7, 1.5590415000915527, {'accuracy': 0.9046, 'data_size': 10000}, 250.96817883301992)
INFO flwr 2024-04-30 20:49:18,059 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:49:18,059 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:49:50,185 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 20:49:51,410 | server.py:125 | fit progress: (8, 1.5589460134506226, {'accuracy': 0.9047, 'data_size': 10000}, 284.3188576179673)
INFO flwr 2024-04-30 20:49:51,410 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 20:49:51,410 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:50:25,076 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 20:50:26,065 | server.py:125 | fit progress: (9, 1.556326985359192, {'accuracy': 0.9045, 'data_size': 10000}, 318.97398746002)
INFO flwr 2024-04-30 20:50:26,065 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 20:50:26,065 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:51:00,181 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 20:51:01,394 | server.py:125 | fit progress: (10, 1.553071141242981, {'accuracy': 0.9085, 'data_size': 10000}, 354.3037843789789)
INFO flwr 2024-04-30 20:51:01,395 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 20:51:01,395 | server.py:153 | FL finished in 354.3041150569916
INFO flwr 2024-04-30 20:51:01,395 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 20:51:01,395 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 20:51:01,395 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 20:51:01,395 | app.py:229 | app_fit: losses_centralized [(0, 2.3023762702941895), (1, 1.8686013221740723), (2, 1.6976970434188843), (3, 1.6223671436309814), (4, 1.5936038494110107), (5, 1.5754177570343018), (6, 1.5593265295028687), (7, 1.5590415000915527), (8, 1.5589460134506226), (9, 1.556326985359192), (10, 1.553071141242981)]
INFO flwr 2024-04-30 20:51:01,395 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0925), (1, 0.7034), (2, 0.7974), (3, 0.8479), (4, 0.8723), (5, 0.8876), (6, 0.9034), (7, 0.9046), (8, 0.9047), (9, 0.9045), (10, 0.9085)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9085
wandb:     loss 1.55307
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_204449-hr67pzb9
wandb: Find logs at: ./wandb/offline-run-20240430_204449-hr67pzb9/logs
INFO flwr 2024-04-30 20:51:04,871 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 20:51:05,564 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1578962)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1578962)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 20:51:10,440	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 20:51:10,539	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 20:51:10,648	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 20:51:10,650	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 20:51:20,253 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69354995712.0, 'memory': 151828323328.0}
INFO flwr 2024-04-30 20:51:20,254 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 20:51:20,254 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 20:51:20,269 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 20:51:20,271 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 20:51:20,271 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 20:51:20,271 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 20:51:22,676 | server.py:94 | initial parameters (loss, other metrics): 2.3067264556884766, {'accuracy': 0.0637, 'data_size': 10000}
INFO flwr 2024-04-30 20:51:22,677 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 20:51:22,677 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1582950)[0m 2024-04-30 20:51:25.699815: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1582950)[0m 2024-04-30 20:51:25.794630: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1582950)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1582950)[0m 2024-04-30 20:51:27.745216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1582950)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1582950)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1582952)[0m 2024-04-30 20:51:25.874797: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1582952)[0m 2024-04-30 20:51:25.967469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1582952)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1582954)[0m 2024-04-30 20:51:27.857903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 20:52:40,335 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 20:52:41,561 | server.py:125 | fit progress: (1, 1.856887936592102, {'accuracy': 0.6796, 'data_size': 10000}, 78.88413239299553)
INFO flwr 2024-04-30 20:52:41,561 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 20:52:41,561 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:53:49,650 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 20:53:50,892 | server.py:125 | fit progress: (2, 1.689527988433838, {'accuracy': 0.7883, 'data_size': 10000}, 148.21561934199417)
INFO flwr 2024-04-30 20:53:50,893 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 20:53:50,893 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:54:54,574 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 20:54:55,804 | server.py:125 | fit progress: (3, 1.6091545820236206, {'accuracy': 0.86, 'data_size': 10000}, 213.1276889939909)
INFO flwr 2024-04-30 20:54:55,805 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 20:54:55,805 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:56:03,578 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 20:56:04,559 | server.py:125 | fit progress: (4, 1.5944260358810425, {'accuracy': 0.874, 'data_size': 10000}, 281.88201192597626)
INFO flwr 2024-04-30 20:56:04,559 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 20:56:04,559 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:57:15,515 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 20:57:16,543 | server.py:125 | fit progress: (5, 1.5632350444793701, {'accuracy': 0.9017, 'data_size': 10000}, 353.86639626696706)
INFO flwr 2024-04-30 20:57:16,543 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 20:57:16,544 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:58:17,649 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 20:58:18,645 | server.py:125 | fit progress: (6, 1.5649850368499756, {'accuracy': 0.9001, 'data_size': 10000}, 415.967873674992)
INFO flwr 2024-04-30 20:58:18,645 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 20:58:18,645 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 20:59:25,318 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 20:59:26,518 | server.py:125 | fit progress: (7, 1.573552131652832, {'accuracy': 0.8894, 'data_size': 10000}, 483.8408309809747)
INFO flwr 2024-04-30 20:59:26,518 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 20:59:26,518 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:00:22,378 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:00:23,594 | server.py:125 | fit progress: (8, 1.566773772239685, {'accuracy': 0.8967, 'data_size': 10000}, 540.9172254530131)
INFO flwr 2024-04-30 21:00:23,594 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:00:23,594 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:01:20,163 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:01:21,172 | server.py:125 | fit progress: (9, 1.5752928256988525, {'accuracy': 0.8873, 'data_size': 10000}, 598.4950000029639)
INFO flwr 2024-04-30 21:01:21,172 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:01:21,172 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:02:29,329 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:02:30,549 | server.py:125 | fit progress: (10, 1.5542751550674438, {'accuracy': 0.9089, 'data_size': 10000}, 667.8721159949782)
INFO flwr 2024-04-30 21:02:30,549 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:02:30,549 | server.py:153 | FL finished in 667.8724903009715
INFO flwr 2024-04-30 21:02:30,549 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:02:30,549 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:02:30,550 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:02:30,550 | app.py:229 | app_fit: losses_centralized [(0, 2.3067264556884766), (1, 1.856887936592102), (2, 1.689527988433838), (3, 1.6091545820236206), (4, 1.5944260358810425), (5, 1.5632350444793701), (6, 1.5649850368499756), (7, 1.573552131652832), (8, 1.566773772239685), (9, 1.5752928256988525), (10, 1.5542751550674438)]
INFO flwr 2024-04-30 21:02:30,550 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0637), (1, 0.6796), (2, 0.7883), (3, 0.86), (4, 0.874), (5, 0.9017), (6, 0.9001), (7, 0.8894), (8, 0.8967), (9, 0.8873), (10, 0.9089)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9089
wandb:     loss 1.55428
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_205105-4fc5ejyo
wandb: Find logs at: ./wandb/offline-run-20240430_205105-4fc5ejyo/logs
INFO flwr 2024-04-30 21:02:34,035 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:02:34,974 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1582947)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1582947)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:02:39,727	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:02:39,818	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:02:39,899	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:02:39,901	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:02:49,464 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 75879799603.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 167052865741.0}
INFO flwr 2024-04-30 21:02:49,465 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:02:49,465 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:02:49,477 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:02:49,480 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:02:49,480 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:02:49,480 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:02:52,028 | server.py:94 | initial parameters (loss, other metrics): 2.3047380447387695, {'accuracy': 0.0849, 'data_size': 10000}
INFO flwr 2024-04-30 21:02:52,028 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:02:52,028 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1587791)[0m 2024-04-30 21:02:54.903423: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1587798)[0m 2024-04-30 21:02:54.999139: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1587798)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1587791)[0m 2024-04-30 21:02:56.928946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1587791)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1587791)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1587803)[0m 2024-04-30 21:02:55.058257: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1587803)[0m 2024-04-30 21:02:55.145192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1587803)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1587803)[0m 2024-04-30 21:02:57.114093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:04:10,624 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:04:11,838 | server.py:125 | fit progress: (1, 1.8611335754394531, {'accuracy': 0.6837, 'data_size': 10000}, 79.80943396000657)
INFO flwr 2024-04-30 21:04:11,838 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:04:11,838 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:05:22,453 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:05:23,439 | server.py:125 | fit progress: (2, 1.7119289636611938, {'accuracy': 0.7651, 'data_size': 10000}, 151.41128734202357)
INFO flwr 2024-04-30 21:05:23,440 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:05:23,440 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:06:23,567 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:06:24,570 | server.py:125 | fit progress: (3, 1.6353189945220947, {'accuracy': 0.8346, 'data_size': 10000}, 212.5420075940201)
INFO flwr 2024-04-30 21:06:24,570 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:06:24,571 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:07:32,804 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:07:33,785 | server.py:125 | fit progress: (4, 1.5919126272201538, {'accuracy': 0.8761, 'data_size': 10000}, 281.7569470880553)
INFO flwr 2024-04-30 21:07:33,785 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:07:33,786 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:08:30,578 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:08:31,596 | server.py:125 | fit progress: (5, 1.5645403861999512, {'accuracy': 0.8998, 'data_size': 10000}, 339.56731898203725)
INFO flwr 2024-04-30 21:08:31,596 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:08:31,596 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:09:31,104 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:09:32,268 | server.py:125 | fit progress: (6, 1.5559213161468506, {'accuracy': 0.9074, 'data_size': 10000}, 400.2399016440031)
INFO flwr 2024-04-30 21:09:32,268 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:09:32,268 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:10:33,390 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:10:34,621 | server.py:125 | fit progress: (7, 1.5545761585235596, {'accuracy': 0.9085, 'data_size': 10000}, 462.5924390030559)
INFO flwr 2024-04-30 21:10:34,621 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:10:34,621 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:11:33,560 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:11:34,788 | server.py:125 | fit progress: (8, 1.5526492595672607, {'accuracy': 0.9104, 'data_size': 10000}, 522.75941810105)
INFO flwr 2024-04-30 21:11:34,788 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:11:34,788 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:12:49,513 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:12:50,538 | server.py:125 | fit progress: (9, 1.5519179105758667, {'accuracy': 0.9103, 'data_size': 10000}, 598.5094869390014)
INFO flwr 2024-04-30 21:12:50,538 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:12:50,538 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:13:52,086 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:13:53,283 | server.py:125 | fit progress: (10, 1.5503759384155273, {'accuracy': 0.9117, 'data_size': 10000}, 661.2546565020457)
INFO flwr 2024-04-30 21:13:53,283 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:13:53,283 | server.py:153 | FL finished in 661.2550157790538
INFO flwr 2024-04-30 21:13:53,283 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:13:53,283 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:13:53,283 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:13:53,284 | app.py:229 | app_fit: losses_centralized [(0, 2.3047380447387695), (1, 1.8611335754394531), (2, 1.7119289636611938), (3, 1.6353189945220947), (4, 1.5919126272201538), (5, 1.5645403861999512), (6, 1.5559213161468506), (7, 1.5545761585235596), (8, 1.5526492595672607), (9, 1.5519179105758667), (10, 1.5503759384155273)]
INFO flwr 2024-04-30 21:13:53,284 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0849), (1, 0.6837), (2, 0.7651), (3, 0.8346), (4, 0.8761), (5, 0.8998), (6, 0.9074), (7, 0.9085), (8, 0.9104), (9, 0.9103), (10, 0.9117)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9117
wandb:     loss 1.55038
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_210234-sk8b5xwl
wandb: Find logs at: ./wandb/offline-run-20240430_210234-sk8b5xwl/logs
INFO flwr 2024-04-30 21:13:56,767 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:13:57,432 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1587795)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1587795)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:14:02,262	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:14:02,358	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:14:02,453	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:14:02,455	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:14:12,080 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 74630785843.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 164138500301.0}
INFO flwr 2024-04-30 21:14:12,081 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:14:12,081 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:14:12,093 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:14:12,094 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:14:12,094 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:14:12,094 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:14:14,504 | server.py:94 | initial parameters (loss, other metrics): 2.3052616119384766, {'accuracy': 0.0876, 'data_size': 10000}
INFO flwr 2024-04-30 21:14:14,505 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:14:14,505 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1593761)[0m 2024-04-30 21:14:17.522934: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1593761)[0m 2024-04-30 21:14:17.618115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1593761)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1593761)[0m 2024-04-30 21:14:19.553801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1593761)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1593761)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1593773)[0m 2024-04-30 21:14:17.666307: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1593773)[0m 2024-04-30 21:14:17.747598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1593773)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1593773)[0m 2024-04-30 21:14:19.780967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:15:31,363 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:15:32,598 | server.py:125 | fit progress: (1, 1.8300353288650513, {'accuracy': 0.7316, 'data_size': 10000}, 78.09290411003167)
INFO flwr 2024-04-30 21:15:32,598 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:15:32,599 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:16:35,177 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:16:36,176 | server.py:125 | fit progress: (2, 1.7063406705856323, {'accuracy': 0.7869, 'data_size': 10000}, 141.67054687900236)
INFO flwr 2024-04-30 21:16:36,176 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:16:36,176 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:17:47,129 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:17:48,113 | server.py:125 | fit progress: (3, 1.6333260536193848, {'accuracy': 0.8391, 'data_size': 10000}, 213.6079308890039)
INFO flwr 2024-04-30 21:17:48,113 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:17:48,114 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:18:49,919 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:18:50,915 | server.py:125 | fit progress: (4, 1.590219497680664, {'accuracy': 0.8767, 'data_size': 10000}, 276.4100406880025)
INFO flwr 2024-04-30 21:18:50,916 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:18:50,916 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:19:58,330 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:19:59,345 | server.py:125 | fit progress: (5, 1.5641406774520874, {'accuracy': 0.9005, 'data_size': 10000}, 344.8396846370306)
INFO flwr 2024-04-30 21:19:59,345 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:19:59,345 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:21:02,437 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:21:03,653 | server.py:125 | fit progress: (6, 1.5584698915481567, {'accuracy': 0.9052, 'data_size': 10000}, 409.14737995603355)
INFO flwr 2024-04-30 21:21:03,653 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:21:03,653 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:22:06,209 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:22:07,399 | server.py:125 | fit progress: (7, 1.5586475133895874, {'accuracy': 0.9036, 'data_size': 10000}, 472.89391424000496)
INFO flwr 2024-04-30 21:22:07,399 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:22:07,400 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:23:08,459 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:23:09,671 | server.py:125 | fit progress: (8, 1.5633294582366943, {'accuracy': 0.8995, 'data_size': 10000}, 535.1654012310319)
INFO flwr 2024-04-30 21:23:09,671 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:23:09,671 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:24:10,304 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:24:11,283 | server.py:125 | fit progress: (9, 1.5566458702087402, {'accuracy': 0.9055, 'data_size': 10000}, 596.7776931130211)
INFO flwr 2024-04-30 21:24:11,283 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:24:11,283 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:25:17,094 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:25:18,324 | server.py:125 | fit progress: (10, 1.5516879558563232, {'accuracy': 0.9093, 'data_size': 10000}, 663.8184510110295)
INFO flwr 2024-04-30 21:25:18,324 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:25:18,324 | server.py:153 | FL finished in 663.8188280449831
INFO flwr 2024-04-30 21:25:18,324 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:25:18,324 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:25:18,324 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:25:18,325 | app.py:229 | app_fit: losses_centralized [(0, 2.3052616119384766), (1, 1.8300353288650513), (2, 1.7063406705856323), (3, 1.6333260536193848), (4, 1.590219497680664), (5, 1.5641406774520874), (6, 1.5584698915481567), (7, 1.5586475133895874), (8, 1.5633294582366943), (9, 1.5566458702087402), (10, 1.5516879558563232)]
INFO flwr 2024-04-30 21:25:18,325 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0876), (1, 0.7316), (2, 0.7869), (3, 0.8391), (4, 0.8767), (5, 0.9005), (6, 0.9052), (7, 0.9036), (8, 0.8995), (9, 0.9055), (10, 0.9093)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9093
wandb:     loss 1.55169
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_211357-3rf2hw6y
wandb: Find logs at: ./wandb/offline-run-20240430_211357-3rf2hw6y/logs
INFO flwr 2024-04-30 21:25:21,836 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:25:24,147 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1593759)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1593759)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:25:29,120	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:25:29,245	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:25:29,325	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:25:29,326	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:25:38,926 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 160255869952.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 72966801408.0}
INFO flwr 2024-04-30 21:25:38,927 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:25:38,927 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:25:38,939 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:25:38,942 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:25:38,942 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:25:38,942 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:25:41,256 | server.py:94 | initial parameters (loss, other metrics): 2.3024184703826904, {'accuracy': 0.0784, 'data_size': 10000}
INFO flwr 2024-04-30 21:25:41,256 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:25:41,257 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1598628)[0m 2024-04-30 21:25:44.421881: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1598628)[0m 2024-04-30 21:25:44.511941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1598628)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1598628)[0m 2024-04-30 21:25:46.560848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1598628)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1598628)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1598629)[0m 2024-04-30 21:25:44.713321: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1598629)[0m 2024-04-30 21:25:44.808820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1598629)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1598629)[0m 2024-04-30 21:25:46.791904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:26:01,287 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:26:02,483 | server.py:125 | fit progress: (1, 1.992501139640808, {'accuracy': 0.5071, 'data_size': 10000}, 21.225873602030333)
INFO flwr 2024-04-30 21:26:02,483 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:26:02,483 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:26:10,268 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:26:11,237 | server.py:125 | fit progress: (2, 1.8811267614364624, {'accuracy': 0.5805, 'data_size': 10000}, 29.980136254045647)
INFO flwr 2024-04-30 21:26:11,237 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:26:11,237 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:26:18,804 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:26:19,812 | server.py:125 | fit progress: (3, 1.8351308107376099, {'accuracy': 0.6266, 'data_size': 10000}, 38.555599840998184)
INFO flwr 2024-04-30 21:26:19,813 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:26:19,813 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:26:27,117 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:26:28,115 | server.py:125 | fit progress: (4, 1.7681703567504883, {'accuracy': 0.6949, 'data_size': 10000}, 46.85828379704617)
INFO flwr 2024-04-30 21:26:28,115 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:26:28,115 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:26:35,485 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:26:36,492 | server.py:125 | fit progress: (5, 1.748579502105713, {'accuracy': 0.7121, 'data_size': 10000}, 55.23528837703634)
INFO flwr 2024-04-30 21:26:36,492 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:26:36,492 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:26:44,020 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:26:45,235 | server.py:125 | fit progress: (6, 1.7218250036239624, {'accuracy': 0.741, 'data_size': 10000}, 63.978512894012965)
INFO flwr 2024-04-30 21:26:45,235 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:26:45,236 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:26:52,581 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:26:53,795 | server.py:125 | fit progress: (7, 1.7090697288513184, {'accuracy': 0.7514, 'data_size': 10000}, 72.53864776802948)
INFO flwr 2024-04-30 21:26:53,796 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:26:53,796 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:27:01,303 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:27:02,505 | server.py:125 | fit progress: (8, 1.6985234022140503, {'accuracy': 0.7622, 'data_size': 10000}, 81.24810790503398)
INFO flwr 2024-04-30 21:27:02,505 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:27:02,505 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:27:09,881 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:27:11,110 | server.py:125 | fit progress: (9, 1.6931437253952026, {'accuracy': 0.7673, 'data_size': 10000}, 89.85327262501232)
INFO flwr 2024-04-30 21:27:11,110 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:27:11,110 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:27:18,656 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:27:19,723 | server.py:125 | fit progress: (10, 1.6876860857009888, {'accuracy': 0.7728, 'data_size': 10000}, 98.46652136399643)
INFO flwr 2024-04-30 21:27:19,724 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:27:19,724 | server.py:153 | FL finished in 98.4669905549963
INFO flwr 2024-04-30 21:27:19,724 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:27:19,724 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:27:19,724 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:27:19,724 | app.py:229 | app_fit: losses_centralized [(0, 2.3024184703826904), (1, 1.992501139640808), (2, 1.8811267614364624), (3, 1.8351308107376099), (4, 1.7681703567504883), (5, 1.748579502105713), (6, 1.7218250036239624), (7, 1.7090697288513184), (8, 1.6985234022140503), (9, 1.6931437253952026), (10, 1.6876860857009888)]
INFO flwr 2024-04-30 21:27:19,724 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0784), (1, 0.5071), (2, 0.5805), (3, 0.6266), (4, 0.6949), (5, 0.7121), (6, 0.741), (7, 0.7514), (8, 0.7622), (9, 0.7673), (10, 0.7728)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7728
wandb:     loss 1.68769
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_212523-bf2ioqdn
wandb: Find logs at: ./wandb/offline-run-20240430_212523-bf2ioqdn/logs
INFO flwr 2024-04-30 21:27:23,231 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:27:24,410 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1598635)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1598635)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:27:32,967	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:27:33,542	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:27:33,667	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:27:33,668	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:27:43,158 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 72938161766.0, 'memory': 160189044122.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 21:27:43,158 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:27:43,158 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:27:43,170 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:27:43,171 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:27:43,172 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:27:43,172 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:27:45,844 | server.py:94 | initial parameters (loss, other metrics): 2.3030407428741455, {'accuracy': 0.1123, 'data_size': 10000}
INFO flwr 2024-04-30 21:27:45,845 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:27:45,846 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1601865)[0m 2024-04-30 21:27:50.822517: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1601865)[0m 2024-04-30 21:27:50.916462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1601865)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1601865)[0m 2024-04-30 21:27:59.048895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1601859)[0m 2024-04-30 21:27:50.870034: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1601858)[0m 2024-04-30 21:27:50.882714: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1601858)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1601865)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1601865)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1601858)[0m 2024-04-30 21:27:59.048895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:28:25,570 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:28:26,791 | server.py:125 | fit progress: (1, 1.9420853853225708, {'accuracy': 0.5516, 'data_size': 10000}, 40.945674291986506)
INFO flwr 2024-04-30 21:28:26,791 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:28:26,792 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:28:34,592 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:28:35,830 | server.py:125 | fit progress: (2, 1.7911174297332764, {'accuracy': 0.6795, 'data_size': 10000}, 49.985050983028486)
INFO flwr 2024-04-30 21:28:35,831 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:28:35,831 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:28:43,303 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:28:44,549 | server.py:125 | fit progress: (3, 1.7465707063674927, {'accuracy': 0.7175, 'data_size': 10000}, 58.70351715100696)
INFO flwr 2024-04-30 21:28:44,549 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:28:44,549 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:28:51,615 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:28:52,840 | server.py:125 | fit progress: (4, 1.745505452156067, {'accuracy': 0.7162, 'data_size': 10000}, 66.99436333501944)
INFO flwr 2024-04-30 21:28:52,840 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:28:52,840 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:29:00,528 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:29:01,563 | server.py:125 | fit progress: (5, 1.733392596244812, {'accuracy': 0.7278, 'data_size': 10000}, 75.717157297011)
INFO flwr 2024-04-30 21:29:01,563 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:29:01,563 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:29:09,141 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:29:10,344 | server.py:125 | fit progress: (6, 1.728512167930603, {'accuracy': 0.733, 'data_size': 10000}, 84.4990587750217)
INFO flwr 2024-04-30 21:29:10,345 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:29:10,345 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:29:17,453 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:29:18,690 | server.py:125 | fit progress: (7, 1.7278273105621338, {'accuracy': 0.7339, 'data_size': 10000}, 92.84469384001568)
INFO flwr 2024-04-30 21:29:18,690 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:29:18,691 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:29:26,008 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:29:27,251 | server.py:125 | fit progress: (8, 1.7144583463668823, {'accuracy': 0.7457, 'data_size': 10000}, 101.40555792802479)
INFO flwr 2024-04-30 21:29:27,251 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:29:27,251 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:29:34,565 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:29:35,777 | server.py:125 | fit progress: (9, 1.7148150205612183, {'accuracy': 0.7462, 'data_size': 10000}, 109.93147373502143)
INFO flwr 2024-04-30 21:29:35,777 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:29:35,777 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:29:43,215 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:29:44,246 | server.py:125 | fit progress: (10, 1.7163000106811523, {'accuracy': 0.7446, 'data_size': 10000}, 118.40090746898204)
INFO flwr 2024-04-30 21:29:44,247 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:29:44,247 | server.py:153 | FL finished in 118.4012822189834
INFO flwr 2024-04-30 21:29:44,247 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:29:44,247 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:29:44,247 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:29:44,247 | app.py:229 | app_fit: losses_centralized [(0, 2.3030407428741455), (1, 1.9420853853225708), (2, 1.7911174297332764), (3, 1.7465707063674927), (4, 1.745505452156067), (5, 1.733392596244812), (6, 1.728512167930603), (7, 1.7278273105621338), (8, 1.7144583463668823), (9, 1.7148150205612183), (10, 1.7163000106811523)]
INFO flwr 2024-04-30 21:29:44,247 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1123), (1, 0.5516), (2, 0.6795), (3, 0.7175), (4, 0.7162), (5, 0.7278), (6, 0.733), (7, 0.7339), (8, 0.7457), (9, 0.7462), (10, 0.7446)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7446
wandb:     loss 1.7163
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_212723-qn5r6vi1
wandb: Find logs at: ./wandb/offline-run-20240430_212723-qn5r6vi1/logs
INFO flwr 2024-04-30 21:29:47,763 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:29:48,802 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1601858)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1601858)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:29:54,213	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:29:54,390	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:29:54,472	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:29:54,473	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:30:04,055 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 158939116954.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 72402478694.0}
INFO flwr 2024-04-30 21:30:04,055 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:30:04,056 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:30:04,068 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:30:04,069 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:30:04,069 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:30:04,070 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:30:06,794 | server.py:94 | initial parameters (loss, other metrics): 2.305694818496704, {'accuracy': 0.1165, 'data_size': 10000}
INFO flwr 2024-04-30 21:30:06,795 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:30:06,795 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1605642)[0m 2024-04-30 21:30:09.569802: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1605642)[0m 2024-04-30 21:30:09.665232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1605642)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1605637)[0m 2024-04-30 21:30:11.692730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1605637)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1605637)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1605638)[0m 2024-04-30 21:30:09.773937: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1605636)[0m 2024-04-30 21:30:09.790141: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1605636)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1605638)[0m 2024-04-30 21:30:11.895984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:30:24,835 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:30:26,042 | server.py:125 | fit progress: (1, 1.8574970960617065, {'accuracy': 0.6543, 'data_size': 10000}, 19.247258837043773)
INFO flwr 2024-04-30 21:30:26,042 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:30:26,043 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:30:34,204 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:30:35,451 | server.py:125 | fit progress: (2, 1.6685253381729126, {'accuracy': 0.8127, 'data_size': 10000}, 28.656594199012034)
INFO flwr 2024-04-30 21:30:35,452 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:30:35,452 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:30:43,056 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:30:44,264 | server.py:125 | fit progress: (3, 1.621990442276001, {'accuracy': 0.848, 'data_size': 10000}, 37.46940095501486)
INFO flwr 2024-04-30 21:30:44,265 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:30:44,265 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:30:51,581 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:30:52,818 | server.py:125 | fit progress: (4, 1.5937565565109253, {'accuracy': 0.874, 'data_size': 10000}, 46.022794171993155)
INFO flwr 2024-04-30 21:30:52,818 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:30:52,818 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:30:59,893 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:31:00,887 | server.py:125 | fit progress: (5, 1.5719552040100098, {'accuracy': 0.8921, 'data_size': 10000}, 54.0917798980372)
INFO flwr 2024-04-30 21:31:00,887 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:31:00,887 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:31:08,295 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:31:09,280 | server.py:125 | fit progress: (6, 1.571427583694458, {'accuracy': 0.894, 'data_size': 10000}, 62.48482817102922)
INFO flwr 2024-04-30 21:31:09,280 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:31:09,280 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:31:16,451 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:31:17,474 | server.py:125 | fit progress: (7, 1.5654252767562866, {'accuracy': 0.8981, 'data_size': 10000}, 70.67867648700485)
INFO flwr 2024-04-30 21:31:17,474 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:31:17,474 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:31:24,973 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:31:25,970 | server.py:125 | fit progress: (8, 1.5642009973526, {'accuracy': 0.8984, 'data_size': 10000}, 79.17479840101441)
INFO flwr 2024-04-30 21:31:25,970 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:31:25,970 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:31:33,327 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:31:34,594 | server.py:125 | fit progress: (9, 1.5621247291564941, {'accuracy': 0.899, 'data_size': 10000}, 87.79924585000845)
INFO flwr 2024-04-30 21:31:34,594 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:31:34,595 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:31:42,130 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:31:43,329 | server.py:125 | fit progress: (10, 1.5561367273330688, {'accuracy': 0.9058, 'data_size': 10000}, 96.53413323900895)
INFO flwr 2024-04-30 21:31:43,329 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:31:43,329 | server.py:153 | FL finished in 96.5345322950161
INFO flwr 2024-04-30 21:31:43,330 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:31:43,330 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:31:43,330 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:31:43,330 | app.py:229 | app_fit: losses_centralized [(0, 2.305694818496704), (1, 1.8574970960617065), (2, 1.6685253381729126), (3, 1.621990442276001), (4, 1.5937565565109253), (5, 1.5719552040100098), (6, 1.571427583694458), (7, 1.5654252767562866), (8, 1.5642009973526), (9, 1.5621247291564941), (10, 1.5561367273330688)]
INFO flwr 2024-04-30 21:31:43,330 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1165), (1, 0.6543), (2, 0.8127), (3, 0.848), (4, 0.874), (5, 0.8921), (6, 0.894), (7, 0.8981), (8, 0.8984), (9, 0.899), (10, 0.9058)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9058
wandb:     loss 1.55614
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_212948-v5jop8zf
wandb: Find logs at: ./wandb/offline-run-20240430_212948-v5jop8zf/logs
INFO flwr 2024-04-30 21:31:46,843 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:31:47,535 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1605632)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1605632)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:31:52,218	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:31:52,316	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:31:52,398	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:31:52,400	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:32:02,099 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 71951942860.0, 'memory': 157887866676.0}
INFO flwr 2024-04-30 21:32:02,099 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:32:02,100 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:32:02,115 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:32:02,117 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:32:02,117 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:32:02,117 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:32:05,187 | server.py:94 | initial parameters (loss, other metrics): 2.2981507778167725, {'accuracy': 0.1221, 'data_size': 10000}
INFO flwr 2024-04-30 21:32:05,187 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:32:05,188 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1608820)[0m 2024-04-30 21:32:07.663658: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1608824)[0m 2024-04-30 21:32:07.742059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1608824)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1608806)[0m 2024-04-30 21:32:09.798379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1608810)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1608810)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1608806)[0m 2024-04-30 21:32:07.830285: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1608806)[0m 2024-04-30 21:32:07.911650: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1608806)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1608810)[0m 2024-04-30 21:32:09.892424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:32:24,028 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:32:25,274 | server.py:125 | fit progress: (1, 1.8333982229232788, {'accuracy': 0.6679, 'data_size': 10000}, 20.08597654500045)
INFO flwr 2024-04-30 21:32:25,274 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:32:25,274 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:32:33,598 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:32:34,587 | server.py:125 | fit progress: (2, 1.8729465007781982, {'accuracy': 0.587, 'data_size': 10000}, 29.399714099999983)
INFO flwr 2024-04-30 21:32:34,587 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:32:34,588 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:32:42,937 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:32:44,167 | server.py:125 | fit progress: (3, 1.8022334575653076, {'accuracy': 0.6611, 'data_size': 10000}, 38.97941503097536)
INFO flwr 2024-04-30 21:32:44,167 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:32:44,167 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:32:51,881 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:32:53,126 | server.py:125 | fit progress: (4, 1.7414947748184204, {'accuracy': 0.7164, 'data_size': 10000}, 47.93857838102849)
INFO flwr 2024-04-30 21:32:53,126 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:32:53,127 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:33:00,959 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:33:01,981 | server.py:125 | fit progress: (5, 1.68098783493042, {'accuracy': 0.7777, 'data_size': 10000}, 56.79362585901981)
INFO flwr 2024-04-30 21:33:01,981 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:33:01,982 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:33:09,722 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:33:10,950 | server.py:125 | fit progress: (6, 1.6423829793930054, {'accuracy': 0.819, 'data_size': 10000}, 65.7626944599906)
INFO flwr 2024-04-30 21:33:10,950 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:33:10,951 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:33:18,842 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:33:20,087 | server.py:125 | fit progress: (7, 1.6612316370010376, {'accuracy': 0.8001, 'data_size': 10000}, 74.89924275298836)
INFO flwr 2024-04-30 21:33:20,087 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:33:20,087 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:33:27,646 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:33:28,868 | server.py:125 | fit progress: (8, 1.6603935956954956, {'accuracy': 0.801, 'data_size': 10000}, 83.68053479900118)
INFO flwr 2024-04-30 21:33:28,868 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:33:28,868 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:33:36,940 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:33:38,192 | server.py:125 | fit progress: (9, 1.6305007934570312, {'accuracy': 0.8318, 'data_size': 10000}, 93.00457792001544)
INFO flwr 2024-04-30 21:33:38,192 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:33:38,193 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:33:45,704 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:33:46,905 | server.py:125 | fit progress: (10, 1.604514479637146, {'accuracy': 0.8561, 'data_size': 10000}, 101.71719581499929)
INFO flwr 2024-04-30 21:33:46,905 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:33:46,905 | server.py:153 | FL finished in 101.71760728297522
INFO flwr 2024-04-30 21:33:46,905 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:33:46,905 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:33:46,906 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:33:46,906 | app.py:229 | app_fit: losses_centralized [(0, 2.2981507778167725), (1, 1.8333982229232788), (2, 1.8729465007781982), (3, 1.8022334575653076), (4, 1.7414947748184204), (5, 1.68098783493042), (6, 1.6423829793930054), (7, 1.6612316370010376), (8, 1.6603935956954956), (9, 1.6305007934570312), (10, 1.604514479637146)]
INFO flwr 2024-04-30 21:33:46,906 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1221), (1, 0.6679), (2, 0.587), (3, 0.6611), (4, 0.7164), (5, 0.7777), (6, 0.819), (7, 0.8001), (8, 0.801), (9, 0.8318), (10, 0.8561)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8561
wandb:     loss 1.60451
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_213147-l4p4yzg2
wandb: Find logs at: ./wandb/offline-run-20240430_213147-l4p4yzg2/logs
INFO flwr 2024-04-30 21:33:50,396 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:33:51,653 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1608815)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1608815)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:33:57,584	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:33:57,823	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:33:57,954	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:33:57,956	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:34:07,681 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 156946642330.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 71548560998.0}
INFO flwr 2024-04-30 21:34:07,681 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:34:07,681 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:34:07,698 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:34:07,700 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:34:07,700 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:34:07,700 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:34:09,748 | server.py:94 | initial parameters (loss, other metrics): 2.3058156967163086, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-30 21:34:09,749 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:34:09,749 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1612682)[0m 2024-04-30 21:34:13.715441: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1612688)[0m 2024-04-30 21:34:13.810021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1612688)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1612682)[0m 2024-04-30 21:34:16.542045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1612682)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1612682)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1612690)[0m 2024-04-30 21:34:13.822097: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1612677)[0m 2024-04-30 21:34:13.879170: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1612677)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1612677)[0m 2024-04-30 21:34:16.542045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:34:36,525 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:34:37,761 | server.py:125 | fit progress: (1, 1.8648370504379272, {'accuracy': 0.6428, 'data_size': 10000}, 28.012261367985047)
INFO flwr 2024-04-30 21:34:37,761 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:34:37,761 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:34:46,240 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:34:47,505 | server.py:125 | fit progress: (2, 1.7262353897094727, {'accuracy': 0.746, 'data_size': 10000}, 37.75678926700493)
INFO flwr 2024-04-30 21:34:47,506 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:34:47,506 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:34:55,122 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:34:56,364 | server.py:125 | fit progress: (3, 1.6761949062347412, {'accuracy': 0.791, 'data_size': 10000}, 46.61552137701074)
INFO flwr 2024-04-30 21:34:56,364 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:34:56,365 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:35:04,383 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:35:05,634 | server.py:125 | fit progress: (4, 1.6553125381469727, {'accuracy': 0.8081, 'data_size': 10000}, 55.885173476999626)
INFO flwr 2024-04-30 21:35:05,634 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:35:05,634 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:35:13,202 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:35:14,194 | server.py:125 | fit progress: (5, 1.6579011678695679, {'accuracy': 0.8049, 'data_size': 10000}, 64.44504086097004)
INFO flwr 2024-04-30 21:35:14,194 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:35:14,194 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:35:22,500 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:35:23,584 | server.py:125 | fit progress: (6, 1.6712576150894165, {'accuracy': 0.7903, 'data_size': 10000}, 73.83571849198779)
INFO flwr 2024-04-30 21:35:23,585 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:35:23,585 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:35:32,132 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:35:33,126 | server.py:125 | fit progress: (7, 1.6428265571594238, {'accuracy': 0.8184, 'data_size': 10000}, 83.37693234300241)
INFO flwr 2024-04-30 21:35:33,126 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:35:33,126 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:35:41,180 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:35:42,192 | server.py:125 | fit progress: (8, 1.6497160196304321, {'accuracy': 0.8105, 'data_size': 10000}, 92.44293160701636)
INFO flwr 2024-04-30 21:35:42,192 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:35:42,192 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:35:50,325 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:35:51,565 | server.py:125 | fit progress: (9, 1.660689115524292, {'accuracy': 0.7994, 'data_size': 10000}, 101.81640705897007)
INFO flwr 2024-04-30 21:35:51,565 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:35:51,566 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:35:59,484 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:36:00,705 | server.py:125 | fit progress: (10, 1.6459894180297852, {'accuracy': 0.8147, 'data_size': 10000}, 110.95659077499295)
INFO flwr 2024-04-30 21:36:00,706 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:36:00,706 | server.py:153 | FL finished in 110.95694096101215
INFO flwr 2024-04-30 21:36:00,706 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:36:00,706 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:36:00,706 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:36:00,706 | app.py:229 | app_fit: losses_centralized [(0, 2.3058156967163086), (1, 1.8648370504379272), (2, 1.7262353897094727), (3, 1.6761949062347412), (4, 1.6553125381469727), (5, 1.6579011678695679), (6, 1.6712576150894165), (7, 1.6428265571594238), (8, 1.6497160196304321), (9, 1.660689115524292), (10, 1.6459894180297852)]
INFO flwr 2024-04-30 21:36:00,706 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.6428), (2, 0.746), (3, 0.791), (4, 0.8081), (5, 0.8049), (6, 0.7903), (7, 0.8184), (8, 0.8105), (9, 0.7994), (10, 0.8147)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8147
wandb:     loss 1.64599
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_213350-nsbh1s3p
wandb: Find logs at: ./wandb/offline-run-20240430_213350-nsbh1s3p/logs
INFO flwr 2024-04-30 21:36:04,214 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:36:04,902 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1612677)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1612677)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:36:10,315	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:36:10,408	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:36:10,492	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:36:10,494	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:36:20,285 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 155314190541.0, 'object_store_memory': 70848938803.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 21:36:20,285 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:36:20,285 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:36:20,298 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:36:20,299 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:36:20,299 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:36:20,300 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:36:22,684 | server.py:94 | initial parameters (loss, other metrics): 2.304542303085327, {'accuracy': 0.0715, 'data_size': 10000}
INFO flwr 2024-04-30 21:36:22,684 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:36:22,685 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1615853)[0m 2024-04-30 21:36:25.774371: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1615853)[0m 2024-04-30 21:36:25.868343: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1615853)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1615851)[0m 2024-04-30 21:36:27.858834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1615851)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1615851)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1615855)[0m 2024-04-30 21:36:25.936152: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1615855)[0m 2024-04-30 21:36:26.019606: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1615855)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1615855)[0m 2024-04-30 21:36:27.902070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:36:41,845 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:36:43,113 | server.py:125 | fit progress: (1, 1.8964840173721313, {'accuracy': 0.5942, 'data_size': 10000}, 20.4278982449905)
INFO flwr 2024-04-30 21:36:43,113 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:36:43,113 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:36:51,659 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:36:52,661 | server.py:125 | fit progress: (2, 1.736161708831787, {'accuracy': 0.7263, 'data_size': 10000}, 29.97669193096226)
INFO flwr 2024-04-30 21:36:52,662 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:36:52,662 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:37:00,794 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:37:02,048 | server.py:125 | fit progress: (3, 1.616202473640442, {'accuracy': 0.8573, 'data_size': 10000}, 39.36299009801587)
INFO flwr 2024-04-30 21:37:02,048 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:37:02,049 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:37:09,992 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:37:11,233 | server.py:125 | fit progress: (4, 1.5997437238693237, {'accuracy': 0.8706, 'data_size': 10000}, 48.54871455801185)
INFO flwr 2024-04-30 21:37:11,234 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:37:11,234 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:37:18,813 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:37:19,843 | server.py:125 | fit progress: (5, 1.5710846185684204, {'accuracy': 0.8953, 'data_size': 10000}, 57.158690462994855)
INFO flwr 2024-04-30 21:37:19,844 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:37:19,844 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:37:27,654 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:37:28,891 | server.py:125 | fit progress: (6, 1.588771939277649, {'accuracy': 0.8755, 'data_size': 10000}, 66.20674578601029)
INFO flwr 2024-04-30 21:37:28,892 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:37:28,892 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:37:36,765 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:37:38,022 | server.py:125 | fit progress: (7, 1.5773324966430664, {'accuracy': 0.8869, 'data_size': 10000}, 75.3372783199884)
INFO flwr 2024-04-30 21:37:38,022 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:37:38,022 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:37:45,553 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:37:46,802 | server.py:125 | fit progress: (8, 1.561131477355957, {'accuracy': 0.9024, 'data_size': 10000}, 84.11770636198344)
INFO flwr 2024-04-30 21:37:46,803 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:37:46,803 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:37:54,430 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:37:55,643 | server.py:125 | fit progress: (9, 1.5695269107818604, {'accuracy': 0.893, 'data_size': 10000}, 92.95845526497578)
INFO flwr 2024-04-30 21:37:55,643 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:37:55,644 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:38:03,329 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:38:04,571 | server.py:125 | fit progress: (10, 1.5581765174865723, {'accuracy': 0.9046, 'data_size': 10000}, 101.8862239929731)
INFO flwr 2024-04-30 21:38:04,571 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:38:04,571 | server.py:153 | FL finished in 101.8865692190011
INFO flwr 2024-04-30 21:38:04,571 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:38:04,571 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:38:04,572 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:38:04,572 | app.py:229 | app_fit: losses_centralized [(0, 2.304542303085327), (1, 1.8964840173721313), (2, 1.736161708831787), (3, 1.616202473640442), (4, 1.5997437238693237), (5, 1.5710846185684204), (6, 1.588771939277649), (7, 1.5773324966430664), (8, 1.561131477355957), (9, 1.5695269107818604), (10, 1.5581765174865723)]
INFO flwr 2024-04-30 21:38:04,572 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0715), (1, 0.5942), (2, 0.7263), (3, 0.8573), (4, 0.8706), (5, 0.8953), (6, 0.8755), (7, 0.8869), (8, 0.9024), (9, 0.893), (10, 0.9046)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9046
wandb:     loss 1.55818
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_213604-2h9q13mj
wandb: Find logs at: ./wandb/offline-run-20240430_213604-2h9q13mj/logs
INFO flwr 2024-04-30 21:38:08,112 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:38:08,749 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1615854)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1615854)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:38:13,785	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:38:13,900	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:38:14,025	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:38:14,027	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:38:23,708 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 154451900212.0, 'CPU': 64.0, 'object_store_memory': 70479385804.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 21:38:23,708 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:38:23,708 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:38:23,723 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:38:23,725 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:38:23,726 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:38:23,726 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:38:25,625 | server.py:94 | initial parameters (loss, other metrics): 2.3042690753936768, {'accuracy': 0.0929, 'data_size': 10000}
INFO flwr 2024-04-30 21:38:25,625 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:38:25,625 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1619598)[0m 2024-04-30 21:38:29.176919: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1619602)[0m 2024-04-30 21:38:29.207539: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1619602)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1619598)[0m 2024-04-30 21:38:31.191964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1619598)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1619598)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1619603)[0m 2024-04-30 21:38:29.239562: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1619603)[0m 2024-04-30 21:38:29.332974: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1619603)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1619603)[0m 2024-04-30 21:38:31.310955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:38:46,343 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:38:47,555 | server.py:125 | fit progress: (1, 1.9634650945663452, {'accuracy': 0.5264, 'data_size': 10000}, 21.92986439500237)
INFO flwr 2024-04-30 21:38:47,555 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:38:47,556 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:38:57,400 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:38:58,642 | server.py:125 | fit progress: (2, 1.7137199640274048, {'accuracy': 0.7651, 'data_size': 10000}, 33.01701943500666)
INFO flwr 2024-04-30 21:38:58,643 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:38:58,643 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:39:07,328 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:39:08,535 | server.py:125 | fit progress: (3, 1.663312315940857, {'accuracy': 0.8053, 'data_size': 10000}, 42.90939367399551)
INFO flwr 2024-04-30 21:39:08,535 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:39:08,535 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:39:17,684 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:39:18,951 | server.py:125 | fit progress: (4, 1.6093946695327759, {'accuracy': 0.8556, 'data_size': 10000}, 53.32584713300457)
INFO flwr 2024-04-30 21:39:18,951 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:39:18,952 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:39:28,227 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:39:29,460 | server.py:125 | fit progress: (5, 1.5946345329284668, {'accuracy': 0.8708, 'data_size': 10000}, 63.83480623702053)
INFO flwr 2024-04-30 21:39:29,460 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:39:29,461 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:39:38,045 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:39:39,258 | server.py:125 | fit progress: (6, 1.5939652919769287, {'accuracy': 0.8696, 'data_size': 10000}, 73.63233120401856)
INFO flwr 2024-04-30 21:39:39,258 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:39:39,258 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:39:48,788 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:39:49,809 | server.py:125 | fit progress: (7, 1.575050711631775, {'accuracy': 0.888, 'data_size': 10000}, 84.18364201899385)
INFO flwr 2024-04-30 21:39:49,809 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:39:49,809 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:39:58,541 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:39:59,569 | server.py:125 | fit progress: (8, 1.5743019580841064, {'accuracy': 0.8892, 'data_size': 10000}, 93.94383653899422)
INFO flwr 2024-04-30 21:39:59,569 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:39:59,570 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:40:08,356 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:40:09,613 | server.py:125 | fit progress: (9, 1.5714057683944702, {'accuracy': 0.8902, 'data_size': 10000}, 103.98782089399174)
INFO flwr 2024-04-30 21:40:09,613 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:40:09,614 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:40:18,565 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:40:19,795 | server.py:125 | fit progress: (10, 1.584877848625183, {'accuracy': 0.878, 'data_size': 10000}, 114.16931378399022)
INFO flwr 2024-04-30 21:40:19,795 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:40:19,795 | server.py:153 | FL finished in 114.1697087600478
INFO flwr 2024-04-30 21:40:19,795 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:40:19,795 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:40:19,795 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:40:19,796 | app.py:229 | app_fit: losses_centralized [(0, 2.3042690753936768), (1, 1.9634650945663452), (2, 1.7137199640274048), (3, 1.663312315940857), (4, 1.6093946695327759), (5, 1.5946345329284668), (6, 1.5939652919769287), (7, 1.575050711631775), (8, 1.5743019580841064), (9, 1.5714057683944702), (10, 1.584877848625183)]
INFO flwr 2024-04-30 21:40:19,796 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0929), (1, 0.5264), (2, 0.7651), (3, 0.8053), (4, 0.8556), (5, 0.8708), (6, 0.8696), (7, 0.888), (8, 0.8892), (9, 0.8902), (10, 0.878)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.878
wandb:     loss 1.58488
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_213808-fgjyboi1
wandb: Find logs at: ./wandb/offline-run-20240430_213808-fgjyboi1/logs
INFO flwr 2024-04-30 21:40:23,307 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:40:24,056 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1619596)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1619596)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:40:28,528	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:40:28,668	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:40:28,787	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:40:28,789	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:40:38,528 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 153867166311.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 70228785561.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 21:40:38,528 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:40:38,528 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:40:38,541 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:40:38,542 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:40:38,543 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:40:38,543 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:40:41,429 | server.py:94 | initial parameters (loss, other metrics): 2.3048839569091797, {'accuracy': 0.0672, 'data_size': 10000}
INFO flwr 2024-04-30 21:40:41,429 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:40:41,430 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1623000)[0m 2024-04-30 21:40:44.059793: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1623000)[0m 2024-04-30 21:40:44.150360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1623000)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1623000)[0m 2024-04-30 21:40:46.214207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1623000)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1623000)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1622994)[0m 2024-04-30 21:40:44.234068: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1622994)[0m 2024-04-30 21:40:44.316714: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1622994)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1623003)[0m 2024-04-30 21:40:46.214391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:41:04,784 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:41:06,061 | server.py:125 | fit progress: (1, 1.8877241611480713, {'accuracy': 0.6412, 'data_size': 10000}, 24.63188151799841)
INFO flwr 2024-04-30 21:41:06,062 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:41:06,062 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:41:15,258 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:41:16,268 | server.py:125 | fit progress: (2, 1.6539318561553955, {'accuracy': 0.8272, 'data_size': 10000}, 34.838849440042395)
INFO flwr 2024-04-30 21:41:16,269 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:41:16,269 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:41:24,957 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:41:26,231 | server.py:125 | fit progress: (3, 1.589808464050293, {'accuracy': 0.8819, 'data_size': 10000}, 44.80111703404691)
INFO flwr 2024-04-30 21:41:26,231 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:41:26,231 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:41:34,883 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:41:36,124 | server.py:125 | fit progress: (4, 1.5758113861083984, {'accuracy': 0.893, 'data_size': 10000}, 54.69437152304454)
INFO flwr 2024-04-30 21:41:36,124 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:41:36,124 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:41:45,402 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:41:46,460 | server.py:125 | fit progress: (5, 1.5879546403884888, {'accuracy': 0.8766, 'data_size': 10000}, 65.03067741804989)
INFO flwr 2024-04-30 21:41:46,460 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:41:46,461 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:41:55,892 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:41:57,152 | server.py:125 | fit progress: (6, 1.565676212310791, {'accuracy': 0.8984, 'data_size': 10000}, 75.72255123802461)
INFO flwr 2024-04-30 21:41:57,152 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:41:57,153 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:42:05,397 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:42:06,656 | server.py:125 | fit progress: (7, 1.570591926574707, {'accuracy': 0.8936, 'data_size': 10000}, 85.22690865304321)
INFO flwr 2024-04-30 21:42:06,657 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:42:06,657 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:42:15,748 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:42:17,018 | server.py:125 | fit progress: (8, 1.5580530166625977, {'accuracy': 0.9061, 'data_size': 10000}, 95.58873013703851)
INFO flwr 2024-04-30 21:42:17,018 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:42:17,019 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:42:25,906 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:42:27,143 | server.py:125 | fit progress: (9, 1.5756572484970093, {'accuracy': 0.8868, 'data_size': 10000}, 105.71313633699901)
INFO flwr 2024-04-30 21:42:27,143 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:42:27,143 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:42:35,898 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:42:37,139 | server.py:125 | fit progress: (10, 1.571777582168579, {'accuracy': 0.8891, 'data_size': 10000}, 115.7095374269993)
INFO flwr 2024-04-30 21:42:37,139 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:42:37,139 | server.py:153 | FL finished in 115.70994346303632
INFO flwr 2024-04-30 21:42:37,140 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:42:37,140 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:42:37,140 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:42:37,140 | app.py:229 | app_fit: losses_centralized [(0, 2.3048839569091797), (1, 1.8877241611480713), (2, 1.6539318561553955), (3, 1.589808464050293), (4, 1.5758113861083984), (5, 1.5879546403884888), (6, 1.565676212310791), (7, 1.570591926574707), (8, 1.5580530166625977), (9, 1.5756572484970093), (10, 1.571777582168579)]
INFO flwr 2024-04-30 21:42:37,140 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0672), (1, 0.6412), (2, 0.8272), (3, 0.8819), (4, 0.893), (5, 0.8766), (6, 0.8984), (7, 0.8936), (8, 0.9061), (9, 0.8868), (10, 0.8891)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8891
wandb:     loss 1.57178
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_214023-fd2nl0me
wandb: Find logs at: ./wandb/offline-run-20240430_214023-fd2nl0me/logs
INFO flwr 2024-04-30 21:42:40,699 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:42:41,345 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1622992)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1622992)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:42:46,133	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:42:46,245	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:42:46,372	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:42:46,374	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:42:56,004 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151282683700.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69121150156.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-30 21:42:56,004 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:42:56,004 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:42:56,019 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:42:56,020 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:42:56,020 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:42:56,020 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:42:58,027 | server.py:94 | initial parameters (loss, other metrics): 2.3012635707855225, {'accuracy': 0.1222, 'data_size': 10000}
INFO flwr 2024-04-30 21:42:58,027 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:42:58,027 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1647141)[0m 2024-04-30 21:43:01.479628: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1647141)[0m 2024-04-30 21:43:01.573041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1647141)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1647141)[0m 2024-04-30 21:43:03.525680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1647141)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1647141)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1647127)[0m 2024-04-30 21:43:01.769967: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1647127)[0m 2024-04-30 21:43:01.857718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1647127)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1647127)[0m 2024-04-30 21:43:03.906311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:43:18,366 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:43:19,592 | server.py:125 | fit progress: (1, 1.9161324501037598, {'accuracy': 0.5928, 'data_size': 10000}, 21.564771863049828)
INFO flwr 2024-04-30 21:43:19,592 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:43:19,592 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:43:29,228 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:43:30,487 | server.py:125 | fit progress: (2, 1.654765009880066, {'accuracy': 0.8368, 'data_size': 10000}, 32.459516197035555)
INFO flwr 2024-04-30 21:43:30,487 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:43:30,487 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:43:39,714 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:43:40,947 | server.py:125 | fit progress: (3, 1.6143921613693237, {'accuracy': 0.8567, 'data_size': 10000}, 42.91932363703381)
INFO flwr 2024-04-30 21:43:40,947 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:43:40,947 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:43:50,574 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:43:51,787 | server.py:125 | fit progress: (4, 1.5833388566970825, {'accuracy': 0.8834, 'data_size': 10000}, 53.75927727500675)
INFO flwr 2024-04-30 21:43:51,787 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:43:51,787 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:44:00,634 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:44:01,673 | server.py:125 | fit progress: (5, 1.5669175386428833, {'accuracy': 0.8978, 'data_size': 10000}, 63.64594334602589)
INFO flwr 2024-04-30 21:44:01,673 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:44:01,674 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:44:10,989 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:44:12,009 | server.py:125 | fit progress: (6, 1.5617684125900269, {'accuracy': 0.9021, 'data_size': 10000}, 73.98206614301307)
INFO flwr 2024-04-30 21:44:12,010 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:44:12,010 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:44:21,349 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:44:22,383 | server.py:125 | fit progress: (7, 1.57048761844635, {'accuracy': 0.893, 'data_size': 10000}, 84.35530659003416)
INFO flwr 2024-04-30 21:44:22,383 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:44:22,383 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:44:31,979 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:44:32,989 | server.py:125 | fit progress: (8, 1.5594195127487183, {'accuracy': 0.9026, 'data_size': 10000}, 94.96184280503076)
INFO flwr 2024-04-30 21:44:32,989 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:44:32,990 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:44:41,654 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:44:42,896 | server.py:125 | fit progress: (9, 1.5529221296310425, {'accuracy': 0.91, 'data_size': 10000}, 104.86836041201605)
INFO flwr 2024-04-30 21:44:42,896 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:44:42,896 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:44:51,914 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:44:53,160 | server.py:125 | fit progress: (10, 1.5512950420379639, {'accuracy': 0.911, 'data_size': 10000}, 115.1327688380261)
INFO flwr 2024-04-30 21:44:53,160 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:44:53,160 | server.py:153 | FL finished in 115.13311401702231
INFO flwr 2024-04-30 21:44:53,160 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:44:53,161 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:44:53,161 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:44:53,161 | app.py:229 | app_fit: losses_centralized [(0, 2.3012635707855225), (1, 1.9161324501037598), (2, 1.654765009880066), (3, 1.6143921613693237), (4, 1.5833388566970825), (5, 1.5669175386428833), (6, 1.5617684125900269), (7, 1.57048761844635), (8, 1.5594195127487183), (9, 1.5529221296310425), (10, 1.5512950420379639)]
INFO flwr 2024-04-30 21:44:53,161 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1222), (1, 0.5928), (2, 0.8368), (3, 0.8567), (4, 0.8834), (5, 0.8978), (6, 0.9021), (7, 0.893), (8, 0.9026), (9, 0.91), (10, 0.911)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.911
wandb:     loss 1.5513
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_214240-zriprpzd
wandb: Find logs at: ./wandb/offline-run-20240430_214240-zriprpzd/logs
INFO flwr 2024-04-30 21:44:56,695 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:44:57,362 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1647125)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1647125)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:45:01,828	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:45:01,948	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:45:02,045	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:45:02,047	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:45:11,710 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 133416868045.0, 'object_store_memory': 61464372019.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 21:45:11,710 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:45:11,710 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:45:11,725 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:45:11,726 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:45:11,726 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:45:11,726 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:45:14,161 | server.py:94 | initial parameters (loss, other metrics): 2.3025295734405518, {'accuracy': 0.081, 'data_size': 10000}
INFO flwr 2024-04-30 21:45:14,161 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:45:14,162 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1678715)[0m 2024-04-30 21:45:17.131812: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1678715)[0m 2024-04-30 21:45:17.277141: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1678715)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1678715)[0m 2024-04-30 21:45:19.184230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1678715)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1678715)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1678714)[0m 2024-04-30 21:45:17.272556: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1678714)[0m 2024-04-30 21:45:17.356408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1678714)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1678714)[0m 2024-04-30 21:45:19.357039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:45:38,036 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:45:39,314 | server.py:125 | fit progress: (1, 1.8673821687698364, {'accuracy': 0.6211, 'data_size': 10000}, 25.152911913988646)
INFO flwr 2024-04-30 21:45:39,315 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:45:39,315 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:45:51,333 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:45:52,364 | server.py:125 | fit progress: (2, 1.7035753726959229, {'accuracy': 0.7829, 'data_size': 10000}, 38.202354081964586)
INFO flwr 2024-04-30 21:45:52,364 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:45:52,364 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:46:03,756 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:46:05,006 | server.py:125 | fit progress: (3, 1.6020358800888062, {'accuracy': 0.8691, 'data_size': 10000}, 50.84494874498341)
INFO flwr 2024-04-30 21:46:05,007 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:46:05,007 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:46:17,076 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:46:18,328 | server.py:125 | fit progress: (4, 1.5970468521118164, {'accuracy': 0.8676, 'data_size': 10000}, 64.16615715296939)
INFO flwr 2024-04-30 21:46:18,328 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:46:18,328 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:46:30,240 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:46:31,265 | server.py:125 | fit progress: (5, 1.6225626468658447, {'accuracy': 0.8403, 'data_size': 10000}, 77.10404498898424)
INFO flwr 2024-04-30 21:46:31,266 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:46:31,266 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:46:43,190 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:46:44,438 | server.py:125 | fit progress: (6, 1.613240361213684, {'accuracy': 0.8501, 'data_size': 10000}, 90.27657963399542)
INFO flwr 2024-04-30 21:46:44,438 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:46:44,438 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:46:56,025 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:46:57,294 | server.py:125 | fit progress: (7, 1.5819393396377563, {'accuracy': 0.8817, 'data_size': 10000}, 103.13221594999777)
INFO flwr 2024-04-30 21:46:57,294 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:46:57,294 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:47:08,059 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:47:09,286 | server.py:125 | fit progress: (8, 1.573933720588684, {'accuracy': 0.8885, 'data_size': 10000}, 115.12421280000126)
INFO flwr 2024-04-30 21:47:09,286 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:47:09,286 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:47:20,752 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:47:22,029 | server.py:125 | fit progress: (9, 1.5551074743270874, {'accuracy': 0.9083, 'data_size': 10000}, 127.86714076099452)
INFO flwr 2024-04-30 21:47:22,029 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:47:22,029 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:47:33,570 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:47:34,795 | server.py:125 | fit progress: (10, 1.5640753507614136, {'accuracy': 0.8975, 'data_size': 10000}, 140.6337166839512)
INFO flwr 2024-04-30 21:47:34,795 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:47:34,795 | server.py:153 | FL finished in 140.63410723098787
INFO flwr 2024-04-30 21:47:34,796 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:47:34,796 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:47:34,796 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:47:34,796 | app.py:229 | app_fit: losses_centralized [(0, 2.3025295734405518), (1, 1.8673821687698364), (2, 1.7035753726959229), (3, 1.6020358800888062), (4, 1.5970468521118164), (5, 1.6225626468658447), (6, 1.613240361213684), (7, 1.5819393396377563), (8, 1.573933720588684), (9, 1.5551074743270874), (10, 1.5640753507614136)]
INFO flwr 2024-04-30 21:47:34,796 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.081), (1, 0.6211), (2, 0.7829), (3, 0.8691), (4, 0.8676), (5, 0.8403), (6, 0.8501), (7, 0.8817), (8, 0.8885), (9, 0.9083), (10, 0.8975)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8975
wandb:     loss 1.56408
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_214456-jixdfity
wandb: Find logs at: ./wandb/offline-run-20240430_214456-jixdfity/logs
INFO flwr 2024-04-30 21:47:38,314 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:47:38,991 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1678714)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1678714)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:47:43,692	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:47:43,796	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:47:43,891	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:47:43,893	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:47:53,541 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 132398475879.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61027918233.0}
INFO flwr 2024-04-30 21:47:53,542 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:47:53,542 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:47:53,559 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:47:53,560 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:47:53,560 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:47:53,560 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:47:55,418 | server.py:94 | initial parameters (loss, other metrics): 2.3045308589935303, {'accuracy': 0.0968, 'data_size': 10000}
INFO flwr 2024-04-30 21:47:55,418 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:47:55,419 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1710161)[0m 2024-04-30 21:47:59.088547: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1710161)[0m 2024-04-30 21:47:59.194162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1710161)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1710161)[0m 2024-04-30 21:48:01.118937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1710163)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1710163)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1710168)[0m 2024-04-30 21:47:59.333027: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1710168)[0m 2024-04-30 21:47:59.424583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1710168)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1710163)[0m 2024-04-30 21:48:01.318643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:48:19,131 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:48:20,348 | server.py:125 | fit progress: (1, 1.8177582025527954, {'accuracy': 0.709, 'data_size': 10000}, 24.92942437599413)
INFO flwr 2024-04-30 21:48:20,348 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:48:20,349 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:48:32,521 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:48:33,794 | server.py:125 | fit progress: (2, 1.6544914245605469, {'accuracy': 0.8299, 'data_size': 10000}, 38.37490853498457)
INFO flwr 2024-04-30 21:48:33,794 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:48:33,794 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:48:45,397 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:48:46,660 | server.py:125 | fit progress: (3, 1.6050124168395996, {'accuracy': 0.8659, 'data_size': 10000}, 51.24126225296641)
INFO flwr 2024-04-30 21:48:46,660 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:48:46,660 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:48:58,337 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:48:59,586 | server.py:125 | fit progress: (4, 1.5755590200424194, {'accuracy': 0.8925, 'data_size': 10000}, 64.1667786979815)
INFO flwr 2024-04-30 21:48:59,586 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:48:59,586 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:49:10,043 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:49:11,064 | server.py:125 | fit progress: (5, 1.5752062797546387, {'accuracy': 0.8886, 'data_size': 10000}, 75.64534340100363)
INFO flwr 2024-04-30 21:49:11,064 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:49:11,065 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:49:22,661 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:49:23,704 | server.py:125 | fit progress: (6, 1.5691564083099365, {'accuracy': 0.8942, 'data_size': 10000}, 88.28572717600036)
INFO flwr 2024-04-30 21:49:23,705 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:49:23,705 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:49:35,920 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:49:36,933 | server.py:125 | fit progress: (7, 1.5561556816101074, {'accuracy': 0.9059, 'data_size': 10000}, 101.51418410998303)
INFO flwr 2024-04-30 21:49:36,933 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:49:36,933 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:49:48,864 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:49:49,915 | server.py:125 | fit progress: (8, 1.5620707273483276, {'accuracy': 0.9006, 'data_size': 10000}, 114.49636636598734)
INFO flwr 2024-04-30 21:49:49,915 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:49:49,916 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:50:02,233 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:50:03,510 | server.py:125 | fit progress: (9, 1.5562390089035034, {'accuracy': 0.9056, 'data_size': 10000}, 128.09170081600314)
INFO flwr 2024-04-30 21:50:03,511 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:50:03,511 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:50:15,161 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:50:16,415 | server.py:125 | fit progress: (10, 1.5559885501861572, {'accuracy': 0.9066, 'data_size': 10000}, 140.99612947000423)
INFO flwr 2024-04-30 21:50:16,415 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:50:16,415 | server.py:153 | FL finished in 140.99653252598364
INFO flwr 2024-04-30 21:50:16,415 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:50:16,415 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:50:16,416 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:50:16,416 | app.py:229 | app_fit: losses_centralized [(0, 2.3045308589935303), (1, 1.8177582025527954), (2, 1.6544914245605469), (3, 1.6050124168395996), (4, 1.5755590200424194), (5, 1.5752062797546387), (6, 1.5691564083099365), (7, 1.5561556816101074), (8, 1.5620707273483276), (9, 1.5562390089035034), (10, 1.5559885501861572)]
INFO flwr 2024-04-30 21:50:16,416 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0968), (1, 0.709), (2, 0.8299), (3, 0.8659), (4, 0.8925), (5, 0.8886), (6, 0.8942), (7, 0.9059), (8, 0.9006), (9, 0.9056), (10, 0.9066)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9066
wandb:     loss 1.55599
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_214738-6dila1t4
wandb: Find logs at: ./wandb/offline-run-20240430_214738-6dila1t4/logs
INFO flwr 2024-04-30 21:50:19,875 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:50:20,551 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1710158)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1710158)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:50:25,551	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:50:25,653	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:50:25,746	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:50:25,748	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:50:35,478 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 131319803700.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60565630156.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-30 21:50:35,479 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:50:35,479 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:50:35,494 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:50:35,495 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:50:35,495 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:50:35,495 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:50:38,014 | server.py:94 | initial parameters (loss, other metrics): 2.3030409812927246, {'accuracy': 0.1002, 'data_size': 10000}
INFO flwr 2024-04-30 21:50:38,015 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:50:38,015 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1749109)[0m 2024-04-30 21:50:40.984260: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1749109)[0m 2024-04-30 21:50:41.078538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1749109)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1749109)[0m 2024-04-30 21:50:43.089267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1749109)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1749109)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1749115)[0m 2024-04-30 21:50:41.113785: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1749115)[0m 2024-04-30 21:50:41.199677: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1749115)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1749115)[0m 2024-04-30 21:50:43.089156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:51:02,461 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:51:03,708 | server.py:125 | fit progress: (1, 1.9214081764221191, {'accuracy': 0.6146, 'data_size': 10000}, 25.69319647899829)
INFO flwr 2024-04-30 21:51:03,708 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:51:03,708 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:51:15,895 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:51:16,932 | server.py:125 | fit progress: (2, 1.6594388484954834, {'accuracy': 0.8276, 'data_size': 10000}, 38.9172347279964)
INFO flwr 2024-04-30 21:51:16,932 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:51:16,932 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:51:28,452 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:51:29,699 | server.py:125 | fit progress: (3, 1.6056249141693115, {'accuracy': 0.8691, 'data_size': 10000}, 51.68452418700326)
INFO flwr 2024-04-30 21:51:29,699 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:51:29,700 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:51:41,302 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:51:42,552 | server.py:125 | fit progress: (4, 1.576872706413269, {'accuracy': 0.8878, 'data_size': 10000}, 64.53735503403004)
INFO flwr 2024-04-30 21:51:42,552 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:51:42,552 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:51:54,482 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:51:55,534 | server.py:125 | fit progress: (5, 1.5647900104522705, {'accuracy': 0.8998, 'data_size': 10000}, 77.51881597103784)
INFO flwr 2024-04-30 21:51:55,534 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:51:55,534 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:52:07,475 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:52:08,719 | server.py:125 | fit progress: (6, 1.5571056604385376, {'accuracy': 0.9075, 'data_size': 10000}, 90.70434882101836)
INFO flwr 2024-04-30 21:52:08,719 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:52:08,719 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:52:20,297 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:52:21,556 | server.py:125 | fit progress: (7, 1.559601068496704, {'accuracy': 0.9026, 'data_size': 10000}, 103.54111473099329)
INFO flwr 2024-04-30 21:52:21,556 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:52:21,556 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:52:33,371 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:52:34,634 | server.py:125 | fit progress: (8, 1.5639702081680298, {'accuracy': 0.8991, 'data_size': 10000}, 116.61939289799193)
INFO flwr 2024-04-30 21:52:34,634 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:52:34,634 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:52:45,350 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:52:46,631 | server.py:125 | fit progress: (9, 1.5626155138015747, {'accuracy': 0.8992, 'data_size': 10000}, 128.61610626999754)
INFO flwr 2024-04-30 21:52:46,631 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:52:46,631 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:52:58,525 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:52:59,752 | server.py:125 | fit progress: (10, 1.5637266635894775, {'accuracy': 0.8979, 'data_size': 10000}, 141.73758311604615)
INFO flwr 2024-04-30 21:52:59,753 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:52:59,753 | server.py:153 | FL finished in 141.73793142504292
INFO flwr 2024-04-30 21:52:59,753 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:52:59,753 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:52:59,753 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:52:59,753 | app.py:229 | app_fit: losses_centralized [(0, 2.3030409812927246), (1, 1.9214081764221191), (2, 1.6594388484954834), (3, 1.6056249141693115), (4, 1.576872706413269), (5, 1.5647900104522705), (6, 1.5571056604385376), (7, 1.559601068496704), (8, 1.5639702081680298), (9, 1.5626155138015747), (10, 1.5637266635894775)]
INFO flwr 2024-04-30 21:52:59,753 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1002), (1, 0.6146), (2, 0.8276), (3, 0.8691), (4, 0.8878), (5, 0.8998), (6, 0.9075), (7, 0.9026), (8, 0.8991), (9, 0.8992), (10, 0.8979)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8979
wandb:     loss 1.56373
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_215020-ep34fu71
wandb: Find logs at: ./wandb/offline-run-20240430_215020-ep34fu71/logs
INFO flwr 2024-04-30 21:53:03,303 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:53:03,986 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1749107)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1749107)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:53:08,773	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:53:08,863	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:53:08,943	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:53:08,945	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:53:18,620 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 60306092851.0, 'node:__internal_head__': 1.0, 'memory': 130714216653.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 21:53:18,620 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:53:18,621 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:53:18,636 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:53:18,638 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:53:18,638 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:53:18,638 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:53:20,470 | server.py:94 | initial parameters (loss, other metrics): 2.3033287525177, {'accuracy': 0.0786, 'data_size': 10000}
INFO flwr 2024-04-30 21:53:20,471 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:53:20,473 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1788000)[0m 2024-04-30 21:53:24.086275: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1788000)[0m 2024-04-30 21:53:24.177598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1788000)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1788000)[0m 2024-04-30 21:53:26.172604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1788000)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1788000)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1787993)[0m 2024-04-30 21:53:24.212447: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1787993)[0m 2024-04-30 21:53:24.301835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1787993)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1787993)[0m 2024-04-30 21:53:26.156673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:53:52,296 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:53:53,513 | server.py:125 | fit progress: (1, 1.8652642965316772, {'accuracy': 0.6345, 'data_size': 10000}, 33.04034263099311)
INFO flwr 2024-04-30 21:53:53,513 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:53:53,514 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:54:14,443 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:54:15,724 | server.py:125 | fit progress: (2, 1.661927580833435, {'accuracy': 0.8199, 'data_size': 10000}, 55.25172444898635)
INFO flwr 2024-04-30 21:54:15,725 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:54:15,725 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:54:34,674 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:54:35,963 | server.py:125 | fit progress: (3, 1.6143041849136353, {'accuracy': 0.8541, 'data_size': 10000}, 75.4907813820173)
INFO flwr 2024-04-30 21:54:35,964 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:54:35,964 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:54:53,742 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:54:55,024 | server.py:125 | fit progress: (4, 1.5987581014633179, {'accuracy': 0.8703, 'data_size': 10000}, 94.55123287200695)
INFO flwr 2024-04-30 21:54:55,024 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:54:55,024 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:55:12,456 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:55:13,476 | server.py:125 | fit progress: (5, 1.6400675773620605, {'accuracy': 0.8235, 'data_size': 10000}, 113.00342541199643)
INFO flwr 2024-04-30 21:55:13,476 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:55:13,477 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:55:31,454 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:55:32,493 | server.py:125 | fit progress: (6, 1.5806779861450195, {'accuracy': 0.8829, 'data_size': 10000}, 132.0206371860113)
INFO flwr 2024-04-30 21:55:32,494 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:55:32,494 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:55:50,471 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:55:51,491 | server.py:125 | fit progress: (7, 1.5669983625411987, {'accuracy': 0.8961, 'data_size': 10000}, 151.01843522797571)
INFO flwr 2024-04-30 21:55:51,491 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:55:51,492 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:56:09,898 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:56:10,956 | server.py:125 | fit progress: (8, 1.5898233652114868, {'accuracy': 0.8736, 'data_size': 10000}, 170.4837098030257)
INFO flwr 2024-04-30 21:56:10,957 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:56:10,957 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:56:28,373 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 21:56:29,602 | server.py:125 | fit progress: (9, 1.5664135217666626, {'accuracy': 0.8954, 'data_size': 10000}, 189.12950391200138)
INFO flwr 2024-04-30 21:56:29,602 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 21:56:29,603 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:56:46,583 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 21:56:47,829 | server.py:125 | fit progress: (10, 1.5594325065612793, {'accuracy': 0.9031, 'data_size': 10000}, 207.35654667700874)
INFO flwr 2024-04-30 21:56:47,830 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 21:56:47,830 | server.py:153 | FL finished in 207.3569642510265
INFO flwr 2024-04-30 21:56:47,830 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 21:56:47,830 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 21:56:47,830 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 21:56:47,830 | app.py:229 | app_fit: losses_centralized [(0, 2.3033287525177), (1, 1.8652642965316772), (2, 1.661927580833435), (3, 1.6143041849136353), (4, 1.5987581014633179), (5, 1.6400675773620605), (6, 1.5806779861450195), (7, 1.5669983625411987), (8, 1.5898233652114868), (9, 1.5664135217666626), (10, 1.5594325065612793)]
INFO flwr 2024-04-30 21:56:47,830 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0786), (1, 0.6345), (2, 0.8199), (3, 0.8541), (4, 0.8703), (5, 0.8235), (6, 0.8829), (7, 0.8961), (8, 0.8736), (9, 0.8954), (10, 0.9031)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9031
wandb:     loss 1.55943
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_215303-slzjji5i
wandb: Find logs at: ./wandb/offline-run-20240430_215303-slzjji5i/logs
INFO flwr 2024-04-30 21:56:51,299 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 21:56:52,144 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1787993)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1787993)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 21:56:56,915	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 21:56:57,028	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 21:56:57,131	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 21:56:57,132	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 21:57:06,982 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 91301983642.0, 'object_store_memory': 43415135846.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-30 21:57:06,983 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 21:57:06,983 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 21:57:07,001 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 21:57:07,003 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 21:57:07,004 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 21:57:07,004 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 21:57:09,635 | server.py:94 | initial parameters (loss, other metrics): 2.3034565448760986, {'accuracy': 0.0994, 'data_size': 10000}
INFO flwr 2024-04-30 21:57:09,636 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 21:57:09,636 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1835042)[0m 2024-04-30 21:57:12.576803: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1835042)[0m 2024-04-30 21:57:12.681561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1835042)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1835042)[0m 2024-04-30 21:57:14.691066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1835057)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1835057)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1835055)[0m 2024-04-30 21:57:12.775196: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1835055)[0m 2024-04-30 21:57:12.860739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1835055)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1835055)[0m 2024-04-30 21:57:14.798215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 21:57:40,013 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 21:57:41,322 | server.py:125 | fit progress: (1, 1.8203614950180054, {'accuracy': 0.7083, 'data_size': 10000}, 31.68585119699128)
INFO flwr 2024-04-30 21:57:41,322 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 21:57:41,323 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:58:00,010 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 21:58:01,074 | server.py:125 | fit progress: (2, 1.6673516035079956, {'accuracy': 0.8171, 'data_size': 10000}, 51.43773850001162)
INFO flwr 2024-04-30 21:58:01,074 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 21:58:01,074 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:58:18,353 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 21:58:19,642 | server.py:125 | fit progress: (3, 1.5867156982421875, {'accuracy': 0.8862, 'data_size': 10000}, 70.00591022701701)
INFO flwr 2024-04-30 21:58:19,642 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 21:58:19,643 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:58:38,686 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 21:58:39,952 | server.py:125 | fit progress: (4, 1.5815540552139282, {'accuracy': 0.8861, 'data_size': 10000}, 90.31574990402441)
INFO flwr 2024-04-30 21:58:39,952 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 21:58:39,952 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:58:58,265 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 21:58:59,318 | server.py:125 | fit progress: (5, 1.5701433420181274, {'accuracy': 0.8954, 'data_size': 10000}, 109.68154684099136)
INFO flwr 2024-04-30 21:58:59,318 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 21:58:59,318 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:59:17,416 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 21:59:18,632 | server.py:125 | fit progress: (6, 1.5648860931396484, {'accuracy': 0.8993, 'data_size': 10000}, 128.99531584401848)
INFO flwr 2024-04-30 21:59:18,632 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 21:59:18,632 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:59:38,268 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 21:59:39,553 | server.py:125 | fit progress: (7, 1.5704249143600464, {'accuracy': 0.8924, 'data_size': 10000}, 149.91692121198867)
INFO flwr 2024-04-30 21:59:39,553 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 21:59:39,554 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 21:59:58,326 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 21:59:59,612 | server.py:125 | fit progress: (8, 1.5592267513275146, {'accuracy': 0.9036, 'data_size': 10000}, 169.97546618699562)
INFO flwr 2024-04-30 21:59:59,612 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 21:59:59,612 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:00:16,511 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:00:17,794 | server.py:125 | fit progress: (9, 1.5515964031219482, {'accuracy': 0.9115, 'data_size': 10000}, 188.15732680703513)
INFO flwr 2024-04-30 22:00:17,794 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:00:17,794 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:00:36,271 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:00:37,565 | server.py:125 | fit progress: (10, 1.5522582530975342, {'accuracy': 0.9108, 'data_size': 10000}, 207.928379169025)
INFO flwr 2024-04-30 22:00:37,565 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:00:37,565 | server.py:153 | FL finished in 207.92875983502017
INFO flwr 2024-04-30 22:00:37,565 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:00:37,565 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:00:37,565 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:00:37,565 | app.py:229 | app_fit: losses_centralized [(0, 2.3034565448760986), (1, 1.8203614950180054), (2, 1.6673516035079956), (3, 1.5867156982421875), (4, 1.5815540552139282), (5, 1.5701433420181274), (6, 1.5648860931396484), (7, 1.5704249143600464), (8, 1.5592267513275146), (9, 1.5515964031219482), (10, 1.5522582530975342)]
INFO flwr 2024-04-30 22:00:37,565 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0994), (1, 0.7083), (2, 0.8171), (3, 0.8862), (4, 0.8861), (5, 0.8954), (6, 0.8993), (7, 0.8924), (8, 0.9036), (9, 0.9115), (10, 0.9108)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9108
wandb:     loss 1.55226
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_215651-pgc5s4s0
wandb: Find logs at: ./wandb/offline-run-20240430_215651-pgc5s4s0/logs
INFO flwr 2024-04-30 22:00:41,087 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:00:42,035 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1835050)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1835050)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:00:46,920	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:00:47,023	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:00:47,103	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:00:47,104	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:00:56,754 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 26829184204.0, 'node:10.20.240.18': 1.0, 'memory': 53658368411.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-30 22:00:56,754 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:00:56,755 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:00:56,770 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:00:56,771 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:00:56,771 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:00:56,771 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:00:58,783 | server.py:94 | initial parameters (loss, other metrics): 2.30328106880188, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-30 22:00:58,784 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:00:58,784 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1923979)[0m 2024-04-30 22:01:02.447357: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1923979)[0m 2024-04-30 22:01:02.529363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1923979)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1923979)[0m 2024-04-30 22:01:04.594760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1923979)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1923979)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1923976)[0m 2024-04-30 22:01:02.595020: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1923976)[0m 2024-04-30 22:01:02.682701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1923976)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1923976)[0m 2024-04-30 22:01:04.697390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:01:32,707 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:01:33,978 | server.py:125 | fit progress: (1, 1.795223593711853, {'accuracy': 0.7993, 'data_size': 10000}, 35.194365817005746)
INFO flwr 2024-04-30 22:01:33,979 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:01:33,979 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:01:52,766 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:01:54,037 | server.py:125 | fit progress: (2, 1.6697757244110107, {'accuracy': 0.8203, 'data_size': 10000}, 55.25266611203551)
INFO flwr 2024-04-30 22:01:54,037 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:01:54,037 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:02:11,775 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:02:13,048 | server.py:125 | fit progress: (3, 1.604802131652832, {'accuracy': 0.868, 'data_size': 10000}, 74.26427131105447)
INFO flwr 2024-04-30 22:02:13,049 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:02:13,049 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:02:32,127 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:02:33,403 | server.py:125 | fit progress: (4, 1.579116940498352, {'accuracy': 0.8868, 'data_size': 10000}, 94.61900638003135)
INFO flwr 2024-04-30 22:02:33,403 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:02:33,403 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:02:51,810 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:02:52,865 | server.py:125 | fit progress: (5, 1.5644795894622803, {'accuracy': 0.9006, 'data_size': 10000}, 114.08087361603975)
INFO flwr 2024-04-30 22:02:52,865 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:02:52,865 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:03:11,269 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:03:12,326 | server.py:125 | fit progress: (6, 1.5598464012145996, {'accuracy': 0.9037, 'data_size': 10000}, 133.54234146804083)
INFO flwr 2024-04-30 22:03:12,327 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:03:12,327 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:03:30,680 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:03:31,711 | server.py:125 | fit progress: (7, 1.5569660663604736, {'accuracy': 0.9064, 'data_size': 10000}, 152.92752087302506)
INFO flwr 2024-04-30 22:03:31,712 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:03:31,712 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:03:49,968 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:03:51,063 | server.py:125 | fit progress: (8, 1.5536795854568481, {'accuracy': 0.9095, 'data_size': 10000}, 172.27856395801064)
INFO flwr 2024-04-30 22:03:51,063 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:03:51,063 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:04:08,854 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:04:10,116 | server.py:125 | fit progress: (9, 1.5567045211791992, {'accuracy': 0.9051, 'data_size': 10000}, 191.33209358202294)
INFO flwr 2024-04-30 22:04:10,116 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:04:10,116 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:04:27,994 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:04:29,263 | server.py:125 | fit progress: (10, 1.5643558502197266, {'accuracy': 0.8972, 'data_size': 10000}, 210.47858616703888)
INFO flwr 2024-04-30 22:04:29,263 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:04:29,263 | server.py:153 | FL finished in 210.47895665100077
INFO flwr 2024-04-30 22:04:29,263 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:04:29,263 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:04:29,263 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:04:29,263 | app.py:229 | app_fit: losses_centralized [(0, 2.30328106880188), (1, 1.795223593711853), (2, 1.6697757244110107), (3, 1.604802131652832), (4, 1.579116940498352), (5, 1.5644795894622803), (6, 1.5598464012145996), (7, 1.5569660663604736), (8, 1.5536795854568481), (9, 1.5567045211791992), (10, 1.5643558502197266)]
INFO flwr 2024-04-30 22:04:29,263 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.7993), (2, 0.8203), (3, 0.868), (4, 0.8868), (5, 0.9006), (6, 0.9037), (7, 0.9064), (8, 0.9095), (9, 0.9051), (10, 0.8972)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8972
wandb:     loss 1.56436
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_220041-jy7xlsho
wandb: Find logs at: ./wandb/offline-run-20240430_220041-jy7xlsho/logs
INFO flwr 2024-04-30 22:04:32,863 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:04:33,560 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1923972)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1923972)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:04:38,624	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:04:38,732	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:04:38,814	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:04:38,816	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:04:48,494 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 47898410190.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 23949205094.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-30 22:04:48,494 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:04:48,494 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:04:48,509 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:04:48,510 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:04:48,510 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:04:48,510 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:04:50,940 | server.py:94 | initial parameters (loss, other metrics): 2.298717498779297, {'accuracy': 0.1272, 'data_size': 10000}
INFO flwr 2024-04-30 22:04:50,941 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:04:50,941 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2009543)[0m 2024-04-30 22:04:54.063640: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2009543)[0m 2024-04-30 22:04:54.159340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2009543)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2009543)[0m 2024-04-30 22:04:56.238099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2009543)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2009543)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2009537)[0m 2024-04-30 22:04:54.261600: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2009537)[0m 2024-04-30 22:04:54.349186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2009537)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2009539)[0m 2024-04-30 22:04:56.235964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:05:38,339 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:05:39,587 | server.py:125 | fit progress: (1, 1.8832989931106567, {'accuracy': 0.6253, 'data_size': 10000}, 48.64650990703376)
INFO flwr 2024-04-30 22:05:39,588 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:05:39,588 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:06:15,754 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:06:16,803 | server.py:125 | fit progress: (2, 1.6563366651535034, {'accuracy': 0.8269, 'data_size': 10000}, 85.86171799898148)
INFO flwr 2024-04-30 22:06:16,803 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:06:16,803 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:06:51,158 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:06:52,403 | server.py:125 | fit progress: (3, 1.6009480953216553, {'accuracy': 0.8757, 'data_size': 10000}, 121.46179179899627)
INFO flwr 2024-04-30 22:06:52,403 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:06:52,403 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:07:29,331 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:07:30,634 | server.py:125 | fit progress: (4, 1.5869531631469727, {'accuracy': 0.8799, 'data_size': 10000}, 159.69295685202815)
INFO flwr 2024-04-30 22:07:30,634 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:07:30,634 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:08:02,017 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:08:03,045 | server.py:125 | fit progress: (5, 1.5663056373596191, {'accuracy': 0.8994, 'data_size': 10000}, 192.10379309701966)
INFO flwr 2024-04-30 22:08:03,045 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:08:03,045 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:08:36,317 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:08:37,571 | server.py:125 | fit progress: (6, 1.5653560161590576, {'accuracy': 0.8978, 'data_size': 10000}, 226.63018372101942)
INFO flwr 2024-04-30 22:08:37,571 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:08:37,571 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:09:10,228 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:09:11,533 | server.py:125 | fit progress: (7, 1.5790131092071533, {'accuracy': 0.8841, 'data_size': 10000}, 260.59198208298767)
INFO flwr 2024-04-30 22:09:11,533 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:09:11,533 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:09:44,778 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:09:46,080 | server.py:125 | fit progress: (8, 1.5633985996246338, {'accuracy': 0.8999, 'data_size': 10000}, 295.13935634400696)
INFO flwr 2024-04-30 22:09:46,081 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:09:46,081 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:10:20,254 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:10:21,546 | server.py:125 | fit progress: (9, 1.560025930404663, {'accuracy': 0.9018, 'data_size': 10000}, 330.60530918702716)
INFO flwr 2024-04-30 22:10:21,546 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:10:21,547 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:10:55,287 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:10:56,600 | server.py:125 | fit progress: (10, 1.5573636293411255, {'accuracy': 0.9049, 'data_size': 10000}, 365.65905658499105)
INFO flwr 2024-04-30 22:10:56,600 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:10:56,600 | server.py:153 | FL finished in 365.65940154198324
INFO flwr 2024-04-30 22:10:56,600 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:10:56,600 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:10:56,601 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:10:56,601 | app.py:229 | app_fit: losses_centralized [(0, 2.298717498779297), (1, 1.8832989931106567), (2, 1.6563366651535034), (3, 1.6009480953216553), (4, 1.5869531631469727), (5, 1.5663056373596191), (6, 1.5653560161590576), (7, 1.5790131092071533), (8, 1.5633985996246338), (9, 1.560025930404663), (10, 1.5573636293411255)]
INFO flwr 2024-04-30 22:10:56,601 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1272), (1, 0.6253), (2, 0.8269), (3, 0.8757), (4, 0.8799), (5, 0.8994), (6, 0.8978), (7, 0.8841), (8, 0.8999), (9, 0.9018), (10, 0.9049)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9049
wandb:     loss 1.55736
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_220433-1t0nvhj6
wandb: Find logs at: ./wandb/offline-run-20240430_220433-1t0nvhj6/logs
INFO flwr 2024-04-30 22:11:00,087 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:11:00,802 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2009539)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2009539)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:11:05,848	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:11:05,954	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:11:06,061	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:11:06,062	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:11:16,335 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 45118046208.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 22559023104.0}
INFO flwr 2024-04-30 22:11:16,336 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:11:16,336 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:11:16,353 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:11:16,354 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:11:16,354 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:11:16,354 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:11:18,225 | server.py:94 | initial parameters (loss, other metrics): 2.307264804840088, {'accuracy': 0.0483, 'data_size': 10000}
INFO flwr 2024-04-30 22:11:18,225 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:11:18,225 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2072370)[0m 2024-04-30 22:11:21.883414: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2072370)[0m 2024-04-30 22:11:21.979026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2072370)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2072370)[0m 2024-04-30 22:11:23.964438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2072370)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2072370)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2072374)[0m 2024-04-30 22:11:22.057177: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2072374)[0m 2024-04-30 22:11:22.141277: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2072374)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2072374)[0m 2024-04-30 22:11:23.994858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:12:03,977 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:12:05,260 | server.py:125 | fit progress: (1, 1.8805203437805176, {'accuracy': 0.6567, 'data_size': 10000}, 47.03457256697584)
INFO flwr 2024-04-30 22:12:05,260 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:12:05,260 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:12:38,407 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:12:39,684 | server.py:125 | fit progress: (2, 1.6847734451293945, {'accuracy': 0.8004, 'data_size': 10000}, 81.45887864095857)
INFO flwr 2024-04-30 22:12:39,684 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:12:39,685 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:13:14,254 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:13:15,543 | server.py:125 | fit progress: (3, 1.5848160982131958, {'accuracy': 0.8895, 'data_size': 10000}, 117.31731933396077)
INFO flwr 2024-04-30 22:13:15,543 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:13:15,543 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:13:47,073 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:13:48,430 | server.py:125 | fit progress: (4, 1.5846458673477173, {'accuracy': 0.8814, 'data_size': 10000}, 150.20472671400057)
INFO flwr 2024-04-30 22:13:48,430 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:13:48,430 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:14:22,161 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:14:23,221 | server.py:125 | fit progress: (5, 1.5755056142807007, {'accuracy': 0.8902, 'data_size': 10000}, 184.995253200992)
INFO flwr 2024-04-30 22:14:23,221 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:14:23,221 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:14:55,117 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:14:56,168 | server.py:125 | fit progress: (6, 1.5620064735412598, {'accuracy': 0.9022, 'data_size': 10000}, 217.94272201496642)
INFO flwr 2024-04-30 22:14:56,168 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:14:56,168 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:15:27,982 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:15:29,069 | server.py:125 | fit progress: (7, 1.5660927295684814, {'accuracy': 0.896, 'data_size': 10000}, 250.84334450401366)
INFO flwr 2024-04-30 22:15:29,069 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:15:29,069 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:16:02,307 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:16:03,381 | server.py:125 | fit progress: (8, 1.5662527084350586, {'accuracy': 0.896, 'data_size': 10000}, 285.15519589500036)
INFO flwr 2024-04-30 22:16:03,381 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:16:03,381 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:16:36,553 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:16:37,830 | server.py:125 | fit progress: (9, 1.5585044622421265, {'accuracy': 0.9033, 'data_size': 10000}, 319.60487594100414)
INFO flwr 2024-04-30 22:16:37,830 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:16:37,831 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:17:12,574 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:17:13,850 | server.py:125 | fit progress: (10, 1.5511242151260376, {'accuracy': 0.9115, 'data_size': 10000}, 355.624184870976)
INFO flwr 2024-04-30 22:17:13,850 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:17:13,850 | server.py:153 | FL finished in 355.6245329529629
INFO flwr 2024-04-30 22:17:13,850 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:17:13,850 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:17:13,850 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:17:13,850 | app.py:229 | app_fit: losses_centralized [(0, 2.307264804840088), (1, 1.8805203437805176), (2, 1.6847734451293945), (3, 1.5848160982131958), (4, 1.5846458673477173), (5, 1.5755056142807007), (6, 1.5620064735412598), (7, 1.5660927295684814), (8, 1.5662527084350586), (9, 1.5585044622421265), (10, 1.5511242151260376)]
INFO flwr 2024-04-30 22:17:13,850 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0483), (1, 0.6567), (2, 0.8004), (3, 0.8895), (4, 0.8814), (5, 0.8902), (6, 0.9022), (7, 0.896), (8, 0.896), (9, 0.9033), (10, 0.9115)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9115
wandb:     loss 1.55112
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_221100-515x2nb1
wandb: Find logs at: ./wandb/offline-run-20240430_221100-515x2nb1/logs
INFO flwr 2024-04-30 22:17:17,419 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:17:18,520 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2072377)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2072377)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:17:23,602	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:17:23,728	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:17:23,815	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:17:23,816	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:17:33,601 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 46031941632.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 23015970816.0}
INFO flwr 2024-04-30 22:17:33,602 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:17:33,602 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:17:33,615 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:17:33,616 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:17:33,616 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:17:33,616 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:17:36,389 | server.py:94 | initial parameters (loss, other metrics): 2.302804708480835, {'accuracy': 0.0685, 'data_size': 10000}
INFO flwr 2024-04-30 22:17:36,390 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:17:36,390 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2177885)[0m 2024-04-30 22:17:39.136463: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2177885)[0m 2024-04-30 22:17:39.243829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2177885)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2177892)[0m 2024-04-30 22:17:41.437190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2177892)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2177892)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2177894)[0m 2024-04-30 22:17:39.349867: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2177894)[0m 2024-04-30 22:17:39.436679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2177894)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2177888)[0m 2024-04-30 22:17:41.435750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:18:25,541 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:18:26,809 | server.py:125 | fit progress: (1, 1.848019003868103, {'accuracy': 0.7255, 'data_size': 10000}, 50.418756362982094)
INFO flwr 2024-04-30 22:18:26,809 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:18:26,809 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:19:01,668 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:19:02,727 | server.py:125 | fit progress: (2, 1.6476867198944092, {'accuracy': 0.8486, 'data_size': 10000}, 86.33683541201754)
INFO flwr 2024-04-30 22:19:02,727 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:19:02,727 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:19:38,955 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:19:40,209 | server.py:125 | fit progress: (3, 1.604912519454956, {'accuracy': 0.8672, 'data_size': 10000}, 123.81940965098329)
INFO flwr 2024-04-30 22:19:40,210 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:19:40,210 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:20:15,140 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:20:16,412 | server.py:125 | fit progress: (4, 1.5834424495697021, {'accuracy': 0.8842, 'data_size': 10000}, 160.02250577899395)
INFO flwr 2024-04-30 22:20:16,413 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:20:16,413 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:20:48,315 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:20:49,400 | server.py:125 | fit progress: (5, 1.5875245332717896, {'accuracy': 0.8754, 'data_size': 10000}, 193.01015673601069)
INFO flwr 2024-04-30 22:20:49,400 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:20:49,401 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:21:23,200 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:21:24,487 | server.py:125 | fit progress: (6, 1.5768078565597534, {'accuracy': 0.8863, 'data_size': 10000}, 228.09713863302022)
INFO flwr 2024-04-30 22:21:24,487 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:21:24,488 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:22:01,513 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:22:02,779 | server.py:125 | fit progress: (7, 1.5667877197265625, {'accuracy': 0.8964, 'data_size': 10000}, 266.3892898230115)
INFO flwr 2024-04-30 22:22:02,780 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:22:02,780 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:22:35,314 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:22:36,534 | server.py:125 | fit progress: (8, 1.5503060817718506, {'accuracy': 0.9119, 'data_size': 10000}, 300.14449853799306)
INFO flwr 2024-04-30 22:22:36,535 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:22:36,535 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:23:10,757 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:23:12,047 | server.py:125 | fit progress: (9, 1.550216794013977, {'accuracy': 0.9125, 'data_size': 10000}, 335.65668856498087)
INFO flwr 2024-04-30 22:23:12,047 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:23:12,047 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:23:41,991 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:23:43,209 | server.py:125 | fit progress: (10, 1.5530232191085815, {'accuracy': 0.9088, 'data_size': 10000}, 366.81901481800014)
INFO flwr 2024-04-30 22:23:43,209 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:23:43,209 | server.py:153 | FL finished in 366.8193626939901
INFO flwr 2024-04-30 22:23:43,209 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:23:43,210 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:23:43,210 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:23:43,210 | app.py:229 | app_fit: losses_centralized [(0, 2.302804708480835), (1, 1.848019003868103), (2, 1.6476867198944092), (3, 1.604912519454956), (4, 1.5834424495697021), (5, 1.5875245332717896), (6, 1.5768078565597534), (7, 1.5667877197265625), (8, 1.5503060817718506), (9, 1.550216794013977), (10, 1.5530232191085815)]
INFO flwr 2024-04-30 22:23:43,210 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0685), (1, 0.7255), (2, 0.8486), (3, 0.8672), (4, 0.8842), (5, 0.8754), (6, 0.8863), (7, 0.8964), (8, 0.9119), (9, 0.9125), (10, 0.9088)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9088
wandb:     loss 1.55302
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_221717-y9v72u2c
wandb: Find logs at: ./wandb/offline-run-20240430_221717-y9v72u2c/logs
INFO flwr 2024-04-30 22:23:46,682 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:23:49,520 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2177888)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2177888)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:23:54,460	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:23:54,577	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:23:54,668	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:23:54,670	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:24:04,311 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 133953427252.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61694325964.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-30 22:24:04,311 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:24:04,312 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:24:04,325 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:24:04,326 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:24:04,326 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:24:04,326 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:24:06,147 | server.py:94 | initial parameters (loss, other metrics): 2.3021957874298096, {'accuracy': 0.0855, 'data_size': 10000}
INFO flwr 2024-04-30 22:24:06,147 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:24:06,147 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2242085)[0m 2024-04-30 22:24:11.250717: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2242085)[0m 2024-04-30 22:24:11.355135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2242085)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2242085)[0m 2024-04-30 22:24:13.503156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2242085)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2242085)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2242090)[0m 2024-04-30 22:24:11.438200: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2242090)[0m 2024-04-30 22:24:11.530533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2242090)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2242090)[0m 2024-04-30 22:24:13.532429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:24:26,924 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:24:28,223 | server.py:125 | fit progress: (1, 1.9524203538894653, {'accuracy': 0.5401, 'data_size': 10000}, 22.07580929499818)
INFO flwr 2024-04-30 22:24:28,223 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:24:28,224 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:24:36,414 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:24:37,689 | server.py:125 | fit progress: (2, 1.964889407157898, {'accuracy': 0.4958, 'data_size': 10000}, 31.54135109001072)
INFO flwr 2024-04-30 22:24:37,689 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:24:37,689 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:24:44,798 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:24:46,028 | server.py:125 | fit progress: (3, 1.8634898662567139, {'accuracy': 0.5964, 'data_size': 10000}, 39.880977433989756)
INFO flwr 2024-04-30 22:24:46,029 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:24:46,029 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:24:53,027 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:24:54,316 | server.py:125 | fit progress: (4, 1.8393468856811523, {'accuracy': 0.6211, 'data_size': 10000}, 48.16857026400976)
INFO flwr 2024-04-30 22:24:54,316 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:24:54,316 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:25:01,794 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:25:02,831 | server.py:125 | fit progress: (5, 1.8373421430587769, {'accuracy': 0.622, 'data_size': 10000}, 56.68318851199001)
INFO flwr 2024-04-30 22:25:02,831 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:25:02,831 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:25:10,074 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:25:11,137 | server.py:125 | fit progress: (6, 1.8313318490982056, {'accuracy': 0.6281, 'data_size': 10000}, 64.98921361804241)
INFO flwr 2024-04-30 22:25:11,137 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:25:11,137 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:25:18,350 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:25:19,389 | server.py:125 | fit progress: (7, 1.8168960809707642, {'accuracy': 0.6414, 'data_size': 10000}, 73.24132891598856)
INFO flwr 2024-04-30 22:25:19,389 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:25:19,389 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:25:26,793 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:25:27,820 | server.py:125 | fit progress: (8, 1.8060327768325806, {'accuracy': 0.6527, 'data_size': 10000}, 81.67313975503203)
INFO flwr 2024-04-30 22:25:27,821 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:25:27,821 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:25:34,570 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:25:35,842 | server.py:125 | fit progress: (9, 1.7914448976516724, {'accuracy': 0.6681, 'data_size': 10000}, 89.69460035499651)
INFO flwr 2024-04-30 22:25:35,842 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:25:35,842 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:25:42,865 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:25:44,102 | server.py:125 | fit progress: (10, 1.768194556236267, {'accuracy': 0.6908, 'data_size': 10000}, 97.95509882300394)
INFO flwr 2024-04-30 22:25:44,103 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:25:44,103 | server.py:153 | FL finished in 97.95552191702882
INFO flwr 2024-04-30 22:25:44,103 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:25:44,103 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:25:44,103 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:25:44,103 | app.py:229 | app_fit: losses_centralized [(0, 2.3021957874298096), (1, 1.9524203538894653), (2, 1.964889407157898), (3, 1.8634898662567139), (4, 1.8393468856811523), (5, 1.8373421430587769), (6, 1.8313318490982056), (7, 1.8168960809707642), (8, 1.8060327768325806), (9, 1.7914448976516724), (10, 1.768194556236267)]
INFO flwr 2024-04-30 22:25:44,104 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0855), (1, 0.5401), (2, 0.4958), (3, 0.5964), (4, 0.6211), (5, 0.622), (6, 0.6281), (7, 0.6414), (8, 0.6527), (9, 0.6681), (10, 0.6908)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6908
wandb:     loss 1.76819
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_222348-vij3pjze
wandb: Find logs at: ./wandb/offline-run-20240430_222348-vij3pjze/logs
INFO flwr 2024-04-30 22:25:47,615 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:25:48,415 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2242096)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2242096)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:25:52,975	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:25:53,087	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:25:53,184	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:25:53,185	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:26:02,818 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 130432012493.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60185148211.0}
INFO flwr 2024-04-30 22:26:02,818 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:26:02,818 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:26:02,833 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:26:02,834 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:26:02,834 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:26:02,834 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:26:05,766 | server.py:94 | initial parameters (loss, other metrics): 2.3001112937927246, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-30 22:26:05,766 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:26:05,766 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2263653)[0m 2024-04-30 22:26:08.256370: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2263653)[0m 2024-04-30 22:26:08.361526: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2263653)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2263653)[0m 2024-04-30 22:26:11.652431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2263653)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2263653)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2263652)[0m 2024-04-30 22:26:08.347571: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2263652)[0m 2024-04-30 22:26:08.464404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2263652)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2263652)[0m 2024-04-30 22:26:11.758107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:26:28,725 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:26:29,749 | server.py:125 | fit progress: (1, 1.9853038787841797, {'accuracy': 0.4874, 'data_size': 10000}, 23.983036657969933)
INFO flwr 2024-04-30 22:26:29,749 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:26:29,750 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:26:37,920 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:26:39,146 | server.py:125 | fit progress: (2, 1.7742990255355835, {'accuracy': 0.6981, 'data_size': 10000}, 33.379466970975045)
INFO flwr 2024-04-30 22:26:39,146 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:26:39,146 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:26:46,348 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:26:47,581 | server.py:125 | fit progress: (3, 1.7486000061035156, {'accuracy': 0.7148, 'data_size': 10000}, 41.81503789796261)
INFO flwr 2024-04-30 22:26:47,581 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:26:47,605 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:26:54,469 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:26:55,776 | server.py:125 | fit progress: (4, 1.6834475994110107, {'accuracy': 0.7831, 'data_size': 10000}, 50.01023866201285)
INFO flwr 2024-04-30 22:26:55,777 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:26:55,777 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:27:02,532 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:27:03,804 | server.py:125 | fit progress: (5, 1.6462701559066772, {'accuracy': 0.8177, 'data_size': 10000}, 58.03764224098995)
INFO flwr 2024-04-30 22:27:03,804 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:27:03,804 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:27:10,848 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:27:11,899 | server.py:125 | fit progress: (6, 1.6161659955978394, {'accuracy': 0.8493, 'data_size': 10000}, 66.133236448979)
INFO flwr 2024-04-30 22:27:11,900 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:27:11,900 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:27:19,173 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:27:20,221 | server.py:125 | fit progress: (7, 1.6055195331573486, {'accuracy': 0.8574, 'data_size': 10000}, 74.45486345799873)
INFO flwr 2024-04-30 22:27:20,221 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:27:20,221 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:27:27,603 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:27:28,663 | server.py:125 | fit progress: (8, 1.6108654737472534, {'accuracy': 0.8522, 'data_size': 10000}, 82.89692056598142)
INFO flwr 2024-04-30 22:27:28,663 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:27:28,663 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:27:35,834 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:27:37,066 | server.py:125 | fit progress: (9, 1.614349126815796, {'accuracy': 0.8478, 'data_size': 10000}, 91.30026899900986)
INFO flwr 2024-04-30 22:27:37,067 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:27:37,067 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:27:44,227 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:27:45,458 | server.py:125 | fit progress: (10, 1.5890668630599976, {'accuracy': 0.8722, 'data_size': 10000}, 99.69153530197218)
INFO flwr 2024-04-30 22:27:45,458 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:27:45,458 | server.py:153 | FL finished in 99.69188971800031
INFO flwr 2024-04-30 22:27:45,458 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:27:45,458 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:27:45,458 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:27:45,458 | app.py:229 | app_fit: losses_centralized [(0, 2.3001112937927246), (1, 1.9853038787841797), (2, 1.7742990255355835), (3, 1.7486000061035156), (4, 1.6834475994110107), (5, 1.6462701559066772), (6, 1.6161659955978394), (7, 1.6055195331573486), (8, 1.6108654737472534), (9, 1.614349126815796), (10, 1.5890668630599976)]
INFO flwr 2024-04-30 22:27:45,458 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.4874), (2, 0.6981), (3, 0.7148), (4, 0.7831), (5, 0.8177), (6, 0.8493), (7, 0.8574), (8, 0.8522), (9, 0.8478), (10, 0.8722)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8722
wandb:     loss 1.58907
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_222547-r7jng21u
wandb: Find logs at: ./wandb/offline-run-20240430_222547-r7jng21u/logs
INFO flwr 2024-04-30 22:27:48,938 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:27:49,711 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2263645)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2263645)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:27:54,506	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:27:54,615	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:27:54,744	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:27:54,747	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:28:04,449 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 130413688218.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60177294950.0}
INFO flwr 2024-04-30 22:28:04,450 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:28:04,450 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:28:04,464 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:28:04,465 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:28:04,465 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:28:04,465 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:28:06,922 | server.py:94 | initial parameters (loss, other metrics): 2.3038723468780518, {'accuracy': 0.091, 'data_size': 10000}
INFO flwr 2024-04-30 22:28:06,922 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:28:06,922 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2288750)[0m 2024-04-30 22:28:09.895733: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2288749)[0m 2024-04-30 22:28:09.977396: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2288749)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2288750)[0m 2024-04-30 22:28:11.926192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2288749)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2288749)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2288873)[0m 2024-04-30 22:28:10.163979: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2288873)[0m 2024-04-30 22:28:10.256134: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2288873)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2288873)[0m 2024-04-30 22:28:12.107950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:28:32,271 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:28:33,300 | server.py:125 | fit progress: (1, 1.902721881866455, {'accuracy': 0.5844, 'data_size': 10000}, 26.377557784027886)
INFO flwr 2024-04-30 22:28:33,300 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:28:33,300 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:28:40,783 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:28:42,004 | server.py:125 | fit progress: (2, 1.7323601245880127, {'accuracy': 0.7458, 'data_size': 10000}, 35.08215118001681)
INFO flwr 2024-04-30 22:28:42,005 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:28:42,005 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:28:49,528 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:28:50,761 | server.py:125 | fit progress: (3, 1.715714454650879, {'accuracy': 0.7485, 'data_size': 10000}, 43.83818722702563)
INFO flwr 2024-04-30 22:28:50,761 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:28:50,761 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:28:58,177 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:28:59,428 | server.py:125 | fit progress: (4, 1.6642485857009888, {'accuracy': 0.7985, 'data_size': 10000}, 52.50597158202436)
INFO flwr 2024-04-30 22:28:59,429 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:28:59,429 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:29:06,694 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:29:07,971 | server.py:125 | fit progress: (5, 1.6791411638259888, {'accuracy': 0.7809, 'data_size': 10000}, 61.04840706003597)
INFO flwr 2024-04-30 22:29:07,971 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:29:07,971 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:29:15,030 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:29:16,077 | server.py:125 | fit progress: (6, 1.6407333612442017, {'accuracy': 0.8226, 'data_size': 10000}, 69.15448142105015)
INFO flwr 2024-04-30 22:29:16,077 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:29:16,077 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:29:23,422 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:29:24,475 | server.py:125 | fit progress: (7, 1.5996447801589966, {'accuracy': 0.8635, 'data_size': 10000}, 77.55232032504864)
INFO flwr 2024-04-30 22:29:24,475 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:29:24,475 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:29:32,080 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:29:33,101 | server.py:125 | fit progress: (8, 1.5878084897994995, {'accuracy': 0.8734, 'data_size': 10000}, 86.179141790024)
INFO flwr 2024-04-30 22:29:33,102 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:29:33,102 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:29:40,244 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:29:41,500 | server.py:125 | fit progress: (9, 1.595576524734497, {'accuracy': 0.867, 'data_size': 10000}, 94.57785254903138)
INFO flwr 2024-04-30 22:29:41,500 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:29:41,501 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:29:48,333 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:29:49,565 | server.py:125 | fit progress: (10, 1.5862730741500854, {'accuracy': 0.8757, 'data_size': 10000}, 102.64281798002776)
INFO flwr 2024-04-30 22:29:49,565 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:29:49,565 | server.py:153 | FL finished in 102.64317598904017
INFO flwr 2024-04-30 22:29:49,566 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:29:49,566 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:29:49,566 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:29:49,566 | app.py:229 | app_fit: losses_centralized [(0, 2.3038723468780518), (1, 1.902721881866455), (2, 1.7323601245880127), (3, 1.715714454650879), (4, 1.6642485857009888), (5, 1.6791411638259888), (6, 1.6407333612442017), (7, 1.5996447801589966), (8, 1.5878084897994995), (9, 1.595576524734497), (10, 1.5862730741500854)]
INFO flwr 2024-04-30 22:29:49,566 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.091), (1, 0.5844), (2, 0.7458), (3, 0.7485), (4, 0.7985), (5, 0.7809), (6, 0.8226), (7, 0.8635), (8, 0.8734), (9, 0.867), (10, 0.8757)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8757
wandb:     loss 1.58627
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_222749-ikl93ro3
wandb: Find logs at: ./wandb/offline-run-20240430_222749-ikl93ro3/logs
INFO flwr 2024-04-30 22:29:53,031 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:29:53,730 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2288873)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2288873)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:29:58,714	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:29:58,867	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:29:58,974	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:29:58,975	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:30:08,593 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 134609227572.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61975383244.0}
INFO flwr 2024-04-30 22:30:08,593 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:30:08,593 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:30:08,607 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:30:08,608 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:30:08,608 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:30:08,608 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:30:11,353 | server.py:94 | initial parameters (loss, other metrics): 2.304962635040283, {'accuracy': 0.1067, 'data_size': 10000}
INFO flwr 2024-04-30 22:30:11,353 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:30:11,354 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2314879)[0m 2024-04-30 22:30:14.020019: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2314879)[0m 2024-04-30 22:30:14.111144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2314879)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2314879)[0m 2024-04-30 22:30:16.131336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2314879)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2314879)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2314878)[0m 2024-04-30 22:30:14.199782: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2314878)[0m 2024-04-30 22:30:14.276481: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2314878)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2314878)[0m 2024-04-30 22:30:16.212882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:30:38,181 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:30:39,212 | server.py:125 | fit progress: (1, 1.9438515901565552, {'accuracy': 0.53, 'data_size': 10000}, 27.858794841973577)
INFO flwr 2024-04-30 22:30:39,213 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:30:39,213 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:30:47,142 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:30:48,392 | server.py:125 | fit progress: (2, 1.870993971824646, {'accuracy': 0.5935, 'data_size': 10000}, 37.038857996987645)
INFO flwr 2024-04-30 22:30:48,393 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:30:48,393 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:30:55,768 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:30:57,041 | server.py:125 | fit progress: (3, 1.785874366760254, {'accuracy': 0.6748, 'data_size': 10000}, 45.687760663975496)
INFO flwr 2024-04-30 22:30:57,041 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:30:57,042 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:31:04,092 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:31:05,345 | server.py:125 | fit progress: (4, 1.7790367603302002, {'accuracy': 0.6792, 'data_size': 10000}, 53.99147299397737)
INFO flwr 2024-04-30 22:31:05,345 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:31:05,345 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:31:12,805 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:31:14,065 | server.py:125 | fit progress: (5, 1.7284681797027588, {'accuracy': 0.7311, 'data_size': 10000}, 62.71193214895902)
INFO flwr 2024-04-30 22:31:14,066 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:31:14,066 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:31:21,004 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:31:22,022 | server.py:125 | fit progress: (6, 1.6999638080596924, {'accuracy': 0.7627, 'data_size': 10000}, 70.66867556597572)
INFO flwr 2024-04-30 22:31:22,022 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:31:22,023 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:31:29,925 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:31:30,950 | server.py:125 | fit progress: (7, 1.7050166130065918, {'accuracy': 0.7583, 'data_size': 10000}, 79.5969136619824)
INFO flwr 2024-04-30 22:31:30,951 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:31:30,951 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:31:38,477 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:31:39,506 | server.py:125 | fit progress: (8, 1.7027982473373413, {'accuracy': 0.7554, 'data_size': 10000}, 88.15284436597722)
INFO flwr 2024-04-30 22:31:39,507 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:31:39,507 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:31:46,498 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:31:47,749 | server.py:125 | fit progress: (9, 1.6751458644866943, {'accuracy': 0.785, 'data_size': 10000}, 96.3959088169504)
INFO flwr 2024-04-30 22:31:47,750 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:31:47,750 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:31:55,424 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:31:56,680 | server.py:125 | fit progress: (10, 1.6728490591049194, {'accuracy': 0.7865, 'data_size': 10000}, 105.32657717698021)
INFO flwr 2024-04-30 22:31:56,680 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:31:56,680 | server.py:153 | FL finished in 105.32693653699243
INFO flwr 2024-04-30 22:31:56,681 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:31:56,681 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:31:56,681 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:31:56,681 | app.py:229 | app_fit: losses_centralized [(0, 2.304962635040283), (1, 1.9438515901565552), (2, 1.870993971824646), (3, 1.785874366760254), (4, 1.7790367603302002), (5, 1.7284681797027588), (6, 1.6999638080596924), (7, 1.7050166130065918), (8, 1.7027982473373413), (9, 1.6751458644866943), (10, 1.6728490591049194)]
INFO flwr 2024-04-30 22:31:56,681 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1067), (1, 0.53), (2, 0.5935), (3, 0.6748), (4, 0.6792), (5, 0.7311), (6, 0.7627), (7, 0.7583), (8, 0.7554), (9, 0.785), (10, 0.7865)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7865
wandb:     loss 1.67285
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_222953-dh0tr8kb
wandb: Find logs at: ./wandb/offline-run-20240430_222953-dh0tr8kb/logs
INFO flwr 2024-04-30 22:32:00,279 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:32:01,010 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2314878)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2314878)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:32:05,793	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:32:05,884	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:32:05,964	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:32:05,965	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:32:15,621 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 136974816666.0, 'object_store_memory': 62989207142.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 22:32:15,621 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:32:15,621 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:32:15,634 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:32:15,635 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:32:15,635 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:32:15,635 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:32:18,134 | server.py:94 | initial parameters (loss, other metrics): 2.301093339920044, {'accuracy': 0.0993, 'data_size': 10000}
INFO flwr 2024-04-30 22:32:18,135 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:32:18,135 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2341006)[0m 2024-04-30 22:32:21.183723: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2341006)[0m 2024-04-30 22:32:21.278155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2341006)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2341006)[0m 2024-04-30 22:32:23.204801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2341013)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2341013)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2341009)[0m 2024-04-30 22:32:21.366449: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2341009)[0m 2024-04-30 22:32:21.454579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2341009)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2341009)[0m 2024-04-30 22:32:23.355841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:32:39,137 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:32:40,180 | server.py:125 | fit progress: (1, 1.8854597806930542, {'accuracy': 0.6117, 'data_size': 10000}, 22.044831626000814)
INFO flwr 2024-04-30 22:32:40,180 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:32:40,180 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:32:48,055 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:32:49,294 | server.py:125 | fit progress: (2, 1.745710015296936, {'accuracy': 0.7157, 'data_size': 10000}, 31.159029703994747)
INFO flwr 2024-04-30 22:32:49,294 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:32:49,294 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:32:56,458 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:32:57,710 | server.py:125 | fit progress: (3, 1.7512317895889282, {'accuracy': 0.7157, 'data_size': 10000}, 39.57470125897089)
INFO flwr 2024-04-30 22:32:57,710 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:32:57,710 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:33:05,168 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:33:06,438 | server.py:125 | fit progress: (4, 1.6578582525253296, {'accuracy': 0.8056, 'data_size': 10000}, 48.30310808296781)
INFO flwr 2024-04-30 22:33:06,438 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:33:06,438 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:33:13,913 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:33:15,160 | server.py:125 | fit progress: (5, 1.7104133367538452, {'accuracy': 0.7517, 'data_size': 10000}, 57.02527335198829)
INFO flwr 2024-04-30 22:33:15,160 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:33:15,161 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:33:22,677 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:33:23,727 | server.py:125 | fit progress: (6, 1.6897612810134888, {'accuracy': 0.7706, 'data_size': 10000}, 65.59178644698113)
INFO flwr 2024-04-30 22:33:23,727 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:33:23,727 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:33:30,965 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:33:31,990 | server.py:125 | fit progress: (7, 1.633802890777588, {'accuracy': 0.8285, 'data_size': 10000}, 73.85547530895565)
INFO flwr 2024-04-30 22:33:31,991 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:33:31,991 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:33:39,775 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:33:40,827 | server.py:125 | fit progress: (8, 1.6052241325378418, {'accuracy': 0.8562, 'data_size': 10000}, 82.69232235394884)
INFO flwr 2024-04-30 22:33:40,828 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:33:40,828 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:33:48,209 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:33:49,460 | server.py:125 | fit progress: (9, 1.6057685613632202, {'accuracy': 0.8558, 'data_size': 10000}, 91.32554974395316)
INFO flwr 2024-04-30 22:33:49,461 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:33:49,461 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:33:57,313 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:33:58,566 | server.py:125 | fit progress: (10, 1.6087853908538818, {'accuracy': 0.8519, 'data_size': 10000}, 100.43143549998058)
INFO flwr 2024-04-30 22:33:58,567 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:33:58,567 | server.py:153 | FL finished in 100.43179148394847
INFO flwr 2024-04-30 22:33:58,567 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:33:58,567 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:33:58,567 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:33:58,567 | app.py:229 | app_fit: losses_centralized [(0, 2.301093339920044), (1, 1.8854597806930542), (2, 1.745710015296936), (3, 1.7512317895889282), (4, 1.6578582525253296), (5, 1.7104133367538452), (6, 1.6897612810134888), (7, 1.633802890777588), (8, 1.6052241325378418), (9, 1.6057685613632202), (10, 1.6087853908538818)]
INFO flwr 2024-04-30 22:33:58,567 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0993), (1, 0.6117), (2, 0.7157), (3, 0.7157), (4, 0.8056), (5, 0.7517), (6, 0.7706), (7, 0.8285), (8, 0.8562), (9, 0.8558), (10, 0.8519)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8519
wandb:     loss 1.60879
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_223200-keviycws
wandb: Find logs at: ./wandb/offline-run-20240430_223200-keviycws/logs
INFO flwr 2024-04-30 22:34:02,091 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:34:02,845 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2341011)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2341011)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:34:07,575	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:34:07,681	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:34:07,794	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:34:07,796	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:34:17,454 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 61547789107.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'memory': 133611507917.0}
INFO flwr 2024-04-30 22:34:17,454 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:34:17,454 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:34:17,468 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:34:17,469 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:34:17,469 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:34:17,469 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:34:20,255 | server.py:94 | initial parameters (loss, other metrics): 2.2989494800567627, {'accuracy': 0.1601, 'data_size': 10000}
INFO flwr 2024-04-30 22:34:20,256 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:34:20,256 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2366655)[0m 2024-04-30 22:34:22.919467: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2366655)[0m 2024-04-30 22:34:23.002999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2366655)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2366655)[0m 2024-04-30 22:34:24.992586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2366655)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2366655)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2366654)[0m 2024-04-30 22:34:23.073560: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2366654)[0m 2024-04-30 22:34:23.159093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2366654)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2366654)[0m 2024-04-30 22:34:25.003204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:34:39,920 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:34:40,940 | server.py:125 | fit progress: (1, 1.8467103242874146, {'accuracy': 0.6792, 'data_size': 10000}, 20.683735019003507)
INFO flwr 2024-04-30 22:34:40,940 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:34:40,940 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:34:48,854 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:34:50,100 | server.py:125 | fit progress: (2, 1.7629382610321045, {'accuracy': 0.7045, 'data_size': 10000}, 29.844468166993465)
INFO flwr 2024-04-30 22:34:50,101 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:34:50,101 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:34:57,393 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:34:58,640 | server.py:125 | fit progress: (3, 1.679145336151123, {'accuracy': 0.7903, 'data_size': 10000}, 38.38445630000206)
INFO flwr 2024-04-30 22:34:58,641 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:34:58,641 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:35:06,199 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:35:07,465 | server.py:125 | fit progress: (4, 1.6862670183181763, {'accuracy': 0.7797, 'data_size': 10000}, 47.20926813397091)
INFO flwr 2024-04-30 22:35:07,465 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:35:07,465 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:35:14,954 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:35:16,202 | server.py:125 | fit progress: (5, 1.6456598043441772, {'accuracy': 0.8176, 'data_size': 10000}, 55.94576487096492)
INFO flwr 2024-04-30 22:35:16,202 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:35:16,202 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:35:23,556 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:35:24,586 | server.py:125 | fit progress: (6, 1.6403523683547974, {'accuracy': 0.8208, 'data_size': 10000}, 64.3304886429687)
INFO flwr 2024-04-30 22:35:24,587 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:35:24,587 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:35:32,052 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:35:33,091 | server.py:125 | fit progress: (7, 1.6456923484802246, {'accuracy': 0.815, 'data_size': 10000}, 72.83534170198254)
INFO flwr 2024-04-30 22:35:33,091 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:35:33,092 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:35:40,579 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:35:41,608 | server.py:125 | fit progress: (8, 1.6365848779678345, {'accuracy': 0.8247, 'data_size': 10000}, 81.35221034596907)
INFO flwr 2024-04-30 22:35:41,608 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:35:41,608 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:35:48,930 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:35:50,169 | server.py:125 | fit progress: (9, 1.6277061700820923, {'accuracy': 0.8335, 'data_size': 10000}, 89.91312110400759)
INFO flwr 2024-04-30 22:35:50,169 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:35:50,169 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:35:57,525 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:35:58,761 | server.py:125 | fit progress: (10, 1.6314243078231812, {'accuracy': 0.8296, 'data_size': 10000}, 98.50485027598916)
INFO flwr 2024-04-30 22:35:58,761 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:35:58,761 | server.py:153 | FL finished in 98.50526067096507
INFO flwr 2024-04-30 22:35:58,761 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:35:58,761 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:35:58,761 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:35:58,761 | app.py:229 | app_fit: losses_centralized [(0, 2.2989494800567627), (1, 1.8467103242874146), (2, 1.7629382610321045), (3, 1.679145336151123), (4, 1.6862670183181763), (5, 1.6456598043441772), (6, 1.6403523683547974), (7, 1.6456923484802246), (8, 1.6365848779678345), (9, 1.6277061700820923), (10, 1.6314243078231812)]
INFO flwr 2024-04-30 22:35:58,762 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1601), (1, 0.6792), (2, 0.7045), (3, 0.7903), (4, 0.7797), (5, 0.8176), (6, 0.8208), (7, 0.815), (8, 0.8247), (9, 0.8335), (10, 0.8296)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8296
wandb:     loss 1.63142
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_223402-9vlcfnxk
wandb: Find logs at: ./wandb/offline-run-20240430_223402-9vlcfnxk/logs
INFO flwr 2024-04-30 22:36:02,255 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:36:02,939 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2366638)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2366638)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:36:07,653	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:36:07,762	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:36:07,846	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:36:07,848	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:36:17,513 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 121310445773.0, 'CPU': 64.0, 'object_store_memory': 56275905331.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 22:36:17,513 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:36:17,514 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:36:17,528 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:36:17,529 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:36:17,529 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:36:17,529 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:36:20,078 | server.py:94 | initial parameters (loss, other metrics): 2.3026866912841797, {'accuracy': 0.1045, 'data_size': 10000}
INFO flwr 2024-04-30 22:36:20,078 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:36:20,079 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2391551)[0m 2024-04-30 22:36:23.015581: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2391462)[0m 2024-04-30 22:36:23.085249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2391462)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2391551)[0m 2024-04-30 22:36:25.057100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2391551)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2391551)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2391461)[0m 2024-04-30 22:36:23.120595: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2391459)[0m 2024-04-30 22:36:23.242639: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2391459)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2391459)[0m 2024-04-30 22:36:25.226192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:36:38,885 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:36:39,898 | server.py:125 | fit progress: (1, 1.9053541421890259, {'accuracy': 0.5929, 'data_size': 10000}, 19.819023171032313)
INFO flwr 2024-04-30 22:36:39,898 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:36:39,898 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:36:48,558 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:36:49,830 | server.py:125 | fit progress: (2, 1.8942582607269287, {'accuracy': 0.5671, 'data_size': 10000}, 29.75135991204297)
INFO flwr 2024-04-30 22:36:49,830 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:36:49,830 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:36:58,260 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:36:59,508 | server.py:125 | fit progress: (3, 1.764198660850525, {'accuracy': 0.6968, 'data_size': 10000}, 39.42895314202178)
INFO flwr 2024-04-30 22:36:59,508 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:36:59,508 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:37:07,285 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:37:08,591 | server.py:125 | fit progress: (4, 1.6433255672454834, {'accuracy': 0.8228, 'data_size': 10000}, 48.512820541043766)
INFO flwr 2024-04-30 22:37:08,592 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:37:08,592 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:37:16,567 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:37:17,831 | server.py:125 | fit progress: (5, 1.6758360862731934, {'accuracy': 0.787, 'data_size': 10000}, 57.75225651601795)
INFO flwr 2024-04-30 22:37:17,831 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:37:17,831 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:37:25,767 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:37:26,828 | server.py:125 | fit progress: (6, 1.6589618921279907, {'accuracy': 0.8028, 'data_size': 10000}, 66.74964663101127)
INFO flwr 2024-04-30 22:37:26,828 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:37:26,829 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:37:34,984 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:37:36,011 | server.py:125 | fit progress: (7, 1.6241375207901, {'accuracy': 0.8389, 'data_size': 10000}, 75.93230006599333)
INFO flwr 2024-04-30 22:37:36,011 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:37:36,011 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:37:44,075 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:37:45,096 | server.py:125 | fit progress: (8, 1.6196051836013794, {'accuracy': 0.8424, 'data_size': 10000}, 85.01782440202078)
INFO flwr 2024-04-30 22:37:45,097 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:37:45,097 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:37:52,950 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:37:54,201 | server.py:125 | fit progress: (9, 1.6039706468582153, {'accuracy': 0.8578, 'data_size': 10000}, 94.12225297698751)
INFO flwr 2024-04-30 22:37:54,201 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:37:54,201 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:38:02,011 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:38:03,281 | server.py:125 | fit progress: (10, 1.5907330513000488, {'accuracy': 0.8708, 'data_size': 10000}, 103.20215868804371)
INFO flwr 2024-04-30 22:38:03,281 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:38:03,281 | server.py:153 | FL finished in 103.20253767003305
INFO flwr 2024-04-30 22:38:03,281 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:38:03,281 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:38:03,281 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:38:03,282 | app.py:229 | app_fit: losses_centralized [(0, 2.3026866912841797), (1, 1.9053541421890259), (2, 1.8942582607269287), (3, 1.764198660850525), (4, 1.6433255672454834), (5, 1.6758360862731934), (6, 1.6589618921279907), (7, 1.6241375207901), (8, 1.6196051836013794), (9, 1.6039706468582153), (10, 1.5907330513000488)]
INFO flwr 2024-04-30 22:38:03,282 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1045), (1, 0.5929), (2, 0.5671), (3, 0.6968), (4, 0.8228), (5, 0.787), (6, 0.8028), (7, 0.8389), (8, 0.8424), (9, 0.8578), (10, 0.8708)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8708
wandb:     loss 1.59073
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_223602-wvo5k80u
wandb: Find logs at: ./wandb/offline-run-20240430_223602-wvo5k80u/logs
INFO flwr 2024-04-30 22:38:06,706 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:38:07,524 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2391459)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2391459)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:38:12,610	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:38:12,735	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:38:12,832	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:38:12,833	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:38:22,882 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 53140869120.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 26570434560.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 22:38:22,883 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:38:22,883 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:38:22,900 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:38:22,901 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:38:22,901 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:38:22,901 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:38:25,339 | server.py:94 | initial parameters (loss, other metrics): 2.303633689880371, {'accuracy': 0.0891, 'data_size': 10000}
INFO flwr 2024-04-30 22:38:25,339 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:38:25,339 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2416793)[0m 2024-04-30 22:38:28.456278: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2416793)[0m 2024-04-30 22:38:28.549294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2416793)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2416793)[0m 2024-04-30 22:38:30.570945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2416793)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2416793)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2416792)[0m 2024-04-30 22:38:28.741252: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2416792)[0m 2024-04-30 22:38:28.835323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2416792)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2416792)[0m 2024-04-30 22:38:30.688363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:38:44,393 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:38:45,400 | server.py:125 | fit progress: (1, 1.8599271774291992, {'accuracy': 0.6475, 'data_size': 10000}, 20.060397489054594)
INFO flwr 2024-04-30 22:38:45,400 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:38:45,400 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:38:53,774 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:38:55,044 | server.py:125 | fit progress: (2, 1.6764060258865356, {'accuracy': 0.8033, 'data_size': 10000}, 29.70446095702937)
INFO flwr 2024-04-30 22:38:55,044 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:38:55,044 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:39:02,677 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:39:03,962 | server.py:125 | fit progress: (3, 1.6291640996932983, {'accuracy': 0.8411, 'data_size': 10000}, 38.623073353024665)
INFO flwr 2024-04-30 22:39:03,963 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:39:03,963 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:39:11,620 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:39:12,874 | server.py:125 | fit progress: (4, 1.6103171110153198, {'accuracy': 0.8554, 'data_size': 10000}, 47.534801956033334)
INFO flwr 2024-04-30 22:39:12,874 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:39:12,874 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:39:20,921 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:39:22,182 | server.py:125 | fit progress: (5, 1.5867805480957031, {'accuracy': 0.8789, 'data_size': 10000}, 56.8426061940263)
INFO flwr 2024-04-30 22:39:22,182 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:39:22,182 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:39:30,152 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:39:31,176 | server.py:125 | fit progress: (6, 1.58633553981781, {'accuracy': 0.8787, 'data_size': 10000}, 65.83657368301647)
INFO flwr 2024-04-30 22:39:31,176 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:39:31,176 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:39:39,108 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:39:40,153 | server.py:125 | fit progress: (7, 1.5831186771392822, {'accuracy': 0.8808, 'data_size': 10000}, 74.81414035102352)
INFO flwr 2024-04-30 22:39:40,154 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:39:40,154 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:39:48,140 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:39:49,189 | server.py:125 | fit progress: (8, 1.5628082752227783, {'accuracy': 0.8994, 'data_size': 10000}, 83.85025407804642)
INFO flwr 2024-04-30 22:39:49,190 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:39:49,190 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:39:56,946 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:39:58,207 | server.py:125 | fit progress: (9, 1.5624332427978516, {'accuracy': 0.9005, 'data_size': 10000}, 92.86777568602702)
INFO flwr 2024-04-30 22:39:58,207 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:39:58,207 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:40:06,322 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:40:07,610 | server.py:125 | fit progress: (10, 1.568314790725708, {'accuracy': 0.8938, 'data_size': 10000}, 102.27093563601375)
INFO flwr 2024-04-30 22:40:07,610 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:40:07,611 | server.py:153 | FL finished in 102.27130583301187
INFO flwr 2024-04-30 22:40:07,633 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:40:07,633 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:40:07,633 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:40:07,633 | app.py:229 | app_fit: losses_centralized [(0, 2.303633689880371), (1, 1.8599271774291992), (2, 1.6764060258865356), (3, 1.6291640996932983), (4, 1.6103171110153198), (5, 1.5867805480957031), (6, 1.58633553981781), (7, 1.5831186771392822), (8, 1.5628082752227783), (9, 1.5624332427978516), (10, 1.568314790725708)]
INFO flwr 2024-04-30 22:40:07,634 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0891), (1, 0.6475), (2, 0.8033), (3, 0.8411), (4, 0.8554), (5, 0.8789), (6, 0.8787), (7, 0.8808), (8, 0.8994), (9, 0.9005), (10, 0.8938)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8938
wandb:     loss 1.56831
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_223806-8tjii8a1
wandb: Find logs at: ./wandb/offline-run-20240430_223806-8tjii8a1/logs
INFO flwr 2024-04-30 22:40:11,135 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:40:11,825 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2416773)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2416773)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:40:16,828	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:40:16,954	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:40:17,080	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:40:17,081	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:40:26,692 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 26569909862.0, 'memory': 53139819726.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 22:40:26,692 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:40:26,692 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:40:26,706 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:40:26,707 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:40:26,708 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:40:26,708 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:40:29,307 | server.py:94 | initial parameters (loss, other metrics): 2.3023431301116943, {'accuracy': 0.0648, 'data_size': 10000}
INFO flwr 2024-04-30 22:40:29,307 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:40:29,307 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2442807)[0m 2024-04-30 22:40:32.189583: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2442807)[0m 2024-04-30 22:40:32.316312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2442807)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2442807)[0m 2024-04-30 22:40:34.266325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2442795)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2442795)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2442791)[0m 2024-04-30 22:40:32.257525: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2442791)[0m 2024-04-30 22:40:32.346705: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2442791)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2442800)[0m 2024-04-30 22:40:34.271260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:40:48,299 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:40:49,331 | server.py:125 | fit progress: (1, 1.9290099143981934, {'accuracy': 0.5825, 'data_size': 10000}, 20.024023152014706)
INFO flwr 2024-04-30 22:40:49,332 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:40:49,332 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:40:57,806 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:40:59,044 | server.py:125 | fit progress: (2, 1.6383379697799683, {'accuracy': 0.8444, 'data_size': 10000}, 29.73661304701818)
INFO flwr 2024-04-30 22:40:59,044 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:40:59,044 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:41:06,814 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:41:08,048 | server.py:125 | fit progress: (3, 1.6234701871871948, {'accuracy': 0.8484, 'data_size': 10000}, 38.740492997982074)
INFO flwr 2024-04-30 22:41:08,048 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:41:08,048 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:41:15,925 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:41:17,215 | server.py:125 | fit progress: (4, 1.5869947671890259, {'accuracy': 0.8823, 'data_size': 10000}, 47.907244679983705)
INFO flwr 2024-04-30 22:41:17,215 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:41:17,215 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:41:24,472 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:41:25,735 | server.py:125 | fit progress: (5, 1.5889641046524048, {'accuracy': 0.8762, 'data_size': 10000}, 56.4274768520263)
INFO flwr 2024-04-30 22:41:25,735 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:41:25,735 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:41:33,495 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:41:34,555 | server.py:125 | fit progress: (6, 1.5844109058380127, {'accuracy': 0.8789, 'data_size': 10000}, 65.24798896699212)
INFO flwr 2024-04-30 22:41:34,556 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:41:34,556 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:41:43,240 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:41:44,278 | server.py:125 | fit progress: (7, 1.588395357131958, {'accuracy': 0.8748, 'data_size': 10000}, 74.97054103401024)
INFO flwr 2024-04-30 22:41:44,278 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:41:44,278 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:41:52,094 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:41:53,116 | server.py:125 | fit progress: (8, 1.5625029802322388, {'accuracy': 0.9004, 'data_size': 10000}, 83.80920948903076)
INFO flwr 2024-04-30 22:41:53,117 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:41:53,117 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:42:01,008 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:42:02,262 | server.py:125 | fit progress: (9, 1.5605806112289429, {'accuracy': 0.9013, 'data_size': 10000}, 92.9544980690116)
INFO flwr 2024-04-30 22:42:02,262 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:42:02,262 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:42:10,267 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:42:11,537 | server.py:125 | fit progress: (10, 1.5697218179702759, {'accuracy': 0.8911, 'data_size': 10000}, 102.22964825801319)
INFO flwr 2024-04-30 22:42:11,537 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:42:11,537 | server.py:153 | FL finished in 102.23001716000726
INFO flwr 2024-04-30 22:42:11,537 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:42:11,538 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:42:11,538 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:42:11,538 | app.py:229 | app_fit: losses_centralized [(0, 2.3023431301116943), (1, 1.9290099143981934), (2, 1.6383379697799683), (3, 1.6234701871871948), (4, 1.5869947671890259), (5, 1.5889641046524048), (6, 1.5844109058380127), (7, 1.588395357131958), (8, 1.5625029802322388), (9, 1.5605806112289429), (10, 1.5697218179702759)]
INFO flwr 2024-04-30 22:42:11,538 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0648), (1, 0.5825), (2, 0.8444), (3, 0.8484), (4, 0.8823), (5, 0.8762), (6, 0.8789), (7, 0.8748), (8, 0.9004), (9, 0.9013), (10, 0.8911)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8911
wandb:     loss 1.56972
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_224011-23uqaiid
wandb: Find logs at: ./wandb/offline-run-20240430_224011-23uqaiid/logs
INFO flwr 2024-04-30 22:42:15,011 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:42:15,694 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2442800)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2442800)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:42:20,305	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:42:20,410	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:42:20,496	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:42:20,497	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:42:30,128 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 50209856718.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 25104928358.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 22:42:30,128 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:42:30,128 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:42:30,145 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:42:30,146 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:42:30,146 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:42:30,146 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:42:32,555 | server.py:94 | initial parameters (loss, other metrics): 2.303743839263916, {'accuracy': 0.0817, 'data_size': 10000}
INFO flwr 2024-04-30 22:42:32,555 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:42:32,556 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2468501)[0m 2024-04-30 22:42:35.706199: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2468501)[0m 2024-04-30 22:42:35.813117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2468501)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2468499)[0m 2024-04-30 22:42:38.599419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2468499)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2468499)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2468504)[0m 2024-04-30 22:42:35.923941: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2468504)[0m 2024-04-30 22:42:36.015738: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2468504)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2468504)[0m 2024-04-30 22:42:38.762967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:42:54,473 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:42:55,520 | server.py:125 | fit progress: (1, 1.9913219213485718, {'accuracy': 0.4765, 'data_size': 10000}, 22.96443252102472)
INFO flwr 2024-04-30 22:42:55,520 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:42:55,520 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:43:05,178 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:43:06,410 | server.py:125 | fit progress: (2, 1.8689184188842773, {'accuracy': 0.5937, 'data_size': 10000}, 33.854803171998356)
INFO flwr 2024-04-30 22:43:06,411 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:43:06,411 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:43:14,871 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:43:16,113 | server.py:125 | fit progress: (3, 1.7604405879974365, {'accuracy': 0.7061, 'data_size': 10000}, 43.557238274021074)
INFO flwr 2024-04-30 22:43:16,113 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:43:16,113 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:43:25,231 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:43:26,509 | server.py:125 | fit progress: (4, 1.7930045127868652, {'accuracy': 0.6688, 'data_size': 10000}, 53.9533149980125)
INFO flwr 2024-04-30 22:43:26,509 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:43:26,509 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:43:35,485 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:43:36,760 | server.py:125 | fit progress: (5, 1.7572073936462402, {'accuracy': 0.7041, 'data_size': 10000}, 64.2049604980275)
INFO flwr 2024-04-30 22:43:36,761 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:43:36,761 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:43:45,731 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:43:46,770 | server.py:125 | fit progress: (6, 1.7565191984176636, {'accuracy': 0.7029, 'data_size': 10000}, 74.21422037098091)
INFO flwr 2024-04-30 22:43:46,770 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:43:46,770 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:43:56,170 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:43:57,209 | server.py:125 | fit progress: (7, 1.734625220298767, {'accuracy': 0.7269, 'data_size': 10000}, 84.6531457240344)
INFO flwr 2024-04-30 22:43:57,209 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:43:57,209 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:44:06,732 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:44:07,769 | server.py:125 | fit progress: (8, 1.7308146953582764, {'accuracy': 0.7311, 'data_size': 10000}, 95.21395470103016)
INFO flwr 2024-04-30 22:44:07,770 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:44:07,770 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:44:16,578 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:44:17,847 | server.py:125 | fit progress: (9, 1.7274882793426514, {'accuracy': 0.7342, 'data_size': 10000}, 105.29185226903064)
INFO flwr 2024-04-30 22:44:17,848 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:44:17,848 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:44:26,874 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:44:28,141 | server.py:125 | fit progress: (10, 1.7229552268981934, {'accuracy': 0.7378, 'data_size': 10000}, 115.58530509698903)
INFO flwr 2024-04-30 22:44:28,141 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:44:28,141 | server.py:153 | FL finished in 115.58568027400179
INFO flwr 2024-04-30 22:44:28,141 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:44:28,141 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:44:28,141 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:44:28,142 | app.py:229 | app_fit: losses_centralized [(0, 2.303743839263916), (1, 1.9913219213485718), (2, 1.8689184188842773), (3, 1.7604405879974365), (4, 1.7930045127868652), (5, 1.7572073936462402), (6, 1.7565191984176636), (7, 1.734625220298767), (8, 1.7308146953582764), (9, 1.7274882793426514), (10, 1.7229552268981934)]
INFO flwr 2024-04-30 22:44:28,142 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0817), (1, 0.4765), (2, 0.5937), (3, 0.7061), (4, 0.6688), (5, 0.7041), (6, 0.7029), (7, 0.7269), (8, 0.7311), (9, 0.7342), (10, 0.7378)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7378
wandb:     loss 1.72296
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_224215-bxdj70pf
wandb: Find logs at: ./wandb/offline-run-20240430_224215-bxdj70pf/logs
INFO flwr 2024-04-30 22:44:31,619 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:44:32,502 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2468504)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2468504)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:44:37,230	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:44:37,332	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:44:37,456	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:44:37,458	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:44:47,286 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 53329553819.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 26664776908.0, 'CPU': 64.0}
INFO flwr 2024-04-30 22:44:47,286 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:44:47,287 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:44:47,305 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:44:47,306 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:44:47,306 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:44:47,307 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:44:50,584 | server.py:94 | initial parameters (loss, other metrics): 2.302584409713745, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-30 22:44:50,585 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:44:50,587 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2497170)[0m 2024-04-30 22:44:52.811539: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2497170)[0m 2024-04-30 22:44:52.907123: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2497170)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2497170)[0m 2024-04-30 22:44:54.890274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2497170)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2497170)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2497168)[0m 2024-04-30 22:44:52.976559: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2497168)[0m 2024-04-30 22:44:53.061055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2497168)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2497168)[0m 2024-04-30 22:44:54.983502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:45:12,201 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:45:13,242 | server.py:125 | fit progress: (1, 1.890054702758789, {'accuracy': 0.6118, 'data_size': 10000}, 22.656028338999022)
INFO flwr 2024-04-30 22:45:13,242 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:45:13,242 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:45:23,259 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:45:24,547 | server.py:125 | fit progress: (2, 1.719853162765503, {'accuracy': 0.7504, 'data_size': 10000}, 33.96081194002181)
INFO flwr 2024-04-30 22:45:24,547 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:45:24,547 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:45:33,494 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:45:34,737 | server.py:125 | fit progress: (3, 1.6063470840454102, {'accuracy': 0.8662, 'data_size': 10000}, 44.15148056100588)
INFO flwr 2024-04-30 22:45:34,738 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:45:34,738 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:45:43,601 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:45:44,844 | server.py:125 | fit progress: (4, 1.6001853942871094, {'accuracy': 0.8665, 'data_size': 10000}, 54.25762940500863)
INFO flwr 2024-04-30 22:45:44,844 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:45:44,844 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:45:54,077 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:45:55,342 | server.py:125 | fit progress: (5, 1.5698304176330566, {'accuracy': 0.8958, 'data_size': 10000}, 64.75627281400375)
INFO flwr 2024-04-30 22:45:55,342 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:45:55,343 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:46:05,065 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:46:06,104 | server.py:125 | fit progress: (6, 1.5837346315383911, {'accuracy': 0.8792, 'data_size': 10000}, 75.51775116700446)
INFO flwr 2024-04-30 22:46:06,104 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:46:06,104 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:46:15,351 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:46:16,406 | server.py:125 | fit progress: (7, 1.5680559873580933, {'accuracy': 0.8952, 'data_size': 10000}, 85.81961306603625)
INFO flwr 2024-04-30 22:46:16,406 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:46:16,406 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:46:25,616 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:46:26,653 | server.py:125 | fit progress: (8, 1.5576584339141846, {'accuracy': 0.9057, 'data_size': 10000}, 96.06714672100497)
INFO flwr 2024-04-30 22:46:26,653 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:46:26,653 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:46:35,356 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:46:36,617 | server.py:125 | fit progress: (9, 1.56198251247406, {'accuracy': 0.9007, 'data_size': 10000}, 106.03114617499523)
INFO flwr 2024-04-30 22:46:36,617 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:46:36,617 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:46:45,448 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:46:46,705 | server.py:125 | fit progress: (10, 1.5567829608917236, {'accuracy': 0.9059, 'data_size': 10000}, 116.11911446903832)
INFO flwr 2024-04-30 22:46:46,705 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:46:46,705 | server.py:153 | FL finished in 116.11947020300431
INFO flwr 2024-04-30 22:46:46,705 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:46:46,706 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:46:46,706 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:46:46,706 | app.py:229 | app_fit: losses_centralized [(0, 2.302584409713745), (1, 1.890054702758789), (2, 1.719853162765503), (3, 1.6063470840454102), (4, 1.6001853942871094), (5, 1.5698304176330566), (6, 1.5837346315383911), (7, 1.5680559873580933), (8, 1.5576584339141846), (9, 1.56198251247406), (10, 1.5567829608917236)]
INFO flwr 2024-04-30 22:46:46,706 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.6118), (2, 0.7504), (3, 0.8662), (4, 0.8665), (5, 0.8958), (6, 0.8792), (7, 0.8952), (8, 0.9057), (9, 0.9007), (10, 0.9059)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9059
wandb:     loss 1.55678
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_224431-36f3lq2j
wandb: Find logs at: ./wandb/offline-run-20240430_224431-36f3lq2j/logs
INFO flwr 2024-04-30 22:46:50,208 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:46:51,046 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2497171)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2497171)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:46:55,919	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:46:56,022	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:46:56,123	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:46:56,125	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:47:06,028 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 25341607526.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 50683215054.0}
INFO flwr 2024-04-30 22:47:06,028 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:47:06,029 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:47:06,044 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:47:06,045 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:47:06,045 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:47:06,046 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:47:09,017 | server.py:94 | initial parameters (loss, other metrics): 2.3018460273742676, {'accuracy': 0.0899, 'data_size': 10000}
INFO flwr 2024-04-30 22:47:09,018 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:47:09,018 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2525458)[0m 2024-04-30 22:47:11.543406: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2525458)[0m 2024-04-30 22:47:11.623243: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2525458)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2525460)[0m 2024-04-30 22:47:13.701129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2525460)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2525460)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2525462)[0m 2024-04-30 22:47:11.706698: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2525462)[0m 2024-04-30 22:47:11.792419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2525462)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2525462)[0m 2024-04-30 22:47:13.627962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:47:29,262 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:47:30,319 | server.py:125 | fit progress: (1, 1.8443924188613892, {'accuracy': 0.6886, 'data_size': 10000}, 21.301230644981842)
INFO flwr 2024-04-30 22:47:30,319 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:47:30,319 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:47:39,894 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:47:41,162 | server.py:125 | fit progress: (2, 1.690279245376587, {'accuracy': 0.7857, 'data_size': 10000}, 32.14441305398941)
INFO flwr 2024-04-30 22:47:41,162 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:47:41,163 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:47:50,716 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:47:51,997 | server.py:125 | fit progress: (3, 1.6058093309402466, {'accuracy': 0.8699, 'data_size': 10000}, 42.97878716798732)
INFO flwr 2024-04-30 22:47:51,997 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:47:51,997 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:48:01,448 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:48:02,701 | server.py:125 | fit progress: (4, 1.5781939029693604, {'accuracy': 0.8886, 'data_size': 10000}, 53.68288155499613)
INFO flwr 2024-04-30 22:48:02,701 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:48:02,701 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:48:11,373 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:48:12,652 | server.py:125 | fit progress: (5, 1.5640804767608643, {'accuracy': 0.9037, 'data_size': 10000}, 63.63393732102122)
INFO flwr 2024-04-30 22:48:12,652 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:48:12,652 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:48:22,004 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:48:23,036 | server.py:125 | fit progress: (6, 1.5663505792617798, {'accuracy': 0.8983, 'data_size': 10000}, 74.01784670399502)
INFO flwr 2024-04-30 22:48:23,036 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:48:23,036 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:48:32,244 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:48:33,278 | server.py:125 | fit progress: (7, 1.5610395669937134, {'accuracy': 0.9019, 'data_size': 10000}, 84.26026410597842)
INFO flwr 2024-04-30 22:48:33,278 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:48:33,278 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:48:42,866 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:48:43,904 | server.py:125 | fit progress: (8, 1.558925986289978, {'accuracy': 0.9038, 'data_size': 10000}, 94.88598642800935)
INFO flwr 2024-04-30 22:48:43,904 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:48:43,904 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:48:53,097 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:48:54,351 | server.py:125 | fit progress: (9, 1.5540876388549805, {'accuracy': 0.9075, 'data_size': 10000}, 105.33350351301488)
INFO flwr 2024-04-30 22:48:54,352 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:48:54,352 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:49:03,473 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:49:04,744 | server.py:125 | fit progress: (10, 1.5515198707580566, {'accuracy': 0.9117, 'data_size': 10000}, 115.72620958701009)
INFO flwr 2024-04-30 22:49:04,744 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:49:04,744 | server.py:153 | FL finished in 115.72664729401004
INFO flwr 2024-04-30 22:49:04,745 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:49:04,745 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:49:04,745 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:49:04,745 | app.py:229 | app_fit: losses_centralized [(0, 2.3018460273742676), (1, 1.8443924188613892), (2, 1.690279245376587), (3, 1.6058093309402466), (4, 1.5781939029693604), (5, 1.5640804767608643), (6, 1.5663505792617798), (7, 1.5610395669937134), (8, 1.558925986289978), (9, 1.5540876388549805), (10, 1.5515198707580566)]
INFO flwr 2024-04-30 22:49:04,745 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0899), (1, 0.6886), (2, 0.7857), (3, 0.8699), (4, 0.8886), (5, 0.9037), (6, 0.8983), (7, 0.9019), (8, 0.9038), (9, 0.9075), (10, 0.9117)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9117
wandb:     loss 1.55152
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_224650-sa9ptpmu
wandb: Find logs at: ./wandb/offline-run-20240430_224650-sa9ptpmu/logs
INFO flwr 2024-04-30 22:49:08,267 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:49:09,058 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2525449)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2525449)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:49:13,825	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:49:13,975	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:49:14,071	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:49:14,073	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:49:23,791 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 23393696563.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 46787393127.0, 'CPU': 64.0}
INFO flwr 2024-04-30 22:49:23,791 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:49:23,792 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:49:23,805 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:49:23,806 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:49:23,806 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:49:23,807 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:49:26,227 | server.py:94 | initial parameters (loss, other metrics): 2.3045008182525635, {'accuracy': 0.0786, 'data_size': 10000}
INFO flwr 2024-04-30 22:49:26,228 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:49:26,228 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2554153)[0m 2024-04-30 22:49:29.304562: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2554150)[0m 2024-04-30 22:49:29.397914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2554150)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2554153)[0m 2024-04-30 22:49:31.378841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2554153)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2554153)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2554155)[0m 2024-04-30 22:49:29.504446: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2554155)[0m 2024-04-30 22:49:29.598920: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2554155)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2554159)[0m 2024-04-30 22:49:31.556872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:49:51,115 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:49:52,169 | server.py:125 | fit progress: (1, 1.8295539617538452, {'accuracy': 0.712, 'data_size': 10000}, 25.94090204301756)
INFO flwr 2024-04-30 22:49:52,169 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:49:52,169 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:50:04,488 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:50:05,772 | server.py:125 | fit progress: (2, 1.6668795347213745, {'accuracy': 0.8109, 'data_size': 10000}, 39.54390595800942)
INFO flwr 2024-04-30 22:50:05,772 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:50:05,772 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:50:16,746 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:50:17,985 | server.py:125 | fit progress: (3, 1.6264485120773315, {'accuracy': 0.8446, 'data_size': 10000}, 51.75760142802028)
INFO flwr 2024-04-30 22:50:17,986 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:50:17,986 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:50:30,206 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:50:31,469 | server.py:125 | fit progress: (4, 1.5948810577392578, {'accuracy': 0.8724, 'data_size': 10000}, 65.24092727300012)
INFO flwr 2024-04-30 22:50:31,469 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:50:31,469 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:50:43,731 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:50:44,989 | server.py:125 | fit progress: (5, 1.6070754528045654, {'accuracy': 0.8579, 'data_size': 10000}, 78.76072753802873)
INFO flwr 2024-04-30 22:50:44,989 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:50:44,989 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:50:57,101 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:50:58,156 | server.py:125 | fit progress: (6, 1.5917953252792358, {'accuracy': 0.8719, 'data_size': 10000}, 91.928389978013)
INFO flwr 2024-04-30 22:50:58,156 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:50:58,157 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:51:09,966 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:51:11,037 | server.py:125 | fit progress: (7, 1.5634691715240479, {'accuracy': 0.9, 'data_size': 10000}, 104.80963443504879)
INFO flwr 2024-04-30 22:51:11,038 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:51:11,038 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:51:22,711 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:51:23,759 | server.py:125 | fit progress: (8, 1.5677212476730347, {'accuracy': 0.8947, 'data_size': 10000}, 117.53151076304493)
INFO flwr 2024-04-30 22:51:23,760 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:51:23,760 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:51:34,873 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:51:36,148 | server.py:125 | fit progress: (9, 1.5657471418380737, {'accuracy': 0.8961, 'data_size': 10000}, 129.92001822101884)
INFO flwr 2024-04-30 22:51:36,148 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:51:36,148 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:51:47,960 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:51:49,214 | server.py:125 | fit progress: (10, 1.5644800662994385, {'accuracy': 0.8974, 'data_size': 10000}, 142.98650576901855)
INFO flwr 2024-04-30 22:51:49,215 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:51:49,215 | server.py:153 | FL finished in 142.98687950399471
INFO flwr 2024-04-30 22:51:49,215 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:51:49,215 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:51:49,215 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:51:49,215 | app.py:229 | app_fit: losses_centralized [(0, 2.3045008182525635), (1, 1.8295539617538452), (2, 1.6668795347213745), (3, 1.6264485120773315), (4, 1.5948810577392578), (5, 1.6070754528045654), (6, 1.5917953252792358), (7, 1.5634691715240479), (8, 1.5677212476730347), (9, 1.5657471418380737), (10, 1.5644800662994385)]
INFO flwr 2024-04-30 22:51:49,215 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0786), (1, 0.712), (2, 0.8109), (3, 0.8446), (4, 0.8724), (5, 0.8579), (6, 0.8719), (7, 0.9), (8, 0.8947), (9, 0.8961), (10, 0.8974)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8974
wandb:     loss 1.56448
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_224908-exojml18
wandb: Find logs at: ./wandb/offline-run-20240430_224908-exojml18/logs
INFO flwr 2024-04-30 22:51:52,723 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:51:53,376 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2554145)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2554145)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:51:58,088	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:51:58,188	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:51:58,285	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:51:58,286	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:52:07,920 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 52451185460.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 26225592729.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 22:52:07,921 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:52:07,921 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:52:07,943 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:52:07,945 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:52:07,946 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:52:07,946 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:52:10,841 | server.py:94 | initial parameters (loss, other metrics): 2.3000147342681885, {'accuracy': 0.1103, 'data_size': 10000}
INFO flwr 2024-04-30 22:52:10,842 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:52:10,842 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2587091)[0m 2024-04-30 22:52:13.411646: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2587091)[0m 2024-04-30 22:52:13.508077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2587091)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2587091)[0m 2024-04-30 22:52:15.516970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2587118)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2587118)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2587098)[0m 2024-04-30 22:52:13.673376: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2587120)[0m 2024-04-30 22:52:13.812362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2587120)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2587120)[0m 2024-04-30 22:52:15.635048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:52:34,432 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:52:35,495 | server.py:125 | fit progress: (1, 1.8327003717422485, {'accuracy': 0.6912, 'data_size': 10000}, 24.652680647966918)
INFO flwr 2024-04-30 22:52:35,495 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:52:35,495 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:52:48,125 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:52:49,395 | server.py:125 | fit progress: (2, 1.7436429262161255, {'accuracy': 0.7214, 'data_size': 10000}, 38.55329302098835)
INFO flwr 2024-04-30 22:52:49,396 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:52:49,396 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:53:01,295 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:53:02,548 | server.py:125 | fit progress: (3, 1.6596169471740723, {'accuracy': 0.8078, 'data_size': 10000}, 51.705656550999265)
INFO flwr 2024-04-30 22:53:02,548 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:53:02,548 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:53:14,455 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:53:15,717 | server.py:125 | fit progress: (4, 1.6446887254714966, {'accuracy': 0.8198, 'data_size': 10000}, 64.87551265599905)
INFO flwr 2024-04-30 22:53:15,718 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:53:15,718 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:53:27,870 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:53:29,070 | server.py:125 | fit progress: (5, 1.634446144104004, {'accuracy': 0.8282, 'data_size': 10000}, 78.22796125599416)
INFO flwr 2024-04-30 22:53:29,070 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:53:29,070 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:53:40,801 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:53:41,844 | server.py:125 | fit progress: (6, 1.6390515565872192, {'accuracy': 0.8224, 'data_size': 10000}, 91.00242927798536)
INFO flwr 2024-04-30 22:53:41,845 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:53:41,845 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:53:53,597 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:53:54,667 | server.py:125 | fit progress: (7, 1.6385185718536377, {'accuracy': 0.8229, 'data_size': 10000}, 103.8249380599591)
INFO flwr 2024-04-30 22:53:54,667 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:53:54,667 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:54:06,987 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:54:08,054 | server.py:125 | fit progress: (8, 1.6304728984832764, {'accuracy': 0.8299, 'data_size': 10000}, 117.21218721795594)
INFO flwr 2024-04-30 22:54:08,054 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:54:08,055 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:54:19,892 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:54:21,145 | server.py:125 | fit progress: (9, 1.6269829273223877, {'accuracy': 0.8344, 'data_size': 10000}, 130.30305690399837)
INFO flwr 2024-04-30 22:54:21,145 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:54:21,145 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:54:32,937 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:54:34,208 | server.py:125 | fit progress: (10, 1.633315920829773, {'accuracy': 0.8278, 'data_size': 10000}, 143.36596212297445)
INFO flwr 2024-04-30 22:54:34,208 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:54:34,208 | server.py:153 | FL finished in 143.36634966998827
INFO flwr 2024-04-30 22:54:34,208 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:54:34,208 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:54:34,209 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:54:34,209 | app.py:229 | app_fit: losses_centralized [(0, 2.3000147342681885), (1, 1.8327003717422485), (2, 1.7436429262161255), (3, 1.6596169471740723), (4, 1.6446887254714966), (5, 1.634446144104004), (6, 1.6390515565872192), (7, 1.6385185718536377), (8, 1.6304728984832764), (9, 1.6269829273223877), (10, 1.633315920829773)]
INFO flwr 2024-04-30 22:54:34,209 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1103), (1, 0.6912), (2, 0.7214), (3, 0.8078), (4, 0.8198), (5, 0.8282), (6, 0.8224), (7, 0.8229), (8, 0.8299), (9, 0.8344), (10, 0.8278)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8278
wandb:     loss 1.63332
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_225153-i2zmu9by
wandb: Find logs at: ./wandb/offline-run-20240430_225153-i2zmu9by/logs
INFO flwr 2024-04-30 22:54:37,699 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:54:38,447 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2587091)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2587091)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:54:43,249	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:54:43,390	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:54:43,485	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:54:43,487	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:54:53,239 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 23472150528.0, 'node:__internal_head__': 1.0, 'memory': 46944301056.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 22:54:53,239 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:54:53,239 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:54:53,252 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:54:53,253 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:54:53,254 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:54:53,254 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:54:55,818 | server.py:94 | initial parameters (loss, other metrics): 2.3059470653533936, {'accuracy': 0.0799, 'data_size': 10000}
INFO flwr 2024-04-30 22:54:55,819 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:54:55,819 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2620559)[0m 2024-04-30 22:54:58.750735: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2620559)[0m 2024-04-30 22:54:58.846145: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2620559)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2620559)[0m 2024-04-30 22:55:00.823201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2620559)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2620559)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2620547)[0m 2024-04-30 22:54:58.962636: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2620547)[0m 2024-04-30 22:54:59.060749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2620547)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2620555)[0m 2024-04-30 22:55:00.948493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:55:19,945 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:55:21,009 | server.py:125 | fit progress: (1, 1.9222326278686523, {'accuracy': 0.632, 'data_size': 10000}, 25.189413310028613)
INFO flwr 2024-04-30 22:55:21,009 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:55:21,009 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:55:33,227 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:55:34,495 | server.py:125 | fit progress: (2, 1.6688073873519897, {'accuracy': 0.8174, 'data_size': 10000}, 38.675396607024595)
INFO flwr 2024-04-30 22:55:34,495 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:55:34,495 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:55:45,420 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:55:46,695 | server.py:125 | fit progress: (3, 1.6050455570220947, {'accuracy': 0.8647, 'data_size': 10000}, 50.87579147599172)
INFO flwr 2024-04-30 22:55:46,695 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:55:46,696 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:55:58,362 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:55:59,636 | server.py:125 | fit progress: (4, 1.56743323802948, {'accuracy': 0.8998, 'data_size': 10000}, 63.81663297000341)
INFO flwr 2024-04-30 22:55:59,636 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:55:59,636 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:56:10,977 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:56:12,241 | server.py:125 | fit progress: (5, 1.5601215362548828, {'accuracy': 0.9058, 'data_size': 10000}, 76.42195829801494)
INFO flwr 2024-04-30 22:56:12,242 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:56:12,242 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:56:23,743 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:56:24,785 | server.py:125 | fit progress: (6, 1.5612962245941162, {'accuracy': 0.9018, 'data_size': 10000}, 88.96579442603979)
INFO flwr 2024-04-30 22:56:24,786 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:56:24,786 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:56:35,949 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 22:56:36,989 | server.py:125 | fit progress: (7, 1.5585163831710815, {'accuracy': 0.906, 'data_size': 10000}, 101.16964949300746)
INFO flwr 2024-04-30 22:56:36,989 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 22:56:36,989 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:56:48,414 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 22:56:49,687 | server.py:125 | fit progress: (8, 1.5531197786331177, {'accuracy': 0.9099, 'data_size': 10000}, 113.86811437300639)
INFO flwr 2024-04-30 22:56:49,688 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 22:56:49,688 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:57:01,670 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 22:57:02,929 | server.py:125 | fit progress: (9, 1.5504908561706543, {'accuracy': 0.913, 'data_size': 10000}, 127.10992790904129)
INFO flwr 2024-04-30 22:57:02,930 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 22:57:02,930 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:57:14,927 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 22:57:16,209 | server.py:125 | fit progress: (10, 1.5540924072265625, {'accuracy': 0.9094, 'data_size': 10000}, 140.3897990550031)
INFO flwr 2024-04-30 22:57:16,209 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 22:57:16,210 | server.py:153 | FL finished in 140.39031566301128
INFO flwr 2024-04-30 22:57:16,210 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 22:57:16,210 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 22:57:16,210 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 22:57:16,210 | app.py:229 | app_fit: losses_centralized [(0, 2.3059470653533936), (1, 1.9222326278686523), (2, 1.6688073873519897), (3, 1.6050455570220947), (4, 1.56743323802948), (5, 1.5601215362548828), (6, 1.5612962245941162), (7, 1.5585163831710815), (8, 1.5531197786331177), (9, 1.5504908561706543), (10, 1.5540924072265625)]
INFO flwr 2024-04-30 22:57:16,210 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0799), (1, 0.632), (2, 0.8174), (3, 0.8647), (4, 0.8998), (5, 0.9058), (6, 0.9018), (7, 0.906), (8, 0.9099), (9, 0.913), (10, 0.9094)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9094
wandb:     loss 1.55409
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_225437-nd9ero83
wandb: Find logs at: ./wandb/offline-run-20240430_225437-nd9ero83/logs
INFO flwr 2024-04-30 22:57:19,772 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 22:57:20,453 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2620556)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2620556)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 22:57:25,325	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 22:57:25,419	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 22:57:25,507	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 22:57:25,509	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 22:57:35,462 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 47622323406.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 23811161702.0}
INFO flwr 2024-04-30 22:57:35,462 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 22:57:35,462 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 22:57:35,478 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 22:57:35,480 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 22:57:35,480 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 22:57:35,480 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 22:57:37,989 | server.py:94 | initial parameters (loss, other metrics): 2.300560235977173, {'accuracy': 0.1405, 'data_size': 10000}
INFO flwr 2024-04-30 22:57:37,989 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 22:57:37,990 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2653156)[0m 2024-04-30 22:57:43.494104: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2653156)[0m 2024-04-30 22:57:43.594740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2653156)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2653156)[0m 2024-04-30 22:57:45.981844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2653156)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2653156)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2653163)[0m 2024-04-30 22:57:43.674859: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2653161)[0m 2024-04-30 22:57:43.751000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2653161)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2653160)[0m 2024-04-30 22:57:45.981832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 22:58:11,637 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 22:58:12,669 | server.py:125 | fit progress: (1, 1.9071226119995117, {'accuracy': 0.5823, 'data_size': 10000}, 34.67963883699849)
INFO flwr 2024-04-30 22:58:12,669 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 22:58:12,670 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:58:31,247 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 22:58:32,319 | server.py:125 | fit progress: (2, 1.6826428174972534, {'accuracy': 0.7969, 'data_size': 10000}, 54.329160491004586)
INFO flwr 2024-04-30 22:58:32,319 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 22:58:32,319 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:58:50,330 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 22:58:51,608 | server.py:125 | fit progress: (3, 1.676225185394287, {'accuracy': 0.7924, 'data_size': 10000}, 73.61800787801621)
INFO flwr 2024-04-30 22:58:51,608 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 22:58:51,608 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:59:11,534 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 22:59:12,827 | server.py:125 | fit progress: (4, 1.6804211139678955, {'accuracy': 0.781, 'data_size': 10000}, 94.8370210470166)
INFO flwr 2024-04-30 22:59:12,827 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 22:59:12,827 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:59:29,884 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 22:59:31,145 | server.py:125 | fit progress: (5, 1.6439334154129028, {'accuracy': 0.8187, 'data_size': 10000}, 113.1555044450215)
INFO flwr 2024-04-30 22:59:31,145 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 22:59:31,146 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 22:59:49,988 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 22:59:51,040 | server.py:125 | fit progress: (6, 1.6391427516937256, {'accuracy': 0.8212, 'data_size': 10000}, 133.0502803720301)
INFO flwr 2024-04-30 22:59:51,040 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 22:59:51,040 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:00:10,683 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:00:11,722 | server.py:125 | fit progress: (7, 1.6365635395050049, {'accuracy': 0.8236, 'data_size': 10000}, 153.73275288799778)
INFO flwr 2024-04-30 23:00:11,723 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:00:11,723 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:00:31,673 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:00:32,709 | server.py:125 | fit progress: (8, 1.629744291305542, {'accuracy': 0.8316, 'data_size': 10000}, 174.71953046799172)
INFO flwr 2024-04-30 23:00:32,709 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:00:32,709 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:00:50,891 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:00:52,167 | server.py:125 | fit progress: (9, 1.629109501838684, {'accuracy': 0.8319, 'data_size': 10000}, 194.1778324600309)
INFO flwr 2024-04-30 23:00:52,168 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:00:52,168 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:01:10,809 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:01:12,066 | server.py:125 | fit progress: (10, 1.630423903465271, {'accuracy': 0.8299, 'data_size': 10000}, 214.07669494498987)
INFO flwr 2024-04-30 23:01:12,067 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:01:12,067 | server.py:153 | FL finished in 214.07711899100104
INFO flwr 2024-04-30 23:01:12,067 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:01:12,067 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:01:12,067 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:01:12,067 | app.py:229 | app_fit: losses_centralized [(0, 2.300560235977173), (1, 1.9071226119995117), (2, 1.6826428174972534), (3, 1.676225185394287), (4, 1.6804211139678955), (5, 1.6439334154129028), (6, 1.6391427516937256), (7, 1.6365635395050049), (8, 1.629744291305542), (9, 1.629109501838684), (10, 1.630423903465271)]
INFO flwr 2024-04-30 23:01:12,067 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1405), (1, 0.5823), (2, 0.7969), (3, 0.7924), (4, 0.781), (5, 0.8187), (6, 0.8212), (7, 0.8236), (8, 0.8316), (9, 0.8319), (10, 0.8299)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8299
wandb:     loss 1.63042
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_225720-a7w8juko
wandb: Find logs at: ./wandb/offline-run-20240430_225720-a7w8juko/logs
INFO flwr 2024-04-30 23:01:15,556 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:01:16,316 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2653163)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2653163)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:01:21,139	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:01:21,245	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:01:21,335	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:01:21,337	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:01:30,971 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 132654450893.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 61137621811.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 23:01:30,972 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:01:30,972 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:01:30,984 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:01:30,985 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:01:30,985 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:01:30,986 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:01:33,578 | server.py:94 | initial parameters (loss, other metrics): 2.302933931350708, {'accuracy': 0.1035, 'data_size': 10000}
INFO flwr 2024-04-30 23:01:33,578 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:01:33,579 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2699361)[0m 2024-04-30 23:01:36.427768: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2699361)[0m 2024-04-30 23:01:36.525172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2699361)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2699361)[0m 2024-04-30 23:01:38.482852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2699348)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2699348)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2699358)[0m 2024-04-30 23:01:36.617112: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2699358)[0m 2024-04-30 23:01:36.702529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2699358)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2699348)[0m 2024-04-30 23:01:38.604665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:02:04,158 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:02:05,197 | server.py:125 | fit progress: (1, 1.8036822080612183, {'accuracy': 0.744, 'data_size': 10000}, 31.61882890400011)
INFO flwr 2024-04-30 23:02:05,198 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:02:05,198 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:02:23,414 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:02:24,678 | server.py:125 | fit progress: (2, 1.6638318300247192, {'accuracy': 0.8199, 'data_size': 10000}, 51.09985906298971)
INFO flwr 2024-04-30 23:02:24,679 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:02:24,679 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:02:44,062 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:02:45,310 | server.py:125 | fit progress: (3, 1.607739806175232, {'accuracy': 0.864, 'data_size': 10000}, 71.73179597995477)
INFO flwr 2024-04-30 23:02:45,311 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:02:45,311 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:03:03,664 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:03:04,920 | server.py:125 | fit progress: (4, 1.5714069604873657, {'accuracy': 0.8968, 'data_size': 10000}, 91.3418096829555)
INFO flwr 2024-04-30 23:03:04,921 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:03:04,921 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:03:23,168 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:03:24,469 | server.py:125 | fit progress: (5, 1.5608761310577393, {'accuracy': 0.9052, 'data_size': 10000}, 110.89049604401225)
INFO flwr 2024-04-30 23:03:24,469 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:03:24,469 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:03:42,188 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:03:43,204 | server.py:125 | fit progress: (6, 1.562855839729309, {'accuracy': 0.9003, 'data_size': 10000}, 129.6256367819733)
INFO flwr 2024-04-30 23:03:43,204 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:03:43,205 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:04:01,419 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:04:02,474 | server.py:125 | fit progress: (7, 1.5619959831237793, {'accuracy': 0.9003, 'data_size': 10000}, 148.8950944689568)
INFO flwr 2024-04-30 23:04:02,474 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:04:02,474 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:04:20,859 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:04:22,113 | server.py:125 | fit progress: (8, 1.5551788806915283, {'accuracy': 0.9067, 'data_size': 10000}, 168.53480418300023)
INFO flwr 2024-04-30 23:04:22,114 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:04:22,114 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:04:41,360 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:04:42,758 | server.py:125 | fit progress: (9, 1.5528285503387451, {'accuracy': 0.9093, 'data_size': 10000}, 189.17977449897444)
INFO flwr 2024-04-30 23:04:42,759 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:04:42,759 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:05:04,898 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:05:06,189 | server.py:125 | fit progress: (10, 1.5532675981521606, {'accuracy': 0.9086, 'data_size': 10000}, 212.61036075197626)
INFO flwr 2024-04-30 23:05:06,189 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:05:06,189 | server.py:153 | FL finished in 212.61077161796857
INFO flwr 2024-04-30 23:05:06,189 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:05:06,190 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:05:06,190 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:05:06,190 | app.py:229 | app_fit: losses_centralized [(0, 2.302933931350708), (1, 1.8036822080612183), (2, 1.6638318300247192), (3, 1.607739806175232), (4, 1.5714069604873657), (5, 1.5608761310577393), (6, 1.562855839729309), (7, 1.5619959831237793), (8, 1.5551788806915283), (9, 1.5528285503387451), (10, 1.5532675981521606)]
INFO flwr 2024-04-30 23:05:06,190 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1035), (1, 0.744), (2, 0.8199), (3, 0.864), (4, 0.8968), (5, 0.9052), (6, 0.9003), (7, 0.9003), (8, 0.9067), (9, 0.9093), (10, 0.9086)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9086
wandb:     loss 1.55327
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_230115-9ijrkh0i
wandb: Find logs at: ./wandb/offline-run-20240430_230115-9ijrkh0i/logs
INFO flwr 2024-04-30 23:05:09,672 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:05:10,417 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2699350)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2699350)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:05:15,670	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:05:15,763	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:05:15,863	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:05:15,865	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:05:26,609 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 131316884890.0, 'node:__internal_head__': 1.0, 'object_store_memory': 60564379238.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 23:05:26,609 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:05:26,610 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:05:26,629 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:05:26,630 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:05:26,630 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:05:26,631 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:05:29,534 | server.py:94 | initial parameters (loss, other metrics): 2.302145481109619, {'accuracy': 0.1196, 'data_size': 10000}
INFO flwr 2024-04-30 23:05:29,535 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:05:29,535 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2752484)[0m 2024-04-30 23:05:32.952621: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2752466)[0m 2024-04-30 23:05:33.011115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2752466)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2752484)[0m 2024-04-30 23:05:35.236981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2752484)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2752484)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2752472)[0m 2024-04-30 23:05:33.193877: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2752472)[0m 2024-04-30 23:05:33.288423: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2752472)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2752472)[0m 2024-04-30 23:05:35.378296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:06:03,040 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:06:04,179 | server.py:125 | fit progress: (1, 1.8908357620239258, {'accuracy': 0.6342, 'data_size': 10000}, 34.64384852803778)
INFO flwr 2024-04-30 23:06:04,179 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:06:04,180 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:06:26,504 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:06:27,727 | server.py:125 | fit progress: (2, 1.6865885257720947, {'accuracy': 0.7986, 'data_size': 10000}, 58.191945410042536)
INFO flwr 2024-04-30 23:06:27,727 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:06:27,728 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:06:49,270 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:06:50,572 | server.py:125 | fit progress: (3, 1.5896533727645874, {'accuracy': 0.8826, 'data_size': 10000}, 81.03648674901342)
INFO flwr 2024-04-30 23:06:50,572 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:06:50,572 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:07:14,988 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:07:16,467 | server.py:125 | fit progress: (4, 1.5775338411331177, {'accuracy': 0.8892, 'data_size': 10000}, 106.93211057502776)
INFO flwr 2024-04-30 23:07:16,468 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:07:16,468 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:07:40,576 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:07:41,865 | server.py:125 | fit progress: (5, 1.5680508613586426, {'accuracy': 0.8956, 'data_size': 10000}, 132.33008954604156)
INFO flwr 2024-04-30 23:07:41,866 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:07:41,866 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:08:03,282 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:08:04,668 | server.py:125 | fit progress: (6, 1.5601667165756226, {'accuracy': 0.9029, 'data_size': 10000}, 155.13289301702753)
INFO flwr 2024-04-30 23:08:04,668 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:08:04,669 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:08:31,562 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:08:32,679 | server.py:125 | fit progress: (7, 1.5544334650039673, {'accuracy': 0.9068, 'data_size': 10000}, 183.1440962430206)
INFO flwr 2024-04-30 23:08:32,680 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:08:32,680 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:08:55,932 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:08:57,384 | server.py:125 | fit progress: (8, 1.557396650314331, {'accuracy': 0.9051, 'data_size': 10000}, 207.84870348899858)
INFO flwr 2024-04-30 23:08:57,384 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:08:57,385 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:09:23,492 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:09:24,944 | server.py:125 | fit progress: (9, 1.560805320739746, {'accuracy': 0.9021, 'data_size': 10000}, 235.40869256202132)
INFO flwr 2024-04-30 23:09:24,944 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:09:24,944 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:09:49,255 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:09:50,554 | server.py:125 | fit progress: (10, 1.5553539991378784, {'accuracy': 0.9068, 'data_size': 10000}, 261.0190671920427)
INFO flwr 2024-04-30 23:09:50,555 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:09:50,555 | server.py:153 | FL finished in 261.01950052700704
INFO flwr 2024-04-30 23:09:50,555 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:09:50,555 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:09:50,555 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:09:50,555 | app.py:229 | app_fit: losses_centralized [(0, 2.302145481109619), (1, 1.8908357620239258), (2, 1.6865885257720947), (3, 1.5896533727645874), (4, 1.5775338411331177), (5, 1.5680508613586426), (6, 1.5601667165756226), (7, 1.5544334650039673), (8, 1.557396650314331), (9, 1.560805320739746), (10, 1.5553539991378784)]
INFO flwr 2024-04-30 23:09:50,555 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1196), (1, 0.6342), (2, 0.7986), (3, 0.8826), (4, 0.8892), (5, 0.8956), (6, 0.9029), (7, 0.9068), (8, 0.9051), (9, 0.9021), (10, 0.9068)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9068
wandb:     loss 1.55535
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_230509-5z6f06c4
wandb: Find logs at: ./wandb/offline-run-20240430_230509-5z6f06c4/logs
INFO flwr 2024-04-30 23:09:54,221 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:09:56,909 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2752466)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2752466)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:10:08,328	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:10:09,012	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:10:09,140	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:10:09,141	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:10:20,478 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61182390681.0, 'memory': 132758911591.0}
INFO flwr 2024-04-30 23:10:20,478 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:10:20,478 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:10:20,511 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:10:20,512 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:10:20,513 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:10:20,513 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:10:23,495 | server.py:94 | initial parameters (loss, other metrics): 2.302846670150757, {'accuracy': 0.1046, 'data_size': 10000}
INFO flwr 2024-04-30 23:10:23,495 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:10:23,496 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2846969)[0m 2024-04-30 23:10:31.260561: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2846969)[0m 2024-04-30 23:10:31.463678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2846969)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2846969)[0m 2024-04-30 23:10:40.723415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=2846977)[0m 2024-04-30 23:10:31.310899: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2846971)[0m 2024-04-30 23:10:31.430256: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2846971)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2846978)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2846978)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2846971)[0m 2024-04-30 23:10:40.720441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:11:10,065 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:11:11,512 | server.py:125 | fit progress: (1, 2.232612371444702, {'accuracy': 0.1837, 'data_size': 10000}, 48.01637821900658)
INFO flwr 2024-04-30 23:11:11,512 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:11:11,512 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:11:22,168 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:11:23,453 | server.py:125 | fit progress: (2, 2.0972185134887695, {'accuracy': 0.358, 'data_size': 10000}, 59.957742672006134)
INFO flwr 2024-04-30 23:11:23,454 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:11:23,454 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:11:32,159 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:11:33,412 | server.py:125 | fit progress: (3, 1.9751930236816406, {'accuracy': 0.4821, 'data_size': 10000}, 69.91622663103044)
INFO flwr 2024-04-30 23:11:33,412 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:11:33,412 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:11:42,845 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:11:44,345 | server.py:125 | fit progress: (4, 1.8929060697555542, {'accuracy': 0.5635, 'data_size': 10000}, 80.84896978799952)
INFO flwr 2024-04-30 23:11:44,345 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:11:44,345 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:11:53,025 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:11:54,531 | server.py:125 | fit progress: (5, 1.8667060136795044, {'accuracy': 0.5921, 'data_size': 10000}, 91.03549398103496)
INFO flwr 2024-04-30 23:11:54,531 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:11:54,531 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:12:03,111 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:12:04,157 | server.py:125 | fit progress: (6, 1.8714324235916138, {'accuracy': 0.5868, 'data_size': 10000}, 100.66167182900244)
INFO flwr 2024-04-30 23:12:04,157 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:12:04,158 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:12:12,825 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:12:14,363 | server.py:125 | fit progress: (7, 1.8616482019424438, {'accuracy': 0.5965, 'data_size': 10000}, 110.86780987703241)
INFO flwr 2024-04-30 23:12:14,364 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:12:14,364 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:12:23,374 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:12:24,878 | server.py:125 | fit progress: (8, 1.8430417776107788, {'accuracy': 0.6163, 'data_size': 10000}, 121.38202668703161)
INFO flwr 2024-04-30 23:12:24,878 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:12:24,878 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:12:34,572 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:12:36,760 | server.py:125 | fit progress: (9, 1.826802372932434, {'accuracy': 0.6328, 'data_size': 10000}, 133.26395799504826)
INFO flwr 2024-04-30 23:12:36,760 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:12:36,760 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:12:47,231 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:12:49,522 | server.py:125 | fit progress: (10, 1.8201830387115479, {'accuracy': 0.638, 'data_size': 10000}, 146.0261853420525)
INFO flwr 2024-04-30 23:12:49,522 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:12:49,522 | server.py:153 | FL finished in 146.02663153700996
INFO flwr 2024-04-30 23:12:49,522 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:12:49,522 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:12:49,523 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:12:49,523 | app.py:229 | app_fit: losses_centralized [(0, 2.302846670150757), (1, 2.232612371444702), (2, 2.0972185134887695), (3, 1.9751930236816406), (4, 1.8929060697555542), (5, 1.8667060136795044), (6, 1.8714324235916138), (7, 1.8616482019424438), (8, 1.8430417776107788), (9, 1.826802372932434), (10, 1.8201830387115479)]
INFO flwr 2024-04-30 23:12:49,523 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1046), (1, 0.1837), (2, 0.358), (3, 0.4821), (4, 0.5635), (5, 0.5921), (6, 0.5868), (7, 0.5965), (8, 0.6163), (9, 0.6328), (10, 0.638)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.638
wandb:     loss 1.82018
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_230956-0oe71b9j
wandb: Find logs at: ./wandb/offline-run-20240430_230956-0oe71b9j/logs
INFO flwr 2024-04-30 23:12:53,156 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:12:54,117 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2846971)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2846971)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:12:59,679	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:12:59,820	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:12:59,961	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:12:59,963	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:13:11,364 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 19462386892.0, 'node:__internal_head__': 1.0, 'memory': 38924773787.0}
INFO flwr 2024-04-30 23:13:11,365 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:13:11,365 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:13:11,393 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:13:11,395 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:13:11,395 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:13:11,395 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:13:15,244 | server.py:94 | initial parameters (loss, other metrics): 2.3029401302337646, {'accuracy': 0.0852, 'data_size': 10000}
INFO flwr 2024-04-30 23:13:15,244 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:13:15,244 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2897441)[0m 2024-04-30 23:13:20.437374: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2897439)[0m 2024-04-30 23:13:20.505763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2897439)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2897441)[0m 2024-04-30 23:13:24.550173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2897437)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2897437)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2897437)[0m 2024-04-30 23:13:21.193769: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2897437)[0m 2024-04-30 23:13:21.324353: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2897437)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2897437)[0m 2024-04-30 23:13:24.809961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:13:41,831 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:13:43,072 | server.py:125 | fit progress: (1, 1.9732394218444824, {'accuracy': 0.5045, 'data_size': 10000}, 27.827864113030955)
INFO flwr 2024-04-30 23:13:43,073 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:13:43,073 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:13:54,230 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:13:55,305 | server.py:125 | fit progress: (2, 1.891859531402588, {'accuracy': 0.5677, 'data_size': 10000}, 40.060807758010924)
INFO flwr 2024-04-30 23:13:55,305 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:13:55,306 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:14:03,665 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:14:05,694 | server.py:125 | fit progress: (3, 1.7524380683898926, {'accuracy': 0.7119, 'data_size': 10000}, 50.4500177790178)
INFO flwr 2024-04-30 23:14:05,695 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:14:05,695 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:14:14,386 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:14:15,812 | server.py:125 | fit progress: (4, 1.6716957092285156, {'accuracy': 0.793, 'data_size': 10000}, 60.567705984052736)
INFO flwr 2024-04-30 23:14:15,812 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:14:15,813 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:14:25,554 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:14:27,843 | server.py:125 | fit progress: (5, 1.6631605625152588, {'accuracy': 0.7998, 'data_size': 10000}, 72.59908703004476)
INFO flwr 2024-04-30 23:14:27,844 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:14:27,844 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:14:37,943 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:14:39,023 | server.py:125 | fit progress: (6, 1.6619873046875, {'accuracy': 0.7998, 'data_size': 10000}, 83.77848562801955)
INFO flwr 2024-04-30 23:14:39,023 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:14:39,023 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:14:48,313 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:14:49,381 | server.py:125 | fit progress: (7, 1.6267600059509277, {'accuracy': 0.837, 'data_size': 10000}, 94.13677034102147)
INFO flwr 2024-04-30 23:14:49,381 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:14:49,382 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:14:58,058 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:14:59,107 | server.py:125 | fit progress: (8, 1.610548496246338, {'accuracy': 0.8507, 'data_size': 10000}, 103.86308783001732)
INFO flwr 2024-04-30 23:14:59,108 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:14:59,108 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:15:07,408 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:15:08,829 | server.py:125 | fit progress: (9, 1.6137183904647827, {'accuracy': 0.8457, 'data_size': 10000}, 113.58468446502229)
INFO flwr 2024-04-30 23:15:08,829 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:15:08,830 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:15:16,969 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:15:18,918 | server.py:125 | fit progress: (10, 1.6034773588180542, {'accuracy': 0.8578, 'data_size': 10000}, 123.67348613700597)
INFO flwr 2024-04-30 23:15:18,918 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:15:18,918 | server.py:153 | FL finished in 123.67389841005206
INFO flwr 2024-04-30 23:15:18,918 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:15:18,919 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:15:18,919 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:15:18,919 | app.py:229 | app_fit: losses_centralized [(0, 2.3029401302337646), (1, 1.9732394218444824), (2, 1.891859531402588), (3, 1.7524380683898926), (4, 1.6716957092285156), (5, 1.6631605625152588), (6, 1.6619873046875), (7, 1.6267600059509277), (8, 1.610548496246338), (9, 1.6137183904647827), (10, 1.6034773588180542)]
INFO flwr 2024-04-30 23:15:18,919 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0852), (1, 0.5045), (2, 0.5677), (3, 0.7119), (4, 0.793), (5, 0.7998), (6, 0.7998), (7, 0.837), (8, 0.8507), (9, 0.8457), (10, 0.8578)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8578
wandb:     loss 1.60348
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_231253-wwi3vlbe
wandb: Find logs at: ./wandb/offline-run-20240430_231253-wwi3vlbe/logs
INFO flwr 2024-04-30 23:15:22,708 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:15:23,595 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2897440)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2897440)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:15:29,560	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:15:29,685	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:15:29,817	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:15:29,818	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:15:40,904 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 48807751680.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 24403875840.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-30 23:15:40,904 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:15:40,905 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:15:40,927 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:15:40,928 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:15:40,928 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:15:40,928 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:15:44,385 | server.py:94 | initial parameters (loss, other metrics): 2.3060483932495117, {'accuracy': 0.0384, 'data_size': 10000}
INFO flwr 2024-04-30 23:15:44,386 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:15:44,386 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2940475)[0m 2024-04-30 23:15:49.643327: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2940475)[0m 2024-04-30 23:15:49.747584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2940475)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2940475)[0m 2024-04-30 23:15:54.742190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=2940472)[0m 2024-04-30 23:15:50.424639: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2940472)[0m 2024-04-30 23:15:50.702079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2940472)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2940466)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2940466)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2940472)[0m 2024-04-30 23:15:54.954235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:16:13,512 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:16:15,141 | server.py:125 | fit progress: (1, 1.8440970182418823, {'accuracy': 0.6663, 'data_size': 10000}, 30.75507054803893)
INFO flwr 2024-04-30 23:16:15,142 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:16:15,142 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:16:24,628 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:16:26,610 | server.py:125 | fit progress: (2, 1.7805064916610718, {'accuracy': 0.6873, 'data_size': 10000}, 42.223969606042374)
INFO flwr 2024-04-30 23:16:26,610 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:16:26,611 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:16:35,395 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:16:37,217 | server.py:125 | fit progress: (3, 1.7315870523452759, {'accuracy': 0.7326, 'data_size': 10000}, 52.830441271013115)
INFO flwr 2024-04-30 23:16:37,217 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:16:37,217 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:16:47,119 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:16:48,387 | server.py:125 | fit progress: (4, 1.7422634363174438, {'accuracy': 0.7189, 'data_size': 10000}, 64.00100145098986)
INFO flwr 2024-04-30 23:16:48,387 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:16:48,387 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:16:57,596 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:16:59,391 | server.py:125 | fit progress: (5, 1.683058738708496, {'accuracy': 0.7791, 'data_size': 10000}, 75.00533584802179)
INFO flwr 2024-04-30 23:16:59,392 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:16:59,392 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:17:08,408 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:17:10,097 | server.py:125 | fit progress: (6, 1.6420886516571045, {'accuracy': 0.8172, 'data_size': 10000}, 85.7110930820345)
INFO flwr 2024-04-30 23:17:10,097 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:17:10,098 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:17:19,359 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:17:21,017 | server.py:125 | fit progress: (7, 1.6022671461105347, {'accuracy': 0.8604, 'data_size': 10000}, 96.63108775799628)
INFO flwr 2024-04-30 23:17:21,017 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:17:21,018 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:17:30,998 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:17:32,994 | server.py:125 | fit progress: (8, 1.6092655658721924, {'accuracy': 0.8525, 'data_size': 10000}, 108.60832449403824)
INFO flwr 2024-04-30 23:17:32,995 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:17:32,995 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:17:41,911 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:17:43,176 | server.py:125 | fit progress: (9, 1.6313852071762085, {'accuracy': 0.8299, 'data_size': 10000}, 118.78990011801943)
INFO flwr 2024-04-30 23:17:43,176 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:17:43,176 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:17:51,920 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:17:53,861 | server.py:125 | fit progress: (10, 1.639466643333435, {'accuracy': 0.8212, 'data_size': 10000}, 129.47506332799094)
INFO flwr 2024-04-30 23:17:53,861 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:17:53,862 | server.py:153 | FL finished in 129.475473522034
INFO flwr 2024-04-30 23:17:53,862 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:17:53,862 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:17:53,862 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:17:53,862 | app.py:229 | app_fit: losses_centralized [(0, 2.3060483932495117), (1, 1.8440970182418823), (2, 1.7805064916610718), (3, 1.7315870523452759), (4, 1.7422634363174438), (5, 1.683058738708496), (6, 1.6420886516571045), (7, 1.6022671461105347), (8, 1.6092655658721924), (9, 1.6313852071762085), (10, 1.639466643333435)]
INFO flwr 2024-04-30 23:17:53,862 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0384), (1, 0.6663), (2, 0.6873), (3, 0.7326), (4, 0.7189), (5, 0.7791), (6, 0.8172), (7, 0.8604), (8, 0.8525), (9, 0.8299), (10, 0.8212)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8212
wandb:     loss 1.63947
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_231523-jkjnmi19
wandb: Find logs at: ./wandb/offline-run-20240430_231523-jkjnmi19/logs
INFO flwr 2024-04-30 23:17:57,332 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:17:58,189 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2940472)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2940472)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:18:03,232	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:18:03,371	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:18:03,452	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:18:03,454	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:18:14,065 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 47618541159.0, 'object_store_memory': 23809270579.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-30 23:18:14,065 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:18:14,065 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:18:14,087 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:18:14,088 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:18:14,089 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:18:14,089 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:18:16,881 | server.py:94 | initial parameters (loss, other metrics): 2.3039090633392334, {'accuracy': 0.0686, 'data_size': 10000}
INFO flwr 2024-04-30 23:18:16,884 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:18:16,885 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2990111)[0m 2024-04-30 23:18:22.211727: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2990111)[0m 2024-04-30 23:18:22.313102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2990111)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2990111)[0m 2024-04-30 23:18:25.912883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2990109)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2990109)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2990115)[0m 2024-04-30 23:18:23.091438: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2990112)[0m 2024-04-30 23:18:23.132212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2990112)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2990114)[0m 2024-04-30 23:18:26.977308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:18:43,674 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:18:44,776 | server.py:125 | fit progress: (1, 1.9769673347473145, {'accuracy': 0.4743, 'data_size': 10000}, 27.890520922956057)
INFO flwr 2024-04-30 23:18:44,776 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:18:44,776 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:18:55,128 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:18:56,979 | server.py:125 | fit progress: (2, 1.958923101425171, {'accuracy': 0.4992, 'data_size': 10000}, 40.09406133799348)
INFO flwr 2024-04-30 23:18:56,980 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:18:56,980 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:19:06,916 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:19:08,841 | server.py:125 | fit progress: (3, 1.8258267641067505, {'accuracy': 0.6294, 'data_size': 10000}, 51.95583049498964)
INFO flwr 2024-04-30 23:19:08,841 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:19:08,842 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:19:18,994 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:19:21,460 | server.py:125 | fit progress: (4, 1.8125648498535156, {'accuracy': 0.6435, 'data_size': 10000}, 64.57440576597583)
INFO flwr 2024-04-30 23:19:21,460 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:19:21,460 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:19:31,732 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:19:33,225 | server.py:125 | fit progress: (5, 1.7899322509765625, {'accuracy': 0.67, 'data_size': 10000}, 76.33942520595156)
INFO flwr 2024-04-30 23:19:33,225 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:19:33,225 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:19:41,643 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:19:43,458 | server.py:125 | fit progress: (6, 1.792999029159546, {'accuracy': 0.6649, 'data_size': 10000}, 86.57306047197198)
INFO flwr 2024-04-30 23:19:43,459 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:19:43,459 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:19:52,168 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:19:53,310 | server.py:125 | fit progress: (7, 1.7851874828338623, {'accuracy': 0.674, 'data_size': 10000}, 96.42486885195831)
INFO flwr 2024-04-30 23:19:53,310 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:19:53,311 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:20:03,255 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:20:04,869 | server.py:125 | fit progress: (8, 1.7727400064468384, {'accuracy': 0.6858, 'data_size': 10000}, 107.98352606996195)
INFO flwr 2024-04-30 23:20:04,869 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:20:04,869 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:20:14,668 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:20:16,045 | server.py:125 | fit progress: (9, 1.7586381435394287, {'accuracy': 0.6999, 'data_size': 10000}, 119.15966377896257)
INFO flwr 2024-04-30 23:20:16,045 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:20:16,045 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:20:24,987 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:20:26,454 | server.py:125 | fit progress: (10, 1.7446478605270386, {'accuracy': 0.7151, 'data_size': 10000}, 129.56887513399124)
INFO flwr 2024-04-30 23:20:26,454 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:20:26,455 | server.py:153 | FL finished in 129.56940737896366
INFO flwr 2024-04-30 23:20:26,455 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:20:26,455 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:20:26,455 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:20:26,455 | app.py:229 | app_fit: losses_centralized [(0, 2.3039090633392334), (1, 1.9769673347473145), (2, 1.958923101425171), (3, 1.8258267641067505), (4, 1.8125648498535156), (5, 1.7899322509765625), (6, 1.792999029159546), (7, 1.7851874828338623), (8, 1.7727400064468384), (9, 1.7586381435394287), (10, 1.7446478605270386)]
INFO flwr 2024-04-30 23:20:26,455 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0686), (1, 0.4743), (2, 0.4992), (3, 0.6294), (4, 0.6435), (5, 0.67), (6, 0.6649), (7, 0.674), (8, 0.6858), (9, 0.6999), (10, 0.7151)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7151
wandb:     loss 1.74465
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_231757-7z4d789u
wandb: Find logs at: ./wandb/offline-run-20240430_231757-7z4d789u/logs
INFO flwr 2024-04-30 23:20:30,129 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:20:30,941 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2990111)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2990111)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:20:36,471	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:20:36,634	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:20:36,793	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:20:36,795	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:20:48,411 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 49854050304.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 24927025152.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 23:20:48,412 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:20:48,412 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:20:48,435 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:20:48,436 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:20:48,437 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:20:48,437 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:20:53,200 | server.py:94 | initial parameters (loss, other metrics): 2.30361270904541, {'accuracy': 0.0681, 'data_size': 10000}
INFO flwr 2024-04-30 23:20:53,200 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:20:53,201 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3042588)[0m 2024-04-30 23:20:57.007704: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3042588)[0m 2024-04-30 23:20:57.261778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3042588)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3042578)[0m 2024-04-30 23:21:01.171838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3042588)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3042588)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3042595)[0m 2024-04-30 23:20:57.580166: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3042595)[0m 2024-04-30 23:20:57.721114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3042595)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3042594)[0m 2024-04-30 23:21:01.225625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:21:19,459 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:21:20,549 | server.py:125 | fit progress: (1, 1.9826494455337524, {'accuracy': 0.4756, 'data_size': 10000}, 27.34882228198694)
INFO flwr 2024-04-30 23:21:20,550 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:21:20,550 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:21:30,705 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:21:32,249 | server.py:125 | fit progress: (2, 1.8025550842285156, {'accuracy': 0.6657, 'data_size': 10000}, 39.04805705399485)
INFO flwr 2024-04-30 23:21:32,249 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:21:32,249 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:21:42,504 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:21:43,863 | server.py:125 | fit progress: (3, 1.760994553565979, {'accuracy': 0.7026, 'data_size': 10000}, 50.66287454398116)
INFO flwr 2024-04-30 23:21:43,864 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:21:43,864 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:21:52,429 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:21:53,722 | server.py:125 | fit progress: (4, 1.7252675294876099, {'accuracy': 0.7354, 'data_size': 10000}, 60.52157158800401)
INFO flwr 2024-04-30 23:21:53,722 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:21:53,722 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:22:02,714 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:22:04,345 | server.py:125 | fit progress: (5, 1.7019468545913696, {'accuracy': 0.7589, 'data_size': 10000}, 71.14433470502263)
INFO flwr 2024-04-30 23:22:04,345 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:22:04,345 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:22:13,433 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:22:15,117 | server.py:125 | fit progress: (6, 1.6763445138931274, {'accuracy': 0.7847, 'data_size': 10000}, 81.91644742898643)
INFO flwr 2024-04-30 23:22:15,117 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:22:15,117 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:22:23,979 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:22:25,686 | server.py:125 | fit progress: (7, 1.6709626913070679, {'accuracy': 0.7888, 'data_size': 10000}, 92.48513710300904)
INFO flwr 2024-04-30 23:22:25,686 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:22:25,686 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:22:34,712 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:22:35,995 | server.py:125 | fit progress: (8, 1.6733061075210571, {'accuracy': 0.7874, 'data_size': 10000}, 102.79406669200398)
INFO flwr 2024-04-30 23:22:35,995 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:22:35,995 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:22:44,316 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:22:46,304 | server.py:125 | fit progress: (9, 1.6777582168579102, {'accuracy': 0.7815, 'data_size': 10000}, 113.10301365796477)
INFO flwr 2024-04-30 23:22:46,304 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:22:46,304 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:22:55,857 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:22:58,489 | server.py:125 | fit progress: (10, 1.6751056909561157, {'accuracy': 0.7834, 'data_size': 10000}, 125.2879860610119)
INFO flwr 2024-04-30 23:22:58,489 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:22:58,489 | server.py:153 | FL finished in 125.2886916269781
INFO flwr 2024-04-30 23:22:58,489 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:22:58,490 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:22:58,490 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:22:58,490 | app.py:229 | app_fit: losses_centralized [(0, 2.30361270904541), (1, 1.9826494455337524), (2, 1.8025550842285156), (3, 1.760994553565979), (4, 1.7252675294876099), (5, 1.7019468545913696), (6, 1.6763445138931274), (7, 1.6709626913070679), (8, 1.6733061075210571), (9, 1.6777582168579102), (10, 1.6751056909561157)]
INFO flwr 2024-04-30 23:22:58,490 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0681), (1, 0.4756), (2, 0.6657), (3, 0.7026), (4, 0.7354), (5, 0.7589), (6, 0.7847), (7, 0.7888), (8, 0.7874), (9, 0.7815), (10, 0.7834)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7834
wandb:     loss 1.67511
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_232030-ltawbwty
wandb: Find logs at: ./wandb/offline-run-20240430_232030-ltawbwty/logs
INFO flwr 2024-04-30 23:23:02,009 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:23:03,003 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3042595)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3042595)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:23:08,255	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:23:08,409	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:23:08,548	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:23:08,550	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:23:20,626 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 43391577294.0, 'object_store_memory': 21695788646.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 23:23:20,626 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:23:20,626 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:23:20,667 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:23:20,668 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:23:20,669 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:23:20,669 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:23:25,140 | server.py:94 | initial parameters (loss, other metrics): 2.3072807788848877, {'accuracy': 0.0542, 'data_size': 10000}
INFO flwr 2024-04-30 23:23:25,141 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:23:25,141 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3086622)[0m 2024-04-30 23:23:29.363146: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3086622)[0m 2024-04-30 23:23:29.473576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3086622)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3086621)[0m 2024-04-30 23:23:33.825171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3086620)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3086620)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3086616)[0m 2024-04-30 23:23:29.920045: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3086621)[0m 2024-04-30 23:23:29.993716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3086621)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3086610)[0m 2024-04-30 23:23:34.007864: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:23:53,893 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:23:55,644 | server.py:125 | fit progress: (1, 1.8184349536895752, {'accuracy': 0.6875, 'data_size': 10000}, 30.503477282007225)
INFO flwr 2024-04-30 23:23:55,645 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:23:55,645 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:24:08,443 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:24:09,587 | server.py:125 | fit progress: (2, 1.7710998058319092, {'accuracy': 0.7014, 'data_size': 10000}, 44.446250159002375)
INFO flwr 2024-04-30 23:24:09,588 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:24:09,588 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:24:18,821 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:24:20,850 | server.py:125 | fit progress: (3, 1.6937849521636963, {'accuracy': 0.7731, 'data_size': 10000}, 55.70879737299401)
INFO flwr 2024-04-30 23:24:20,850 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:24:20,850 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:24:30,116 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:24:31,906 | server.py:125 | fit progress: (4, 1.678381085395813, {'accuracy': 0.7849, 'data_size': 10000}, 66.7649308089749)
INFO flwr 2024-04-30 23:24:31,906 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:24:31,907 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:24:41,289 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:24:42,746 | server.py:125 | fit progress: (5, 1.6633548736572266, {'accuracy': 0.7987, 'data_size': 10000}, 77.60544876201311)
INFO flwr 2024-04-30 23:24:42,747 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:24:42,747 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:24:52,515 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:24:54,644 | server.py:125 | fit progress: (6, 1.6555384397506714, {'accuracy': 0.8064, 'data_size': 10000}, 89.50254234398017)
INFO flwr 2024-04-30 23:24:54,644 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:24:54,644 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:25:05,047 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:25:06,231 | server.py:125 | fit progress: (7, 1.659149169921875, {'accuracy': 0.8032, 'data_size': 10000}, 101.08985833899351)
INFO flwr 2024-04-30 23:25:06,231 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:25:06,232 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:25:17,201 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:25:18,520 | server.py:125 | fit progress: (8, 1.6467353105545044, {'accuracy': 0.8151, 'data_size': 10000}, 113.37923113600118)
INFO flwr 2024-04-30 23:25:18,520 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:25:18,521 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:25:27,866 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:25:29,239 | server.py:125 | fit progress: (9, 1.643799066543579, {'accuracy': 0.8174, 'data_size': 10000}, 124.09801851498196)
INFO flwr 2024-04-30 23:25:29,239 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:25:29,239 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:25:38,074 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:25:39,424 | server.py:125 | fit progress: (10, 1.6417003870010376, {'accuracy': 0.8184, 'data_size': 10000}, 134.28322461998323)
INFO flwr 2024-04-30 23:25:39,424 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:25:39,425 | server.py:153 | FL finished in 134.28359946899582
INFO flwr 2024-04-30 23:25:39,425 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:25:39,425 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:25:39,425 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:25:39,425 | app.py:229 | app_fit: losses_centralized [(0, 2.3072807788848877), (1, 1.8184349536895752), (2, 1.7710998058319092), (3, 1.6937849521636963), (4, 1.678381085395813), (5, 1.6633548736572266), (6, 1.6555384397506714), (7, 1.659149169921875), (8, 1.6467353105545044), (9, 1.643799066543579), (10, 1.6417003870010376)]
INFO flwr 2024-04-30 23:25:39,425 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0542), (1, 0.6875), (2, 0.7014), (3, 0.7731), (4, 0.7849), (5, 0.7987), (6, 0.8064), (7, 0.8032), (8, 0.8151), (9, 0.8174), (10, 0.8184)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8184
wandb:     loss 1.6417
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_232302-lwpo356k
wandb: Find logs at: ./wandb/offline-run-20240430_232302-lwpo356k/logs
INFO flwr 2024-04-30 23:25:43,015 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:25:43,694 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3086615)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3086615)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:25:49,633	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:25:49,790	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:25:49,917	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:25:49,918	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:26:00,477 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 43039990580.0, 'object_store_memory': 21519995289.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 23:26:00,477 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:26:00,477 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:26:00,499 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:26:00,500 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:26:00,500 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:26:00,500 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:26:04,636 | server.py:94 | initial parameters (loss, other metrics): 2.3005738258361816, {'accuracy': 0.1305, 'data_size': 10000}
INFO flwr 2024-04-30 23:26:04,636 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:26:04,637 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3153053)[0m 2024-04-30 23:26:07.981722: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3153053)[0m 2024-04-30 23:26:08.078178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3153053)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3153053)[0m 2024-04-30 23:26:11.541501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3153059)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3153059)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3153057)[0m 2024-04-30 23:26:08.251212: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3153051)[0m 2024-04-30 23:26:08.385152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3153051)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3153051)[0m 2024-04-30 23:26:11.536297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:26:32,623 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:26:33,875 | server.py:125 | fit progress: (1, 2.0359325408935547, {'accuracy': 0.4253, 'data_size': 10000}, 29.238233997952193)
INFO flwr 2024-04-30 23:26:33,875 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:26:33,875 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:26:45,322 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:26:47,292 | server.py:125 | fit progress: (2, 2.0676913261413574, {'accuracy': 0.3858, 'data_size': 10000}, 42.65498369297711)
INFO flwr 2024-04-30 23:26:47,292 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:26:47,292 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:26:57,257 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:26:59,297 | server.py:125 | fit progress: (3, 1.9690675735473633, {'accuracy': 0.4852, 'data_size': 10000}, 54.66006136697251)
INFO flwr 2024-04-30 23:26:59,297 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:26:59,297 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:27:08,705 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:27:10,281 | server.py:125 | fit progress: (4, 1.9302433729171753, {'accuracy': 0.5266, 'data_size': 10000}, 65.64410163398134)
INFO flwr 2024-04-30 23:27:10,281 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:27:10,281 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:27:20,331 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:27:21,914 | server.py:125 | fit progress: (5, 1.8949857950210571, {'accuracy': 0.562, 'data_size': 10000}, 77.27702915295959)
INFO flwr 2024-04-30 23:27:21,914 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:27:21,914 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:27:32,157 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:27:34,058 | server.py:125 | fit progress: (6, 1.9173130989074707, {'accuracy': 0.5392, 'data_size': 10000}, 89.4214772330015)
INFO flwr 2024-04-30 23:27:34,058 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:27:34,059 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:27:44,468 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:27:46,181 | server.py:125 | fit progress: (7, 1.905709147453308, {'accuracy': 0.5497, 'data_size': 10000}, 101.54425740195438)
INFO flwr 2024-04-30 23:27:46,181 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:27:46,182 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:27:56,008 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:27:57,948 | server.py:125 | fit progress: (8, 1.8779033422470093, {'accuracy': 0.5793, 'data_size': 10000}, 113.31160806899425)
INFO flwr 2024-04-30 23:27:57,949 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:27:57,949 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:28:07,686 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:28:09,403 | server.py:125 | fit progress: (9, 1.85735023021698, {'accuracy': 0.5998, 'data_size': 10000}, 124.7660142009845)
INFO flwr 2024-04-30 23:28:09,403 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:28:09,403 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:28:19,424 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:28:21,969 | server.py:125 | fit progress: (10, 1.8556058406829834, {'accuracy': 0.6016, 'data_size': 10000}, 137.33221527497517)
INFO flwr 2024-04-30 23:28:21,969 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:28:21,969 | server.py:153 | FL finished in 137.33277841296513
INFO flwr 2024-04-30 23:28:21,970 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:28:21,970 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:28:21,970 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:28:21,970 | app.py:229 | app_fit: losses_centralized [(0, 2.3005738258361816), (1, 2.0359325408935547), (2, 2.0676913261413574), (3, 1.9690675735473633), (4, 1.9302433729171753), (5, 1.8949857950210571), (6, 1.9173130989074707), (7, 1.905709147453308), (8, 1.8779033422470093), (9, 1.85735023021698), (10, 1.8556058406829834)]
INFO flwr 2024-04-30 23:28:21,970 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1305), (1, 0.4253), (2, 0.3858), (3, 0.4852), (4, 0.5266), (5, 0.562), (6, 0.5392), (7, 0.5497), (8, 0.5793), (9, 0.5998), (10, 0.6016)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6016
wandb:     loss 1.85561
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_232543-mb6hh9nq
wandb: Find logs at: ./wandb/offline-run-20240430_232543-mb6hh9nq/logs
INFO flwr 2024-04-30 23:28:25,857 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:28:26,883 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3153053)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3153053)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:28:32,340	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:28:32,438	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:28:32,602	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:28:32,603	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:28:44,879 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 48074777396.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 24037388697.0}
INFO flwr 2024-04-30 23:28:44,879 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:28:44,879 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:28:44,903 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:28:44,905 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:28:44,905 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:28:44,905 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:28:48,592 | server.py:94 | initial parameters (loss, other metrics): 2.3013083934783936, {'accuracy': 0.0975, 'data_size': 10000}
INFO flwr 2024-04-30 23:28:48,593 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:28:48,593 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3218872)[0m 2024-04-30 23:28:53.500781: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3218872)[0m 2024-04-30 23:28:53.622961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3218872)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3218872)[0m 2024-04-30 23:28:57.892502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3218870)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3218870)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3218876)[0m 2024-04-30 23:28:54.145728: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3218876)[0m 2024-04-30 23:28:54.265121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3218876)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3218876)[0m 2024-04-30 23:28:58.204480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:29:21,494 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:29:23,171 | server.py:125 | fit progress: (1, 1.8063141107559204, {'accuracy': 0.7159, 'data_size': 10000}, 34.57819285301957)
INFO flwr 2024-04-30 23:29:23,171 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:29:23,172 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:29:33,869 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:29:35,629 | server.py:125 | fit progress: (2, 1.7018177509307861, {'accuracy': 0.7766, 'data_size': 10000}, 47.03604724799516)
INFO flwr 2024-04-30 23:29:35,629 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:29:35,629 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:29:46,062 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:29:48,226 | server.py:125 | fit progress: (3, 1.720807433128357, {'accuracy': 0.7469, 'data_size': 10000}, 59.633369838004)
INFO flwr 2024-04-30 23:29:48,227 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:29:48,227 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:29:58,047 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:29:59,609 | server.py:125 | fit progress: (4, 1.6054632663726807, {'accuracy': 0.8608, 'data_size': 10000}, 71.01596011198126)
INFO flwr 2024-04-30 23:29:59,609 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:29:59,609 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:30:09,736 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:30:11,324 | server.py:125 | fit progress: (5, 1.6285734176635742, {'accuracy': 0.8348, 'data_size': 10000}, 82.7308994490304)
INFO flwr 2024-04-30 23:30:11,324 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:30:11,324 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:30:21,577 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:30:22,817 | server.py:125 | fit progress: (6, 1.6148273944854736, {'accuracy': 0.8478, 'data_size': 10000}, 94.22429190500407)
INFO flwr 2024-04-30 23:30:22,818 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:30:22,818 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:30:32,874 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:30:34,088 | server.py:125 | fit progress: (7, 1.5881880521774292, {'accuracy': 0.8748, 'data_size': 10000}, 105.49515580700245)
INFO flwr 2024-04-30 23:30:34,088 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:30:34,089 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:30:43,389 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:30:44,712 | server.py:125 | fit progress: (8, 1.5679593086242676, {'accuracy': 0.8939, 'data_size': 10000}, 116.11858313699486)
INFO flwr 2024-04-30 23:30:44,712 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:30:44,712 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:30:54,344 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:30:55,702 | server.py:125 | fit progress: (9, 1.5808157920837402, {'accuracy': 0.8824, 'data_size': 10000}, 127.1087638370227)
INFO flwr 2024-04-30 23:30:55,702 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:30:55,702 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:31:05,287 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:31:07,170 | server.py:125 | fit progress: (10, 1.5875306129455566, {'accuracy': 0.8742, 'data_size': 10000}, 138.57743343600305)
INFO flwr 2024-04-30 23:31:07,171 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:31:07,171 | server.py:153 | FL finished in 138.57785437902203
INFO flwr 2024-04-30 23:31:07,171 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:31:07,171 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:31:07,171 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:31:07,171 | app.py:229 | app_fit: losses_centralized [(0, 2.3013083934783936), (1, 1.8063141107559204), (2, 1.7018177509307861), (3, 1.720807433128357), (4, 1.6054632663726807), (5, 1.6285734176635742), (6, 1.6148273944854736), (7, 1.5881880521774292), (8, 1.5679593086242676), (9, 1.5808157920837402), (10, 1.5875306129455566)]
INFO flwr 2024-04-30 23:31:07,171 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0975), (1, 0.7159), (2, 0.7766), (3, 0.7469), (4, 0.8608), (5, 0.8348), (6, 0.8478), (7, 0.8748), (8, 0.8939), (9, 0.8824), (10, 0.8742)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8742
wandb:     loss 1.58753
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_232826-sm7xt5i5
wandb: Find logs at: ./wandb/offline-run-20240430_232826-sm7xt5i5/logs
INFO flwr 2024-04-30 23:31:10,678 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:31:11,503 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3218864)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3218864)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:31:16,821	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:31:16,957	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:31:17,098	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:31:17,100	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:31:28,815 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 57094742016.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 28547371008.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 23:31:28,815 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:31:28,816 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:31:28,835 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:31:28,836 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:31:28,837 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:31:28,837 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:31:32,147 | server.py:94 | initial parameters (loss, other metrics): 2.304145574569702, {'accuracy': 0.0875, 'data_size': 10000}
INFO flwr 2024-04-30 23:31:32,148 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:31:32,148 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3286356)[0m 2024-04-30 23:31:37.260162: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3286356)[0m 2024-04-30 23:31:37.351966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3286356)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3286370)[0m 2024-04-30 23:31:41.782756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3286363)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3286363)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3286369)[0m 2024-04-30 23:31:38.048667: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3286369)[0m 2024-04-30 23:31:38.287183: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3286369)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3286369)[0m 2024-04-30 23:31:42.219224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:32:02,914 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:32:04,870 | server.py:125 | fit progress: (1, 1.8963922262191772, {'accuracy': 0.5885, 'data_size': 10000}, 32.72162908699829)
INFO flwr 2024-04-30 23:32:04,870 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:32:04,870 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:32:15,062 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:32:16,142 | server.py:125 | fit progress: (2, 1.6717073917388916, {'accuracy': 0.809, 'data_size': 10000}, 43.993776672985405)
INFO flwr 2024-04-30 23:32:16,142 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:32:16,142 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:32:25,471 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:32:26,723 | server.py:125 | fit progress: (3, 1.6369335651397705, {'accuracy': 0.8293, 'data_size': 10000}, 54.57493524096208)
INFO flwr 2024-04-30 23:32:26,723 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:32:26,723 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:32:36,120 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:32:38,280 | server.py:125 | fit progress: (4, 1.601149320602417, {'accuracy': 0.8635, 'data_size': 10000}, 66.13213660399197)
INFO flwr 2024-04-30 23:32:38,280 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:32:38,281 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:32:47,760 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:32:49,455 | server.py:125 | fit progress: (5, 1.5889393091201782, {'accuracy': 0.8741, 'data_size': 10000}, 77.30746688198997)
INFO flwr 2024-04-30 23:32:49,456 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:32:49,456 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:33:00,011 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:33:01,796 | server.py:125 | fit progress: (6, 1.5784220695495605, {'accuracy': 0.8849, 'data_size': 10000}, 89.64855065499432)
INFO flwr 2024-04-30 23:33:01,797 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:33:01,797 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:33:11,710 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:33:13,162 | server.py:125 | fit progress: (7, 1.6074321269989014, {'accuracy': 0.8549, 'data_size': 10000}, 101.01384416298242)
INFO flwr 2024-04-30 23:33:13,162 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:33:13,162 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:33:22,835 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:33:24,277 | server.py:125 | fit progress: (8, 1.5683834552764893, {'accuracy': 0.8937, 'data_size': 10000}, 112.12911582301604)
INFO flwr 2024-04-30 23:33:24,277 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:33:24,278 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:33:34,279 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:33:35,671 | server.py:125 | fit progress: (9, 1.5677918195724487, {'accuracy': 0.8953, 'data_size': 10000}, 123.52306422201218)
INFO flwr 2024-04-30 23:33:35,671 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:33:35,671 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:33:46,039 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:33:48,181 | server.py:125 | fit progress: (10, 1.5749330520629883, {'accuracy': 0.8866, 'data_size': 10000}, 136.03338628896745)
INFO flwr 2024-04-30 23:33:48,182 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:33:48,182 | server.py:153 | FL finished in 136.03390295401914
INFO flwr 2024-04-30 23:33:48,182 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:33:48,182 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:33:48,182 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:33:48,182 | app.py:229 | app_fit: losses_centralized [(0, 2.304145574569702), (1, 1.8963922262191772), (2, 1.6717073917388916), (3, 1.6369335651397705), (4, 1.601149320602417), (5, 1.5889393091201782), (6, 1.5784220695495605), (7, 1.6074321269989014), (8, 1.5683834552764893), (9, 1.5677918195724487), (10, 1.5749330520629883)]
INFO flwr 2024-04-30 23:33:48,183 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0875), (1, 0.5885), (2, 0.809), (3, 0.8293), (4, 0.8635), (5, 0.8741), (6, 0.8849), (7, 0.8549), (8, 0.8937), (9, 0.8953), (10, 0.8866)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8866
wandb:     loss 1.57493
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_233110-zf50zyl2
wandb: Find logs at: ./wandb/offline-run-20240430_233110-zf50zyl2/logs
INFO flwr 2024-04-30 23:33:51,967 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:33:52,674 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3286367)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3286367)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:33:58,489	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:33:58,658	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:33:58,821	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:33:58,823	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:34:10,519 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 142034226176.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65157525504.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-30 23:34:10,519 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:34:10,519 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:34:10,535 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:34:10,536 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:34:10,536 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:34:10,537 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:34:13,227 | server.py:94 | initial parameters (loss, other metrics): 2.3045389652252197, {'accuracy': 0.0831, 'data_size': 10000}
INFO flwr 2024-04-30 23:34:13,228 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:34:13,228 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3353639)[0m 2024-04-30 23:34:18.866345: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3353636)[0m 2024-04-30 23:34:19.015396: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3353636)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3353632)[0m 2024-04-30 23:34:23.054255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=3353640)[0m 2024-04-30 23:34:20.009581: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3353640)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3353640)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3353640)[0m 2024-04-30 23:34:20.110552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3353640)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3353635)[0m 2024-04-30 23:34:24.049859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:34:44,695 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:34:45,904 | server.py:125 | fit progress: (1, 1.988708257675171, {'accuracy': 0.4845, 'data_size': 10000}, 32.67610819503898)
INFO flwr 2024-04-30 23:34:45,904 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:34:45,905 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:34:56,993 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:34:58,079 | server.py:125 | fit progress: (2, 1.929580807685852, {'accuracy': 0.5229, 'data_size': 10000}, 44.8511112020351)
INFO flwr 2024-04-30 23:34:58,079 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:34:58,079 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:35:07,334 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:35:08,601 | server.py:125 | fit progress: (3, 1.7963720560073853, {'accuracy': 0.6664, 'data_size': 10000}, 55.37331617705058)
INFO flwr 2024-04-30 23:35:08,601 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:35:08,602 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:35:17,533 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:35:18,801 | server.py:125 | fit progress: (4, 1.7892025709152222, {'accuracy': 0.6722, 'data_size': 10000}, 65.57296073599719)
INFO flwr 2024-04-30 23:35:18,801 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:35:18,801 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:35:29,148 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:35:31,149 | server.py:125 | fit progress: (5, 1.7813355922698975, {'accuracy': 0.6805, 'data_size': 10000}, 77.92091821704526)
INFO flwr 2024-04-30 23:35:31,149 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:35:31,149 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:35:41,592 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:35:43,285 | server.py:125 | fit progress: (6, 1.765854835510254, {'accuracy': 0.6947, 'data_size': 10000}, 90.05714874103433)
INFO flwr 2024-04-30 23:35:43,285 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:35:43,286 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:35:53,923 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:35:55,681 | server.py:125 | fit progress: (7, 1.7460668087005615, {'accuracy': 0.7137, 'data_size': 10000}, 102.45290661900071)
INFO flwr 2024-04-30 23:35:55,681 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:35:55,681 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:36:05,913 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:36:07,528 | server.py:125 | fit progress: (8, 1.7411677837371826, {'accuracy': 0.7195, 'data_size': 10000}, 114.29972118203295)
INFO flwr 2024-04-30 23:36:07,528 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:36:07,528 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:36:17,912 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:36:19,373 | server.py:125 | fit progress: (9, 1.7340271472930908, {'accuracy': 0.7264, 'data_size': 10000}, 126.14552401704714)
INFO flwr 2024-04-30 23:36:19,374 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:36:19,374 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:36:30,373 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:36:32,526 | server.py:125 | fit progress: (10, 1.7408027648925781, {'accuracy': 0.7191, 'data_size': 10000}, 139.29802887601545)
INFO flwr 2024-04-30 23:36:32,527 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:36:32,527 | server.py:153 | FL finished in 139.2989446450374
INFO flwr 2024-04-30 23:36:32,527 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:36:32,527 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:36:32,528 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:36:32,528 | app.py:229 | app_fit: losses_centralized [(0, 2.3045389652252197), (1, 1.988708257675171), (2, 1.929580807685852), (3, 1.7963720560073853), (4, 1.7892025709152222), (5, 1.7813355922698975), (6, 1.765854835510254), (7, 1.7460668087005615), (8, 1.7411677837371826), (9, 1.7340271472930908), (10, 1.7408027648925781)]
INFO flwr 2024-04-30 23:36:32,528 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0831), (1, 0.4845), (2, 0.5229), (3, 0.6664), (4, 0.6722), (5, 0.6805), (6, 0.6947), (7, 0.7137), (8, 0.7195), (9, 0.7264), (10, 0.7191)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7191
wandb:     loss 1.7408
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_233352-jadsun2y
wandb: Find logs at: ./wandb/offline-run-20240430_233352-jadsun2y/logs
INFO flwr 2024-04-30 23:36:36,120 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:36:37,113 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3353637)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3353637)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:36:43,616	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:36:43,729	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:36:43,908	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:36:43,910	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:36:55,633 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61724778086.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 134024482202.0}
INFO flwr 2024-04-30 23:36:55,634 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:36:55,634 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:36:55,658 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:36:55,659 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:36:55,659 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:36:55,659 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:36:59,520 | server.py:94 | initial parameters (loss, other metrics): 2.2993786334991455, {'accuracy': 0.1101, 'data_size': 10000}
INFO flwr 2024-04-30 23:36:59,520 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:36:59,520 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3415302)[0m 2024-04-30 23:37:04.542064: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3415302)[0m 2024-04-30 23:37:04.672825: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3415302)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3415303)[0m 2024-04-30 23:37:09.289010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=3415305)[0m 2024-04-30 23:37:05.289485: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3415303)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3415303)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3415301)[0m 2024-04-30 23:37:05.469875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3415301)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3415305)[0m 2024-04-30 23:37:09.571012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:37:29,417 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:37:30,933 | server.py:125 | fit progress: (1, 1.8588491678237915, {'accuracy': 0.6516, 'data_size': 10000}, 31.412582740013022)
INFO flwr 2024-04-30 23:37:30,933 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:37:30,934 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:37:41,242 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:37:42,423 | server.py:125 | fit progress: (2, 1.6685785055160522, {'accuracy': 0.8111, 'data_size': 10000}, 42.902822825999465)
INFO flwr 2024-04-30 23:37:42,423 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:37:42,424 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:37:52,023 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:37:53,958 | server.py:125 | fit progress: (3, 1.6396150588989258, {'accuracy': 0.8318, 'data_size': 10000}, 54.437928643019404)
INFO flwr 2024-04-30 23:37:53,958 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:37:53,959 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:38:03,602 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:38:04,971 | server.py:125 | fit progress: (4, 1.5925689935684204, {'accuracy': 0.8754, 'data_size': 10000}, 65.45053079898935)
INFO flwr 2024-04-30 23:38:04,971 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:38:04,971 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:38:14,588 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:38:16,607 | server.py:125 | fit progress: (5, 1.612158179283142, {'accuracy': 0.8524, 'data_size': 10000}, 77.08715947001474)
INFO flwr 2024-04-30 23:38:16,608 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:38:16,608 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:38:27,033 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:38:28,886 | server.py:125 | fit progress: (6, 1.5825543403625488, {'accuracy': 0.8799, 'data_size': 10000}, 89.36527758999728)
INFO flwr 2024-04-30 23:38:28,886 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:38:28,886 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:38:39,641 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:38:41,573 | server.py:125 | fit progress: (7, 1.5692094564437866, {'accuracy': 0.8949, 'data_size': 10000}, 102.0522865470266)
INFO flwr 2024-04-30 23:38:41,573 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:38:41,573 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:38:51,868 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:38:54,448 | server.py:125 | fit progress: (8, 1.5689500570297241, {'accuracy': 0.8955, 'data_size': 10000}, 114.92781759198988)
INFO flwr 2024-04-30 23:38:54,449 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:38:54,449 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:39:04,832 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:39:06,109 | server.py:125 | fit progress: (9, 1.5633050203323364, {'accuracy': 0.8983, 'data_size': 10000}, 126.58841880102409)
INFO flwr 2024-04-30 23:39:06,109 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:39:06,109 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:39:17,479 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:39:18,982 | server.py:125 | fit progress: (10, 1.5639861822128296, {'accuracy': 0.8983, 'data_size': 10000}, 139.46128356299596)
INFO flwr 2024-04-30 23:39:18,982 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:39:18,982 | server.py:153 | FL finished in 139.46176024101442
INFO flwr 2024-04-30 23:39:18,982 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:39:18,982 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:39:18,982 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:39:18,983 | app.py:229 | app_fit: losses_centralized [(0, 2.2993786334991455), (1, 1.8588491678237915), (2, 1.6685785055160522), (3, 1.6396150588989258), (4, 1.5925689935684204), (5, 1.612158179283142), (6, 1.5825543403625488), (7, 1.5692094564437866), (8, 1.5689500570297241), (9, 1.5633050203323364), (10, 1.5639861822128296)]
INFO flwr 2024-04-30 23:39:18,983 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1101), (1, 0.6516), (2, 0.8111), (3, 0.8318), (4, 0.8754), (5, 0.8524), (6, 0.8799), (7, 0.8949), (8, 0.8955), (9, 0.8983), (10, 0.8983)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8983
wandb:     loss 1.56399
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_233636-2dr4jgcf
wandb: Find logs at: ./wandb/offline-run-20240430_233636-2dr4jgcf/logs
INFO flwr 2024-04-30 23:39:22,477 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:39:23,506 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3415282)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3415282)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:39:29,043	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:39:29,194	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:39:29,343	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:39:29,344	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:39:40,739 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 129147544167.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 59634661785.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-30 23:39:40,740 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:39:40,740 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:39:40,767 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:39:40,769 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:39:40,769 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:39:40,769 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:39:44,385 | server.py:94 | initial parameters (loss, other metrics): 2.29986310005188, {'accuracy': 0.1558, 'data_size': 10000}
INFO flwr 2024-04-30 23:39:44,385 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:39:44,386 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3476044)[0m 2024-04-30 23:39:48.700626: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3476044)[0m 2024-04-30 23:39:48.804358: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3476044)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3476044)[0m 2024-04-30 23:39:52.932246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3476048)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3476048)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3476046)[0m 2024-04-30 23:39:49.353708: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3476036)[0m 2024-04-30 23:39:49.581065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3476036)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3476046)[0m 2024-04-30 23:39:53.034901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:40:12,410 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:40:13,581 | server.py:125 | fit progress: (1, 1.7987937927246094, {'accuracy': 0.7387, 'data_size': 10000}, 29.195537842984777)
INFO flwr 2024-04-30 23:40:13,581 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:40:13,582 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:40:23,857 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:40:25,572 | server.py:125 | fit progress: (2, 1.6370577812194824, {'accuracy': 0.8484, 'data_size': 10000}, 41.18659518199274)
INFO flwr 2024-04-30 23:40:25,573 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:40:25,573 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:40:36,665 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:40:38,734 | server.py:125 | fit progress: (3, 1.596596360206604, {'accuracy': 0.8751, 'data_size': 10000}, 54.34875153697794)
INFO flwr 2024-04-30 23:40:38,735 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:40:38,735 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:40:49,309 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:40:51,479 | server.py:125 | fit progress: (4, 1.5815632343292236, {'accuracy': 0.8866, 'data_size': 10000}, 67.09337142598815)
INFO flwr 2024-04-30 23:40:51,479 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:40:51,480 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:41:02,040 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:41:04,214 | server.py:125 | fit progress: (5, 1.5763344764709473, {'accuracy': 0.8882, 'data_size': 10000}, 79.82794429600472)
INFO flwr 2024-04-30 23:41:04,214 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:41:04,214 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:41:14,458 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:41:15,968 | server.py:125 | fit progress: (6, 1.5643523931503296, {'accuracy': 0.8998, 'data_size': 10000}, 91.58229263598332)
INFO flwr 2024-04-30 23:41:15,968 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:41:15,969 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:41:26,509 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:41:27,712 | server.py:125 | fit progress: (7, 1.5702444314956665, {'accuracy': 0.8934, 'data_size': 10000}, 103.3258299579611)
INFO flwr 2024-04-30 23:41:27,712 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:41:27,712 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:41:38,055 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:41:39,452 | server.py:125 | fit progress: (8, 1.566865086555481, {'accuracy': 0.8953, 'data_size': 10000}, 115.06632819696097)
INFO flwr 2024-04-30 23:41:39,452 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:41:39,452 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:41:50,324 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:41:51,719 | server.py:125 | fit progress: (9, 1.5572187900543213, {'accuracy': 0.905, 'data_size': 10000}, 127.33378539601108)
INFO flwr 2024-04-30 23:41:51,720 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:41:51,720 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:42:01,368 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:42:03,781 | server.py:125 | fit progress: (10, 1.565179467201233, {'accuracy': 0.897, 'data_size': 10000}, 139.39499652496306)
INFO flwr 2024-04-30 23:42:03,781 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:42:03,781 | server.py:153 | FL finished in 139.3956998619833
INFO flwr 2024-04-30 23:42:03,782 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:42:03,782 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:42:03,782 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:42:03,782 | app.py:229 | app_fit: losses_centralized [(0, 2.29986310005188), (1, 1.7987937927246094), (2, 1.6370577812194824), (3, 1.596596360206604), (4, 1.5815632343292236), (5, 1.5763344764709473), (6, 1.5643523931503296), (7, 1.5702444314956665), (8, 1.566865086555481), (9, 1.5572187900543213), (10, 1.565179467201233)]
INFO flwr 2024-04-30 23:42:03,782 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1558), (1, 0.7387), (2, 0.8484), (3, 0.8751), (4, 0.8866), (5, 0.8882), (6, 0.8998), (7, 0.8934), (8, 0.8953), (9, 0.905), (10, 0.897)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.897
wandb:     loss 1.56518
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_233922-egx0le09
wandb: Find logs at: ./wandb/offline-run-20240430_233922-egx0le09/logs
INFO flwr 2024-04-30 23:42:07,777 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:42:08,746 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3476036)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3476036)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:42:13,998	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:42:14,177	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:42:14,305	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:42:14,306	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:42:25,183 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 140183419904.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 64364322816.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 23:42:25,183 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:42:25,183 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:42:25,207 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:42:25,208 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:42:25,208 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:42:25,208 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:42:28,354 | server.py:94 | initial parameters (loss, other metrics): 2.304189682006836, {'accuracy': 0.105, 'data_size': 10000}
INFO flwr 2024-04-30 23:42:28,355 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:42:28,355 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3533691)[0m 2024-04-30 23:42:33.305642: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3533691)[0m 2024-04-30 23:42:33.419041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3533691)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3533691)[0m 2024-04-30 23:42:37.448063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3533702)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3533702)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3533701)[0m 2024-04-30 23:42:33.808923: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3533694)[0m 2024-04-30 23:42:34.042118: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3533694)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3533702)[0m 2024-04-30 23:42:37.547752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:43:01,482 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:43:02,546 | server.py:125 | fit progress: (1, 1.9381300210952759, {'accuracy': 0.5617, 'data_size': 10000}, 34.19166411500191)
INFO flwr 2024-04-30 23:43:02,547 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:43:02,547 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:43:15,655 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:43:16,889 | server.py:125 | fit progress: (2, 1.7836339473724365, {'accuracy': 0.6812, 'data_size': 10000}, 48.53436571499333)
INFO flwr 2024-04-30 23:43:16,890 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:43:16,890 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:43:29,075 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:43:31,183 | server.py:125 | fit progress: (3, 1.712556004524231, {'accuracy': 0.7519, 'data_size': 10000}, 62.8282191180042)
INFO flwr 2024-04-30 23:43:31,183 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:43:31,184 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:43:41,609 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:43:43,001 | server.py:125 | fit progress: (4, 1.687882661819458, {'accuracy': 0.773, 'data_size': 10000}, 74.64585963601712)
INFO flwr 2024-04-30 23:43:43,001 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:43:43,001 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:43:54,011 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:43:56,229 | server.py:125 | fit progress: (5, 1.6365735530853271, {'accuracy': 0.8284, 'data_size': 10000}, 87.87379207601771)
INFO flwr 2024-04-30 23:43:56,229 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:43:56,229 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:44:07,313 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:44:09,070 | server.py:125 | fit progress: (6, 1.6240533590316772, {'accuracy': 0.8395, 'data_size': 10000}, 100.71565020200796)
INFO flwr 2024-04-30 23:44:09,071 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:44:09,071 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:44:20,501 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:44:22,215 | server.py:125 | fit progress: (7, 1.60867178440094, {'accuracy': 0.8523, 'data_size': 10000}, 113.85970341600478)
INFO flwr 2024-04-30 23:44:22,215 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:44:22,215 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:44:34,408 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:44:36,417 | server.py:125 | fit progress: (8, 1.5970505475997925, {'accuracy': 0.8655, 'data_size': 10000}, 128.06227663799655)
INFO flwr 2024-04-30 23:44:36,417 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:44:36,418 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:44:47,236 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:44:48,697 | server.py:125 | fit progress: (9, 1.6025625467300415, {'accuracy': 0.8586, 'data_size': 10000}, 140.34194376401138)
INFO flwr 2024-04-30 23:44:48,697 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:44:48,697 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:44:58,359 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:44:59,628 | server.py:125 | fit progress: (10, 1.6038713455200195, {'accuracy': 0.8569, 'data_size': 10000}, 151.2727103990037)
INFO flwr 2024-04-30 23:44:59,628 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:44:59,628 | server.py:153 | FL finished in 151.27308830199763
INFO flwr 2024-04-30 23:44:59,628 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:44:59,628 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:44:59,628 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:44:59,628 | app.py:229 | app_fit: losses_centralized [(0, 2.304189682006836), (1, 1.9381300210952759), (2, 1.7836339473724365), (3, 1.712556004524231), (4, 1.687882661819458), (5, 1.6365735530853271), (6, 1.6240533590316772), (7, 1.60867178440094), (8, 1.5970505475997925), (9, 1.6025625467300415), (10, 1.6038713455200195)]
INFO flwr 2024-04-30 23:44:59,628 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.105), (1, 0.5617), (2, 0.6812), (3, 0.7519), (4, 0.773), (5, 0.8284), (6, 0.8395), (7, 0.8523), (8, 0.8655), (9, 0.8586), (10, 0.8569)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8569
wandb:     loss 1.60387
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_234208-2kfmyst3
wandb: Find logs at: ./wandb/offline-run-20240430_234208-2kfmyst3/logs
INFO flwr 2024-04-30 23:45:03,098 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:45:03,896 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3533697)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3533697)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:45:09,305	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:45:09,460	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:45:09,593	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:45:09,595	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:45:20,897 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 65161702195.0, 'node:10.20.240.18': 1.0, 'memory': 142043971789.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 23:45:20,898 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:45:20,898 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:45:20,938 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:45:20,940 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:45:20,941 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:45:20,941 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:45:24,654 | server.py:94 | initial parameters (loss, other metrics): 2.3055524826049805, {'accuracy': 0.0545, 'data_size': 10000}
INFO flwr 2024-04-30 23:45:24,654 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:45:24,654 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3594661)[0m 2024-04-30 23:45:29.937041: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3594661)[0m 2024-04-30 23:45:30.029092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3594661)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3594664)[0m 2024-04-30 23:45:34.564708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=3594657)[0m 2024-04-30 23:45:30.653792: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3594671)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3594671)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3594680)[0m 2024-04-30 23:45:30.952888: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3594680)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3594680)[0m 2024-04-30 23:45:35.059164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:46:00,024 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:46:01,246 | server.py:125 | fit progress: (1, 1.8796570301055908, {'accuracy': 0.6434, 'data_size': 10000}, 36.591349983995315)
INFO flwr 2024-04-30 23:46:01,246 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:46:01,246 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:46:14,537 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:46:15,620 | server.py:125 | fit progress: (2, 1.662191390991211, {'accuracy': 0.8126, 'data_size': 10000}, 50.96560417203)
INFO flwr 2024-04-30 23:46:15,620 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:46:15,620 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:46:28,397 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:46:31,129 | server.py:125 | fit progress: (3, 1.591089129447937, {'accuracy': 0.8821, 'data_size': 10000}, 66.47512346901931)
INFO flwr 2024-04-30 23:46:31,130 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:46:31,130 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:46:42,772 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:46:44,864 | server.py:125 | fit progress: (4, 1.5841283798217773, {'accuracy': 0.8827, 'data_size': 10000}, 80.20977142301854)
INFO flwr 2024-04-30 23:46:44,864 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:46:44,865 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:46:56,330 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:46:58,471 | server.py:125 | fit progress: (5, 1.5782405138015747, {'accuracy': 0.8869, 'data_size': 10000}, 93.81697547103977)
INFO flwr 2024-04-30 23:46:58,472 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:46:58,472 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:47:09,170 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:47:10,922 | server.py:125 | fit progress: (6, 1.571850299835205, {'accuracy': 0.8918, 'data_size': 10000}, 106.26763425004901)
INFO flwr 2024-04-30 23:47:10,922 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:47:10,923 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:47:22,222 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:47:23,615 | server.py:125 | fit progress: (7, 1.5624428987503052, {'accuracy': 0.9002, 'data_size': 10000}, 118.96082791202934)
INFO flwr 2024-04-30 23:47:23,615 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:47:23,616 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:47:33,908 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:47:35,923 | server.py:125 | fit progress: (8, 1.557564616203308, {'accuracy': 0.9042, 'data_size': 10000}, 131.2689961339929)
INFO flwr 2024-04-30 23:47:35,924 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:47:35,924 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:47:46,262 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:47:47,552 | server.py:125 | fit progress: (9, 1.5640207529067993, {'accuracy': 0.8987, 'data_size': 10000}, 142.89737548900302)
INFO flwr 2024-04-30 23:47:47,552 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:47:47,552 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:47:58,070 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:47:59,351 | server.py:125 | fit progress: (10, 1.55265474319458, {'accuracy': 0.9088, 'data_size': 10000}, 154.69708674703725)
INFO flwr 2024-04-30 23:47:59,352 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:47:59,352 | server.py:153 | FL finished in 154.697443897021
INFO flwr 2024-04-30 23:47:59,352 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:47:59,352 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:47:59,352 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:47:59,352 | app.py:229 | app_fit: losses_centralized [(0, 2.3055524826049805), (1, 1.8796570301055908), (2, 1.662191390991211), (3, 1.591089129447937), (4, 1.5841283798217773), (5, 1.5782405138015747), (6, 1.571850299835205), (7, 1.5624428987503052), (8, 1.557564616203308), (9, 1.5640207529067993), (10, 1.55265474319458)]
INFO flwr 2024-04-30 23:47:59,352 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0545), (1, 0.6434), (2, 0.8126), (3, 0.8821), (4, 0.8827), (5, 0.8869), (6, 0.8918), (7, 0.9002), (8, 0.9042), (9, 0.8987), (10, 0.9088)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9088
wandb:     loss 1.55265
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_234503-8hf5pgqd
wandb: Find logs at: ./wandb/offline-run-20240430_234503-8hf5pgqd/logs
INFO flwr 2024-04-30 23:48:02,820 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:48:03,660 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3594663)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3594663)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:48:09,853	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:48:09,988	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:48:10,129	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:48:10,130	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:48:22,937 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 39543885006.0, 'object_store_memory': 19771942502.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 23:48:22,937 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:48:22,938 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:48:22,960 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:48:22,961 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:48:22,962 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:48:22,962 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:48:25,968 | server.py:94 | initial parameters (loss, other metrics): 2.2976372241973877, {'accuracy': 0.1731, 'data_size': 10000}
INFO flwr 2024-04-30 23:48:25,970 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:48:25,970 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3664988)[0m 2024-04-30 23:48:31.326364: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3664988)[0m 2024-04-30 23:48:31.439156: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3664988)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3664985)[0m 2024-04-30 23:48:35.751382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3664974)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3664974)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3664977)[0m 2024-04-30 23:48:32.132559: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3664977)[0m 2024-04-30 23:48:32.283654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3664977)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3664977)[0m 2024-04-30 23:48:36.067289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:49:00,990 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:49:02,090 | server.py:125 | fit progress: (1, 1.8506664037704468, {'accuracy': 0.6622, 'data_size': 10000}, 36.12023912096629)
INFO flwr 2024-04-30 23:49:02,090 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:49:02,091 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:49:14,715 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:49:16,371 | server.py:125 | fit progress: (2, 1.6423909664154053, {'accuracy': 0.843, 'data_size': 10000}, 50.40127824398223)
INFO flwr 2024-04-30 23:49:16,372 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:49:16,372 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:49:29,061 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:49:30,505 | server.py:125 | fit progress: (3, 1.5873292684555054, {'accuracy': 0.8846, 'data_size': 10000}, 64.53487286699237)
INFO flwr 2024-04-30 23:49:30,505 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:49:30,505 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:49:42,856 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:49:45,472 | server.py:125 | fit progress: (4, 1.587769627571106, {'accuracy': 0.879, 'data_size': 10000}, 79.50248531199759)
INFO flwr 2024-04-30 23:49:45,473 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:49:45,473 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:49:57,364 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:50:00,073 | server.py:125 | fit progress: (5, 1.5984679460525513, {'accuracy': 0.8652, 'data_size': 10000}, 94.10282356396783)
INFO flwr 2024-04-30 23:50:00,073 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:50:00,074 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:50:11,108 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:50:12,784 | server.py:125 | fit progress: (6, 1.5766748189926147, {'accuracy': 0.8868, 'data_size': 10000}, 106.81416648300365)
INFO flwr 2024-04-30 23:50:12,784 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:50:12,785 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:50:24,480 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:50:26,301 | server.py:125 | fit progress: (7, 1.5579214096069336, {'accuracy': 0.9067, 'data_size': 10000}, 120.33071131701581)
INFO flwr 2024-04-30 23:50:26,301 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:50:26,301 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:50:37,275 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:50:38,547 | server.py:125 | fit progress: (8, 1.561933159828186, {'accuracy': 0.9012, 'data_size': 10000}, 132.57738790899748)
INFO flwr 2024-04-30 23:50:38,548 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:50:38,548 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:50:49,765 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:50:51,738 | server.py:125 | fit progress: (9, 1.5575729608535767, {'accuracy': 0.9053, 'data_size': 10000}, 145.76758502196753)
INFO flwr 2024-04-30 23:50:51,738 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:50:51,738 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:51:02,000 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:51:03,345 | server.py:125 | fit progress: (10, 1.5529295206069946, {'accuracy': 0.9099, 'data_size': 10000}, 157.3749979819986)
INFO flwr 2024-04-30 23:51:03,345 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:51:03,345 | server.py:153 | FL finished in 157.37542263697833
INFO flwr 2024-04-30 23:51:03,346 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:51:03,346 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:51:03,346 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:51:03,346 | app.py:229 | app_fit: losses_centralized [(0, 2.2976372241973877), (1, 1.8506664037704468), (2, 1.6423909664154053), (3, 1.5873292684555054), (4, 1.587769627571106), (5, 1.5984679460525513), (6, 1.5766748189926147), (7, 1.5579214096069336), (8, 1.561933159828186), (9, 1.5575729608535767), (10, 1.5529295206069946)]
INFO flwr 2024-04-30 23:51:03,346 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1731), (1, 0.6622), (2, 0.843), (3, 0.8846), (4, 0.879), (5, 0.8652), (6, 0.8868), (7, 0.9067), (8, 0.9012), (9, 0.9053), (10, 0.9099)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9099
wandb:     loss 1.55293
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_234803-9vt6m6sg
wandb: Find logs at: ./wandb/offline-run-20240430_234803-9vt6m6sg/logs
INFO flwr 2024-04-30 23:51:07,000 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:51:07,901 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3664988)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3664988)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:51:13,008	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:51:13,153	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:51:13,296	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:51:13,297	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:51:25,358 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 55012977870.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 27506488934.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 23:51:25,359 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:51:25,359 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:51:25,376 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:51:25,377 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:51:25,377 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:51:25,377 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:51:28,510 | server.py:94 | initial parameters (loss, other metrics): 2.3003292083740234, {'accuracy': 0.1099, 'data_size': 10000}
INFO flwr 2024-04-30 23:51:28,510 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:51:28,510 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3737889)[0m 2024-04-30 23:51:34.006876: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3737886)[0m 2024-04-30 23:51:34.142841: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3737886)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3737879)[0m 2024-04-30 23:51:40.000305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=3737882)[0m 2024-04-30 23:51:34.644212: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3737885)[0m 2024-04-30 23:51:34.808557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3737885)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3737882)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3737882)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3737887)[0m 2024-04-30 23:51:40.018384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:52:03,928 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:52:05,830 | server.py:125 | fit progress: (1, 1.8824793100357056, {'accuracy': 0.6092, 'data_size': 10000}, 37.319505456020124)
INFO flwr 2024-04-30 23:52:05,830 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:52:05,830 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:52:20,545 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:52:22,462 | server.py:125 | fit progress: (2, 1.6884156465530396, {'accuracy': 0.793, 'data_size': 10000}, 53.95124893204775)
INFO flwr 2024-04-30 23:52:22,462 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:52:22,462 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:52:35,800 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:52:37,440 | server.py:125 | fit progress: (3, 1.650192379951477, {'accuracy': 0.8165, 'data_size': 10000}, 68.92945772001985)
INFO flwr 2024-04-30 23:52:37,440 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:52:37,440 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:52:51,760 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:52:53,222 | server.py:125 | fit progress: (4, 1.5865404605865479, {'accuracy': 0.8796, 'data_size': 10000}, 84.7118282870506)
INFO flwr 2024-04-30 23:52:53,222 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:52:53,223 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:53:09,161 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:53:10,594 | server.py:125 | fit progress: (5, 1.599362850189209, {'accuracy': 0.8634, 'data_size': 10000}, 102.08384185301838)
INFO flwr 2024-04-30 23:53:10,594 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:53:10,595 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:53:27,013 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:53:29,302 | server.py:125 | fit progress: (6, 1.5776139497756958, {'accuracy': 0.885, 'data_size': 10000}, 120.79185450205114)
INFO flwr 2024-04-30 23:53:29,302 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:53:29,303 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:53:46,180 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:53:48,438 | server.py:125 | fit progress: (7, 1.5879676342010498, {'accuracy': 0.8751, 'data_size': 10000}, 139.9275932260207)
INFO flwr 2024-04-30 23:53:48,438 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:53:48,439 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:54:02,612 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:54:05,269 | server.py:125 | fit progress: (8, 1.5824499130249023, {'accuracy': 0.879, 'data_size': 10000}, 156.75896732800175)
INFO flwr 2024-04-30 23:54:05,270 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:54:05,270 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:54:20,437 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:54:22,206 | server.py:125 | fit progress: (9, 1.5670596361160278, {'accuracy': 0.8949, 'data_size': 10000}, 173.69598429405596)
INFO flwr 2024-04-30 23:54:22,207 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:54:22,207 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:54:35,655 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:54:37,613 | server.py:125 | fit progress: (10, 1.5609948635101318, {'accuracy': 0.8998, 'data_size': 10000}, 189.1028072470217)
INFO flwr 2024-04-30 23:54:37,613 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:54:37,614 | server.py:153 | FL finished in 189.10322628600989
INFO flwr 2024-04-30 23:54:37,614 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:54:37,614 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:54:37,614 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:54:37,614 | app.py:229 | app_fit: losses_centralized [(0, 2.3003292083740234), (1, 1.8824793100357056), (2, 1.6884156465530396), (3, 1.650192379951477), (4, 1.5865404605865479), (5, 1.599362850189209), (6, 1.5776139497756958), (7, 1.5879676342010498), (8, 1.5824499130249023), (9, 1.5670596361160278), (10, 1.5609948635101318)]
INFO flwr 2024-04-30 23:54:37,614 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1099), (1, 0.6092), (2, 0.793), (3, 0.8165), (4, 0.8796), (5, 0.8634), (6, 0.885), (7, 0.8751), (8, 0.879), (9, 0.8949), (10, 0.8998)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8998
wandb:     loss 1.56099
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_235107-vdf3uu9n
wandb: Find logs at: ./wandb/offline-run-20240430_235107-vdf3uu9n/logs
INFO flwr 2024-04-30 23:54:41,324 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:54:42,154 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3737889)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3737889)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:54:47,854	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:54:47,982	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:54:48,105	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:54:48,106	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:54:58,871 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 56970598811.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 28485299404.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-30 23:54:58,871 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:54:58,872 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:54:58,892 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:54:58,894 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:54:58,894 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:54:58,894 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:55:03,583 | server.py:94 | initial parameters (loss, other metrics): 2.30330753326416, {'accuracy': 0.0551, 'data_size': 10000}
INFO flwr 2024-04-30 23:55:03,583 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:55:03,583 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3814383)[0m 2024-04-30 23:55:06.695628: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3814383)[0m 2024-04-30 23:55:06.802374: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3814383)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3814383)[0m 2024-04-30 23:55:11.085315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3814384)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3814384)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3814388)[0m 2024-04-30 23:55:07.424152: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3814388)[0m 2024-04-30 23:55:07.635029: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3814388)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3814386)[0m 2024-04-30 23:55:11.150628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:55:38,246 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:55:39,915 | server.py:125 | fit progress: (1, 1.9119561910629272, {'accuracy': 0.5826, 'data_size': 10000}, 36.33154723100597)
INFO flwr 2024-04-30 23:55:39,915 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:55:39,916 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:55:55,417 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:55:56,653 | server.py:125 | fit progress: (2, 1.6500256061553955, {'accuracy': 0.8361, 'data_size': 10000}, 53.069681701017544)
INFO flwr 2024-04-30 23:55:56,653 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:55:56,653 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:56:12,098 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:56:13,546 | server.py:125 | fit progress: (3, 1.5947374105453491, {'accuracy': 0.8792, 'data_size': 10000}, 69.96242380898912)
INFO flwr 2024-04-30 23:56:13,546 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:56:13,546 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:56:27,815 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:56:29,098 | server.py:125 | fit progress: (4, 1.587661623954773, {'accuracy': 0.8801, 'data_size': 10000}, 85.51478606899036)
INFO flwr 2024-04-30 23:56:29,098 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:56:29,098 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:56:42,384 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 23:56:43,897 | server.py:125 | fit progress: (5, 1.5636414289474487, {'accuracy': 0.9025, 'data_size': 10000}, 100.31368472898612)
INFO flwr 2024-04-30 23:56:43,897 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 23:56:43,897 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:56:57,790 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 23:56:59,262 | server.py:125 | fit progress: (6, 1.5859386920928955, {'accuracy': 0.8777, 'data_size': 10000}, 115.67911305499729)
INFO flwr 2024-04-30 23:56:59,263 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 23:56:59,263 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:57:16,672 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 23:57:17,861 | server.py:125 | fit progress: (7, 1.5856107473373413, {'accuracy': 0.8775, 'data_size': 10000}, 134.27775908599142)
INFO flwr 2024-04-30 23:57:17,861 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 23:57:17,862 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:57:33,389 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 23:57:34,797 | server.py:125 | fit progress: (8, 1.5542267560958862, {'accuracy': 0.9079, 'data_size': 10000}, 151.21354756498476)
INFO flwr 2024-04-30 23:57:34,797 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 23:57:34,797 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:57:49,428 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 23:57:50,763 | server.py:125 | fit progress: (9, 1.5590325593948364, {'accuracy': 0.904, 'data_size': 10000}, 167.1796919060289)
INFO flwr 2024-04-30 23:57:50,763 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 23:57:50,763 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:58:04,963 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 23:58:07,234 | server.py:125 | fit progress: (10, 1.5528532266616821, {'accuracy': 0.9094, 'data_size': 10000}, 183.65039972000523)
INFO flwr 2024-04-30 23:58:07,234 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 23:58:07,234 | server.py:153 | FL finished in 183.65095507598016
INFO flwr 2024-04-30 23:58:07,234 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 23:58:07,234 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 23:58:07,235 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 23:58:07,235 | app.py:229 | app_fit: losses_centralized [(0, 2.30330753326416), (1, 1.9119561910629272), (2, 1.6500256061553955), (3, 1.5947374105453491), (4, 1.587661623954773), (5, 1.5636414289474487), (6, 1.5859386920928955), (7, 1.5856107473373413), (8, 1.5542267560958862), (9, 1.5590325593948364), (10, 1.5528532266616821)]
INFO flwr 2024-04-30 23:58:07,235 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0551), (1, 0.5826), (2, 0.8361), (3, 0.8792), (4, 0.8801), (5, 0.9025), (6, 0.8777), (7, 0.8775), (8, 0.9079), (9, 0.904), (10, 0.9094)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9094
wandb:     loss 1.55285
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_235441-b4vvtgt2
wandb: Find logs at: ./wandb/offline-run-20240430_235441-b4vvtgt2/logs
INFO flwr 2024-04-30 23:58:10,796 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 23:58:11,690 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3814388)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3814388)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 23:58:17,137	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 23:58:17,283	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 23:58:17,441	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-04-30 23:58:17,442	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-04-30 23:58:29,380 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 59571454772.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 29785727385.0}
INFO flwr 2024-04-30 23:58:29,381 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 23:58:29,381 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 23:58:29,407 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 23:58:29,409 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 23:58:29,410 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 23:58:29,410 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 23:58:34,234 | server.py:94 | initial parameters (loss, other metrics): 2.3016932010650635, {'accuracy': 0.0532, 'data_size': 10000}
INFO flwr 2024-04-30 23:58:34,235 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 23:58:34,235 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3886549)[0m 2024-04-30 23:58:37.895627: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3886549)[0m 2024-04-30 23:58:37.984717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3886549)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3886545)[0m 2024-04-30 23:58:42.650263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3886549)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3886549)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3886543)[0m 2024-04-30 23:58:38.322708: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3886543)[0m 2024-04-30 23:58:38.470844: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3886543)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3886532)[0m 2024-04-30 23:58:42.826461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 23:59:10,559 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 23:59:11,846 | server.py:125 | fit progress: (1, 1.8445371389389038, {'accuracy': 0.7226, 'data_size': 10000}, 37.61117981100688)
INFO flwr 2024-04-30 23:59:11,847 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 23:59:11,847 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:59:26,960 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 23:59:28,262 | server.py:125 | fit progress: (2, 1.6468257904052734, {'accuracy': 0.8406, 'data_size': 10000}, 54.027316292980686)
INFO flwr 2024-04-30 23:59:28,263 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 23:59:28,263 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:59:41,649 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 23:59:43,831 | server.py:125 | fit progress: (3, 1.5832470655441284, {'accuracy': 0.8887, 'data_size': 10000}, 69.59604306897381)
INFO flwr 2024-04-30 23:59:43,832 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 23:59:43,832 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 23:59:57,765 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 23:59:59,130 | server.py:125 | fit progress: (4, 1.5688198804855347, {'accuracy': 0.899, 'data_size': 10000}, 84.89438475301722)
INFO flwr 2024-04-30 23:59:59,130 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 23:59:59,130 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:00:12,148 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:00:13,618 | server.py:125 | fit progress: (5, 1.559594988822937, {'accuracy': 0.9043, 'data_size': 10000}, 99.38280749798287)
INFO flwr 2024-05-01 00:00:13,618 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:00:13,619 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:00:27,437 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:00:29,328 | server.py:125 | fit progress: (6, 1.5566813945770264, {'accuracy': 0.9061, 'data_size': 10000}, 115.09269108599983)
INFO flwr 2024-05-01 00:00:29,328 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:00:29,328 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:00:45,336 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:00:47,009 | server.py:125 | fit progress: (7, 1.560441017150879, {'accuracy': 0.9026, 'data_size': 10000}, 132.773961969011)
INFO flwr 2024-05-01 00:00:47,009 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:00:47,010 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:01:00,890 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:01:03,002 | server.py:125 | fit progress: (8, 1.5701860189437866, {'accuracy': 0.8923, 'data_size': 10000}, 148.76645933598047)
INFO flwr 2024-05-01 00:01:03,002 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:01:03,002 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:01:15,721 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:01:17,180 | server.py:125 | fit progress: (9, 1.5671195983886719, {'accuracy': 0.8945, 'data_size': 10000}, 162.94527960801497)
INFO flwr 2024-05-01 00:01:17,181 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:01:17,181 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:01:30,979 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:01:32,271 | server.py:125 | fit progress: (10, 1.556646704673767, {'accuracy': 0.9052, 'data_size': 10000}, 178.03545314696385)
INFO flwr 2024-05-01 00:01:32,271 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:01:32,271 | server.py:153 | FL finished in 178.03590241499478
INFO flwr 2024-05-01 00:01:32,271 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:01:32,271 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:01:32,271 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:01:32,271 | app.py:229 | app_fit: losses_centralized [(0, 2.3016932010650635), (1, 1.8445371389389038), (2, 1.6468257904052734), (3, 1.5832470655441284), (4, 1.5688198804855347), (5, 1.559594988822937), (6, 1.5566813945770264), (7, 1.560441017150879), (8, 1.5701860189437866), (9, 1.5671195983886719), (10, 1.556646704673767)]
INFO flwr 2024-05-01 00:01:32,272 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0532), (1, 0.7226), (2, 0.8406), (3, 0.8887), (4, 0.899), (5, 0.9043), (6, 0.9061), (7, 0.9026), (8, 0.8923), (9, 0.8945), (10, 0.9052)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9052
wandb:     loss 1.55665
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_235811-is4iug1o
wandb: Find logs at: ./wandb/offline-run-20240430_235811-is4iug1o/logs
INFO flwr 2024-05-01 00:01:35,850 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:01:37,987 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3886545)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3886545)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:01:43,031	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:01:43,177	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:01:43,312	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:01:43,314	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:01:54,388 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 56462836532.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 28231418265.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-05-01 00:01:54,388 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:01:54,388 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:01:54,408 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:01:54,410 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:01:54,410 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:01:54,410 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:01:57,257 | server.py:94 | initial parameters (loss, other metrics): 2.30074405670166, {'accuracy': 0.1815, 'data_size': 10000}
INFO flwr 2024-05-01 00:01:57,265 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:01:57,266 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3964962)[0m 2024-05-01 00:02:01.815857: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3964962)[0m 2024-05-01 00:02:01.918833: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3964962)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3964960)[0m 2024-05-01 00:02:05.560198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3964962)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3964962)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3964973)[0m 2024-05-01 00:02:02.430610: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3964973)[0m 2024-05-01 00:02:02.533266: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3964973)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3964973)[0m 2024-05-01 00:02:05.637830: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:02:23,756 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:02:24,883 | server.py:125 | fit progress: (1, 2.1527915000915527, {'accuracy': 0.3015, 'data_size': 10000}, 27.616643798013683)
INFO flwr 2024-05-01 00:02:24,883 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:02:24,883 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:02:36,231 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:02:37,470 | server.py:125 | fit progress: (2, 2.0668485164642334, {'accuracy': 0.3798, 'data_size': 10000}, 40.20435447600903)
INFO flwr 2024-05-01 00:02:37,471 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:02:37,471 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:02:47,392 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:02:48,837 | server.py:125 | fit progress: (3, 1.96775484085083, {'accuracy': 0.4897, 'data_size': 10000}, 51.571093993028626)
INFO flwr 2024-05-01 00:02:48,837 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:02:48,838 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:02:59,156 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:03:01,929 | server.py:125 | fit progress: (4, 1.9563541412353516, {'accuracy': 0.5033, 'data_size': 10000}, 64.66294575802749)
INFO flwr 2024-05-01 00:03:01,929 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:03:01,930 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:03:12,284 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:03:14,378 | server.py:125 | fit progress: (5, 1.9173152446746826, {'accuracy': 0.543, 'data_size': 10000}, 77.11253185203532)
INFO flwr 2024-05-01 00:03:14,379 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:03:14,379 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:03:24,483 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:03:25,758 | server.py:125 | fit progress: (6, 1.8881616592407227, {'accuracy': 0.5719, 'data_size': 10000}, 88.49183950398583)
INFO flwr 2024-05-01 00:03:25,758 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:03:25,758 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:03:35,784 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:03:37,486 | server.py:125 | fit progress: (7, 1.8562042713165283, {'accuracy': 0.6032, 'data_size': 10000}, 100.22032929799752)
INFO flwr 2024-05-01 00:03:37,486 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:03:37,487 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:03:47,792 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:03:49,865 | server.py:125 | fit progress: (8, 1.8376257419586182, {'accuracy': 0.6225, 'data_size': 10000}, 112.59891122399131)
INFO flwr 2024-05-01 00:03:49,865 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:03:49,865 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:03:59,306 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:04:01,279 | server.py:125 | fit progress: (9, 1.8318562507629395, {'accuracy': 0.6269, 'data_size': 10000}, 124.0129996460164)
INFO flwr 2024-05-01 00:04:01,279 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:04:01,279 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:04:10,010 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:04:11,393 | server.py:125 | fit progress: (10, 1.831086277961731, {'accuracy': 0.6293, 'data_size': 10000}, 134.12704000301892)
INFO flwr 2024-05-01 00:04:11,393 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:04:11,393 | server.py:153 | FL finished in 134.12758659903193
INFO flwr 2024-05-01 00:04:11,394 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:04:11,394 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:04:11,394 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:04:11,394 | app.py:229 | app_fit: losses_centralized [(0, 2.30074405670166), (1, 2.1527915000915527), (2, 2.0668485164642334), (3, 1.96775484085083), (4, 1.9563541412353516), (5, 1.9173152446746826), (6, 1.8881616592407227), (7, 1.8562042713165283), (8, 1.8376257419586182), (9, 1.8318562507629395), (10, 1.831086277961731)]
INFO flwr 2024-05-01 00:04:11,394 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1815), (1, 0.3015), (2, 0.3798), (3, 0.4897), (4, 0.5033), (5, 0.543), (6, 0.5719), (7, 0.6032), (8, 0.6225), (9, 0.6269), (10, 0.6293)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6293
wandb:     loss 1.83109
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_000137-v0gd6w3u
wandb: Find logs at: ./wandb/offline-run-20240501_000137-v0gd6w3u/logs
INFO flwr 2024-05-01 00:04:15,113 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:04:15,957 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=3964973)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=3964973)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:04:21,289	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:04:21,430	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:04:21,563	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:04:21,564	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:04:32,836 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 49689024923.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 24844512460.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-05-01 00:04:32,837 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:04:32,837 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:04:32,858 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:04:32,860 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:04:32,860 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:04:32,860 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:04:36,220 | server.py:94 | initial parameters (loss, other metrics): 2.298464059829712, {'accuracy': 0.1477, 'data_size': 10000}
INFO flwr 2024-05-01 00:04:36,220 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:04:36,220 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=4022152)[0m 2024-05-01 00:04:40.708186: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=4022152)[0m 2024-05-01 00:04:40.800086: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=4022152)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=4022148)[0m 2024-05-01 00:04:44.288022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=4022148)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=4022148)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=4022144)[0m 2024-05-01 00:04:41.188717: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4022144)[0m 2024-05-01 00:04:41.448567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4022144)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4022144)[0m 2024-05-01 00:04:44.551288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:05:01,267 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:05:02,346 | server.py:125 | fit progress: (1, 2.0877623558044434, {'accuracy': 0.3702, 'data_size': 10000}, 26.12583228503354)
INFO flwr 2024-05-01 00:05:02,347 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:05:02,347 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:05:12,310 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:05:13,537 | server.py:125 | fit progress: (2, 1.9833167791366577, {'accuracy': 0.4731, 'data_size': 10000}, 37.31673687801231)
INFO flwr 2024-05-01 00:05:13,537 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:05:13,538 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:05:22,363 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:05:24,331 | server.py:125 | fit progress: (3, 1.9952967166900635, {'accuracy': 0.4624, 'data_size': 10000}, 48.11071769800037)
INFO flwr 2024-05-01 00:05:24,331 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:05:24,332 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:05:33,567 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:05:35,512 | server.py:125 | fit progress: (4, 1.9840644598007202, {'accuracy': 0.4737, 'data_size': 10000}, 59.291631150001194)
INFO flwr 2024-05-01 00:05:35,512 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:05:35,513 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:05:43,714 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:05:45,655 | server.py:125 | fit progress: (5, 1.9734385013580322, {'accuracy': 0.4837, 'data_size': 10000}, 69.43507640302414)
INFO flwr 2024-05-01 00:05:45,656 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:05:45,656 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:05:54,178 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:05:55,390 | server.py:125 | fit progress: (6, 1.9714957475662231, {'accuracy': 0.4857, 'data_size': 10000}, 79.16945167601807)
INFO flwr 2024-05-01 00:05:55,390 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:05:55,390 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:06:04,331 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:06:05,381 | server.py:125 | fit progress: (7, 1.9710636138916016, {'accuracy': 0.4871, 'data_size': 10000}, 89.16065153799718)
INFO flwr 2024-05-01 00:06:05,381 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:06:05,382 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:06:13,584 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:06:15,014 | server.py:125 | fit progress: (8, 1.9701402187347412, {'accuracy': 0.4882, 'data_size': 10000}, 98.79375916800927)
INFO flwr 2024-05-01 00:06:15,014 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:06:15,015 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:06:24,040 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:06:26,297 | server.py:125 | fit progress: (9, 1.969048023223877, {'accuracy': 0.489, 'data_size': 10000}, 110.07647721201647)
INFO flwr 2024-05-01 00:06:26,297 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:06:26,297 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:06:35,904 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:06:38,162 | server.py:125 | fit progress: (10, 1.9673993587493896, {'accuracy': 0.4905, 'data_size': 10000}, 121.94187704002252)
INFO flwr 2024-05-01 00:06:38,163 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:06:38,163 | server.py:153 | FL finished in 121.94235173502238
INFO flwr 2024-05-01 00:06:38,163 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:06:38,163 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:06:38,163 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:06:38,163 | app.py:229 | app_fit: losses_centralized [(0, 2.298464059829712), (1, 2.0877623558044434), (2, 1.9833167791366577), (3, 1.9952967166900635), (4, 1.9840644598007202), (5, 1.9734385013580322), (6, 1.9714957475662231), (7, 1.9710636138916016), (8, 1.9701402187347412), (9, 1.969048023223877), (10, 1.9673993587493896)]
INFO flwr 2024-05-01 00:06:38,163 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1477), (1, 0.3702), (2, 0.4731), (3, 0.4624), (4, 0.4737), (5, 0.4837), (6, 0.4857), (7, 0.4871), (8, 0.4882), (9, 0.489), (10, 0.4905)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4905
wandb:     loss 1.9674
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_000415-az2p22hn
wandb: Find logs at: ./wandb/offline-run-20240501_000415-az2p22hn/logs
INFO flwr 2024-05-01 00:06:42,055 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:06:42,877 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=4022144)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=4022144)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:06:48,260	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:06:48,423	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:06:48,569	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:06:48,570	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:07:00,169 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 50063093760.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 25031546880.0}
INFO flwr 2024-05-01 00:07:00,170 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:07:00,170 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:07:00,187 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:07:00,188 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:07:00,188 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:07:00,188 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:07:03,020 | server.py:94 | initial parameters (loss, other metrics): 2.3036158084869385, {'accuracy': 0.1139, 'data_size': 10000}
INFO flwr 2024-05-01 00:07:03,020 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:07:03,021 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=4069765)[0m 2024-05-01 00:07:08.149303: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=4069765)[0m 2024-05-01 00:07:08.257207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=4069765)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=4069765)[0m 2024-05-01 00:07:12.959103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=4069761)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=4069761)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=4069766)[0m 2024-05-01 00:07:08.826252: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4069766)[0m 2024-05-01 00:07:08.948990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4069766)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4069763)[0m 2024-05-01 00:07:12.963095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:07:33,848 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:07:36,009 | server.py:125 | fit progress: (1, 1.961187481880188, {'accuracy': 0.519, 'data_size': 10000}, 32.98821158398641)
INFO flwr 2024-05-01 00:07:36,009 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:07:36,009 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:07:47,553 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:07:49,859 | server.py:125 | fit progress: (2, 1.8461371660232544, {'accuracy': 0.6201, 'data_size': 10000}, 46.838053117040545)
INFO flwr 2024-05-01 00:07:49,859 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:07:49,859 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:07:58,406 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:07:59,870 | server.py:125 | fit progress: (3, 1.788756251335144, {'accuracy': 0.6765, 'data_size': 10000}, 56.84965906600701)
INFO flwr 2024-05-01 00:07:59,871 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:07:59,871 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:08:09,839 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:08:11,118 | server.py:125 | fit progress: (4, 1.683933973312378, {'accuracy': 0.7812, 'data_size': 10000}, 68.09713586099679)
INFO flwr 2024-05-01 00:08:11,118 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:08:11,118 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:08:22,566 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:08:24,827 | server.py:125 | fit progress: (5, 1.6752396821975708, {'accuracy': 0.7892, 'data_size': 10000}, 81.80587606603513)
INFO flwr 2024-05-01 00:08:24,827 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:08:24,827 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:08:34,523 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:08:36,512 | server.py:125 | fit progress: (6, 1.6853396892547607, {'accuracy': 0.7769, 'data_size': 10000}, 93.49151880200952)
INFO flwr 2024-05-01 00:08:36,512 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:08:36,513 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:08:45,969 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:08:48,186 | server.py:125 | fit progress: (7, 1.671186089515686, {'accuracy': 0.7911, 'data_size': 10000}, 105.16577338502975)
INFO flwr 2024-05-01 00:08:48,187 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:08:48,187 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:08:57,727 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:09:00,467 | server.py:125 | fit progress: (8, 1.6757115125656128, {'accuracy': 0.7849, 'data_size': 10000}, 117.44628950499464)
INFO flwr 2024-05-01 00:09:00,467 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:09:00,468 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:09:09,142 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:09:11,332 | server.py:125 | fit progress: (9, 1.659317135810852, {'accuracy': 0.8013, 'data_size': 10000}, 128.31141220399877)
INFO flwr 2024-05-01 00:09:11,332 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:09:11,333 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:09:19,780 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:09:21,178 | server.py:125 | fit progress: (10, 1.647730827331543, {'accuracy': 0.8137, 'data_size': 10000}, 138.1578267129953)
INFO flwr 2024-05-01 00:09:21,179 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:09:21,179 | server.py:153 | FL finished in 138.15836602798663
INFO flwr 2024-05-01 00:09:21,179 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:09:21,179 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:09:21,179 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:09:21,180 | app.py:229 | app_fit: losses_centralized [(0, 2.3036158084869385), (1, 1.961187481880188), (2, 1.8461371660232544), (3, 1.788756251335144), (4, 1.683933973312378), (5, 1.6752396821975708), (6, 1.6853396892547607), (7, 1.671186089515686), (8, 1.6757115125656128), (9, 1.659317135810852), (10, 1.647730827331543)]
INFO flwr 2024-05-01 00:09:21,180 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1139), (1, 0.519), (2, 0.6201), (3, 0.6765), (4, 0.7812), (5, 0.7892), (6, 0.7769), (7, 0.7911), (8, 0.7849), (9, 0.8013), (10, 0.8137)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8137
wandb:     loss 1.64773
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_000642-rhfeaq5x
wandb: Find logs at: ./wandb/offline-run-20240501_000642-rhfeaq5x/logs
INFO flwr 2024-05-01 00:09:24,916 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:09:25,976 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=4069759)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=4069759)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:09:31,188	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:09:31,331	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:09:31,475	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:09:31,477	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:09:42,900 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 134439635559.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61902700953.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-05-01 00:09:42,900 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:09:42,900 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:09:42,924 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:09:42,925 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:09:42,925 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:09:42,925 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:09:46,659 | server.py:94 | initial parameters (loss, other metrics): 2.303546190261841, {'accuracy': 0.0802, 'data_size': 10000}
INFO flwr 2024-05-01 00:09:46,659 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:09:46,659 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=4133458)[0m 2024-05-01 00:09:51.129890: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=4133458)[0m 2024-05-01 00:09:51.361715: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=4133458)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=4133456)[0m 2024-05-01 00:09:54.688457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=4133452)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=4133452)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=4133454)[0m 2024-05-01 00:09:51.543525: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4133454)[0m 2024-05-01 00:09:51.650059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4133454)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4133453)[0m 2024-05-01 00:09:54.763950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:10:11,606 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:10:13,494 | server.py:125 | fit progress: (1, 2.229140520095825, {'accuracy': 0.213, 'data_size': 10000}, 26.8348420310067)
INFO flwr 2024-05-01 00:10:13,495 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:10:13,495 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:10:23,917 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:10:25,597 | server.py:125 | fit progress: (2, 2.114835262298584, {'accuracy': 0.3316, 'data_size': 10000}, 38.93718069395982)
INFO flwr 2024-05-01 00:10:25,597 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:10:25,597 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:10:34,939 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:10:37,005 | server.py:125 | fit progress: (3, 2.0417191982269287, {'accuracy': 0.4169, 'data_size': 10000}, 50.34550637000939)
INFO flwr 2024-05-01 00:10:37,005 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:10:37,005 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:10:46,490 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:10:48,529 | server.py:125 | fit progress: (4, 2.003319501876831, {'accuracy': 0.4538, 'data_size': 10000}, 61.86944245296763)
INFO flwr 2024-05-01 00:10:48,529 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:10:48,529 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:10:56,897 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:10:58,632 | server.py:125 | fit progress: (5, 1.9885523319244385, {'accuracy': 0.4681, 'data_size': 10000}, 71.9725854449789)
INFO flwr 2024-05-01 00:10:58,632 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:10:58,632 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:11:07,665 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:11:09,338 | server.py:125 | fit progress: (6, 1.9796922206878662, {'accuracy': 0.4777, 'data_size': 10000}, 82.67818567197537)
INFO flwr 2024-05-01 00:11:09,338 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:11:09,338 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:11:18,181 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:11:19,414 | server.py:125 | fit progress: (7, 1.976517915725708, {'accuracy': 0.4815, 'data_size': 10000}, 92.75487179198535)
INFO flwr 2024-05-01 00:11:19,415 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:11:19,415 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:11:27,994 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:11:29,620 | server.py:125 | fit progress: (8, 1.9748197793960571, {'accuracy': 0.4833, 'data_size': 10000}, 102.96013414399931)
INFO flwr 2024-05-01 00:11:29,620 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:11:29,620 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:11:38,521 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:11:39,863 | server.py:125 | fit progress: (9, 1.9749574661254883, {'accuracy': 0.4836, 'data_size': 10000}, 113.20372373395367)
INFO flwr 2024-05-01 00:11:39,863 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:11:39,864 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:11:48,050 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:11:50,215 | server.py:125 | fit progress: (10, 1.9750429391860962, {'accuracy': 0.4837, 'data_size': 10000}, 123.5553262429894)
INFO flwr 2024-05-01 00:11:50,215 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:11:50,215 | server.py:153 | FL finished in 123.55607513996074
INFO flwr 2024-05-01 00:11:50,216 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:11:50,216 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:11:50,216 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:11:50,216 | app.py:229 | app_fit: losses_centralized [(0, 2.303546190261841), (1, 2.229140520095825), (2, 2.114835262298584), (3, 2.0417191982269287), (4, 2.003319501876831), (5, 1.9885523319244385), (6, 1.9796922206878662), (7, 1.976517915725708), (8, 1.9748197793960571), (9, 1.9749574661254883), (10, 1.9750429391860962)]
INFO flwr 2024-05-01 00:11:50,216 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0802), (1, 0.213), (2, 0.3316), (3, 0.4169), (4, 0.4538), (5, 0.4681), (6, 0.4777), (7, 0.4815), (8, 0.4833), (9, 0.4836), (10, 0.4837)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4837
wandb:     loss 1.97504
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_000925-nr8b17tz
wandb: Find logs at: ./wandb/offline-run-20240501_000925-nr8b17tz/logs
INFO flwr 2024-05-01 00:11:54,004 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:11:54,724 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=4133454)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=4133454)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:11:59,807	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:11:59,913	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:12:00,010	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:12:00,013	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:12:11,057 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64702439424.0, 'memory': 140972358656.0, 'CPU': 64.0}
INFO flwr 2024-05-01 00:12:11,057 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:12:11,057 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:12:11,073 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:12:11,074 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:12:11,074 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:12:11,074 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:12:13,933 | server.py:94 | initial parameters (loss, other metrics): 2.3028182983398438, {'accuracy': 0.0882, 'data_size': 10000}
INFO flwr 2024-05-01 00:12:13,934 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:12:13,934 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=4183874)[0m 2024-05-01 00:12:17.237798: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=4183874)[0m 2024-05-01 00:12:17.337339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=4183874)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=4183874)[0m 2024-05-01 00:12:19.545322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=4183889)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=4183889)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=4183881)[0m 2024-05-01 00:12:17.680194: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4183881)[0m 2024-05-01 00:12:17.786744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4183881)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=4183881)[0m 2024-05-01 00:12:20.030996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:12:33,231 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:12:34,404 | server.py:125 | fit progress: (1, 2.032593011856079, {'accuracy': 0.4353, 'data_size': 10000}, 20.470305787050165)
INFO flwr 2024-05-01 00:12:34,405 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:12:34,405 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:12:42,942 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:12:44,076 | server.py:125 | fit progress: (2, 1.8984031677246094, {'accuracy': 0.5638, 'data_size': 10000}, 30.142447528021876)
INFO flwr 2024-05-01 00:12:44,077 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:12:44,077 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:12:52,117 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:12:53,464 | server.py:125 | fit progress: (3, 1.8273109197616577, {'accuracy': 0.6388, 'data_size': 10000}, 39.53021027200157)
INFO flwr 2024-05-01 00:12:53,465 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:12:53,465 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:13:00,506 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:13:01,940 | server.py:125 | fit progress: (4, 1.8248062133789062, {'accuracy': 0.6326, 'data_size': 10000}, 48.00610725703882)
INFO flwr 2024-05-01 00:13:01,940 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:13:01,941 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:13:09,659 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:13:11,071 | server.py:125 | fit progress: (5, 1.8217599391937256, {'accuracy': 0.6378, 'data_size': 10000}, 57.13674180302769)
INFO flwr 2024-05-01 00:13:11,071 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:13:11,071 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:13:18,676 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:13:19,814 | server.py:125 | fit progress: (6, 1.7820298671722412, {'accuracy': 0.6773, 'data_size': 10000}, 65.88037798402365)
INFO flwr 2024-05-01 00:13:19,815 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:13:19,815 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:13:27,365 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:13:28,583 | server.py:125 | fit progress: (7, 1.7530908584594727, {'accuracy': 0.7081, 'data_size': 10000}, 74.6490108340513)
INFO flwr 2024-05-01 00:13:28,583 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:13:28,584 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:13:36,147 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:13:37,622 | server.py:125 | fit progress: (8, 1.7531025409698486, {'accuracy': 0.7082, 'data_size': 10000}, 83.68805360503029)
INFO flwr 2024-05-01 00:13:37,622 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:13:37,623 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:13:45,613 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:13:47,754 | server.py:125 | fit progress: (9, 1.7649492025375366, {'accuracy': 0.6952, 'data_size': 10000}, 93.82041932200082)
INFO flwr 2024-05-01 00:13:47,755 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:13:47,755 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:13:56,538 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:13:58,717 | server.py:125 | fit progress: (10, 1.7583167552947998, {'accuracy': 0.7018, 'data_size': 10000}, 104.78340866504004)
INFO flwr 2024-05-01 00:13:58,718 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:13:58,718 | server.py:153 | FL finished in 104.78386772301747
INFO flwr 2024-05-01 00:13:58,718 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:13:58,718 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:13:58,718 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:13:58,718 | app.py:229 | app_fit: losses_centralized [(0, 2.3028182983398438), (1, 2.032593011856079), (2, 1.8984031677246094), (3, 1.8273109197616577), (4, 1.8248062133789062), (5, 1.8217599391937256), (6, 1.7820298671722412), (7, 1.7530908584594727), (8, 1.7531025409698486), (9, 1.7649492025375366), (10, 1.7583167552947998)]
INFO flwr 2024-05-01 00:13:58,719 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0882), (1, 0.4353), (2, 0.5638), (3, 0.6388), (4, 0.6326), (5, 0.6378), (6, 0.6773), (7, 0.7081), (8, 0.7082), (9, 0.6952), (10, 0.7018)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7018
wandb:     loss 1.75832
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_001154-5efylgjb
wandb: Find logs at: ./wandb/offline-run-20240501_001154-5efylgjb/logs
INFO flwr 2024-05-01 00:14:02,528 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:14:03,835 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=4183881)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=4183881)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:14:13,948	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:14:14,762	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:14:14,923	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:14:14,925	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:14:26,284 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 62806711910.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 136548994458.0}
INFO flwr 2024-05-01 00:14:26,285 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:14:26,285 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:14:26,310 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:14:26,312 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:14:26,312 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:14:26,313 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:14:30,092 | server.py:94 | initial parameters (loss, other metrics): 2.3042218685150146, {'accuracy': 0.052, 'data_size': 10000}
INFO flwr 2024-05-01 00:14:30,093 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:14:30,093 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=42295)[0m 2024-05-01 00:14:36.161435: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=42295)[0m 2024-05-01 00:14:36.424689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=42295)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=42295)[0m 2024-05-01 00:14:43.665221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=42312)[0m 2024-05-01 00:14:36.535293: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=42300)[0m 2024-05-01 00:14:36.642443: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=42300)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=42304)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=42304)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=42290)[0m 2024-05-01 00:14:43.665233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:15:08,186 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:15:09,409 | server.py:125 | fit progress: (1, 1.9373828172683716, {'accuracy': 0.5446, 'data_size': 10000}, 39.31541320600081)
INFO flwr 2024-05-01 00:15:09,409 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:15:09,409 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:15:20,051 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:15:21,926 | server.py:125 | fit progress: (2, 1.8554435968399048, {'accuracy': 0.6125, 'data_size': 10000}, 51.83279598603258)
INFO flwr 2024-05-01 00:15:21,926 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:15:21,927 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:15:31,863 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:15:33,245 | server.py:125 | fit progress: (3, 1.7965681552886963, {'accuracy': 0.6669, 'data_size': 10000}, 63.15195243404014)
INFO flwr 2024-05-01 00:15:33,245 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:15:33,246 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:15:42,895 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:15:45,470 | server.py:125 | fit progress: (4, 1.7379539012908936, {'accuracy': 0.7241, 'data_size': 10000}, 75.37663795403205)
INFO flwr 2024-05-01 00:15:45,470 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:15:45,471 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:15:55,573 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:15:57,016 | server.py:125 | fit progress: (5, 1.7592449188232422, {'accuracy': 0.7023, 'data_size': 10000}, 86.92266027803998)
INFO flwr 2024-05-01 00:15:57,016 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:15:57,016 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:16:05,810 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:16:07,188 | server.py:125 | fit progress: (6, 1.7366312742233276, {'accuracy': 0.7245, 'data_size': 10000}, 97.0948948150035)
INFO flwr 2024-05-01 00:16:07,188 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:16:07,188 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:16:16,407 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:16:17,471 | server.py:125 | fit progress: (7, 1.7322818040847778, {'accuracy': 0.7278, 'data_size': 10000}, 107.3778834870318)
INFO flwr 2024-05-01 00:16:17,471 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:16:17,471 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:16:26,399 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:16:28,490 | server.py:125 | fit progress: (8, 1.7348999977111816, {'accuracy': 0.7255, 'data_size': 10000}, 118.39708863204578)
INFO flwr 2024-05-01 00:16:28,490 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:16:28,491 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:16:38,976 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:16:40,416 | server.py:125 | fit progress: (9, 1.7234470844268799, {'accuracy': 0.7366, 'data_size': 10000}, 130.32303029100876)
INFO flwr 2024-05-01 00:16:40,416 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:16:40,417 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:16:50,660 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:16:52,013 | server.py:125 | fit progress: (10, 1.7159615755081177, {'accuracy': 0.744, 'data_size': 10000}, 141.91978233802365)
INFO flwr 2024-05-01 00:16:52,013 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:16:52,013 | server.py:153 | FL finished in 141.92018521099817
INFO flwr 2024-05-01 00:16:52,013 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:16:52,013 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:16:52,014 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:16:52,014 | app.py:229 | app_fit: losses_centralized [(0, 2.3042218685150146), (1, 1.9373828172683716), (2, 1.8554435968399048), (3, 1.7965681552886963), (4, 1.7379539012908936), (5, 1.7592449188232422), (6, 1.7366312742233276), (7, 1.7322818040847778), (8, 1.7348999977111816), (9, 1.7234470844268799), (10, 1.7159615755081177)]
INFO flwr 2024-05-01 00:16:52,014 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.052), (1, 0.5446), (2, 0.6125), (3, 0.6669), (4, 0.7241), (5, 0.7023), (6, 0.7245), (7, 0.7278), (8, 0.7255), (9, 0.7366), (10, 0.744)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.744
wandb:     loss 1.71596
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_001403-5fn87xoj
wandb: Find logs at: ./wandb/offline-run-20240501_001403-5fn87xoj/logs
INFO flwr 2024-05-01 00:16:55,579 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:16:56,303 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=42307)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=42307)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:17:01,691	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:17:01,829	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:17:01,965	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:17:01,966	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:17:13,256 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 141589021696.0, 'CPU': 64.0, 'object_store_memory': 64966723584.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-05-01 00:17:13,257 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:17:13,257 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:17:13,304 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:17:13,305 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:17:13,306 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:17:13,306 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:17:16,954 | server.py:94 | initial parameters (loss, other metrics): 2.3044025897979736, {'accuracy': 0.0855, 'data_size': 10000}
INFO flwr 2024-05-01 00:17:16,954 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:17:16,955 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=102037)[0m 2024-05-01 00:17:21.630156: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=102037)[0m 2024-05-01 00:17:21.742152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=102037)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=102031)[0m 2024-05-01 00:17:25.833203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=102028)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=102028)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=102041)[0m 2024-05-01 00:17:22.260052: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=102041)[0m 2024-05-01 00:17:22.372236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=102041)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=102041)[0m 2024-05-01 00:17:25.835882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:17:43,201 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:17:44,591 | server.py:125 | fit progress: (1, 1.9864288568496704, {'accuracy': 0.4859, 'data_size': 10000}, 27.636813609977253)
INFO flwr 2024-05-01 00:17:44,592 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:17:44,592 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:17:55,382 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:17:56,560 | server.py:125 | fit progress: (2, 2.087301731109619, {'accuracy': 0.3559, 'data_size': 10000}, 39.60512645402923)
INFO flwr 2024-05-01 00:17:56,560 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:17:56,560 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:18:07,028 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:18:08,689 | server.py:125 | fit progress: (3, 1.8446400165557861, {'accuracy': 0.6158, 'data_size': 10000}, 51.734655901032966)
INFO flwr 2024-05-01 00:18:08,689 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:18:08,690 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:18:18,213 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:18:20,716 | server.py:125 | fit progress: (4, 1.7584171295166016, {'accuracy': 0.7021, 'data_size': 10000}, 63.76181281299796)
INFO flwr 2024-05-01 00:18:20,717 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:18:20,717 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:18:30,715 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:18:32,459 | server.py:125 | fit progress: (5, 1.749727725982666, {'accuracy': 0.7092, 'data_size': 10000}, 75.50493395101512)
INFO flwr 2024-05-01 00:18:32,460 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:18:32,460 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:18:42,778 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:18:44,736 | server.py:125 | fit progress: (6, 1.7536115646362305, {'accuracy': 0.7072, 'data_size': 10000}, 87.78155037999386)
INFO flwr 2024-05-01 00:18:44,736 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:18:44,737 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:18:52,843 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:18:54,034 | server.py:125 | fit progress: (7, 1.7168868780136108, {'accuracy': 0.7426, 'data_size': 10000}, 97.07939527300186)
INFO flwr 2024-05-01 00:18:54,034 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:18:54,034 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:19:02,112 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:19:03,473 | server.py:125 | fit progress: (8, 1.6914591789245605, {'accuracy': 0.77, 'data_size': 10000}, 106.5187812250224)
INFO flwr 2024-05-01 00:19:03,473 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:19:03,474 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:19:10,866 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:19:12,312 | server.py:125 | fit progress: (9, 1.6891893148422241, {'accuracy': 0.7688, 'data_size': 10000}, 115.35776398901362)
INFO flwr 2024-05-01 00:19:12,312 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:19:12,313 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:19:19,815 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:19:21,192 | server.py:125 | fit progress: (10, 1.6832218170166016, {'accuracy': 0.7766, 'data_size': 10000}, 124.23769557097694)
INFO flwr 2024-05-01 00:19:21,192 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:19:21,193 | server.py:153 | FL finished in 124.23815089598065
INFO flwr 2024-05-01 00:19:21,193 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:19:21,193 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:19:21,193 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:19:21,193 | app.py:229 | app_fit: losses_centralized [(0, 2.3044025897979736), (1, 1.9864288568496704), (2, 2.087301731109619), (3, 1.8446400165557861), (4, 1.7584171295166016), (5, 1.749727725982666), (6, 1.7536115646362305), (7, 1.7168868780136108), (8, 1.6914591789245605), (9, 1.6891893148422241), (10, 1.6832218170166016)]
INFO flwr 2024-05-01 00:19:21,193 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0855), (1, 0.4859), (2, 0.3559), (3, 0.6158), (4, 0.7021), (5, 0.7092), (6, 0.7072), (7, 0.7426), (8, 0.77), (9, 0.7688), (10, 0.7766)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7766
wandb:     loss 1.68322
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_001655-t6ejqk46
wandb: Find logs at: ./wandb/offline-run-20240501_001655-t6ejqk46/logs
INFO flwr 2024-05-01 00:19:24,997 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:19:28,311 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=102031)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=102031)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:19:33,524	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:19:33,631	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:19:33,740	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:19:33,741	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:19:43,368 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 137407869952.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63174801408.0}
INFO flwr 2024-05-01 00:19:43,369 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:19:43,369 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:19:43,385 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:19:43,387 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:19:43,387 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:19:43,388 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:19:46,602 | server.py:94 | initial parameters (loss, other metrics): 2.3015620708465576, {'accuracy': 0.1224, 'data_size': 10000}
INFO flwr 2024-05-01 00:19:46,604 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:19:46,609 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=159529)[0m 2024-05-01 00:19:48.942162: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=159529)[0m 2024-05-01 00:19:49.042435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=159529)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=159529)[0m 2024-05-01 00:19:51.058162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=159532)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=159532)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=159536)[0m 2024-05-01 00:19:49.258994: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=159536)[0m 2024-05-01 00:19:49.358454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=159536)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=159530)[0m 2024-05-01 00:19:51.218059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:20:03,940 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:20:05,036 | server.py:125 | fit progress: (1, 1.8761895895004272, {'accuracy': 0.6301, 'data_size': 10000}, 18.426474292995408)
INFO flwr 2024-05-01 00:20:05,036 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:20:05,036 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:20:13,686 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:20:14,896 | server.py:125 | fit progress: (2, 1.801519513130188, {'accuracy': 0.6684, 'data_size': 10000}, 28.2870411319891)
INFO flwr 2024-05-01 00:20:14,897 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:20:14,897 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:20:22,743 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:20:24,116 | server.py:125 | fit progress: (3, 1.7548277378082275, {'accuracy': 0.7127, 'data_size': 10000}, 37.506437965959776)
INFO flwr 2024-05-01 00:20:24,116 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:20:24,116 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:20:31,616 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:20:32,930 | server.py:125 | fit progress: (4, 1.7120364904403687, {'accuracy': 0.7522, 'data_size': 10000}, 46.320309069007635)
INFO flwr 2024-05-01 00:20:32,930 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:20:32,930 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:20:40,530 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:20:41,968 | server.py:125 | fit progress: (5, 1.6572824716567993, {'accuracy': 0.8078, 'data_size': 10000}, 55.35904535098234)
INFO flwr 2024-05-01 00:20:41,969 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:20:41,969 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:20:49,912 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:20:51,038 | server.py:125 | fit progress: (6, 1.693839430809021, {'accuracy': 0.7658, 'data_size': 10000}, 64.42890676599927)
INFO flwr 2024-05-01 00:20:51,039 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:20:51,039 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:20:59,108 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:21:00,312 | server.py:125 | fit progress: (7, 1.6842410564422607, {'accuracy': 0.7791, 'data_size': 10000}, 73.7029259519768)
INFO flwr 2024-05-01 00:21:00,313 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:21:00,313 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:21:08,070 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:21:09,397 | server.py:125 | fit progress: (8, 1.6584469079971313, {'accuracy': 0.803, 'data_size': 10000}, 82.78762026299955)
INFO flwr 2024-05-01 00:21:09,397 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:21:09,397 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:21:17,297 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:21:18,612 | server.py:125 | fit progress: (9, 1.6485902070999146, {'accuracy': 0.8132, 'data_size': 10000}, 92.00310397200519)
INFO flwr 2024-05-01 00:21:18,613 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:21:18,613 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:21:26,171 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:21:27,547 | server.py:125 | fit progress: (10, 1.6396123170852661, {'accuracy': 0.8209, 'data_size': 10000}, 100.93735492799897)
INFO flwr 2024-05-01 00:21:27,547 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:21:27,547 | server.py:153 | FL finished in 100.93775521597126
INFO flwr 2024-05-01 00:21:27,547 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:21:27,547 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:21:27,547 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:21:27,547 | app.py:229 | app_fit: losses_centralized [(0, 2.3015620708465576), (1, 1.8761895895004272), (2, 1.801519513130188), (3, 1.7548277378082275), (4, 1.7120364904403687), (5, 1.6572824716567993), (6, 1.693839430809021), (7, 1.6842410564422607), (8, 1.6584469079971313), (9, 1.6485902070999146), (10, 1.6396123170852661)]
INFO flwr 2024-05-01 00:21:27,548 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1224), (1, 0.6301), (2, 0.6684), (3, 0.7127), (4, 0.7522), (5, 0.8078), (6, 0.7658), (7, 0.7791), (8, 0.803), (9, 0.8132), (10, 0.8209)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8209
wandb:     loss 1.63961
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_001925-pbe4gne9
wandb: Find logs at: ./wandb/offline-run-20240501_001925-pbe4gne9/logs
INFO flwr 2024-05-01 00:21:31,032 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:21:31,776 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=159528)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=159528)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:21:36,807	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:21:36,926	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:21:37,030	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:21:37,032	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:21:47,163 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 81190708224.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 39081732096.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-05-01 00:21:47,164 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:21:47,164 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:21:47,180 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:21:47,181 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:21:47,181 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:21:47,182 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:21:50,188 | server.py:94 | initial parameters (loss, other metrics): 2.305995225906372, {'accuracy': 0.0691, 'data_size': 10000}
INFO flwr 2024-05-01 00:21:50,189 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:21:50,189 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=208750)[0m 2024-05-01 00:21:52.683796: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=208750)[0m 2024-05-01 00:21:52.777055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=208750)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=208750)[0m 2024-05-01 00:21:54.795852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=208753)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=208753)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=208758)[0m 2024-05-01 00:21:52.885841: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=208758)[0m 2024-05-01 00:21:52.973209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=208758)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=208758)[0m 2024-05-01 00:21:54.934773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:22:08,405 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:22:09,649 | server.py:125 | fit progress: (1, 1.8712819814682007, {'accuracy': 0.6538, 'data_size': 10000}, 19.459585043019615)
INFO flwr 2024-05-01 00:22:09,649 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:22:09,649 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:22:18,714 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:22:19,946 | server.py:125 | fit progress: (2, 1.7366355657577515, {'accuracy': 0.7312, 'data_size': 10000}, 29.757368471997324)
INFO flwr 2024-05-01 00:22:19,947 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:22:19,947 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:22:28,144 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:22:29,583 | server.py:125 | fit progress: (3, 1.6261948347091675, {'accuracy': 0.8429, 'data_size': 10000}, 39.394081604026724)
INFO flwr 2024-05-01 00:22:29,583 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:22:29,584 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:22:37,318 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:22:38,762 | server.py:125 | fit progress: (4, 1.6187795400619507, {'accuracy': 0.8476, 'data_size': 10000}, 48.572848308016546)
INFO flwr 2024-05-01 00:22:38,762 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:22:38,762 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:22:46,128 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:22:47,599 | server.py:125 | fit progress: (5, 1.589613914489746, {'accuracy': 0.8741, 'data_size': 10000}, 57.4095582900336)
INFO flwr 2024-05-01 00:22:47,599 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:22:47,599 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:22:55,110 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:22:56,296 | server.py:125 | fit progress: (6, 1.6018598079681396, {'accuracy': 0.8626, 'data_size': 10000}, 66.10723838402191)
INFO flwr 2024-05-01 00:22:56,296 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:22:56,297 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:23:04,323 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:23:05,458 | server.py:125 | fit progress: (7, 1.598501205444336, {'accuracy': 0.8635, 'data_size': 10000}, 75.26917273999425)
INFO flwr 2024-05-01 00:23:05,458 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:23:05,459 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:23:13,046 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:23:14,496 | server.py:125 | fit progress: (8, 1.5720158815383911, {'accuracy': 0.8905, 'data_size': 10000}, 84.30658412002958)
INFO flwr 2024-05-01 00:23:14,496 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:23:14,496 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:23:21,870 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:23:23,335 | server.py:125 | fit progress: (9, 1.5771132707595825, {'accuracy': 0.8856, 'data_size': 10000}, 93.1465187559952)
INFO flwr 2024-05-01 00:23:23,336 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:23:23,336 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:23:30,880 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:23:32,311 | server.py:125 | fit progress: (10, 1.5762747526168823, {'accuracy': 0.8854, 'data_size': 10000}, 102.12221937102731)
INFO flwr 2024-05-01 00:23:32,311 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:23:32,312 | server.py:153 | FL finished in 102.1226329660276
INFO flwr 2024-05-01 00:23:32,312 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:23:32,312 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:23:32,312 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:23:32,312 | app.py:229 | app_fit: losses_centralized [(0, 2.305995225906372), (1, 1.8712819814682007), (2, 1.7366355657577515), (3, 1.6261948347091675), (4, 1.6187795400619507), (5, 1.589613914489746), (6, 1.6018598079681396), (7, 1.598501205444336), (8, 1.5720158815383911), (9, 1.5771132707595825), (10, 1.5762747526168823)]
INFO flwr 2024-05-01 00:23:32,312 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0691), (1, 0.6538), (2, 0.7312), (3, 0.8429), (4, 0.8476), (5, 0.8741), (6, 0.8626), (7, 0.8635), (8, 0.8905), (9, 0.8856), (10, 0.8854)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8854
wandb:     loss 1.57627
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_002131-pfh8u0et
wandb: Find logs at: ./wandb/offline-run-20240501_002131-pfh8u0et/logs
INFO flwr 2024-05-01 00:23:35,839 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:23:36,724 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=208750)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=208750)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:23:41,455	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:23:41,560	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:23:41,653	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:23:41,655	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:23:51,779 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 62233177498.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 30957076070.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-05-01 00:23:51,779 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:23:51,779 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:23:51,793 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:23:51,794 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:23:51,795 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:23:51,795 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:23:54,271 | server.py:94 | initial parameters (loss, other metrics): 2.301478147506714, {'accuracy': 0.1227, 'data_size': 10000}
INFO flwr 2024-05-01 00:23:54,271 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:23:54,271 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=257922)[0m 2024-05-01 00:23:57.573802: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=257918)[0m 2024-05-01 00:23:57.685047: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=257918)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=257918)[0m 2024-05-01 00:23:59.770377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=257918)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=257918)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=257925)[0m 2024-05-01 00:23:57.836499: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=257925)[0m 2024-05-01 00:23:57.937065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=257925)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=257925)[0m 2024-05-01 00:23:59.964241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:24:13,390 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:24:14,513 | server.py:125 | fit progress: (1, 1.9174226522445679, {'accuracy': 0.5813, 'data_size': 10000}, 20.241875358042307)
INFO flwr 2024-05-01 00:24:14,513 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:24:14,513 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:24:22,937 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:24:24,030 | server.py:125 | fit progress: (2, 1.853555679321289, {'accuracy': 0.6082, 'data_size': 10000}, 29.758544320997316)
INFO flwr 2024-05-01 00:24:24,030 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:24:24,030 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:24:31,696 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:24:33,015 | server.py:125 | fit progress: (3, 1.768021821975708, {'accuracy': 0.6924, 'data_size': 10000}, 38.74412588000996)
INFO flwr 2024-05-01 00:24:33,015 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:24:33,016 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:24:41,061 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:24:42,554 | server.py:125 | fit progress: (4, 1.7156857252120972, {'accuracy': 0.7443, 'data_size': 10000}, 48.2830858870293)
INFO flwr 2024-05-01 00:24:42,554 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:24:42,555 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:24:50,623 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:24:52,085 | server.py:125 | fit progress: (5, 1.7136826515197754, {'accuracy': 0.7462, 'data_size': 10000}, 57.81348013301613)
INFO flwr 2024-05-01 00:24:52,085 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:24:52,085 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:25:00,173 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:25:01,332 | server.py:125 | fit progress: (6, 1.653098225593567, {'accuracy': 0.8079, 'data_size': 10000}, 67.06144020601641)
INFO flwr 2024-05-01 00:25:01,333 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:25:01,333 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:25:10,014 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:25:11,223 | server.py:125 | fit progress: (7, 1.61881685256958, {'accuracy': 0.8445, 'data_size': 10000}, 76.95200519502396)
INFO flwr 2024-05-01 00:25:11,223 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:25:11,224 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:25:19,316 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:25:20,640 | server.py:125 | fit progress: (8, 1.624356985092163, {'accuracy': 0.8383, 'data_size': 10000}, 86.3684759750031)
INFO flwr 2024-05-01 00:25:20,640 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:25:20,640 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:25:28,641 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:25:30,027 | server.py:125 | fit progress: (9, 1.6172634363174438, {'accuracy': 0.8442, 'data_size': 10000}, 95.75583491899306)
INFO flwr 2024-05-01 00:25:30,027 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:25:30,027 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:25:38,091 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:25:39,556 | server.py:125 | fit progress: (10, 1.6034867763519287, {'accuracy': 0.8582, 'data_size': 10000}, 105.28457910602447)
INFO flwr 2024-05-01 00:25:39,556 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:25:39,556 | server.py:153 | FL finished in 105.28500328800874
INFO flwr 2024-05-01 00:25:39,556 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:25:39,556 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:25:39,556 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:25:39,557 | app.py:229 | app_fit: losses_centralized [(0, 2.301478147506714), (1, 1.9174226522445679), (2, 1.853555679321289), (3, 1.768021821975708), (4, 1.7156857252120972), (5, 1.7136826515197754), (6, 1.653098225593567), (7, 1.61881685256958), (8, 1.624356985092163), (9, 1.6172634363174438), (10, 1.6034867763519287)]
INFO flwr 2024-05-01 00:25:39,557 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1227), (1, 0.5813), (2, 0.6082), (3, 0.6924), (4, 0.7443), (5, 0.7462), (6, 0.8079), (7, 0.8445), (8, 0.8383), (9, 0.8442), (10, 0.8582)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8582
wandb:     loss 1.60349
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_002336-a2yc925i
wandb: Find logs at: ./wandb/offline-run-20240501_002336-a2yc925i/logs
INFO flwr 2024-05-01 00:25:43,202 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:25:44,018 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=257929)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=257929)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:25:49,203	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:25:49,312	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:25:49,407	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:25:49,410	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:26:00,295 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 30099476889.0, 'node:10.20.240.18': 1.0, 'memory': 60232112743.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-05-01 00:26:00,295 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:26:00,295 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:26:00,314 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:26:00,316 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:26:00,316 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:26:00,316 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:26:03,356 | server.py:94 | initial parameters (loss, other metrics): 2.3012304306030273, {'accuracy': 0.12, 'data_size': 10000}
INFO flwr 2024-05-01 00:26:03,356 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:26:03,357 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=309904)[0m 2024-05-01 00:26:06.271425: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=309904)[0m 2024-05-01 00:26:06.368001: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=309904)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=309899)[0m 2024-05-01 00:26:08.554714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=309904)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=309904)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=309908)[0m 2024-05-01 00:26:06.473791: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=309908)[0m 2024-05-01 00:26:06.573633: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=309908)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=309908)[0m 2024-05-01 00:26:08.739538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:26:22,211 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:26:23,411 | server.py:125 | fit progress: (1, 1.9000403881072998, {'accuracy': 0.6031, 'data_size': 10000}, 20.054396057967097)
INFO flwr 2024-05-01 00:26:23,411 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:26:23,411 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:26:32,253 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:26:33,351 | server.py:125 | fit progress: (2, 1.79063081741333, {'accuracy': 0.6802, 'data_size': 10000}, 29.99462015798781)
INFO flwr 2024-05-01 00:26:33,351 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:26:33,352 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:26:41,471 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:26:42,777 | server.py:125 | fit progress: (3, 1.7074732780456543, {'accuracy': 0.7606, 'data_size': 10000}, 39.42018665996147)
INFO flwr 2024-05-01 00:26:42,777 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:26:42,777 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:26:50,297 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:26:51,583 | server.py:125 | fit progress: (4, 1.6230714321136475, {'accuracy': 0.8416, 'data_size': 10000}, 48.22651515400503)
INFO flwr 2024-05-01 00:26:51,583 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:26:51,583 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:26:59,157 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:27:00,463 | server.py:125 | fit progress: (5, 1.6185072660446167, {'accuracy': 0.8458, 'data_size': 10000}, 57.10679498396348)
INFO flwr 2024-05-01 00:27:00,464 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:27:00,464 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:27:08,273 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:27:09,445 | server.py:125 | fit progress: (6, 1.6239558458328247, {'accuracy': 0.8376, 'data_size': 10000}, 66.08840017201146)
INFO flwr 2024-05-01 00:27:09,445 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:27:09,445 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:27:17,515 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:27:18,629 | server.py:125 | fit progress: (7, 1.6066818237304688, {'accuracy': 0.8548, 'data_size': 10000}, 75.27212009200593)
INFO flwr 2024-05-01 00:27:18,629 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:27:18,629 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:27:26,529 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:27:27,883 | server.py:125 | fit progress: (8, 1.580549716949463, {'accuracy': 0.8814, 'data_size': 10000}, 84.52601454599062)
INFO flwr 2024-05-01 00:27:27,883 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:27:27,883 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:27:35,999 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:27:37,364 | server.py:125 | fit progress: (9, 1.5861443281173706, {'accuracy': 0.8773, 'data_size': 10000}, 94.00755431095604)
INFO flwr 2024-05-01 00:27:37,364 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:27:37,365 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:27:45,357 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:27:46,811 | server.py:125 | fit progress: (10, 1.5832289457321167, {'accuracy': 0.8797, 'data_size': 10000}, 103.45490735897329)
INFO flwr 2024-05-01 00:27:46,812 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:27:46,812 | server.py:153 | FL finished in 103.45548915897962
INFO flwr 2024-05-01 00:27:46,812 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:27:46,812 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:27:46,812 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:27:46,812 | app.py:229 | app_fit: losses_centralized [(0, 2.3012304306030273), (1, 1.9000403881072998), (2, 1.79063081741333), (3, 1.7074732780456543), (4, 1.6230714321136475), (5, 1.6185072660446167), (6, 1.6239558458328247), (7, 1.6066818237304688), (8, 1.580549716949463), (9, 1.5861443281173706), (10, 1.5832289457321167)]
INFO flwr 2024-05-01 00:27:46,813 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.12), (1, 0.6031), (2, 0.6802), (3, 0.7606), (4, 0.8416), (5, 0.8458), (6, 0.8376), (7, 0.8548), (8, 0.8814), (9, 0.8773), (10, 0.8797)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8797
wandb:     loss 1.58323
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_002543-id9nm86a
wandb: Find logs at: ./wandb/offline-run-20240501_002543-id9nm86a/logs
INFO flwr 2024-05-01 00:27:50,468 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:27:51,319 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=309899)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=309899)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:27:56,217	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:27:56,372	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:27:56,472	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:27:56,473	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:28:07,229 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 30653665689.0, 'node:10.20.240.18': 1.0, 'memory': 61525219943.0}
INFO flwr 2024-05-01 00:28:07,229 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:28:07,229 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:28:07,248 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:28:07,249 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:28:07,249 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:28:07,250 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:28:10,101 | server.py:94 | initial parameters (loss, other metrics): 2.300086259841919, {'accuracy': 0.1141, 'data_size': 10000}
INFO flwr 2024-05-01 00:28:10,101 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:28:10,101 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=362989)[0m 2024-05-01 00:28:13.421861: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=362989)[0m 2024-05-01 00:28:13.522461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=362989)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=362989)[0m 2024-05-01 00:28:15.642591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=362989)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=362989)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=362988)[0m 2024-05-01 00:28:13.604396: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=362988)[0m 2024-05-01 00:28:13.706258: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=362988)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=362999)[0m 2024-05-01 00:28:15.923080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:28:29,711 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:28:30,884 | server.py:125 | fit progress: (1, 1.8914684057235718, {'accuracy': 0.601, 'data_size': 10000}, 20.782167088997085)
INFO flwr 2024-05-01 00:28:30,884 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:28:30,884 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:28:39,936 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:28:41,000 | server.py:125 | fit progress: (2, 1.6697025299072266, {'accuracy': 0.8157, 'data_size': 10000}, 30.89865790202748)
INFO flwr 2024-05-01 00:28:41,000 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:28:41,000 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:28:49,512 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:28:50,934 | server.py:125 | fit progress: (3, 1.6242411136627197, {'accuracy': 0.8475, 'data_size': 10000}, 40.832204658014234)
INFO flwr 2024-05-01 00:28:50,934 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:28:50,934 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:28:58,987 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:29:00,346 | server.py:125 | fit progress: (4, 1.6306021213531494, {'accuracy': 0.8336, 'data_size': 10000}, 50.24418517301092)
INFO flwr 2024-05-01 00:29:00,346 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:29:00,346 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:29:08,303 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:29:09,737 | server.py:125 | fit progress: (5, 1.5851153135299683, {'accuracy': 0.8795, 'data_size': 10000}, 59.6352463500225)
INFO flwr 2024-05-01 00:29:09,737 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:29:09,737 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:29:17,663 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:29:18,916 | server.py:125 | fit progress: (6, 1.585155963897705, {'accuracy': 0.8766, 'data_size': 10000}, 68.81499333202373)
INFO flwr 2024-05-01 00:29:18,917 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:29:18,917 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:29:27,095 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:29:28,270 | server.py:125 | fit progress: (7, 1.5869534015655518, {'accuracy': 0.8749, 'data_size': 10000}, 78.16913746698992)
INFO flwr 2024-05-01 00:29:28,271 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:29:28,271 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:29:36,168 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:29:37,552 | server.py:125 | fit progress: (8, 1.5632960796356201, {'accuracy': 0.8996, 'data_size': 10000}, 87.45113792899065)
INFO flwr 2024-05-01 00:29:37,553 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:29:37,553 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:29:45,085 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:29:46,369 | server.py:125 | fit progress: (9, 1.5675305128097534, {'accuracy': 0.8961, 'data_size': 10000}, 96.26805293303914)
INFO flwr 2024-05-01 00:29:46,370 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:29:46,370 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:29:53,830 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:29:55,116 | server.py:125 | fit progress: (10, 1.5683021545410156, {'accuracy': 0.8948, 'data_size': 10000}, 105.01451471901964)
INFO flwr 2024-05-01 00:29:55,116 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:29:55,116 | server.py:153 | FL finished in 105.01486846199259
INFO flwr 2024-05-01 00:29:55,116 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:29:55,116 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:29:55,117 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:29:55,117 | app.py:229 | app_fit: losses_centralized [(0, 2.300086259841919), (1, 1.8914684057235718), (2, 1.6697025299072266), (3, 1.6242411136627197), (4, 1.6306021213531494), (5, 1.5851153135299683), (6, 1.585155963897705), (7, 1.5869534015655518), (8, 1.5632960796356201), (9, 1.5675305128097534), (10, 1.5683021545410156)]
INFO flwr 2024-05-01 00:29:55,117 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1141), (1, 0.601), (2, 0.8157), (3, 0.8475), (4, 0.8336), (5, 0.8795), (6, 0.8766), (7, 0.8749), (8, 0.8996), (9, 0.8961), (10, 0.8948)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8948
wandb:     loss 1.5683
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_002750-rgfkg2by
wandb: Find logs at: ./wandb/offline-run-20240501_002750-rgfkg2by/logs
INFO flwr 2024-05-01 00:29:58,624 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:29:59,312 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=362986)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=362986)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:30:04,292	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:30:04,394	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:30:04,504	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:30:04,505	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:30:16,335 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 56257511424.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 28128755712.0}
INFO flwr 2024-05-01 00:30:16,335 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:30:16,335 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:30:16,353 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:30:16,355 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:30:16,355 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:30:16,356 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:30:18,950 | server.py:94 | initial parameters (loss, other metrics): 2.3009493350982666, {'accuracy': 0.1242, 'data_size': 10000}
INFO flwr 2024-05-01 00:30:18,950 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:30:18,951 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=415320)[0m 2024-05-01 00:30:22.978891: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=415324)[0m 2024-05-01 00:30:23.058540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=415324)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=415319)[0m 2024-05-01 00:30:25.167916: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=415327)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=415327)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=415327)[0m 2024-05-01 00:30:23.177550: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=415327)[0m 2024-05-01 00:30:23.272751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=415327)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=415327)[0m 2024-05-01 00:30:25.337790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:30:40,420 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:30:41,543 | server.py:125 | fit progress: (1, 1.8962218761444092, {'accuracy': 0.6028, 'data_size': 10000}, 22.591759220988024)
INFO flwr 2024-05-01 00:30:41,543 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:30:41,543 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:30:51,172 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:30:52,337 | server.py:125 | fit progress: (2, 1.9101210832595825, {'accuracy': 0.5416, 'data_size': 10000}, 33.386398844013456)
INFO flwr 2024-05-01 00:30:52,337 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:30:52,338 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:31:00,624 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:31:02,113 | server.py:125 | fit progress: (3, 1.736131191253662, {'accuracy': 0.7286, 'data_size': 10000}, 43.16194424399873)
INFO flwr 2024-05-01 00:31:02,113 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:31:02,113 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:31:10,679 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:31:12,146 | server.py:125 | fit progress: (4, 1.6841140985488892, {'accuracy': 0.7815, 'data_size': 10000}, 53.19556251197355)
INFO flwr 2024-05-01 00:31:12,147 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:31:12,147 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:31:20,655 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:31:21,987 | server.py:125 | fit progress: (5, 1.6837409734725952, {'accuracy': 0.7805, 'data_size': 10000}, 63.03607996599749)
INFO flwr 2024-05-01 00:31:21,987 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:31:21,987 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:31:30,663 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:31:31,818 | server.py:125 | fit progress: (6, 1.6722480058670044, {'accuracy': 0.7899, 'data_size': 10000}, 72.86749296297785)
INFO flwr 2024-05-01 00:31:31,819 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:31:31,819 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:31:40,399 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:31:41,597 | server.py:125 | fit progress: (7, 1.6347084045410156, {'accuracy': 0.8263, 'data_size': 10000}, 82.64610962796723)
INFO flwr 2024-05-01 00:31:41,597 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:31:41,597 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:31:50,489 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:31:51,817 | server.py:125 | fit progress: (8, 1.6043024063110352, {'accuracy': 0.8582, 'data_size': 10000}, 92.86614581401227)
INFO flwr 2024-05-01 00:31:51,817 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:31:51,817 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:32:00,072 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:32:01,435 | server.py:125 | fit progress: (9, 1.591253638267517, {'accuracy': 0.8702, 'data_size': 10000}, 102.48403865500586)
INFO flwr 2024-05-01 00:32:01,435 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:32:01,435 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:32:09,716 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:32:11,096 | server.py:125 | fit progress: (10, 1.59470534324646, {'accuracy': 0.8676, 'data_size': 10000}, 112.14487481501419)
INFO flwr 2024-05-01 00:32:11,096 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:32:11,096 | server.py:153 | FL finished in 112.14524586999323
INFO flwr 2024-05-01 00:32:11,096 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:32:11,096 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:32:11,096 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:32:11,096 | app.py:229 | app_fit: losses_centralized [(0, 2.3009493350982666), (1, 1.8962218761444092), (2, 1.9101210832595825), (3, 1.736131191253662), (4, 1.6841140985488892), (5, 1.6837409734725952), (6, 1.6722480058670044), (7, 1.6347084045410156), (8, 1.6043024063110352), (9, 1.591253638267517), (10, 1.59470534324646)]
INFO flwr 2024-05-01 00:32:11,097 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1242), (1, 0.6028), (2, 0.5416), (3, 0.7286), (4, 0.7815), (5, 0.7805), (6, 0.7899), (7, 0.8263), (8, 0.8582), (9, 0.8702), (10, 0.8676)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8676
wandb:     loss 1.59471
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_002958-1ykvehxd
wandb: Find logs at: ./wandb/offline-run-20240501_002958-1ykvehxd/logs
INFO flwr 2024-05-01 00:32:14,679 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:32:15,357 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=415326)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=415326)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:32:20,140	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:32:20,236	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:32:20,348	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:32:20,350	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:32:29,985 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 58309860558.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 29154930278.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-05-01 00:32:29,986 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:32:29,986 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:32:30,000 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:32:30,001 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:32:30,001 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:32:30,001 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:32:32,641 | server.py:94 | initial parameters (loss, other metrics): 2.306201457977295, {'accuracy': 0.0772, 'data_size': 10000}
INFO flwr 2024-05-01 00:32:32,641 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:32:32,641 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=469469)[0m 2024-05-01 00:32:35.568554: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=469469)[0m 2024-05-01 00:32:35.658566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=469469)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=469469)[0m 2024-05-01 00:32:37.641157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=469463)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=469463)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=469471)[0m 2024-05-01 00:32:35.790355: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=469471)[0m 2024-05-01 00:32:35.890738: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=469471)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=469463)[0m 2024-05-01 00:32:37.817759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:32:52,774 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:32:54,017 | server.py:125 | fit progress: (1, 1.8439255952835083, {'accuracy': 0.6652, 'data_size': 10000}, 21.375387868960388)
INFO flwr 2024-05-01 00:32:54,017 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:32:54,017 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:33:03,354 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:33:04,562 | server.py:125 | fit progress: (2, 1.6775104999542236, {'accuracy': 0.8114, 'data_size': 10000}, 31.92117717198562)
INFO flwr 2024-05-01 00:33:04,563 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:33:04,563 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:33:13,256 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:33:14,718 | server.py:125 | fit progress: (3, 1.616049885749817, {'accuracy': 0.8533, 'data_size': 10000}, 42.0764704629546)
INFO flwr 2024-05-01 00:33:14,718 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:33:14,718 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:33:23,194 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:33:24,543 | server.py:125 | fit progress: (4, 1.6055635213851929, {'accuracy': 0.8612, 'data_size': 10000}, 51.90169331996003)
INFO flwr 2024-05-01 00:33:24,543 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:33:24,543 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:33:33,390 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:33:34,866 | server.py:125 | fit progress: (5, 1.6004672050476074, {'accuracy': 0.8625, 'data_size': 10000}, 62.224415674980264)
INFO flwr 2024-05-01 00:33:34,866 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:33:34,866 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:33:43,001 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:33:44,204 | server.py:125 | fit progress: (6, 1.5942281484603882, {'accuracy': 0.8688, 'data_size': 10000}, 71.5623272069497)
INFO flwr 2024-05-01 00:33:44,204 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:33:44,204 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:33:53,045 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:33:54,101 | server.py:125 | fit progress: (7, 1.588212013244629, {'accuracy': 0.8755, 'data_size': 10000}, 81.45986800396349)
INFO flwr 2024-05-01 00:33:54,101 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:33:54,102 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:34:02,235 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:34:03,509 | server.py:125 | fit progress: (8, 1.5636712312698364, {'accuracy': 0.8979, 'data_size': 10000}, 90.86744043097133)
INFO flwr 2024-05-01 00:34:03,509 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:34:03,509 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:34:11,719 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:34:12,991 | server.py:125 | fit progress: (9, 1.5622658729553223, {'accuracy': 0.8997, 'data_size': 10000}, 100.34980267594801)
INFO flwr 2024-05-01 00:34:12,991 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:34:12,991 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:34:20,518 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:34:21,857 | server.py:125 | fit progress: (10, 1.5634604692459106, {'accuracy': 0.8994, 'data_size': 10000}, 109.21600101498188)
INFO flwr 2024-05-01 00:34:21,858 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:34:21,858 | server.py:153 | FL finished in 109.21641403296962
INFO flwr 2024-05-01 00:34:21,858 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:34:21,858 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:34:21,858 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:34:21,858 | app.py:229 | app_fit: losses_centralized [(0, 2.306201457977295), (1, 1.8439255952835083), (2, 1.6775104999542236), (3, 1.616049885749817), (4, 1.6055635213851929), (5, 1.6004672050476074), (6, 1.5942281484603882), (7, 1.588212013244629), (8, 1.5636712312698364), (9, 1.5622658729553223), (10, 1.5634604692459106)]
INFO flwr 2024-05-01 00:34:21,858 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0772), (1, 0.6652), (2, 0.8114), (3, 0.8533), (4, 0.8612), (5, 0.8625), (6, 0.8688), (7, 0.8755), (8, 0.8979), (9, 0.8997), (10, 0.8994)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8994
wandb:     loss 1.56346
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_003214-x6wc3egm
wandb: Find logs at: ./wandb/offline-run-20240501_003214-x6wc3egm/logs
INFO flwr 2024-05-01 00:34:25,415 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:34:26,151 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=469462)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=469462)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:34:31,301	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:34:31,404	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:34:31,516	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:34:31,518	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:34:42,191 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 57214178919.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 28607089459.0}
INFO flwr 2024-05-01 00:34:42,192 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:34:42,192 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:34:42,211 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:34:42,213 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:34:42,214 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:34:42,214 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:34:45,610 | server.py:94 | initial parameters (loss, other metrics): 2.302257537841797, {'accuracy': 0.1089, 'data_size': 10000}
INFO flwr 2024-05-01 00:34:45,611 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:34:45,611 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=525355)[0m 2024-05-01 00:34:48.610075: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=525355)[0m 2024-05-01 00:34:48.729006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=525355)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=525355)[0m 2024-05-01 00:34:50.816685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=525355)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=525355)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=525346)[0m 2024-05-01 00:34:48.835401: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=525346)[0m 2024-05-01 00:34:48.925074: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=525346)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=525346)[0m 2024-05-01 00:34:50.901981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:35:05,832 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:35:07,015 | server.py:125 | fit progress: (1, 1.9031519889831543, {'accuracy': 0.5904, 'data_size': 10000}, 21.40396900201449)
INFO flwr 2024-05-01 00:35:07,015 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:35:07,015 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:35:16,301 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:35:17,409 | server.py:125 | fit progress: (2, 1.6971373558044434, {'accuracy': 0.7811, 'data_size': 10000}, 31.798048279015347)
INFO flwr 2024-05-01 00:35:17,409 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:35:17,409 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:35:26,245 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:35:27,595 | server.py:125 | fit progress: (3, 1.6719111204147339, {'accuracy': 0.7975, 'data_size': 10000}, 41.984537444019224)
INFO flwr 2024-05-01 00:35:27,596 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:35:27,596 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:35:35,779 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:35:37,113 | server.py:125 | fit progress: (4, 1.659693717956543, {'accuracy': 0.8047, 'data_size': 10000}, 51.501784358988516)
INFO flwr 2024-05-01 00:35:37,113 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:35:37,113 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:35:46,017 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:35:47,439 | server.py:125 | fit progress: (5, 1.6371763944625854, {'accuracy': 0.8253, 'data_size': 10000}, 61.82856674003415)
INFO flwr 2024-05-01 00:35:47,440 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:35:47,440 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:35:55,592 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:35:56,677 | server.py:125 | fit progress: (6, 1.6352728605270386, {'accuracy': 0.827, 'data_size': 10000}, 71.06673552899156)
INFO flwr 2024-05-01 00:35:56,678 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:35:56,678 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:36:04,664 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:36:05,744 | server.py:125 | fit progress: (7, 1.6304049491882324, {'accuracy': 0.8315, 'data_size': 10000}, 80.13314969703788)
INFO flwr 2024-05-01 00:36:05,744 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:36:05,744 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:36:13,840 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:36:15,139 | server.py:125 | fit progress: (8, 1.6190061569213867, {'accuracy': 0.8433, 'data_size': 10000}, 89.5278156180284)
INFO flwr 2024-05-01 00:36:15,139 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:36:15,139 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:36:24,043 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:36:25,538 | server.py:125 | fit progress: (9, 1.5986008644104004, {'accuracy': 0.8632, 'data_size': 10000}, 99.9276650880347)
INFO flwr 2024-05-01 00:36:25,539 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:36:25,539 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:36:34,188 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:36:35,471 | server.py:125 | fit progress: (10, 1.5964443683624268, {'accuracy': 0.8653, 'data_size': 10000}, 109.86042724101571)
INFO flwr 2024-05-01 00:36:35,471 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:36:35,472 | server.py:153 | FL finished in 109.86077197099803
INFO flwr 2024-05-01 00:36:35,472 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:36:35,472 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:36:35,472 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:36:35,472 | app.py:229 | app_fit: losses_centralized [(0, 2.302257537841797), (1, 1.9031519889831543), (2, 1.6971373558044434), (3, 1.6719111204147339), (4, 1.659693717956543), (5, 1.6371763944625854), (6, 1.6352728605270386), (7, 1.6304049491882324), (8, 1.6190061569213867), (9, 1.5986008644104004), (10, 1.5964443683624268)]
INFO flwr 2024-05-01 00:36:35,472 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1089), (1, 0.5904), (2, 0.7811), (3, 0.7975), (4, 0.8047), (5, 0.8253), (6, 0.827), (7, 0.8315), (8, 0.8433), (9, 0.8632), (10, 0.8653)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8653
wandb:     loss 1.59644
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_003425-tf0tgrah
wandb: Find logs at: ./wandb/offline-run-20240501_003425-tf0tgrah/logs
INFO flwr 2024-05-01 00:36:38,976 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:36:39,697 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=525341)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=525341)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:36:44,702	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:36:44,808	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:36:44,932	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:36:44,934	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:36:55,806 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 52982619342.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 26491309670.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-05-01 00:36:55,806 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:36:55,807 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:36:55,824 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:36:55,827 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:36:55,828 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:36:55,828 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:36:59,862 | server.py:94 | initial parameters (loss, other metrics): 2.30629301071167, {'accuracy': 0.0896, 'data_size': 10000}
INFO flwr 2024-05-01 00:36:59,863 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:36:59,864 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=580205)[0m 2024-05-01 00:37:02.054390: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=580205)[0m 2024-05-01 00:37:02.151861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=580205)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=580207)[0m 2024-05-01 00:37:04.134363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=580205)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=580205)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=580203)[0m 2024-05-01 00:37:02.307813: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=580203)[0m 2024-05-01 00:37:02.401057: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=580203)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=580203)[0m 2024-05-01 00:37:04.221791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:37:21,183 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:37:22,324 | server.py:125 | fit progress: (1, 1.9235976934432983, {'accuracy': 0.5585, 'data_size': 10000}, 22.46054960798938)
INFO flwr 2024-05-01 00:37:22,324 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:37:22,325 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:37:33,263 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:37:34,487 | server.py:125 | fit progress: (2, 1.7577472925186157, {'accuracy': 0.7044, 'data_size': 10000}, 34.62342343799537)
INFO flwr 2024-05-01 00:37:34,487 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:37:34,487 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:37:43,757 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:37:45,071 | server.py:125 | fit progress: (3, 1.6664316654205322, {'accuracy': 0.8035, 'data_size': 10000}, 45.20726764801657)
INFO flwr 2024-05-01 00:37:45,071 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:37:45,071 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:37:54,730 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:37:56,123 | server.py:125 | fit progress: (4, 1.6592414379119873, {'accuracy': 0.8054, 'data_size': 10000}, 56.2596221989952)
INFO flwr 2024-05-01 00:37:56,123 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:37:56,124 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:38:05,704 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:38:07,046 | server.py:125 | fit progress: (5, 1.5972243547439575, {'accuracy': 0.8677, 'data_size': 10000}, 67.18281365500297)
INFO flwr 2024-05-01 00:38:07,047 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:38:07,047 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:38:15,961 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:38:17,044 | server.py:125 | fit progress: (6, 1.5925683975219727, {'accuracy': 0.8715, 'data_size': 10000}, 77.18089492496802)
INFO flwr 2024-05-01 00:38:17,045 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:38:17,045 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:38:26,527 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:38:27,658 | server.py:125 | fit progress: (7, 1.585234522819519, {'accuracy': 0.8782, 'data_size': 10000}, 87.79434562899405)
INFO flwr 2024-05-01 00:38:27,658 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:38:27,658 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:38:36,745 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:38:38,064 | server.py:125 | fit progress: (8, 1.5787837505340576, {'accuracy': 0.8829, 'data_size': 10000}, 98.20024876302341)
INFO flwr 2024-05-01 00:38:38,064 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:38:38,064 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:38:48,105 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:38:49,470 | server.py:125 | fit progress: (9, 1.5762938261032104, {'accuracy': 0.8865, 'data_size': 10000}, 109.60618003597483)
INFO flwr 2024-05-01 00:38:49,470 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:38:49,470 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:38:58,587 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:38:59,925 | server.py:125 | fit progress: (10, 1.57319974899292, {'accuracy': 0.8878, 'data_size': 10000}, 120.06194283202058)
INFO flwr 2024-05-01 00:38:59,926 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:38:59,926 | server.py:153 | FL finished in 120.06236165901646
INFO flwr 2024-05-01 00:38:59,926 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:38:59,926 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:38:59,926 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:38:59,926 | app.py:229 | app_fit: losses_centralized [(0, 2.30629301071167), (1, 1.9235976934432983), (2, 1.7577472925186157), (3, 1.6664316654205322), (4, 1.6592414379119873), (5, 1.5972243547439575), (6, 1.5925683975219727), (7, 1.585234522819519), (8, 1.5787837505340576), (9, 1.5762938261032104), (10, 1.57319974899292)]
INFO flwr 2024-05-01 00:38:59,927 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0896), (1, 0.5585), (2, 0.7044), (3, 0.8035), (4, 0.8054), (5, 0.8677), (6, 0.8715), (7, 0.8782), (8, 0.8829), (9, 0.8865), (10, 0.8878)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8878
wandb:     loss 1.5732
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_003639-u7drvev2
wandb: Find logs at: ./wandb/offline-run-20240501_003639-u7drvev2/logs
INFO flwr 2024-05-01 00:39:03,452 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:39:04,128 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=580191)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=580191)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:39:08,850	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:39:08,955	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:39:09,058	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:39:09,059	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:39:19,488 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 48460369920.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 24230184960.0, 'CPU': 64.0}
INFO flwr 2024-05-01 00:39:19,488 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:39:19,488 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:39:19,502 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:39:19,503 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:39:19,503 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:39:19,503 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:39:21,851 | server.py:94 | initial parameters (loss, other metrics): 2.301058769226074, {'accuracy': 0.1481, 'data_size': 10000}
INFO flwr 2024-05-01 00:39:21,852 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:39:21,852 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=631019)[0m 2024-05-01 00:39:25.379878: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=631019)[0m 2024-05-01 00:39:25.494472: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=631019)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=631011)[0m 2024-05-01 00:39:27.535759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=631018)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=631018)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=631016)[0m 2024-05-01 00:39:25.633533: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=631016)[0m 2024-05-01 00:39:25.728344: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=631016)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=631016)[0m 2024-05-01 00:39:27.677947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:39:43,871 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:39:45,092 | server.py:125 | fit progress: (1, 1.8264878988265991, {'accuracy': 0.7025, 'data_size': 10000}, 23.239953397016507)
INFO flwr 2024-05-01 00:39:45,093 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:39:45,093 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:39:56,213 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:39:57,443 | server.py:125 | fit progress: (2, 1.6411778926849365, {'accuracy': 0.8381, 'data_size': 10000}, 35.59026567899855)
INFO flwr 2024-05-01 00:39:57,443 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:39:57,443 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:40:07,117 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:40:08,587 | server.py:125 | fit progress: (3, 1.6005181074142456, {'accuracy': 0.8733, 'data_size': 10000}, 46.734456130012404)
INFO flwr 2024-05-01 00:40:08,587 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:40:08,587 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:40:19,481 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:40:20,953 | server.py:125 | fit progress: (4, 1.6148240566253662, {'accuracy': 0.8492, 'data_size': 10000}, 59.100813767989166)
INFO flwr 2024-05-01 00:40:20,953 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:40:20,954 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:40:30,599 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:40:32,065 | server.py:125 | fit progress: (5, 1.5692535638809204, {'accuracy': 0.8959, 'data_size': 10000}, 70.21272201603279)
INFO flwr 2024-05-01 00:40:32,065 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:40:32,066 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:40:42,742 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:40:43,872 | server.py:125 | fit progress: (6, 1.5632375478744507, {'accuracy': 0.8999, 'data_size': 10000}, 82.01929865201237)
INFO flwr 2024-05-01 00:40:43,872 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:40:43,872 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:40:54,029 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:40:55,272 | server.py:125 | fit progress: (7, 1.5767165422439575, {'accuracy': 0.8856, 'data_size': 10000}, 93.42007485101931)
INFO flwr 2024-05-01 00:40:55,273 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:40:55,273 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:41:05,894 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:41:07,263 | server.py:125 | fit progress: (8, 1.5806885957717896, {'accuracy': 0.8807, 'data_size': 10000}, 105.41080918099033)
INFO flwr 2024-05-01 00:41:07,263 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:41:07,264 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:41:16,562 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:41:17,836 | server.py:125 | fit progress: (9, 1.5562163591384888, {'accuracy': 0.9058, 'data_size': 10000}, 115.98356588702882)
INFO flwr 2024-05-01 00:41:17,836 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:41:17,836 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:41:27,222 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:41:28,516 | server.py:125 | fit progress: (10, 1.5589344501495361, {'accuracy': 0.9024, 'data_size': 10000}, 126.66348815598758)
INFO flwr 2024-05-01 00:41:28,516 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:41:28,516 | server.py:153 | FL finished in 126.66385012801038
INFO flwr 2024-05-01 00:41:28,516 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:41:28,516 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:41:28,516 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:41:28,516 | app.py:229 | app_fit: losses_centralized [(0, 2.301058769226074), (1, 1.8264878988265991), (2, 1.6411778926849365), (3, 1.6005181074142456), (4, 1.6148240566253662), (5, 1.5692535638809204), (6, 1.5632375478744507), (7, 1.5767165422439575), (8, 1.5806885957717896), (9, 1.5562163591384888), (10, 1.5589344501495361)]
INFO flwr 2024-05-01 00:41:28,517 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1481), (1, 0.7025), (2, 0.8381), (3, 0.8733), (4, 0.8492), (5, 0.8959), (6, 0.8999), (7, 0.8856), (8, 0.8807), (9, 0.9058), (10, 0.9024)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9024
wandb:     loss 1.55893
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_003903-k0wkqv0z
wandb: Find logs at: ./wandb/offline-run-20240501_003903-k0wkqv0z/logs
INFO flwr 2024-05-01 00:41:32,016 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:41:32,781 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=631016)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=631016)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:41:38,535	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:41:38,714	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:41:38,854	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:41:38,856	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:41:48,584 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 159826340455.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 72782717337.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-05-01 00:41:48,584 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:41:48,584 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:41:48,599 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:41:48,600 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:41:48,601 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:41:48,601 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:41:51,507 | server.py:94 | initial parameters (loss, other metrics): 2.2983670234680176, {'accuracy': 0.1205, 'data_size': 10000}
INFO flwr 2024-05-01 00:41:51,508 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:41:51,508 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=687467)[0m 2024-05-01 00:41:54.852661: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=687464)[0m 2024-05-01 00:41:54.946168: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=687464)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=687467)[0m 2024-05-01 00:41:57.375556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=687464)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=687464)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=687458)[0m 2024-05-01 00:41:54.943669: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=687458)[0m 2024-05-01 00:41:55.043722: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=687458)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=687469)[0m 2024-05-01 00:41:57.369050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:42:14,733 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:42:15,795 | server.py:125 | fit progress: (1, 1.8490532636642456, {'accuracy': 0.6832, 'data_size': 10000}, 24.287039301998448)
INFO flwr 2024-05-01 00:42:15,796 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:42:15,796 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:42:26,446 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:42:27,531 | server.py:125 | fit progress: (2, 1.679795503616333, {'accuracy': 0.7994, 'data_size': 10000}, 36.02253941696836)
INFO flwr 2024-05-01 00:42:27,531 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:42:27,531 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:42:38,728 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:42:40,176 | server.py:125 | fit progress: (3, 1.6515926122665405, {'accuracy': 0.8153, 'data_size': 10000}, 48.66789208201226)
INFO flwr 2024-05-01 00:42:40,177 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:42:40,177 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:42:50,746 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:42:52,193 | server.py:125 | fit progress: (4, 1.6429312229156494, {'accuracy': 0.8209, 'data_size': 10000}, 60.68433985399315)
INFO flwr 2024-05-01 00:42:52,193 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:42:52,193 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:43:02,043 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:43:03,534 | server.py:125 | fit progress: (5, 1.6391338109970093, {'accuracy': 0.823, 'data_size': 10000}, 72.02593272499507)
INFO flwr 2024-05-01 00:43:03,535 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:43:03,535 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:43:12,801 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:43:14,029 | server.py:125 | fit progress: (6, 1.654280424118042, {'accuracy': 0.8084, 'data_size': 10000}, 82.52108498499729)
INFO flwr 2024-05-01 00:43:14,030 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:43:14,030 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:43:24,035 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:43:25,108 | server.py:125 | fit progress: (7, 1.6469779014587402, {'accuracy': 0.8145, 'data_size': 10000}, 93.59995305998018)
INFO flwr 2024-05-01 00:43:25,109 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:43:25,109 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:43:34,322 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:43:35,768 | server.py:125 | fit progress: (8, 1.6336116790771484, {'accuracy': 0.8278, 'data_size': 10000}, 104.25954968499718)
INFO flwr 2024-05-01 00:43:35,768 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:43:35,768 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:43:44,706 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:43:46,047 | server.py:125 | fit progress: (9, 1.640160322189331, {'accuracy': 0.8212, 'data_size': 10000}, 114.53867714200169)
INFO flwr 2024-05-01 00:43:46,047 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:43:46,047 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:43:55,913 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:43:57,259 | server.py:125 | fit progress: (10, 1.6324111223220825, {'accuracy': 0.8294, 'data_size': 10000}, 125.75077151297592)
INFO flwr 2024-05-01 00:43:57,259 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:43:57,260 | server.py:153 | FL finished in 125.75115633098176
INFO flwr 2024-05-01 00:43:57,260 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:43:57,260 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:43:57,260 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:43:57,260 | app.py:229 | app_fit: losses_centralized [(0, 2.2983670234680176), (1, 1.8490532636642456), (2, 1.679795503616333), (3, 1.6515926122665405), (4, 1.6429312229156494), (5, 1.6391338109970093), (6, 1.654280424118042), (7, 1.6469779014587402), (8, 1.6336116790771484), (9, 1.640160322189331), (10, 1.6324111223220825)]
INFO flwr 2024-05-01 00:43:57,260 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1205), (1, 0.6832), (2, 0.7994), (3, 0.8153), (4, 0.8209), (5, 0.823), (6, 0.8084), (7, 0.8145), (8, 0.8278), (9, 0.8212), (10, 0.8294)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8294
wandb:     loss 1.63241
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_004132-w4xr44mb
wandb: Find logs at: ./wandb/offline-run-20240501_004132-w4xr44mb/logs
INFO flwr 2024-05-01 00:44:00,864 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:44:03,161 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=687458)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=687458)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:44:07,962	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:44:08,111	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:44:08,249	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:44:08,251	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:44:17,843 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 159973929575.0, 'object_store_memory': 72845969817.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-05-01 00:44:17,843 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:44:17,843 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:44:17,864 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:44:17,865 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:44:17,865 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:44:17,865 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:44:20,572 | server.py:94 | initial parameters (loss, other metrics): 2.302539825439453, {'accuracy': 0.1459, 'data_size': 10000}
INFO flwr 2024-05-01 00:44:20,572 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:44:20,572 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=747433)[0m 2024-05-01 00:44:23.498031: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=747433)[0m 2024-05-01 00:44:23.603827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=747433)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=747435)[0m 2024-05-01 00:44:25.554351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=747444)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=747444)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=747441)[0m 2024-05-01 00:44:23.555735: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=747441)[0m 2024-05-01 00:44:23.653436: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=747441)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=747442)[0m 2024-05-01 00:44:25.676401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:44:38,942 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:44:40,170 | server.py:125 | fit progress: (1, 1.9937189817428589, {'accuracy': 0.4885, 'data_size': 10000}, 19.598222043015994)
INFO flwr 2024-05-01 00:44:40,171 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:44:40,171 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:44:48,595 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:44:49,800 | server.py:125 | fit progress: (2, 2.0268306732177734, {'accuracy': 0.4342, 'data_size': 10000}, 29.227409189974423)
INFO flwr 2024-05-01 00:44:49,800 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:44:49,800 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:44:57,423 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:44:58,885 | server.py:125 | fit progress: (3, 1.9322476387023926, {'accuracy': 0.5225, 'data_size': 10000}, 38.31288121896796)
INFO flwr 2024-05-01 00:44:58,885 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:44:58,886 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:45:06,262 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:45:07,715 | server.py:125 | fit progress: (4, 1.8489863872528076, {'accuracy': 0.611, 'data_size': 10000}, 47.14251627901103)
INFO flwr 2024-05-01 00:45:07,715 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:45:07,715 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:45:15,299 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:45:16,748 | server.py:125 | fit progress: (5, 1.8371394872665405, {'accuracy': 0.6231, 'data_size': 10000}, 56.17590405297233)
INFO flwr 2024-05-01 00:45:16,748 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:45:16,749 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:45:24,376 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:45:25,539 | server.py:125 | fit progress: (6, 1.865666389465332, {'accuracy': 0.5923, 'data_size': 10000}, 64.96663518098649)
INFO flwr 2024-05-01 00:45:25,539 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:45:25,539 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:45:32,941 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:45:34,173 | server.py:125 | fit progress: (7, 1.8220425844192505, {'accuracy': 0.6366, 'data_size': 10000}, 73.6008766189916)
INFO flwr 2024-05-01 00:45:34,173 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:45:34,174 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:45:41,287 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:45:42,686 | server.py:125 | fit progress: (8, 1.7561928033828735, {'accuracy': 0.7033, 'data_size': 10000}, 82.11376854300033)
INFO flwr 2024-05-01 00:45:42,686 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:45:42,686 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:45:50,377 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:45:51,801 | server.py:125 | fit progress: (9, 1.7362360954284668, {'accuracy': 0.7241, 'data_size': 10000}, 91.22894897998776)
INFO flwr 2024-05-01 00:45:51,801 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:45:51,802 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:45:58,809 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:46:00,105 | server.py:125 | fit progress: (10, 1.7275124788284302, {'accuracy': 0.7319, 'data_size': 10000}, 99.53309711196925)
INFO flwr 2024-05-01 00:46:00,106 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:46:00,106 | server.py:153 | FL finished in 99.53345527499914
INFO flwr 2024-05-01 00:46:00,106 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:46:00,106 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:46:00,106 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:46:00,106 | app.py:229 | app_fit: losses_centralized [(0, 2.302539825439453), (1, 1.9937189817428589), (2, 2.0268306732177734), (3, 1.9322476387023926), (4, 1.8489863872528076), (5, 1.8371394872665405), (6, 1.865666389465332), (7, 1.8220425844192505), (8, 1.7561928033828735), (9, 1.7362360954284668), (10, 1.7275124788284302)]
INFO flwr 2024-05-01 00:46:00,106 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1459), (1, 0.4885), (2, 0.4342), (3, 0.5225), (4, 0.611), (5, 0.6231), (6, 0.5923), (7, 0.6366), (8, 0.7033), (9, 0.7241), (10, 0.7319)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7319
wandb:     loss 1.72751
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_004402-ka9r1q4b
wandb: Find logs at: ./wandb/offline-run-20240501_004402-ka9r1q4b/logs
INFO flwr 2024-05-01 00:46:03,644 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:46:04,413 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=747433)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=747433)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:46:09,059	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:46:09,172	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:46:09,257	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:46:09,258	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:46:18,859 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 160860872090.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 73226088038.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-05-01 00:46:18,860 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:46:18,860 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:46:18,873 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:46:18,874 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:46:18,874 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:46:18,874 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:46:21,335 | server.py:94 | initial parameters (loss, other metrics): 2.3054745197296143, {'accuracy': 0.0666, 'data_size': 10000}
INFO flwr 2024-05-01 00:46:21,335 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:46:21,335 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=795303)[0m 2024-05-01 00:46:24.538600: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=795303)[0m 2024-05-01 00:46:24.632656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=795303)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=795310)[0m 2024-05-01 00:46:26.808638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=795306)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=795306)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=795306)[0m 2024-05-01 00:46:24.761576: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=795306)[0m 2024-05-01 00:46:24.854845: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=795306)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=795303)[0m 2024-05-01 00:46:26.887006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:46:40,173 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:46:41,390 | server.py:125 | fit progress: (1, 1.987952709197998, {'accuracy': 0.4794, 'data_size': 10000}, 20.0547331260168)
INFO flwr 2024-05-01 00:46:41,390 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:46:41,390 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:46:49,722 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:46:50,853 | server.py:125 | fit progress: (2, 1.9129085540771484, {'accuracy': 0.5554, 'data_size': 10000}, 29.51737154700095)
INFO flwr 2024-05-01 00:46:50,853 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:46:50,853 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:46:58,536 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:46:59,925 | server.py:125 | fit progress: (3, 1.8392302989959717, {'accuracy': 0.6205, 'data_size': 10000}, 38.59025492903311)
INFO flwr 2024-05-01 00:46:59,926 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:46:59,926 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:47:07,538 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:47:08,873 | server.py:125 | fit progress: (4, 1.8019373416900635, {'accuracy': 0.6569, 'data_size': 10000}, 47.537845896033105)
INFO flwr 2024-05-01 00:47:08,873 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:47:08,873 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:47:16,602 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:47:18,056 | server.py:125 | fit progress: (5, 1.7944905757904053, {'accuracy': 0.6651, 'data_size': 10000}, 56.72131611703662)
INFO flwr 2024-05-01 00:47:18,057 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:47:18,057 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:47:25,719 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:47:26,817 | server.py:125 | fit progress: (6, 1.7969838380813599, {'accuracy': 0.6631, 'data_size': 10000}, 65.48145872703753)
INFO flwr 2024-05-01 00:47:26,817 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:47:26,817 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:47:34,242 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:47:35,361 | server.py:125 | fit progress: (7, 1.7800124883651733, {'accuracy': 0.6783, 'data_size': 10000}, 74.02542748005362)
INFO flwr 2024-05-01 00:47:35,361 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:47:35,361 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:47:42,771 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:47:44,129 | server.py:125 | fit progress: (8, 1.7658896446228027, {'accuracy': 0.6944, 'data_size': 10000}, 82.79378430300858)
INFO flwr 2024-05-01 00:47:44,129 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:47:44,129 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:47:51,793 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:47:53,144 | server.py:125 | fit progress: (9, 1.7555619478225708, {'accuracy': 0.7028, 'data_size': 10000}, 91.80884722905466)
INFO flwr 2024-05-01 00:47:53,144 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:47:53,144 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:48:00,588 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:48:01,890 | server.py:125 | fit progress: (10, 1.768392562866211, {'accuracy': 0.6914, 'data_size': 10000}, 100.55533038103022)
INFO flwr 2024-05-01 00:48:01,891 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:48:01,891 | server.py:153 | FL finished in 100.55567537603201
INFO flwr 2024-05-01 00:48:01,891 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:48:01,891 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:48:01,891 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:48:01,891 | app.py:229 | app_fit: losses_centralized [(0, 2.3054745197296143), (1, 1.987952709197998), (2, 1.9129085540771484), (3, 1.8392302989959717), (4, 1.8019373416900635), (5, 1.7944905757904053), (6, 1.7969838380813599), (7, 1.7800124883651733), (8, 1.7658896446228027), (9, 1.7555619478225708), (10, 1.768392562866211)]
INFO flwr 2024-05-01 00:48:01,891 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0666), (1, 0.4794), (2, 0.5554), (3, 0.6205), (4, 0.6569), (5, 0.6651), (6, 0.6631), (7, 0.6783), (8, 0.6944), (9, 0.7028), (10, 0.6914)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6914
wandb:     loss 1.76839
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_004603-riuqtutz
wandb: Find logs at: ./wandb/offline-run-20240501_004603-riuqtutz/logs
INFO flwr 2024-05-01 00:48:05,456 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:48:06,171 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=795301)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=795301)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:48:10,793	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:48:10,934	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:48:11,020	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:48:11,021	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:48:20,632 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 153309234996.0, 'object_store_memory': 69989672140.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-05-01 00:48:20,632 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:48:20,632 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:48:20,648 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:48:20,650 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:48:20,650 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:48:20,650 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:48:23,237 | server.py:94 | initial parameters (loss, other metrics): 2.2977893352508545, {'accuracy': 0.1196, 'data_size': 10000}
INFO flwr 2024-05-01 00:48:23,237 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:48:23,237 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=841146)[0m 2024-05-01 00:48:27.619610: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=841154)[0m 2024-05-01 00:48:27.756790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=841154)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=841154)[0m 2024-05-01 00:48:32.512987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=841146)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=841146)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=841156)[0m 2024-05-01 00:48:27.764138: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=841156)[0m 2024-05-01 00:48:27.835228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=841156)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=841143)[0m 2024-05-01 00:48:32.516567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:48:46,326 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:48:47,556 | server.py:125 | fit progress: (1, 1.9348994493484497, {'accuracy': 0.5406, 'data_size': 10000}, 24.318621044047177)
INFO flwr 2024-05-01 00:48:47,556 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:48:47,556 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:48:55,793 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:48:57,021 | server.py:125 | fit progress: (2, 1.9262863397598267, {'accuracy': 0.5302, 'data_size': 10000}, 33.78356665704632)
INFO flwr 2024-05-01 00:48:57,021 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:48:57,021 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:49:04,669 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:49:06,121 | server.py:125 | fit progress: (3, 1.7824138402938843, {'accuracy': 0.6791, 'data_size': 10000}, 42.883805019024294)
INFO flwr 2024-05-01 00:49:06,121 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:49:06,122 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:49:13,611 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:49:15,086 | server.py:125 | fit progress: (4, 1.7028870582580566, {'accuracy': 0.7602, 'data_size': 10000}, 51.848466954019386)
INFO flwr 2024-05-01 00:49:15,086 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:49:15,086 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:49:22,698 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:49:24,038 | server.py:125 | fit progress: (5, 1.6783185005187988, {'accuracy': 0.7831, 'data_size': 10000}, 60.80099778599106)
INFO flwr 2024-05-01 00:49:24,039 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:49:24,039 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:49:31,594 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:49:32,814 | server.py:125 | fit progress: (6, 1.645123839378357, {'accuracy': 0.8168, 'data_size': 10000}, 69.57652250304818)
INFO flwr 2024-05-01 00:49:32,814 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:49:32,814 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:49:40,252 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:49:41,316 | server.py:125 | fit progress: (7, 1.623749852180481, {'accuracy': 0.8378, 'data_size': 10000}, 78.07918842299841)
INFO flwr 2024-05-01 00:49:41,317 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:49:41,317 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:49:48,534 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:49:49,926 | server.py:125 | fit progress: (8, 1.6255974769592285, {'accuracy': 0.8363, 'data_size': 10000}, 86.68903316301294)
INFO flwr 2024-05-01 00:49:49,927 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:49:49,927 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:49:57,225 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:49:58,589 | server.py:125 | fit progress: (9, 1.6281681060791016, {'accuracy': 0.8334, 'data_size': 10000}, 95.35137372004101)
INFO flwr 2024-05-01 00:49:58,589 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:49:58,589 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:50:05,992 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:50:07,387 | server.py:125 | fit progress: (10, 1.6184747219085693, {'accuracy': 0.8428, 'data_size': 10000}, 104.15004806203069)
INFO flwr 2024-05-01 00:50:07,388 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:50:07,388 | server.py:153 | FL finished in 104.15048976504477
INFO flwr 2024-05-01 00:50:07,388 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:50:07,388 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:50:07,388 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:50:07,388 | app.py:229 | app_fit: losses_centralized [(0, 2.2977893352508545), (1, 1.9348994493484497), (2, 1.9262863397598267), (3, 1.7824138402938843), (4, 1.7028870582580566), (5, 1.6783185005187988), (6, 1.645123839378357), (7, 1.623749852180481), (8, 1.6255974769592285), (9, 1.6281681060791016), (10, 1.6184747219085693)]
INFO flwr 2024-05-01 00:50:07,388 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1196), (1, 0.5406), (2, 0.5302), (3, 0.6791), (4, 0.7602), (5, 0.7831), (6, 0.8168), (7, 0.8378), (8, 0.8363), (9, 0.8334), (10, 0.8428)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8428
wandb:     loss 1.61847
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_004805-yi03w8om
wandb: Find logs at: ./wandb/offline-run-20240501_004805-yi03w8om/logs
INFO flwr 2024-05-01 00:50:11,000 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:50:11,733 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=841157)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=841157)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:50:16,758	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:50:16,856	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:50:16,958	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:50:16,960	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:50:27,349 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69978624000.0, 'memory': 153283456000.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-05-01 00:50:27,349 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:50:27,349 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:50:27,368 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:50:27,369 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:50:27,369 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:50:27,369 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:50:30,100 | server.py:94 | initial parameters (loss, other metrics): 2.302748918533325, {'accuracy': 0.0903, 'data_size': 10000}
INFO flwr 2024-05-01 00:50:30,101 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:50:30,101 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=896600)[0m 2024-05-01 00:50:32.882659: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=896600)[0m 2024-05-01 00:50:32.975170: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=896600)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=896600)[0m 2024-05-01 00:50:35.091278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=896600)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=896600)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=896598)[0m 2024-05-01 00:50:33.059467: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=896598)[0m 2024-05-01 00:50:33.151491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=896598)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=896593)[0m 2024-05-01 00:50:35.237582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:50:47,712 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:50:48,785 | server.py:125 | fit progress: (1, 2.137376070022583, {'accuracy': 0.3094, 'data_size': 10000}, 18.684289188997354)
INFO flwr 2024-05-01 00:50:48,786 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:50:48,786 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:50:56,635 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:50:57,701 | server.py:125 | fit progress: (2, 2.0062036514282227, {'accuracy': 0.4561, 'data_size': 10000}, 27.59985262498958)
INFO flwr 2024-05-01 00:50:57,701 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:50:57,701 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:51:04,602 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:51:06,030 | server.py:125 | fit progress: (3, 1.9220459461212158, {'accuracy': 0.5374, 'data_size': 10000}, 35.928635591990314)
INFO flwr 2024-05-01 00:51:06,030 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:51:06,030 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:51:13,406 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:51:14,743 | server.py:125 | fit progress: (4, 1.922333836555481, {'accuracy': 0.5369, 'data_size': 10000}, 44.64234331599437)
INFO flwr 2024-05-01 00:51:14,744 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:51:14,744 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:51:22,393 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:51:23,794 | server.py:125 | fit progress: (5, 1.9179219007492065, {'accuracy': 0.5409, 'data_size': 10000}, 53.692559300980065)
INFO flwr 2024-05-01 00:51:23,794 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:51:23,794 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:51:31,094 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:51:32,244 | server.py:125 | fit progress: (6, 1.904561161994934, {'accuracy': 0.5537, 'data_size': 10000}, 62.14311856497079)
INFO flwr 2024-05-01 00:51:32,244 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:51:32,245 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:51:39,701 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:51:40,832 | server.py:125 | fit progress: (7, 1.8850027322769165, {'accuracy': 0.5738, 'data_size': 10000}, 70.73068434098968)
INFO flwr 2024-05-01 00:51:40,832 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:51:40,832 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:51:48,159 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:51:49,523 | server.py:125 | fit progress: (8, 1.8597745895385742, {'accuracy': 0.5995, 'data_size': 10000}, 79.42185297497781)
INFO flwr 2024-05-01 00:51:49,523 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:51:49,523 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:51:56,822 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:51:58,188 | server.py:125 | fit progress: (9, 1.8420768976211548, {'accuracy': 0.6175, 'data_size': 10000}, 88.08663946401794)
INFO flwr 2024-05-01 00:51:58,188 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:51:58,188 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:52:05,459 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:52:06,821 | server.py:125 | fit progress: (10, 1.8426618576049805, {'accuracy': 0.617, 'data_size': 10000}, 96.72043192200363)
INFO flwr 2024-05-01 00:52:06,822 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:52:06,822 | server.py:153 | FL finished in 96.72081014601281
INFO flwr 2024-05-01 00:52:06,822 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:52:06,822 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:52:06,822 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:52:06,822 | app.py:229 | app_fit: losses_centralized [(0, 2.302748918533325), (1, 2.137376070022583), (2, 2.0062036514282227), (3, 1.9220459461212158), (4, 1.922333836555481), (5, 1.9179219007492065), (6, 1.904561161994934), (7, 1.8850027322769165), (8, 1.8597745895385742), (9, 1.8420768976211548), (10, 1.8426618576049805)]
INFO flwr 2024-05-01 00:52:06,822 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0903), (1, 0.3094), (2, 0.4561), (3, 0.5374), (4, 0.5369), (5, 0.5409), (6, 0.5537), (7, 0.5738), (8, 0.5995), (9, 0.6175), (10, 0.617)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.617
wandb:     loss 1.84266
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_005011-cykllsoo
wandb: Find logs at: ./wandb/offline-run-20240501_005011-cykllsoo/logs
INFO flwr 2024-05-01 00:52:10,375 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:52:11,113 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=896589)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=896589)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:52:15,954	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:52:16,058	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:52:16,164	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:52:16,166	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:52:26,345 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 72691725926.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 159614027162.0}
INFO flwr 2024-05-01 00:52:26,345 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:52:26,345 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:52:26,359 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:52:26,360 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:52:26,360 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:52:26,361 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:52:29,182 | server.py:94 | initial parameters (loss, other metrics): 2.3050355911254883, {'accuracy': 0.0425, 'data_size': 10000}
INFO flwr 2024-05-01 00:52:29,183 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:52:29,183 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=929683)[0m 2024-05-01 00:52:32.022055: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=929683)[0m 2024-05-01 00:52:32.125350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=929683)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=929683)[0m 2024-05-01 00:52:34.101890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=929690)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=929690)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=929679)[0m 2024-05-01 00:52:32.216527: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=929699)[0m 2024-05-01 00:52:32.402647: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=929699)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=929699)[0m 2024-05-01 00:52:34.303665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:52:46,658 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:52:47,858 | server.py:125 | fit progress: (1, 2.0191903114318848, {'accuracy': 0.4478, 'data_size': 10000}, 18.67545981297735)
INFO flwr 2024-05-01 00:52:47,859 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:52:47,859 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:52:55,771 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:52:56,942 | server.py:125 | fit progress: (2, 1.9747430086135864, {'accuracy': 0.4781, 'data_size': 10000}, 27.758559843990952)
INFO flwr 2024-05-01 00:52:56,942 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:52:56,942 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:53:04,618 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:53:06,060 | server.py:125 | fit progress: (3, 1.8573466539382935, {'accuracy': 0.6019, 'data_size': 10000}, 36.877452393993735)
INFO flwr 2024-05-01 00:53:06,061 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:53:06,061 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:53:13,693 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:53:15,185 | server.py:125 | fit progress: (4, 1.8067904710769653, {'accuracy': 0.6531, 'data_size': 10000}, 46.001592462998815)
INFO flwr 2024-05-01 00:53:15,185 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:53:15,185 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:53:22,630 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:53:24,012 | server.py:125 | fit progress: (5, 1.7644191980361938, {'accuracy': 0.6964, 'data_size': 10000}, 54.82867728499696)
INFO flwr 2024-05-01 00:53:24,012 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:53:24,012 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:53:31,346 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:53:32,507 | server.py:125 | fit progress: (6, 1.7445480823516846, {'accuracy': 0.715, 'data_size': 10000}, 63.32424775999971)
INFO flwr 2024-05-01 00:53:32,508 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:53:32,508 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:53:40,164 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:53:41,363 | server.py:125 | fit progress: (7, 1.7409316301345825, {'accuracy': 0.719, 'data_size': 10000}, 72.17992049898021)
INFO flwr 2024-05-01 00:53:41,363 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:53:41,363 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:53:48,724 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:53:50,137 | server.py:125 | fit progress: (8, 1.7407214641571045, {'accuracy': 0.7182, 'data_size': 10000}, 80.953894374019)
INFO flwr 2024-05-01 00:53:50,137 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:53:50,137 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:53:57,729 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:53:59,122 | server.py:125 | fit progress: (9, 1.7405834197998047, {'accuracy': 0.7185, 'data_size': 10000}, 89.93868096300866)
INFO flwr 2024-05-01 00:53:59,122 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:53:59,122 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:54:06,455 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:54:07,866 | server.py:125 | fit progress: (10, 1.7384556531906128, {'accuracy': 0.7215, 'data_size': 10000}, 98.68345689296257)
INFO flwr 2024-05-01 00:54:07,867 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:54:07,867 | server.py:153 | FL finished in 98.6838623600197
INFO flwr 2024-05-01 00:54:07,867 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:54:07,867 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:54:07,867 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:54:07,867 | app.py:229 | app_fit: losses_centralized [(0, 2.3050355911254883), (1, 2.0191903114318848), (2, 1.9747430086135864), (3, 1.8573466539382935), (4, 1.8067904710769653), (5, 1.7644191980361938), (6, 1.7445480823516846), (7, 1.7409316301345825), (8, 1.7407214641571045), (9, 1.7405834197998047), (10, 1.7384556531906128)]
INFO flwr 2024-05-01 00:54:07,867 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0425), (1, 0.4478), (2, 0.4781), (3, 0.6019), (4, 0.6531), (5, 0.6964), (6, 0.715), (7, 0.719), (8, 0.7182), (9, 0.7185), (10, 0.7215)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7215
wandb:     loss 1.73846
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_005210-cav5c36p
wandb: Find logs at: ./wandb/offline-run-20240501_005210-cav5c36p/logs
INFO flwr 2024-05-01 00:54:11,483 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:54:12,169 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=929679)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=929679)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:54:17,355	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:54:17,496	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:54:17,641	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:54:17,643	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:54:27,702 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 164516523418.0, 'object_store_memory': 74792795750.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-05-01 00:54:27,703 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:54:27,703 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:54:27,720 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:54:27,722 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:54:27,722 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:54:27,722 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:54:30,148 | server.py:94 | initial parameters (loss, other metrics): 2.3066885471343994, {'accuracy': 0.0505, 'data_size': 10000}
INFO flwr 2024-05-01 00:54:30,148 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:54:30,148 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=974597)[0m 2024-05-01 00:54:33.200723: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=974597)[0m 2024-05-01 00:54:33.298788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=974597)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=974595)[0m 2024-05-01 00:54:35.279446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=974597)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=974597)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=974600)[0m 2024-05-01 00:54:33.375049: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=974600)[0m 2024-05-01 00:54:33.468285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=974600)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=974603)[0m 2024-05-01 00:54:35.429118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:54:48,355 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:54:49,535 | server.py:125 | fit progress: (1, 1.958274006843567, {'accuracy': 0.5318, 'data_size': 10000}, 19.38703483704012)
INFO flwr 2024-05-01 00:54:49,536 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:54:49,536 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:54:57,592 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:54:58,715 | server.py:125 | fit progress: (2, 1.9193058013916016, {'accuracy': 0.5345, 'data_size': 10000}, 28.56719526904635)
INFO flwr 2024-05-01 00:54:58,716 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:54:58,716 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:55:06,161 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:55:07,536 | server.py:125 | fit progress: (3, 1.7560306787490845, {'accuracy': 0.7076, 'data_size': 10000}, 37.387622219044715)
INFO flwr 2024-05-01 00:55:07,536 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:55:07,536 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:55:14,698 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:55:16,080 | server.py:125 | fit progress: (4, 1.7249563932418823, {'accuracy': 0.7367, 'data_size': 10000}, 45.931921428011265)
INFO flwr 2024-05-01 00:55:16,080 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:55:16,081 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:55:23,457 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:55:24,863 | server.py:125 | fit progress: (5, 1.726134181022644, {'accuracy': 0.7351, 'data_size': 10000}, 54.715263363032136)
INFO flwr 2024-05-01 00:55:24,864 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:55:24,864 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:55:32,289 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:55:33,475 | server.py:125 | fit progress: (6, 1.686058759689331, {'accuracy': 0.7745, 'data_size': 10000}, 63.32703800901072)
INFO flwr 2024-05-01 00:55:33,475 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:55:33,476 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:55:41,155 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:55:42,308 | server.py:125 | fit progress: (7, 1.6739917993545532, {'accuracy': 0.788, 'data_size': 10000}, 72.15995500201825)
INFO flwr 2024-05-01 00:55:42,308 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:55:42,309 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:55:50,150 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:55:51,575 | server.py:125 | fit progress: (8, 1.6624290943145752, {'accuracy': 0.7982, 'data_size': 10000}, 81.42699565505609)
INFO flwr 2024-05-01 00:55:51,575 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:55:51,576 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:55:59,188 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:56:00,574 | server.py:125 | fit progress: (9, 1.6623975038528442, {'accuracy': 0.7988, 'data_size': 10000}, 90.42532569600735)
INFO flwr 2024-05-01 00:56:00,574 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:56:00,574 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:56:08,026 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:56:09,421 | server.py:125 | fit progress: (10, 1.6525068283081055, {'accuracy': 0.8086, 'data_size': 10000}, 99.27293582301354)
INFO flwr 2024-05-01 00:56:09,421 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:56:09,422 | server.py:153 | FL finished in 99.27334299503127
INFO flwr 2024-05-01 00:56:09,422 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:56:09,422 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:56:09,422 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:56:09,422 | app.py:229 | app_fit: losses_centralized [(0, 2.3066885471343994), (1, 1.958274006843567), (2, 1.9193058013916016), (3, 1.7560306787490845), (4, 1.7249563932418823), (5, 1.726134181022644), (6, 1.686058759689331), (7, 1.6739917993545532), (8, 1.6624290943145752), (9, 1.6623975038528442), (10, 1.6525068283081055)]
INFO flwr 2024-05-01 00:56:09,422 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0505), (1, 0.5318), (2, 0.5345), (3, 0.7076), (4, 0.7367), (5, 0.7351), (6, 0.7745), (7, 0.788), (8, 0.7982), (9, 0.7988), (10, 0.8086)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8086
wandb:     loss 1.65251
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_005411-bsqohd3g
wandb: Find logs at: ./wandb/offline-run-20240501_005411-bsqohd3g/logs
INFO flwr 2024-05-01 00:56:12,975 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:56:13,828 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=974592)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=974592)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:56:18,882	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:56:18,979	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:56:19,070	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:56:19,072	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:56:29,117 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 75384782438.0, 'memory': 165897825690.0}
INFO flwr 2024-05-01 00:56:29,117 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:56:29,117 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:56:29,131 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:56:29,132 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:56:29,132 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:56:29,132 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:56:31,597 | server.py:94 | initial parameters (loss, other metrics): 2.3050174713134766, {'accuracy': 0.0858, 'data_size': 10000}
INFO flwr 2024-05-01 00:56:31,597 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:56:31,598 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1015849)[0m 2024-05-01 00:56:34.879686: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1015849)[0m 2024-05-01 00:56:34.979133: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1015849)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1015849)[0m 2024-05-01 00:56:36.977977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1015843)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1015843)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1015845)[0m 2024-05-01 00:56:35.060585: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1015845)[0m 2024-05-01 00:56:35.147565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1015845)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1015845)[0m 2024-05-01 00:56:37.094494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:56:49,602 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:56:50,713 | server.py:125 | fit progress: (1, 2.0668413639068604, {'accuracy': 0.3991, 'data_size': 10000}, 19.11545128695434)
INFO flwr 2024-05-01 00:56:50,713 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:56:50,713 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:56:59,328 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:57:00,547 | server.py:125 | fit progress: (2, 2.0377542972564697, {'accuracy': 0.4202, 'data_size': 10000}, 28.94965185597539)
INFO flwr 2024-05-01 00:57:00,548 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:57:00,548 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:57:08,073 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:57:09,381 | server.py:125 | fit progress: (3, 2.003748893737793, {'accuracy': 0.4566, 'data_size': 10000}, 37.78375728300307)
INFO flwr 2024-05-01 00:57:09,382 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:57:09,382 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:57:17,033 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:57:18,474 | server.py:125 | fit progress: (4, 1.9366657733917236, {'accuracy': 0.5232, 'data_size': 10000}, 46.87595827400219)
INFO flwr 2024-05-01 00:57:18,474 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:57:18,474 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:57:26,476 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:57:27,936 | server.py:125 | fit progress: (5, 1.9045416116714478, {'accuracy': 0.5547, 'data_size': 10000}, 56.33790471998509)
INFO flwr 2024-05-01 00:57:27,936 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:57:27,936 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:57:35,621 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:57:36,694 | server.py:125 | fit progress: (6, 1.9053407907485962, {'accuracy': 0.5536, 'data_size': 10000}, 65.09595295897452)
INFO flwr 2024-05-01 00:57:36,694 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:57:36,694 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:57:44,778 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:57:46,019 | server.py:125 | fit progress: (7, 1.9084603786468506, {'accuracy': 0.551, 'data_size': 10000}, 74.42187685897807)
INFO flwr 2024-05-01 00:57:46,020 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:57:46,020 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:57:53,564 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:57:54,952 | server.py:125 | fit progress: (8, 1.9069130420684814, {'accuracy': 0.5517, 'data_size': 10000}, 83.35430771496613)
INFO flwr 2024-05-01 00:57:54,952 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:57:54,952 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:58:02,533 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 00:58:03,904 | server.py:125 | fit progress: (9, 1.9024832248687744, {'accuracy': 0.5572, 'data_size': 10000}, 92.30641778599238)
INFO flwr 2024-05-01 00:58:03,904 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 00:58:03,905 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:58:11,183 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 00:58:12,637 | server.py:125 | fit progress: (10, 1.8986537456512451, {'accuracy': 0.561, 'data_size': 10000}, 101.0392519699526)
INFO flwr 2024-05-01 00:58:12,637 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 00:58:12,637 | server.py:153 | FL finished in 101.03964380000252
INFO flwr 2024-05-01 00:58:12,637 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 00:58:12,638 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 00:58:12,638 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 00:58:12,638 | app.py:229 | app_fit: losses_centralized [(0, 2.3050174713134766), (1, 2.0668413639068604), (2, 2.0377542972564697), (3, 2.003748893737793), (4, 1.9366657733917236), (5, 1.9045416116714478), (6, 1.9053407907485962), (7, 1.9084603786468506), (8, 1.9069130420684814), (9, 1.9024832248687744), (10, 1.8986537456512451)]
INFO flwr 2024-05-01 00:58:12,638 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0858), (1, 0.3991), (2, 0.4202), (3, 0.4566), (4, 0.5232), (5, 0.5547), (6, 0.5536), (7, 0.551), (8, 0.5517), (9, 0.5572), (10, 0.561)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.561
wandb:     loss 1.89865
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_005613-3aj2fuku
wandb: Find logs at: ./wandb/offline-run-20240501_005613-3aj2fuku/logs
INFO flwr 2024-05-01 00:58:16,281 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 00:58:16,980 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1015842)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1015842)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 00:58:21,887	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 00:58:21,997	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 00:58:22,094	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 00:58:22,096	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 00:58:32,855 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 74670208204.0, 'memory': 164230485812.0}
INFO flwr 2024-05-01 00:58:32,855 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 00:58:32,855 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 00:58:32,869 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 00:58:32,870 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 00:58:32,870 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 00:58:32,870 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 00:58:35,891 | server.py:94 | initial parameters (loss, other metrics): 2.299117088317871, {'accuracy': 0.1409, 'data_size': 10000}
INFO flwr 2024-05-01 00:58:35,892 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 00:58:35,892 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1066833)[0m 2024-05-01 00:58:38.677864: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1066833)[0m 2024-05-01 00:58:38.781691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1066833)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1066833)[0m 2024-05-01 00:58:41.008426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1066839)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1066839)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1066837)[0m 2024-05-01 00:58:38.820642: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1066839)[0m 2024-05-01 00:58:39.018707: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1066839)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1066839)[0m 2024-05-01 00:58:41.236408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 00:58:54,515 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 00:58:55,582 | server.py:125 | fit progress: (1, 2.0234272480010986, {'accuracy': 0.4595, 'data_size': 10000}, 19.690145205997396)
INFO flwr 2024-05-01 00:58:55,582 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 00:58:55,583 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:59:03,995 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 00:59:05,078 | server.py:125 | fit progress: (2, 1.8937054872512817, {'accuracy': 0.571, 'data_size': 10000}, 29.186164029000793)
INFO flwr 2024-05-01 00:59:05,079 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 00:59:05,079 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:59:12,398 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 00:59:13,728 | server.py:125 | fit progress: (3, 1.861010193824768, {'accuracy': 0.5937, 'data_size': 10000}, 37.835514693986624)
INFO flwr 2024-05-01 00:59:13,728 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 00:59:13,728 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:59:20,743 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 00:59:22,031 | server.py:125 | fit progress: (4, 1.7986984252929688, {'accuracy': 0.6613, 'data_size': 10000}, 46.13852368295193)
INFO flwr 2024-05-01 00:59:22,031 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 00:59:22,031 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:59:29,742 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 00:59:31,114 | server.py:125 | fit progress: (5, 1.7482099533081055, {'accuracy': 0.7136, 'data_size': 10000}, 55.2219759519794)
INFO flwr 2024-05-01 00:59:31,114 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 00:59:31,114 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:59:38,149 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 00:59:39,228 | server.py:125 | fit progress: (6, 1.7000970840454102, {'accuracy': 0.7604, 'data_size': 10000}, 63.33599306398537)
INFO flwr 2024-05-01 00:59:39,228 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 00:59:39,229 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:59:46,890 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 00:59:47,966 | server.py:125 | fit progress: (7, 1.684687852859497, {'accuracy': 0.7761, 'data_size': 10000}, 72.07403934997274)
INFO flwr 2024-05-01 00:59:47,966 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 00:59:47,967 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 00:59:54,910 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 00:59:56,194 | server.py:125 | fit progress: (8, 1.6966904401779175, {'accuracy': 0.7646, 'data_size': 10000}, 80.3022556270007)
INFO flwr 2024-05-01 00:59:56,195 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 00:59:56,195 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:00:03,341 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:00:04,646 | server.py:125 | fit progress: (9, 1.6778708696365356, {'accuracy': 0.7827, 'data_size': 10000}, 88.75354794395389)
INFO flwr 2024-05-01 01:00:04,646 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:00:04,646 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:00:11,885 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:00:13,160 | server.py:125 | fit progress: (10, 1.6708170175552368, {'accuracy': 0.7887, 'data_size': 10000}, 97.26762867096113)
INFO flwr 2024-05-01 01:00:13,160 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:00:13,160 | server.py:153 | FL finished in 97.26803984498838
INFO flwr 2024-05-01 01:00:13,160 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:00:13,160 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:00:13,160 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:00:13,161 | app.py:229 | app_fit: losses_centralized [(0, 2.299117088317871), (1, 2.0234272480010986), (2, 1.8937054872512817), (3, 1.861010193824768), (4, 1.7986984252929688), (5, 1.7482099533081055), (6, 1.7000970840454102), (7, 1.684687852859497), (8, 1.6966904401779175), (9, 1.6778708696365356), (10, 1.6708170175552368)]
INFO flwr 2024-05-01 01:00:13,161 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1409), (1, 0.4595), (2, 0.571), (3, 0.5937), (4, 0.6613), (5, 0.7136), (6, 0.7604), (7, 0.7761), (8, 0.7646), (9, 0.7827), (10, 0.7887)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7887
wandb:     loss 1.67082
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_005816-ljr38tuh
wandb: Find logs at: ./wandb/offline-run-20240501_005816-ljr38tuh/logs
INFO flwr 2024-05-01 01:00:16,687 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:00:17,427 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1066831)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1066831)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:00:22,442	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:00:22,549	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:00:22,643	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:00:22,645	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:00:33,411 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 75466685644.0, 'node:10.20.240.18': 1.0, 'memory': 166088933172.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-05-01 01:00:33,411 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:00:33,411 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:00:33,426 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:00:33,427 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:00:33,427 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:00:33,428 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:00:36,447 | server.py:94 | initial parameters (loss, other metrics): 2.3044211864471436, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-05-01 01:00:36,447 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:00:36,447 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1106602)[0m 2024-05-01 01:00:39.653984: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1106602)[0m 2024-05-01 01:00:39.751742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1106602)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1106603)[0m 2024-05-01 01:00:41.765051: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1106602)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1106602)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1106601)[0m 2024-05-01 01:00:39.825051: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1106601)[0m 2024-05-01 01:00:39.920741: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1106601)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1106601)[0m 2024-05-01 01:00:41.891207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:00:55,377 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:00:56,469 | server.py:125 | fit progress: (1, 1.9602047204971313, {'accuracy': 0.5218, 'data_size': 10000}, 20.021431635017507)
INFO flwr 2024-05-01 01:00:56,469 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:00:56,469 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:01:04,586 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:01:05,751 | server.py:125 | fit progress: (2, 1.9176719188690186, {'accuracy': 0.5402, 'data_size': 10000}, 29.303387197025586)
INFO flwr 2024-05-01 01:01:05,751 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:01:05,751 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:01:13,810 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:01:15,245 | server.py:125 | fit progress: (3, 1.8143588304519653, {'accuracy': 0.645, 'data_size': 10000}, 38.797685660014395)
INFO flwr 2024-05-01 01:01:15,245 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:01:15,245 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:01:22,525 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:01:23,856 | server.py:125 | fit progress: (4, 1.757918357849121, {'accuracy': 0.7064, 'data_size': 10000}, 47.40827480197186)
INFO flwr 2024-05-01 01:01:23,856 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:01:23,856 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:01:31,841 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:01:33,324 | server.py:125 | fit progress: (5, 1.7435678243637085, {'accuracy': 0.7196, 'data_size': 10000}, 56.87670566601446)
INFO flwr 2024-05-01 01:01:33,324 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:01:33,325 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:01:40,727 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:01:41,867 | server.py:125 | fit progress: (6, 1.7143704891204834, {'accuracy': 0.748, 'data_size': 10000}, 65.41995718399994)
INFO flwr 2024-05-01 01:01:41,868 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:01:41,868 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:01:52,143 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:01:53,240 | server.py:125 | fit progress: (7, 1.6731089353561401, {'accuracy': 0.7877, 'data_size': 10000}, 76.79261291597504)
INFO flwr 2024-05-01 01:01:53,240 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:01:53,240 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:02:03,469 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:02:04,800 | server.py:125 | fit progress: (8, 1.6855456829071045, {'accuracy': 0.7767, 'data_size': 10000}, 88.35312155098654)
INFO flwr 2024-05-01 01:02:04,801 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:02:04,801 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:02:14,493 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:02:16,747 | server.py:125 | fit progress: (9, 1.6922928094863892, {'accuracy': 0.7692, 'data_size': 10000}, 100.29938502499135)
INFO flwr 2024-05-01 01:02:16,747 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:02:16,747 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:02:26,774 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:02:28,578 | server.py:125 | fit progress: (10, 1.6679060459136963, {'accuracy': 0.7929, 'data_size': 10000}, 112.13089661800768)
INFO flwr 2024-05-01 01:02:28,578 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:02:28,579 | server.py:153 | FL finished in 112.13137444702443
INFO flwr 2024-05-01 01:02:28,579 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:02:28,579 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:02:28,579 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:02:28,579 | app.py:229 | app_fit: losses_centralized [(0, 2.3044211864471436), (1, 1.9602047204971313), (2, 1.9176719188690186), (3, 1.8143588304519653), (4, 1.757918357849121), (5, 1.7435678243637085), (6, 1.7143704891204834), (7, 1.6731089353561401), (8, 1.6855456829071045), (9, 1.6922928094863892), (10, 1.6679060459136963)]
INFO flwr 2024-05-01 01:02:28,579 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.5218), (2, 0.5402), (3, 0.645), (4, 0.7064), (5, 0.7196), (6, 0.748), (7, 0.7877), (8, 0.7767), (9, 0.7692), (10, 0.7929)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7929
wandb:     loss 1.66791
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_010016-hry24dia
wandb: Find logs at: ./wandb/offline-run-20240501_010016-hry24dia/logs
INFO flwr 2024-05-01 01:02:32,116 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:02:33,577 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1106595)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1106595)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:02:43,203	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:02:44,313	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:02:44,415	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:02:44,416	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:02:55,414 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 75336383692.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 165784895284.0}
INFO flwr 2024-05-01 01:02:55,414 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:02:55,414 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:02:55,428 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:02:55,429 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:02:55,430 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:02:55,430 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:02:59,778 | server.py:94 | initial parameters (loss, other metrics): 2.3049814701080322, {'accuracy': 0.0688, 'data_size': 10000}
INFO flwr 2024-05-01 01:02:59,778 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:02:59,779 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1162251)[0m 2024-05-01 01:03:04.786042: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1162250)[0m 2024-05-01 01:03:05.066633: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1162250)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1162247)[0m 2024-05-01 01:03:12.278968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1162248)[0m 2024-05-01 01:03:05.122769: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1162253)[0m 2024-05-01 01:03:05.333659: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1162253)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1162246)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1162246)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1162251)[0m 2024-05-01 01:03:12.275902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:03:36,779 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:03:38,995 | server.py:125 | fit progress: (1, 2.011404275894165, {'accuracy': 0.4722, 'data_size': 10000}, 39.21689206804149)
INFO flwr 2024-05-01 01:03:38,996 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:03:38,996 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:03:49,285 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:03:50,356 | server.py:125 | fit progress: (2, 2.0488057136535645, {'accuracy': 0.4044, 'data_size': 10000}, 50.577310976048466)
INFO flwr 2024-05-01 01:03:50,356 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:03:50,356 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:04:00,094 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:04:01,519 | server.py:125 | fit progress: (3, 1.9933595657348633, {'accuracy': 0.4668, 'data_size': 10000}, 61.74023941101041)
INFO flwr 2024-05-01 01:04:01,519 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:04:01,519 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:04:10,901 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:04:12,241 | server.py:125 | fit progress: (4, 2.0411922931671143, {'accuracy': 0.4124, 'data_size': 10000}, 72.46254033804871)
INFO flwr 2024-05-01 01:04:12,241 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:04:12,241 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:04:20,858 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:04:22,303 | server.py:125 | fit progress: (5, 2.0200459957122803, {'accuracy': 0.4373, 'data_size': 10000}, 82.52500134601723)
INFO flwr 2024-05-01 01:04:22,304 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:04:22,304 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:04:30,305 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:04:31,398 | server.py:125 | fit progress: (6, 1.9928972721099854, {'accuracy': 0.4645, 'data_size': 10000}, 91.6193809810211)
INFO flwr 2024-05-01 01:04:31,398 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:04:31,398 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:04:40,916 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:04:43,069 | server.py:125 | fit progress: (7, 1.970401406288147, {'accuracy': 0.4869, 'data_size': 10000}, 103.29037189501105)
INFO flwr 2024-05-01 01:04:43,069 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:04:43,069 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:04:52,706 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:04:54,065 | server.py:125 | fit progress: (8, 1.9484652280807495, {'accuracy': 0.5101, 'data_size': 10000}, 114.28677878004964)
INFO flwr 2024-05-01 01:04:54,066 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:04:54,066 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:05:03,142 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:05:05,362 | server.py:125 | fit progress: (9, 1.9405722618103027, {'accuracy': 0.5173, 'data_size': 10000}, 125.58382833003998)
INFO flwr 2024-05-01 01:05:05,363 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:05:05,363 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:05:15,100 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:05:17,564 | server.py:125 | fit progress: (10, 1.9298685789108276, {'accuracy': 0.5287, 'data_size': 10000}, 137.78507806401467)
INFO flwr 2024-05-01 01:05:17,564 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:05:17,564 | server.py:153 | FL finished in 137.78555539104855
INFO flwr 2024-05-01 01:05:17,564 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:05:17,564 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:05:17,564 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:05:17,564 | app.py:229 | app_fit: losses_centralized [(0, 2.3049814701080322), (1, 2.011404275894165), (2, 2.0488057136535645), (3, 1.9933595657348633), (4, 2.0411922931671143), (5, 2.0200459957122803), (6, 1.9928972721099854), (7, 1.970401406288147), (8, 1.9484652280807495), (9, 1.9405722618103027), (10, 1.9298685789108276)]
INFO flwr 2024-05-01 01:05:17,565 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0688), (1, 0.4722), (2, 0.4044), (3, 0.4668), (4, 0.4124), (5, 0.4373), (6, 0.4645), (7, 0.4869), (8, 0.5101), (9, 0.5173), (10, 0.5287)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5287
wandb:     loss 1.92987
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_010232-9kvnup8i
wandb: Find logs at: ./wandb/offline-run-20240501_010232-9kvnup8i/logs
INFO flwr 2024-05-01 01:05:21,244 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:05:22,100 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1162251)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1162251)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:05:27,793	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:05:27,934	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:05:28,051	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:05:28,052	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:05:39,663 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 155133857997.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 70771653427.0}
INFO flwr 2024-05-01 01:05:39,663 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:05:39,663 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:05:39,690 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:05:39,691 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:05:39,692 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:05:39,692 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:05:43,943 | server.py:94 | initial parameters (loss, other metrics): 2.299114227294922, {'accuracy': 0.1118, 'data_size': 10000}
INFO flwr 2024-05-01 01:05:43,944 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:05:43,944 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1219185)[0m 2024-05-01 01:05:48.326955: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1219185)[0m 2024-05-01 01:05:48.433361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1219185)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1219174)[0m 2024-05-01 01:05:52.149833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1219179)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1219179)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1219181)[0m 2024-05-01 01:05:49.082222: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1219181)[0m 2024-05-01 01:05:49.311647: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1219181)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1219181)[0m 2024-05-01 01:05:52.571448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:06:11,511 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:06:13,360 | server.py:125 | fit progress: (1, 1.9407742023468018, {'accuracy': 0.5483, 'data_size': 10000}, 29.415877623017877)
INFO flwr 2024-05-01 01:06:13,361 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:06:13,361 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:06:23,568 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:06:24,890 | server.py:125 | fit progress: (2, 1.8301986455917358, {'accuracy': 0.6317, 'data_size': 10000}, 40.945972552057356)
INFO flwr 2024-05-01 01:06:24,891 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:06:24,891 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:06:34,380 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:06:36,002 | server.py:125 | fit progress: (3, 1.736291766166687, {'accuracy': 0.7287, 'data_size': 10000}, 52.05752343701897)
INFO flwr 2024-05-01 01:06:36,002 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:06:36,002 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:06:44,658 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:06:46,105 | server.py:125 | fit progress: (4, 1.6797773838043213, {'accuracy': 0.7853, 'data_size': 10000}, 62.16068016202189)
INFO flwr 2024-05-01 01:06:46,105 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:06:46,105 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:06:55,737 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:06:57,950 | server.py:125 | fit progress: (5, 1.6658529043197632, {'accuracy': 0.7967, 'data_size': 10000}, 74.00568477401976)
INFO flwr 2024-05-01 01:06:57,950 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:06:57,951 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:07:06,855 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:07:08,810 | server.py:125 | fit progress: (6, 1.6550204753875732, {'accuracy': 0.8075, 'data_size': 10000}, 84.86600168800214)
INFO flwr 2024-05-01 01:07:08,811 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:07:08,811 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:07:18,689 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:07:20,814 | server.py:125 | fit progress: (7, 1.6542127132415771, {'accuracy': 0.8066, 'data_size': 10000}, 96.86936975701246)
INFO flwr 2024-05-01 01:07:20,814 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:07:20,814 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:07:29,515 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:07:31,675 | server.py:125 | fit progress: (8, 1.6503775119781494, {'accuracy': 0.8108, 'data_size': 10000}, 107.73071189800976)
INFO flwr 2024-05-01 01:07:31,676 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:07:31,676 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:07:40,738 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:07:42,891 | server.py:125 | fit progress: (9, 1.6471720933914185, {'accuracy': 0.814, 'data_size': 10000}, 118.94707065704279)
INFO flwr 2024-05-01 01:07:42,892 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:07:42,892 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:07:52,100 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:07:54,163 | server.py:125 | fit progress: (10, 1.6374573707580566, {'accuracy': 0.824, 'data_size': 10000}, 130.21875751100015)
INFO flwr 2024-05-01 01:07:54,163 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:07:54,163 | server.py:153 | FL finished in 130.2191852030228
INFO flwr 2024-05-01 01:07:54,164 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:07:54,164 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:07:54,164 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:07:54,164 | app.py:229 | app_fit: losses_centralized [(0, 2.299114227294922), (1, 1.9407742023468018), (2, 1.8301986455917358), (3, 1.736291766166687), (4, 1.6797773838043213), (5, 1.6658529043197632), (6, 1.6550204753875732), (7, 1.6542127132415771), (8, 1.6503775119781494), (9, 1.6471720933914185), (10, 1.6374573707580566)]
INFO flwr 2024-05-01 01:07:54,164 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1118), (1, 0.5483), (2, 0.6317), (3, 0.7287), (4, 0.7853), (5, 0.7967), (6, 0.8075), (7, 0.8066), (8, 0.8108), (9, 0.814), (10, 0.824)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.824
wandb:     loss 1.63746
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_010521-w5r9u3wy
wandb: Find logs at: ./wandb/offline-run-20240501_010521-w5r9u3wy/logs
INFO flwr 2024-05-01 01:07:57,708 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:07:58,561 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1219188)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1219188)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:08:03,946	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:08:04,062	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:08:04,186	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:08:04,188	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:08:16,275 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 151338115277.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69144906547.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-05-01 01:08:16,276 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:08:16,276 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:08:16,304 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:08:16,306 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:08:16,307 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:08:16,307 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:08:19,769 | server.py:94 | initial parameters (loss, other metrics): 2.303743839263916, {'accuracy': 0.0869, 'data_size': 10000}
INFO flwr 2024-05-01 01:08:19,770 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:08:19,770 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1271295)[0m 2024-05-01 01:08:24.507956: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1271261)[0m 2024-05-01 01:08:24.691206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1271261)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1271260)[0m 2024-05-01 01:08:29.106063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1271291)[0m 2024-05-01 01:08:25.297646: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1271260)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1271260)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1271291)[0m 2024-05-01 01:08:25.475694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1271291)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1271295)[0m 2024-05-01 01:08:29.560251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:08:48,049 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:08:49,184 | server.py:125 | fit progress: (1, 1.888926386833191, {'accuracy': 0.6237, 'data_size': 10000}, 29.41363275400363)
INFO flwr 2024-05-01 01:08:49,184 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:08:49,184 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:09:00,231 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:09:02,003 | server.py:125 | fit progress: (2, 1.7001910209655762, {'accuracy': 0.7718, 'data_size': 10000}, 42.233124862017576)
INFO flwr 2024-05-01 01:09:02,003 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:09:02,004 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:09:11,341 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:09:13,507 | server.py:125 | fit progress: (3, 1.6679203510284424, {'accuracy': 0.7997, 'data_size': 10000}, 53.737077715981286)
INFO flwr 2024-05-01 01:09:13,507 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:09:13,508 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:09:23,422 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:09:25,275 | server.py:125 | fit progress: (4, 1.6734243631362915, {'accuracy': 0.7908, 'data_size': 10000}, 65.50507378496695)
INFO flwr 2024-05-01 01:09:25,275 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:09:25,276 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:09:34,574 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:09:36,082 | server.py:125 | fit progress: (5, 1.647652506828308, {'accuracy': 0.815, 'data_size': 10000}, 76.31195224396652)
INFO flwr 2024-05-01 01:09:36,082 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:09:36,082 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:09:45,580 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:09:47,032 | server.py:125 | fit progress: (6, 1.6578583717346191, {'accuracy': 0.8028, 'data_size': 10000}, 87.2614975880133)
INFO flwr 2024-05-01 01:09:47,032 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:09:47,032 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:09:56,558 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:09:57,752 | server.py:125 | fit progress: (7, 1.6491777896881104, {'accuracy': 0.8123, 'data_size': 10000}, 97.98190717201214)
INFO flwr 2024-05-01 01:09:57,752 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:09:57,753 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:10:07,701 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:10:09,157 | server.py:125 | fit progress: (8, 1.6380444765090942, {'accuracy': 0.8226, 'data_size': 10000}, 109.3870142030064)
INFO flwr 2024-05-01 01:10:09,157 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:10:09,157 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:10:18,838 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:10:20,343 | server.py:125 | fit progress: (9, 1.64913010597229, {'accuracy': 0.8116, 'data_size': 10000}, 120.57316361099947)
INFO flwr 2024-05-01 01:10:20,343 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:10:20,344 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:10:29,049 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:10:30,442 | server.py:125 | fit progress: (10, 1.6517869234085083, {'accuracy': 0.8067, 'data_size': 10000}, 130.67240908101667)
INFO flwr 2024-05-01 01:10:30,443 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:10:30,443 | server.py:153 | FL finished in 130.67286984698148
INFO flwr 2024-05-01 01:10:30,443 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:10:30,443 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:10:30,443 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:10:30,443 | app.py:229 | app_fit: losses_centralized [(0, 2.303743839263916), (1, 1.888926386833191), (2, 1.7001910209655762), (3, 1.6679203510284424), (4, 1.6734243631362915), (5, 1.647652506828308), (6, 1.6578583717346191), (7, 1.6491777896881104), (8, 1.6380444765090942), (9, 1.64913010597229), (10, 1.6517869234085083)]
INFO flwr 2024-05-01 01:10:30,443 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0869), (1, 0.6237), (2, 0.7718), (3, 0.7997), (4, 0.7908), (5, 0.815), (6, 0.8028), (7, 0.8123), (8, 0.8226), (9, 0.8116), (10, 0.8067)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8067
wandb:     loss 1.65179
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_010758-vt6jz7d3
wandb: Find logs at: ./wandb/offline-run-20240501_010758-vt6jz7d3/logs
INFO flwr 2024-05-01 01:10:34,169 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:10:35,175 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1271272)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1271272)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:10:40,328	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:10:40,462	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:10:40,612	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:10:40,613	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:10:51,450 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 155040590848.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 70731681792.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-05-01 01:10:51,451 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:10:51,451 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:10:51,475 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:10:51,476 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:10:51,477 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:10:51,477 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:10:54,713 | server.py:94 | initial parameters (loss, other metrics): 2.304412364959717, {'accuracy': 0.1169, 'data_size': 10000}
INFO flwr 2024-05-01 01:10:54,713 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:10:54,713 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1333181)[0m 2024-05-01 01:10:59.038450: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1333181)[0m 2024-05-01 01:10:59.143873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1333181)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1333181)[0m 2024-05-01 01:11:02.788097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1333181)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1333181)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1333184)[0m 2024-05-01 01:10:59.758354: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1333184)[0m 2024-05-01 01:10:59.858535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1333184)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1333179)[0m 2024-05-01 01:11:02.951314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:11:21,217 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:11:22,433 | server.py:125 | fit progress: (1, 1.981050968170166, {'accuracy': 0.4891, 'data_size': 10000}, 27.71962921100203)
INFO flwr 2024-05-01 01:11:22,433 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:11:22,433 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:11:34,499 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:11:36,479 | server.py:125 | fit progress: (2, 1.9078097343444824, {'accuracy': 0.5485, 'data_size': 10000}, 41.76564178103581)
INFO flwr 2024-05-01 01:11:36,479 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:11:36,480 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:11:46,003 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:11:47,955 | server.py:125 | fit progress: (3, 1.8638793230056763, {'accuracy': 0.5939, 'data_size': 10000}, 53.242073437024374)
INFO flwr 2024-05-01 01:11:47,956 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:11:47,956 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:11:58,429 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:11:59,926 | server.py:125 | fit progress: (4, 1.7828701734542847, {'accuracy': 0.6751, 'data_size': 10000}, 65.21238894603448)
INFO flwr 2024-05-01 01:11:59,926 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:11:59,926 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:12:09,823 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:12:11,482 | server.py:125 | fit progress: (5, 1.755632758140564, {'accuracy': 0.7053, 'data_size': 10000}, 76.76856003201101)
INFO flwr 2024-05-01 01:12:11,482 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:12:11,482 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:12:22,090 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:12:24,011 | server.py:125 | fit progress: (6, 1.6982064247131348, {'accuracy': 0.7623, 'data_size': 10000}, 89.29736701899674)
INFO flwr 2024-05-01 01:12:24,011 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:12:24,011 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:12:33,961 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:12:35,407 | server.py:125 | fit progress: (7, 1.7229716777801514, {'accuracy': 0.7367, 'data_size': 10000}, 100.69328328501433)
INFO flwr 2024-05-01 01:12:35,407 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:12:35,407 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:12:45,397 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:12:47,781 | server.py:125 | fit progress: (8, 1.7530324459075928, {'accuracy': 0.7072, 'data_size': 10000}, 113.06765023001935)
INFO flwr 2024-05-01 01:12:47,781 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:12:47,782 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:12:58,031 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:13:00,491 | server.py:125 | fit progress: (9, 1.7324702739715576, {'accuracy': 0.7268, 'data_size': 10000}, 125.77772189903772)
INFO flwr 2024-05-01 01:13:00,491 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:13:00,492 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:13:09,774 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:13:11,191 | server.py:125 | fit progress: (10, 1.7010506391525269, {'accuracy': 0.7581, 'data_size': 10000}, 136.47791795403464)
INFO flwr 2024-05-01 01:13:11,191 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:13:11,192 | server.py:153 | FL finished in 136.47830976400292
INFO flwr 2024-05-01 01:13:11,192 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:13:11,192 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:13:11,192 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:13:11,192 | app.py:229 | app_fit: losses_centralized [(0, 2.304412364959717), (1, 1.981050968170166), (2, 1.9078097343444824), (3, 1.8638793230056763), (4, 1.7828701734542847), (5, 1.755632758140564), (6, 1.6982064247131348), (7, 1.7229716777801514), (8, 1.7530324459075928), (9, 1.7324702739715576), (10, 1.7010506391525269)]
INFO flwr 2024-05-01 01:13:11,192 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1169), (1, 0.4891), (2, 0.5485), (3, 0.5939), (4, 0.6751), (5, 0.7053), (6, 0.7623), (7, 0.7367), (8, 0.7072), (9, 0.7268), (10, 0.7581)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7581
wandb:     loss 1.70105
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_011034-qic7wnxs
wandb: Find logs at: ./wandb/offline-run-20240501_011034-qic7wnxs/logs
INFO flwr 2024-05-01 01:13:14,764 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:13:15,608 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1333183)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1333183)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:13:20,990	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:13:21,129	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:13:21,274	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:13:21,275	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:13:32,297 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 152198430106.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69513612902.0}
INFO flwr 2024-05-01 01:13:32,297 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:13:32,297 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:13:32,327 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:13:32,328 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:13:32,329 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:13:32,329 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:13:35,486 | server.py:94 | initial parameters (loss, other metrics): 2.3005549907684326, {'accuracy': 0.1038, 'data_size': 10000}
INFO flwr 2024-05-01 01:13:35,487 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:13:35,487 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1392204)[0m 2024-05-01 01:13:40.371328: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1392204)[0m 2024-05-01 01:13:40.471964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1392204)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1392195)[0m 2024-05-01 01:13:44.281543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1392204)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1392204)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1392206)[0m 2024-05-01 01:13:40.890921: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1392206)[0m 2024-05-01 01:13:41.109204: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1392206)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1392206)[0m 2024-05-01 01:13:44.309739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:14:01,987 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:14:03,426 | server.py:125 | fit progress: (1, 1.8758164644241333, {'accuracy': 0.6438, 'data_size': 10000}, 27.938936685968656)
INFO flwr 2024-05-01 01:14:03,426 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:14:03,427 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:14:13,199 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:14:15,237 | server.py:125 | fit progress: (2, 1.7188760042190552, {'accuracy': 0.7562, 'data_size': 10000}, 39.750087607011665)
INFO flwr 2024-05-01 01:14:15,238 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:14:15,238 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:14:24,877 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:14:26,621 | server.py:125 | fit progress: (3, 1.688535213470459, {'accuracy': 0.7792, 'data_size': 10000}, 51.13382738101063)
INFO flwr 2024-05-01 01:14:26,621 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:14:26,621 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:14:36,437 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:14:38,308 | server.py:125 | fit progress: (4, 1.6378709077835083, {'accuracy': 0.8292, 'data_size': 10000}, 62.82113269896945)
INFO flwr 2024-05-01 01:14:38,309 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:14:38,309 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:14:49,055 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:14:51,496 | server.py:125 | fit progress: (5, 1.6025782823562622, {'accuracy': 0.8622, 'data_size': 10000}, 76.00871575297788)
INFO flwr 2024-05-01 01:14:51,496 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:14:51,496 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:15:01,088 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:15:02,353 | server.py:125 | fit progress: (6, 1.5902496576309204, {'accuracy': 0.8721, 'data_size': 10000}, 86.86558635701658)
INFO flwr 2024-05-01 01:15:02,353 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:15:02,353 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:15:13,244 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:15:14,501 | server.py:125 | fit progress: (7, 1.5765225887298584, {'accuracy': 0.8866, 'data_size': 10000}, 99.01345402601874)
INFO flwr 2024-05-01 01:15:14,501 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:15:14,501 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:15:24,324 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:15:26,466 | server.py:125 | fit progress: (8, 1.57310152053833, {'accuracy': 0.8901, 'data_size': 10000}, 110.97874698601663)
INFO flwr 2024-05-01 01:15:26,466 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:15:26,467 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:15:36,151 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:15:37,939 | server.py:125 | fit progress: (9, 1.5772145986557007, {'accuracy': 0.8859, 'data_size': 10000}, 122.4514675239916)
INFO flwr 2024-05-01 01:15:37,939 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:15:37,939 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:15:48,050 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:15:49,348 | server.py:125 | fit progress: (10, 1.5745161771774292, {'accuracy': 0.8877, 'data_size': 10000}, 133.86059485096484)
INFO flwr 2024-05-01 01:15:49,348 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:15:49,348 | server.py:153 | FL finished in 133.861002982012
INFO flwr 2024-05-01 01:15:49,348 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:15:49,348 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:15:49,348 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:15:49,349 | app.py:229 | app_fit: losses_centralized [(0, 2.3005549907684326), (1, 1.8758164644241333), (2, 1.7188760042190552), (3, 1.688535213470459), (4, 1.6378709077835083), (5, 1.6025782823562622), (6, 1.5902496576309204), (7, 1.5765225887298584), (8, 1.57310152053833), (9, 1.5772145986557007), (10, 1.5745161771774292)]
INFO flwr 2024-05-01 01:15:49,349 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1038), (1, 0.6438), (2, 0.7562), (3, 0.7792), (4, 0.8292), (5, 0.8622), (6, 0.8721), (7, 0.8866), (8, 0.8901), (9, 0.8859), (10, 0.8877)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8877
wandb:     loss 1.57452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_011315-6gscqibb
wandb: Find logs at: ./wandb/offline-run-20240501_011315-6gscqibb/logs
INFO flwr 2024-05-01 01:15:52,810 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 16
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:15:53,633 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1392202)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1392202)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:15:58,984	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:15:59,132	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:15:59,268	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:15:59,269	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:16:10,577 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 72344054169.0, 'memory': 158802793063.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-05-01 01:16:10,578 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:16:10,578 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:16:10,599 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:16:10,600 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:16:10,600 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:16:10,600 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:16:13,868 | server.py:94 | initial parameters (loss, other metrics): 2.3018124103546143, {'accuracy': 0.077, 'data_size': 10000}
INFO flwr 2024-05-01 01:16:13,868 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:16:13,869 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1456274)[0m 2024-05-01 01:16:18.388578: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1456274)[0m 2024-05-01 01:16:18.505944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1456274)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1456281)[0m 2024-05-01 01:16:22.173306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1456280)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1456280)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1456279)[0m 2024-05-01 01:16:18.805723: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1456279)[0m 2024-05-01 01:16:18.900836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1456279)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1456278)[0m 2024-05-01 01:16:22.197055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:16:40,899 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:16:42,605 | server.py:125 | fit progress: (1, 1.950571060180664, {'accuracy': 0.5328, 'data_size': 10000}, 28.736010561988223)
INFO flwr 2024-05-01 01:16:42,605 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:16:42,605 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:16:54,069 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:16:55,624 | server.py:125 | fit progress: (2, 1.824747085571289, {'accuracy': 0.6409, 'data_size': 10000}, 41.755794018972665)
INFO flwr 2024-05-01 01:16:55,625 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:16:55,625 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:17:06,010 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:17:07,509 | server.py:125 | fit progress: (3, 1.8093286752700806, {'accuracy': 0.6551, 'data_size': 10000}, 53.64014491898706)
INFO flwr 2024-05-01 01:17:07,509 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:17:07,509 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:17:17,405 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:17:19,395 | server.py:125 | fit progress: (4, 1.7414883375167847, {'accuracy': 0.7234, 'data_size': 10000}, 65.52648201596458)
INFO flwr 2024-05-01 01:17:19,395 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:17:19,396 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:17:30,639 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:17:33,080 | server.py:125 | fit progress: (5, 1.7415168285369873, {'accuracy': 0.7198, 'data_size': 10000}, 79.21143819100689)
INFO flwr 2024-05-01 01:17:33,080 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:17:33,081 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:17:43,083 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:17:44,543 | server.py:125 | fit progress: (6, 1.7397050857543945, {'accuracy': 0.7228, 'data_size': 10000}, 90.67454893898685)
INFO flwr 2024-05-01 01:17:44,543 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:17:44,544 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:17:55,385 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:17:57,370 | server.py:125 | fit progress: (7, 1.736930251121521, {'accuracy': 0.724, 'data_size': 10000}, 103.50099725695327)
INFO flwr 2024-05-01 01:17:57,370 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:17:57,370 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:18:07,935 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:18:09,911 | server.py:125 | fit progress: (8, 1.7412402629852295, {'accuracy': 0.719, 'data_size': 10000}, 116.04192851297557)
INFO flwr 2024-05-01 01:18:09,911 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:18:09,911 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:18:19,285 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:18:20,816 | server.py:125 | fit progress: (9, 1.7263256311416626, {'accuracy': 0.7337, 'data_size': 10000}, 126.9476142450003)
INFO flwr 2024-05-01 01:18:20,816 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:18:20,817 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:18:31,171 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:18:33,256 | server.py:125 | fit progress: (10, 1.7275714874267578, {'accuracy': 0.7335, 'data_size': 10000}, 139.38685337099014)
INFO flwr 2024-05-01 01:18:33,256 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:18:33,256 | server.py:153 | FL finished in 139.387733534968
INFO flwr 2024-05-01 01:18:33,257 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:18:33,257 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:18:33,257 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:18:33,257 | app.py:229 | app_fit: losses_centralized [(0, 2.3018124103546143), (1, 1.950571060180664), (2, 1.824747085571289), (3, 1.8093286752700806), (4, 1.7414883375167847), (5, 1.7415168285369873), (6, 1.7397050857543945), (7, 1.736930251121521), (8, 1.7412402629852295), (9, 1.7263256311416626), (10, 1.7275714874267578)]
INFO flwr 2024-05-01 01:18:33,257 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.077), (1, 0.5328), (2, 0.6409), (3, 0.6551), (4, 0.7234), (5, 0.7198), (6, 0.7228), (7, 0.724), (8, 0.719), (9, 0.7337), (10, 0.7335)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7335
wandb:     loss 1.72757
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_011553-5mwqi001
wandb: Find logs at: ./wandb/offline-run-20240501_011553-5mwqi001/logs
INFO flwr 2024-05-01 01:18:37,107 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:18:37,901 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1456282)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1456282)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:18:43,164	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:18:43,317	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:18:43,462	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:18:43,464	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:18:54,438 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 65223587020.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 142188369716.0}
INFO flwr 2024-05-01 01:18:54,438 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:18:54,438 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:18:54,460 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:18:54,462 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:18:54,462 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:18:54,462 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:18:57,457 | server.py:94 | initial parameters (loss, other metrics): 2.302661657333374, {'accuracy': 0.0757, 'data_size': 10000}
INFO flwr 2024-05-01 01:18:57,458 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:18:57,458 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1519332)[0m 2024-05-01 01:19:02.095717: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1519334)[0m 2024-05-01 01:19:02.262214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1519334)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1519326)[0m 2024-05-01 01:19:05.638928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1519326)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1519326)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1519321)[0m 2024-05-01 01:19:02.406667: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1519321)[0m 2024-05-01 01:19:02.523961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1519321)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1519332)[0m 2024-05-01 01:19:05.706402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:19:25,730 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:19:27,001 | server.py:125 | fit progress: (1, 1.9030324220657349, {'accuracy': 0.593, 'data_size': 10000}, 29.543195377045777)
INFO flwr 2024-05-01 01:19:27,002 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:19:27,002 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:19:38,489 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:19:40,097 | server.py:125 | fit progress: (2, 1.8847500085830688, {'accuracy': 0.5846, 'data_size': 10000}, 42.639362393005285)
INFO flwr 2024-05-01 01:19:40,098 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:19:40,098 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:19:50,613 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:19:52,146 | server.py:125 | fit progress: (3, 1.7517509460449219, {'accuracy': 0.704, 'data_size': 10000}, 54.688355085032526)
INFO flwr 2024-05-01 01:19:52,147 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:19:52,147 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:20:03,036 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:20:05,229 | server.py:125 | fit progress: (4, 1.7574814558029175, {'accuracy': 0.7034, 'data_size': 10000}, 67.77114375302335)
INFO flwr 2024-05-01 01:20:05,229 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:20:05,230 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:20:16,035 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:20:17,815 | server.py:125 | fit progress: (5, 1.7088561058044434, {'accuracy': 0.7524, 'data_size': 10000}, 80.35710156505229)
INFO flwr 2024-05-01 01:20:17,815 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:20:17,816 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:20:29,513 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:20:30,897 | server.py:125 | fit progress: (6, 1.6812328100204468, {'accuracy': 0.7779, 'data_size': 10000}, 93.43907062103972)
INFO flwr 2024-05-01 01:20:30,897 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:20:30,898 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:20:41,023 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:20:42,216 | server.py:125 | fit progress: (7, 1.675759196281433, {'accuracy': 0.7857, 'data_size': 10000}, 104.75817819801159)
INFO flwr 2024-05-01 01:20:42,216 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:20:42,217 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:20:53,214 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:20:54,592 | server.py:125 | fit progress: (8, 1.6907888650894165, {'accuracy': 0.7703, 'data_size': 10000}, 117.13413717300864)
INFO flwr 2024-05-01 01:20:54,592 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:20:54,593 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:21:04,219 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:21:06,658 | server.py:125 | fit progress: (9, 1.6988589763641357, {'accuracy': 0.7623, 'data_size': 10000}, 129.1998477500165)
INFO flwr 2024-05-01 01:21:06,658 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:21:06,658 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:21:16,870 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:21:18,868 | server.py:125 | fit progress: (10, 1.6535427570343018, {'accuracy': 0.8076, 'data_size': 10000}, 141.40992207202362)
INFO flwr 2024-05-01 01:21:18,868 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:21:18,868 | server.py:153 | FL finished in 141.41035206400556
INFO flwr 2024-05-01 01:21:18,868 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:21:18,869 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:21:18,869 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:21:18,869 | app.py:229 | app_fit: losses_centralized [(0, 2.302661657333374), (1, 1.9030324220657349), (2, 1.8847500085830688), (3, 1.7517509460449219), (4, 1.7574814558029175), (5, 1.7088561058044434), (6, 1.6812328100204468), (7, 1.675759196281433), (8, 1.6907888650894165), (9, 1.6988589763641357), (10, 1.6535427570343018)]
INFO flwr 2024-05-01 01:21:18,869 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0757), (1, 0.593), (2, 0.5846), (3, 0.704), (4, 0.7034), (5, 0.7524), (6, 0.7779), (7, 0.7857), (8, 0.7703), (9, 0.7623), (10, 0.8076)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8076
wandb:     loss 1.65354
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_011837-xt1bd5rb
wandb: Find logs at: ./wandb/offline-run-20240501_011837-xt1bd5rb/logs
INFO flwr 2024-05-01 01:21:22,437 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:21:23,268 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1519334)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1519334)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:21:28,586	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:21:28,706	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:21:28,842	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:21:28,844	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:21:39,572 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 135460057703.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62340024729.0, 'CPU': 64.0}
INFO flwr 2024-05-01 01:21:39,572 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:21:39,572 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:21:39,592 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:21:39,593 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:21:39,594 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:21:39,594 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:21:42,691 | server.py:94 | initial parameters (loss, other metrics): 2.300398111343384, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-05-01 01:21:42,691 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:21:42,691 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1581863)[0m 2024-05-01 01:21:47.475262: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1581863)[0m 2024-05-01 01:21:47.571746: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1581863)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1581859)[0m 2024-05-01 01:21:50.071021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1581859)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1581859)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1581864)[0m 2024-05-01 01:21:47.708783: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1581864)[0m 2024-05-01 01:21:47.799786: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1581864)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1581864)[0m 2024-05-01 01:21:50.144030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:22:06,232 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:22:07,449 | server.py:125 | fit progress: (1, 1.8939136266708374, {'accuracy': 0.5997, 'data_size': 10000}, 24.757310688029975)
INFO flwr 2024-05-01 01:22:07,480 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:22:07,480 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:22:17,188 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:22:18,459 | server.py:125 | fit progress: (2, 1.6568621397018433, {'accuracy': 0.8285, 'data_size': 10000}, 35.767387324012816)
INFO flwr 2024-05-01 01:22:18,459 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:22:18,459 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:22:27,457 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:22:28,894 | server.py:125 | fit progress: (3, 1.6247339248657227, {'accuracy': 0.8459, 'data_size': 10000}, 46.202654730004724)
INFO flwr 2024-05-01 01:22:28,894 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:22:28,894 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:22:36,904 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:22:38,373 | server.py:125 | fit progress: (4, 1.6296882629394531, {'accuracy': 0.8349, 'data_size': 10000}, 55.6821130340104)
INFO flwr 2024-05-01 01:22:38,374 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:22:38,374 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:22:47,169 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:22:48,641 | server.py:125 | fit progress: (5, 1.604887843132019, {'accuracy': 0.8579, 'data_size': 10000}, 65.94968988903565)
INFO flwr 2024-05-01 01:22:48,641 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:22:48,641 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:22:57,873 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:22:59,095 | server.py:125 | fit progress: (6, 1.5707807540893555, {'accuracy': 0.892, 'data_size': 10000}, 76.40402524900855)
INFO flwr 2024-05-01 01:22:59,096 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:22:59,096 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:23:07,671 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:23:08,894 | server.py:125 | fit progress: (7, 1.5704408884048462, {'accuracy': 0.8923, 'data_size': 10000}, 86.20274803205393)
INFO flwr 2024-05-01 01:23:08,894 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:23:08,895 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:23:18,192 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:23:19,655 | server.py:125 | fit progress: (8, 1.566633701324463, {'accuracy': 0.8965, 'data_size': 10000}, 96.96396437403746)
INFO flwr 2024-05-01 01:23:19,656 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:23:19,656 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:23:28,253 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:23:29,720 | server.py:125 | fit progress: (9, 1.5651278495788574, {'accuracy': 0.8971, 'data_size': 10000}, 107.02897324902005)
INFO flwr 2024-05-01 01:23:29,721 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:23:29,721 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:23:38,120 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:23:39,415 | server.py:125 | fit progress: (10, 1.561529517173767, {'accuracy': 0.9019, 'data_size': 10000}, 116.72316675801994)
INFO flwr 2024-05-01 01:23:39,415 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:23:39,415 | server.py:153 | FL finished in 116.72352682403289
INFO flwr 2024-05-01 01:23:39,415 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:23:39,415 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:23:39,415 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:23:39,415 | app.py:229 | app_fit: losses_centralized [(0, 2.300398111343384), (1, 1.8939136266708374), (2, 1.6568621397018433), (3, 1.6247339248657227), (4, 1.6296882629394531), (5, 1.604887843132019), (6, 1.5707807540893555), (7, 1.5704408884048462), (8, 1.566633701324463), (9, 1.5651278495788574), (10, 1.561529517173767)]
INFO flwr 2024-05-01 01:23:39,415 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.5997), (2, 0.8285), (3, 0.8459), (4, 0.8349), (5, 0.8579), (6, 0.892), (7, 0.8923), (8, 0.8965), (9, 0.8971), (10, 0.9019)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9019
wandb:     loss 1.56153
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_012122-8ujymg4d
wandb: Find logs at: ./wandb/offline-run-20240501_012122-8ujymg4d/logs
INFO flwr 2024-05-01 01:23:42,952 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 32
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-01 01:23:43,741 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1581865)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1581865)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-05-01 01:23:48,930	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-01 01:23:49,035	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-01 01:23:49,144	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip' (0.20MiB) to Ray cluster...
2024-05-01 01:23:49,146	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_770cb769bbc6fb79.zip'.
INFO flwr 2024-05-01 01:24:00,002 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 62616717312.0, 'node:10.20.240.18': 1.0, 'memory': 136105673728.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-05-01 01:24:00,002 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-01 01:24:00,002 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-01 01:24:00,020 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-01 01:24:00,021 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-01 01:24:00,021 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-01 01:24:00,021 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-01 01:24:02,793 | server.py:94 | initial parameters (loss, other metrics): 2.301098585128784, {'accuracy': 0.1207, 'data_size': 10000}
INFO flwr 2024-05-01 01:24:02,793 | server.py:104 | FL starting
DEBUG flwr 2024-05-01 01:24:02,794 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1644331)[0m 2024-05-01 01:24:05.964864: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1644331)[0m 2024-05-01 01:24:06.071510: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1644331)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1644331)[0m 2024-05-01 01:24:08.038416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1644331)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1644331)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1644332)[0m 2024-05-01 01:24:06.131176: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1644333)[0m 2024-05-01 01:24:06.203056: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1644333)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1644333)[0m 2024-05-01 01:24:08.181344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-05-01 01:24:22,544 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-01 01:24:23,607 | server.py:125 | fit progress: (1, 1.8164927959442139, {'accuracy': 0.7086, 'data_size': 10000}, 20.81338747899281)
INFO flwr 2024-05-01 01:24:23,607 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-01 01:24:23,607 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:24:32,369 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-01 01:24:33,465 | server.py:125 | fit progress: (2, 1.6734402179718018, {'accuracy': 0.7985, 'data_size': 10000}, 30.671321731992066)
INFO flwr 2024-05-01 01:24:33,465 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-05-01 01:24:33,465 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:24:41,916 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-05-01 01:24:43,201 | server.py:125 | fit progress: (3, 1.6074368953704834, {'accuracy': 0.8654, 'data_size': 10000}, 40.407636480988)
INFO flwr 2024-05-01 01:24:43,202 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-05-01 01:24:43,202 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:24:51,507 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-05-01 01:24:52,880 | server.py:125 | fit progress: (4, 1.6189603805541992, {'accuracy': 0.8461, 'data_size': 10000}, 50.08656713797245)
INFO flwr 2024-05-01 01:24:52,880 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-05-01 01:24:52,881 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:25:01,894 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-05-01 01:25:03,371 | server.py:125 | fit progress: (5, 1.5846428871154785, {'accuracy': 0.8825, 'data_size': 10000}, 60.57745457201963)
INFO flwr 2024-05-01 01:25:03,371 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-05-01 01:25:03,372 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:25:12,329 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-05-01 01:25:13,560 | server.py:125 | fit progress: (6, 1.5820908546447754, {'accuracy': 0.88, 'data_size': 10000}, 70.76611479598796)
INFO flwr 2024-05-01 01:25:13,560 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-05-01 01:25:13,560 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:25:22,285 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-05-01 01:25:23,500 | server.py:125 | fit progress: (7, 1.5692991018295288, {'accuracy': 0.894, 'data_size': 10000}, 80.70591318496736)
INFO flwr 2024-05-01 01:25:23,500 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-05-01 01:25:23,500 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:25:32,382 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-05-01 01:25:33,841 | server.py:125 | fit progress: (8, 1.5753463506698608, {'accuracy': 0.8873, 'data_size': 10000}, 91.04766556801042)
INFO flwr 2024-05-01 01:25:33,842 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-05-01 01:25:33,842 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:25:42,348 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-05-01 01:25:43,776 | server.py:125 | fit progress: (9, 1.5577998161315918, {'accuracy': 0.9038, 'data_size': 10000}, 100.98267582297558)
INFO flwr 2024-05-01 01:25:43,777 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-05-01 01:25:43,777 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-01 01:25:52,468 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-05-01 01:25:53,856 | server.py:125 | fit progress: (10, 1.5629587173461914, {'accuracy': 0.8985, 'data_size': 10000}, 111.06198888801737)
INFO flwr 2024-05-01 01:25:53,856 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-05-01 01:25:53,856 | server.py:153 | FL finished in 111.06240259797778
INFO flwr 2024-05-01 01:25:53,856 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-01 01:25:53,856 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-01 01:25:53,856 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-01 01:25:53,856 | app.py:229 | app_fit: losses_centralized [(0, 2.301098585128784), (1, 1.8164927959442139), (2, 1.6734402179718018), (3, 1.6074368953704834), (4, 1.6189603805541992), (5, 1.5846428871154785), (6, 1.5820908546447754), (7, 1.5692991018295288), (8, 1.5753463506698608), (9, 1.5577998161315918), (10, 1.5629587173461914)]
INFO flwr 2024-05-01 01:25:53,857 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1207), (1, 0.7086), (2, 0.7985), (3, 0.8654), (4, 0.8461), (5, 0.8825), (6, 0.88), (7, 0.894), (8, 0.8873), (9, 0.9038), (10, 0.8985)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8985
wandb:     loss 1.56296
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240501_012343-ywufum2i
wandb: Find logs at: ./wandb/offline-run-20240501_012343-ywufum2i/logs
[2m[36m(DefaultActor pid=1644322)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1644322)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
