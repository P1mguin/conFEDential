ctit083
2024-05-15 12:08:08.355706: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-15 12:08:57.325479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-15 12:11:32,097 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-15 12:11:32,146 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-15 12:11:44,502 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-15 12:11:44,507 | Simulation.py:240 | Found previously split dataloaders, loading them
INFO flwr 2024-05-15 12:12:19,611 | main.py:70 | Loaded 1 configs with name MNIST-LOGISTICREGRESSION-FEDNL, running...
INFO flwr 2024-05-15 12:12:19,611 | main.py:72 | Config:
	Simulation:
		Data:
			dataset_name: mnist
			batch_size: -1
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["image"].reshape(784) / 255.,
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 10
			local_rounds: 10
		Model:
			optimizer_name: FedNL
			model_name: Logistic Regression
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				
			model_architecture:
				type: Linear
					in_features: 784
					out_features: 10
				type: Softmax
					dim: -1
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-15 12:12:19,611 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-15 12:12:19,881 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-15 12:12:34,421 | Simulation.py:365 | Created 8 clients with resources 8 CPUs and 0.125 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-15 12:12:34,423 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-15 12:12:34,424 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-05-15 12:12:54,513	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-15 12:12:58,499	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-15 12:12:58,569	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cbafdf10a2a492f0.zip' (0.16MiB) to Ray cluster...
2024-05-15 12:12:58,570	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cbafdf10a2a492f0.zip'.
INFO flwr 2024-05-15 12:13:13,060 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.13': 1.0, 'memory': 172921697280.0, 'object_store_memory': 78395013120.0}
INFO flwr 2024-05-15 12:13:13,061 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-15 12:13:13,061 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-15 12:13:13,081 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-15 12:13:13,082 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-15 12:13:13,082 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-15 12:13:13,082 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=905549)[0m 2024-05-15 12:13:20.355655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=905549)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=905549)[0m 2024-05-15 12:13:26.586800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=905546)[0m 2024-05-15 12:13:20.375783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=905546)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
INFO flwr 2024-05-15 12:13:27,100 | server.py:94 | initial parameters (loss, other metrics): 0.00023044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-05-15 12:13:27,100 | server.py:104 | FL starting
DEBUG flwr 2024-05-15 12:13:27,100 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=905549)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=905549)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=905546)[0m 2024-05-15 12:13:26.586948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
ERROR flwr 2024-05-15 12:14:49,458 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 92 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:49,474 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 92 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,163 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 64 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,164 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 64 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,165 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905549, ip=10.20.240.13, actor_id=fc2f8cebd0eb99621a1ad06801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8aaf7de4d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905549, ip=10.20.240.13, actor_id=fc2f8cebd0eb99621a1ad06801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8aaf7de4d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905549, ip=10.20.240.13, actor_id=fc2f8cebd0eb99621a1ad06801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8aaf7de4d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 80 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,165 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905549, ip=10.20.240.13, actor_id=fc2f8cebd0eb99621a1ad06801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8aaf7de4d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905549, ip=10.20.240.13, actor_id=fc2f8cebd0eb99621a1ad06801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8aaf7de4d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905549, ip=10.20.240.13, actor_id=fc2f8cebd0eb99621a1ad06801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8aaf7de4d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 80 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,168 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905539, ip=10.20.240.13, actor_id=6f1f7b2c18a1b01fb74ca5de01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f48bcffe560>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905539, ip=10.20.240.13, actor_id=6f1f7b2c18a1b01fb74ca5de01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f48bcffe560>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905539, ip=10.20.240.13, actor_id=6f1f7b2c18a1b01fb74ca5de01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f48bcffe560>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 49 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,169 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905539, ip=10.20.240.13, actor_id=6f1f7b2c18a1b01fb74ca5de01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f48bcffe560>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905539, ip=10.20.240.13, actor_id=6f1f7b2c18a1b01fb74ca5de01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f48bcffe560>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905539, ip=10.20.240.13, actor_id=6f1f7b2c18a1b01fb74ca5de01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f48bcffe560>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 49 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,169 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905544, ip=10.20.240.13, actor_id=ba10be64c253c2eded2a855801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f84f35228f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905544, ip=10.20.240.13, actor_id=ba10be64c253c2eded2a855801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f84f35228f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905544, ip=10.20.240.13, actor_id=ba10be64c253c2eded2a855801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f84f35228f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 89 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,170 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905544, ip=10.20.240.13, actor_id=ba10be64c253c2eded2a855801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f84f35228f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905544, ip=10.20.240.13, actor_id=ba10be64c253c2eded2a855801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f84f35228f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905544, ip=10.20.240.13, actor_id=ba10be64c253c2eded2a855801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f84f35228f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 89 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,169 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905546, ip=10.20.240.13, actor_id=92f4d91b1bf21cebd253272201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbc31ada8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905546, ip=10.20.240.13, actor_id=92f4d91b1bf21cebd253272201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbc31ada8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905546, ip=10.20.240.13, actor_id=92f4d91b1bf21cebd253272201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbc31ada8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 85 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,172 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905546, ip=10.20.240.13, actor_id=92f4d91b1bf21cebd253272201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbc31ada8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905546, ip=10.20.240.13, actor_id=92f4d91b1bf21cebd253272201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbc31ada8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905546, ip=10.20.240.13, actor_id=92f4d91b1bf21cebd253272201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbc31ada8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 85 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,173 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905545, ip=10.20.240.13, actor_id=712a79f907c8bed6726211b201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8219142710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905545, ip=10.20.240.13, actor_id=712a79f907c8bed6726211b201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8219142710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905545, ip=10.20.240.13, actor_id=712a79f907c8bed6726211b201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8219142710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 95 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,174 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905536, ip=10.20.240.13, actor_id=57d693c6d0ed78d9ba744c7e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f65b229a9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905536, ip=10.20.240.13, actor_id=57d693c6d0ed78d9ba744c7e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f65b229a9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905536, ip=10.20.240.13, actor_id=57d693c6d0ed78d9ba744c7e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f65b229a9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,174 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.36 MiB is allocated by PyTorch, and 148.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.36 MiB is allocated by PyTorch, and 148.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,174 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905545, ip=10.20.240.13, actor_id=712a79f907c8bed6726211b201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8219142710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905545, ip=10.20.240.13, actor_id=712a79f907c8bed6726211b201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8219142710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905545, ip=10.20.240.13, actor_id=712a79f907c8bed6726211b201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8219142710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 95 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,174 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905536, ip=10.20.240.13, actor_id=57d693c6d0ed78d9ba744c7e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f65b229a9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905536, ip=10.20.240.13, actor_id=57d693c6d0ed78d9ba744c7e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f65b229a9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905536, ip=10.20.240.13, actor_id=57d693c6d0ed78d9ba744c7e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f65b229a9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,175 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.36 MiB is allocated by PyTorch, and 148.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905538, ip=10.20.240.13, actor_id=4fbe5ce617fab560ec7925f701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f714551a6e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Process 905548 has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.36 MiB is allocated by PyTorch, and 148.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-15 12:14:50,878 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.36 MiB is allocated by PyTorch, and 148.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 14 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.36 MiB is allocated by PyTorch, and 148.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-15 12:14:50,879 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.36 MiB is allocated by PyTorch, and 148.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=905548, ip=10.20.240.13, actor_id=3a5ce8feb71aecb5ec8e774001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6516fa64d0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 14 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-15_12-12-36_612283_868558/runtime_resources/working_dir_files/_ray_pkg_cbafdf10a2a492f0/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 141.12 MiB is free. Process 868558 has 214.00 MiB memory in use. Including non-PyTorch memory, this process has 1.30 GiB memory in use. Process 905549 has 1.30 GiB memory in use. Process 905546 has 1.30 GiB memory in use. Process 905545 has 1.30 GiB memory in use. Process 905544 has 1.30 GiB memory in use. Process 905539 has 1.47 GiB memory in use. Process 905538 has 1.30 GiB memory in use. Process 905536 has 1.30 GiB memory in use. Of the allocated memory 1009.36 MiB is allocated by PyTorch, and 148.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
DEBUG flwr 2024-05-15 12:14:50,880 | server.py:236 | fit_round 1 received 0 results and 10 failures
ERROR flwr 2024-05-15 12:14:50,881 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-05-15 12:14:51,010 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 79, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 99, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 150, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-05-15 12:14:51,010 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-15 12:14:51,010 | Simulation.py:168 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0654
wandb:     loss 0.00023
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240515_121226-vuqnp5st
wandb: Find logs at: ./wandb/offline-run-20240515_121226-vuqnp5st/logs
[2m[36m(DefaultActor pid=905546)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=905546)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
