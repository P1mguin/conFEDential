ctit088
	Adding python 3.10.7 (ubuntu 20.04) to your environment
	Adding slurm utilities
	Adding compute-node cpu/gpu monitoring utilities
2024-05-14 15:00:25.552635: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-14 15:00:27.257523: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-14 15:00:29.323632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-14 15:00:36,924 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-14 15:00:36,926 | Data.py:112 | No preprocessed data found for the given preprocess function, preprocessing now
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/data/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Mon May  6 18:38:11 2024).
Map:   0%|          | 0/60000 [00:00<?, ? examples/s]Map:   0%|          | 1/60000 [00:00<4:55:06,  3.39 examples/s]Map:   1%|          | 314/60000 [00:00<00:58, 1022.53 examples/s]Map:   1%|1         | 669/60000 [00:00<00:32, 1823.06 examples/s]Map:   2%|1         | 1000/60000 [00:01<01:23, 708.54 examples/s]Map:   2%|2         | 1379/60000 [00:01<00:54, 1082.00 examples/s]Map:   3%|2         | 1761/60000 [00:01<00:39, 1486.11 examples/s]Map:   4%|3         | 2190/60000 [00:01<00:34, 1677.70 examples/s]Map:   4%|4         | 2574/60000 [00:01<00:27, 2053.69 examples/s]Map:   5%|4         | 2957/60000 [00:01<00:23, 2406.42 examples/s]Map:   6%|5         | 3387/60000 [00:02<00:22, 2541.27 examples/s]Map:   6%|6         | 3756/60000 [00:02<00:20, 2794.44 examples/s]Map:   7%|6         | 4191/60000 [00:02<00:20, 2762.16 examples/s]Map:   8%|7         | 4575/60000 [00:02<00:18, 3007.48 examples/s]Map:   8%|8         | 4955/60000 [00:02<00:17, 3201.19 examples/s]Map:   9%|8         | 5381/60000 [00:02<00:17, 3056.28 examples/s]Map:  10%|9         | 5761/60000 [00:02<00:16, 3237.68 examples/s]Map:  10%|#         | 6190/60000 [00:02<00:17, 3069.19 examples/s]Map:  11%|#         | 6574/60000 [00:03<00:16, 3255.93 examples/s]Map:  12%|#1        | 6956/60000 [00:03<00:15, 3401.29 examples/s]Map:  12%|#2        | 7385/60000 [00:03<00:16, 3201.40 examples/s]Map:  13%|#2        | 7767/60000 [00:03<00:15, 3354.77 examples/s]Map:  14%|#3        | 8193/60000 [00:03<00:16, 3170.61 examples/s]Map:  14%|#4        | 8576/60000 [00:03<00:15, 3335.04 examples/s]Map:  15%|#4        | 8956/60000 [00:03<00:14, 3455.58 examples/s]Map:  16%|#5        | 9386/60000 [00:03<00:15, 3203.89 examples/s]Map:  16%|#6        | 9769/60000 [00:03<00:14, 3361.48 examples/s]Map:  17%|#6        | 10193/60000 [00:04<00:15, 3172.35 examples/s]Map:  18%|#7        | 10575/60000 [00:04<00:14, 3331.65 examples/s]Map:  18%|#8        | 10940/60000 [00:04<00:14, 3415.22 examples/s]Map:  19%|#8        | 11384/60000 [00:04<00:15, 3209.98 examples/s]Map:  20%|#9        | 11767/60000 [00:04<00:14, 3364.03 examples/s]Map:  20%|##        | 12190/60000 [00:04<00:15, 3171.40 examples/s]Map:  21%|##        | 12574/60000 [00:04<00:14, 3337.02 examples/s]Map:  22%|##1       | 12958/60000 [00:04<00:13, 3466.54 examples/s]Map:  22%|##2       | 13381/60000 [00:05<00:14, 3197.63 examples/s]Map:  23%|##2       | 13762/60000 [00:05<00:13, 3352.20 examples/s]Map:  24%|##3       | 14190/60000 [00:05<00:14, 3171.32 examples/s]Map:  24%|##4       | 14576/60000 [00:05<00:13, 3340.90 examples/s]Map:  25%|##4       | 14960/60000 [00:05<00:12, 3470.17 examples/s]Map:  26%|##5       | 15389/60000 [00:05<00:13, 3245.45 examples/s]Map:  26%|##6       | 15772/60000 [00:05<00:13, 3390.71 examples/s]Map:  27%|##6       | 16195/60000 [00:05<00:13, 3185.59 examples/s]Map:  28%|##7       | 16578/60000 [00:06<00:12, 3346.54 examples/s]Map:  28%|##8       | 16960/60000 [00:06<00:12, 3469.59 examples/s]Map:  29%|##8       | 17385/60000 [00:06<00:13, 3214.57 examples/s]Map:  30%|##9       | 17769/60000 [00:06<00:12, 3372.04 examples/s]Map:  30%|###       | 18191/60000 [00:06<00:13, 3172.64 examples/s]Map:  31%|###       | 18578/60000 [00:06<00:12, 3345.19 examples/s]Map:  32%|###1      | 18961/60000 [00:06<00:11, 3469.83 examples/s]Map:  32%|###2      | 19387/60000 [00:06<00:12, 3239.27 examples/s]Map:  33%|###2      | 19767/60000 [00:07<00:11, 3379.99 examples/s]Map:  34%|###3      | 20192/60000 [00:07<00:12, 3182.37 examples/s]Map:  34%|###4      | 20575/60000 [00:07<00:11, 3345.22 examples/s]Map:  35%|###4      | 20958/60000 [00:07<00:11, 3471.97 examples/s]Map:  36%|###5      | 21386/60000 [00:07<00:11, 3229.18 examples/s]Map:  36%|###6      | 21771/60000 [00:07<00:11, 3386.07 examples/s]Map:  37%|###6      | 22192/60000 [00:07<00:11, 3179.96 examples/s]Map:  38%|###7      | 22573/60000 [00:07<00:11, 3336.38 examples/s]Map:  38%|###8      | 22952/60000 [00:07<00:10, 3452.34 examples/s]Map:  39%|###8      | 23382/60000 [00:08<00:11, 3225.40 examples/s]Map:  40%|###9      | 23764/60000 [00:08<00:10, 3375.58 examples/s]Map:  40%|####      | 24191/60000 [00:08<00:11, 3166.65 examples/s]Map:  41%|####      | 24577/60000 [00:08<00:10, 3338.17 examples/s]Map:  42%|####1     | 24959/60000 [00:08<00:10, 3463.22 examples/s]Map:  42%|####2     | 25385/60000 [00:08<00:10, 3235.36 examples/s]Map:  43%|####2     | 25763/60000 [00:08<00:10, 3371.37 examples/s]Map:  44%|####3     | 26190/60000 [00:08<00:10, 3148.81 examples/s]Map:  44%|####4     | 26575/60000 [00:09<00:10, 3322.78 examples/s]Map:  45%|####4     | 26957/60000 [00:09<00:09, 3452.57 examples/s]Map:  46%|####5     | 27387/60000 [00:09<00:10, 3237.01 examples/s]Map:  46%|####6     | 27773/60000 [00:09<00:09, 3393.70 examples/s]Map:  47%|####6     | 28198/60000 [00:09<00:09, 3193.95 examples/s]Map:  48%|####7     | 28580/60000 [00:09<00:09, 3349.94 examples/s]Map:  48%|####8     | 28959/60000 [00:09<00:08, 3463.78 examples/s]Map:  49%|####8     | 29386/60000 [00:09<00:09, 3231.66 examples/s]Map:  50%|####9     | 29769/60000 [00:10<00:08, 3382.62 examples/s]Map:  50%|#####     | 30192/60000 [00:10<00:09, 3155.28 examples/s]Map:  51%|#####     | 30578/60000 [00:10<00:08, 3329.73 examples/s]Map:  52%|#####1    | 30962/60000 [00:10<00:08, 3462.90 examples/s]Map:  52%|#####2    | 31384/60000 [00:10<00:08, 3226.19 examples/s]Map:  53%|#####2    | 31768/60000 [00:10<00:08, 3380.69 examples/s]Map:  54%|#####3    | 32195/60000 [00:10<00:08, 3189.24 examples/s]Map:  54%|#####4    | 32575/60000 [00:10<00:08, 3339.93 examples/s]Map:  55%|#####4    | 32959/60000 [00:10<00:07, 3468.97 examples/s]Map:  56%|#####5    | 33388/60000 [00:11<00:08, 3227.74 examples/s]Map:  56%|#####6    | 33775/60000 [00:11<00:07, 3389.47 examples/s]Map:  57%|#####7    | 34201/60000 [00:11<00:08, 3192.02 examples/s]Map:  58%|#####7    | 34583/60000 [00:11<00:07, 3349.56 examples/s]Map:  58%|#####8    | 34963/60000 [00:11<00:07, 3467.39 examples/s]Map:  59%|#####8    | 35384/60000 [00:11<00:07, 3222.96 examples/s]Map:  60%|#####9    | 35765/60000 [00:11<00:07, 3372.20 examples/s]Map:  60%|######    | 36192/60000 [00:11<00:07, 3163.86 examples/s]Map:  61%|######    | 36573/60000 [00:12<00:07, 3324.83 examples/s]Map:  62%|######1   | 36952/60000 [00:12<00:06, 3444.91 examples/s]Map:  62%|######2   | 37367/60000 [00:12<00:07, 3190.40 examples/s]Map:  63%|######2   | 37741/60000 [00:12<00:06, 3329.99 examples/s]Map:  64%|######3   | 38192/60000 [00:12<00:06, 3119.01 examples/s]Map:  64%|######4   | 38577/60000 [00:12<00:06, 3297.78 examples/s]Map:  65%|######4   | 38963/60000 [00:13<00:09, 2107.72 examples/s]Map:  66%|######5   | 39383/60000 [00:13<00:09, 2270.09 examples/s]Map:  66%|######6   | 39766/60000 [00:13<00:07, 2570.47 examples/s]Map:  67%|######6   | 40192/60000 [00:13<00:07, 2647.38 examples/s]Map:  68%|######7   | 40576/60000 [00:13<00:06, 2905.87 examples/s]Map:  68%|######8   | 40957/60000 [00:13<00:06, 3119.07 examples/s]Map:  69%|######8   | 41383/60000 [00:13<00:06, 2993.33 examples/s]Map:  70%|######9   | 41767/60000 [00:13<00:05, 3194.63 examples/s]Map:  70%|#######   | 42194/60000 [00:14<00:05, 3073.83 examples/s]Map:  71%|#######   | 42579/60000 [00:14<00:05, 3261.66 examples/s]Map:  72%|#######1  | 42962/60000 [00:14<00:05, 3406.85 examples/s]Map:  72%|#######2  | 43388/60000 [00:14<00:05, 3202.22 examples/s]Map:  73%|#######2  | 43770/60000 [00:14<00:04, 3356.72 examples/s]Map:  74%|#######3  | 44197/60000 [00:14<00:04, 3175.53 examples/s]Map:  74%|#######4  | 44581/60000 [00:14<00:04, 3341.33 examples/s]Map:  75%|#######4  | 44964/60000 [00:14<00:04, 3468.13 examples/s]Map:  76%|#######5  | 45384/60000 [00:15<00:04, 3208.80 examples/s]Map:  76%|#######6  | 45769/60000 [00:15<00:04, 3370.92 examples/s]Map:  77%|#######6  | 46189/60000 [00:15<00:04, 3150.51 examples/s]Map:  78%|#######7  | 46574/60000 [00:15<00:04, 3322.01 examples/s]Map:  78%|#######8  | 46954/60000 [00:15<00:03, 3445.56 examples/s]Map:  79%|#######8  | 47387/60000 [00:15<00:03, 3231.66 examples/s]Map:  80%|#######9  | 47768/60000 [00:15<00:03, 3377.41 examples/s]Map:  80%|########  | 48192/60000 [00:15<00:03, 3179.83 examples/s]Map:  81%|########  | 48575/60000 [00:15<00:03, 3341.13 examples/s]Map:  82%|########1 | 48957/60000 [00:16<00:03, 3465.10 examples/s]Map:  82%|########2 | 49379/60000 [00:16<00:03, 3198.42 examples/s]Map:  83%|########2 | 49761/60000 [00:16<00:03, 3354.21 examples/s]Map:  84%|########3 | 50191/60000 [00:16<00:03, 3172.50 examples/s]Map:  84%|########4 | 50575/60000 [00:16<00:02, 3337.51 examples/s]Map:  85%|########4 | 50957/60000 [00:16<00:02, 3463.29 examples/s]Map:  86%|########5 | 51385/60000 [00:16<00:02, 3236.87 examples/s]Map:  86%|########6 | 51768/60000 [00:16<00:02, 3388.01 examples/s]Map:  87%|########6 | 52189/60000 [00:17<00:02, 3162.29 examples/s]Map:  88%|########7 | 52572/60000 [00:17<00:02, 3327.25 examples/s]Map:  88%|########8 | 52953/60000 [00:17<00:02, 3451.56 examples/s]Map:  89%|########8 | 53380/60000 [00:17<00:02, 3228.80 examples/s]Map:  90%|########9 | 53763/60000 [00:17<00:01, 3380.72 examples/s]Map:  90%|######### | 54189/60000 [00:17<00:01, 3183.03 examples/s]Map:  91%|######### | 54575/60000 [00:17<00:01, 3349.61 examples/s]Map:  92%|#########1| 54960/60000 [00:17<00:01, 3477.59 examples/s]Map:  92%|#########2| 55385/60000 [00:18<00:01, 3242.32 examples/s]Map:  93%|#########2| 55765/60000 [00:18<00:01, 3383.47 examples/s]Map:  94%|#########3| 56189/60000 [00:18<00:01, 3154.30 examples/s]Map:  94%|#########4| 56574/60000 [00:18<00:01, 3327.55 examples/s]Map:  95%|#########4| 56957/60000 [00:18<00:00, 3456.84 examples/s]Map:  96%|#########5| 57388/60000 [00:18<00:00, 3239.51 examples/s]Map:  96%|#########6| 57771/60000 [00:18<00:00, 3388.97 examples/s]Map:  97%|#########6| 58182/60000 [00:18<00:00, 3157.30 examples/s]Map:  98%|#########7| 58555/60000 [00:18<00:00, 3300.62 examples/s]Map:  98%|#########8| 58928/60000 [00:19<00:00, 3411.73 examples/s]Map:  99%|#########8| 59381/60000 [00:19<00:00, 3172.38 examples/s]Map: 100%|#########9| 59763/60000 [00:19<00:00, 3333.99 examples/s]Map: 100%|##########| 60000/60000 [00:20<00:00, 2919.49 examples/s]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:   4%|3         | 376/10000 [00:00<00:02, 3720.55 examples/s]Map:   8%|7         | 759/10000 [00:00<00:02, 3776.80 examples/s]Map:  12%|#1        | 1188/10000 [00:00<00:02, 3181.35 examples/s]Map:  16%|#5        | 1571/10000 [00:00<00:02, 3401.11 examples/s]Map:  20%|#9        | 1953/10000 [00:00<00:02, 3535.78 examples/s]Map:  24%|##3       | 2381/10000 [00:00<00:02, 3242.66 examples/s]Map:  28%|##7       | 2762/10000 [00:00<00:02, 3400.56 examples/s]Map:  32%|###1      | 3188/10000 [00:00<00:02, 3187.19 examples/s]Map:  36%|###5      | 3572/10000 [00:01<00:01, 3356.18 examples/s]Map:  40%|###9      | 3956/10000 [00:01<00:01, 3485.81 examples/s]Map:  44%|####3     | 4388/10000 [00:01<00:01, 3251.94 examples/s]Map:  48%|####7     | 4771/10000 [00:01<00:01, 3400.46 examples/s]Map:  52%|#####1    | 5191/10000 [00:01<00:01, 3175.34 examples/s]Map:  56%|#####5    | 5572/10000 [00:01<00:01, 3332.48 examples/s]Map:  60%|#####9    | 5954/10000 [00:01<00:01, 3459.60 examples/s]Map:  64%|######3   | 6377/10000 [00:01<00:01, 3208.19 examples/s]Map:  68%|######7   | 6758/10000 [00:02<00:00, 3359.70 examples/s]Map:  72%|#######1  | 7188/10000 [00:02<00:00, 3153.67 examples/s]Map:  76%|#######5  | 7571/10000 [00:02<00:00, 3319.61 examples/s]Map:  79%|#######9  | 7949/10000 [00:02<00:00, 3438.20 examples/s]Map:  84%|########3 | 8380/10000 [00:02<00:00, 3188.23 examples/s]Map:  88%|########7 | 8760/10000 [00:02<00:00, 3340.04 examples/s]Map:  92%|#########1| 9187/10000 [00:02<00:00, 3158.42 examples/s]Map:  96%|#########5| 9569/10000 [00:02<00:00, 3322.55 examples/s]Map:  99%|#########9| 9944/10000 [00:02<00:00, 3432.89 examples/s]Map: 100%|##########| 10000/10000 [00:03<00:00, 2511.32 examples/s]
INFO flwr 2024-05-14 15:07:46,144 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-14 15:07:46,145 | Simulation.py:247 | Found no previously split dataloaders, splitting the data now
INFO flwr 2024-05-14 15:08:14,087 | main.py:70 | Loaded 1 configs with name MNIST-LOGISTICREGRESSION-FEDNL, running...
INFO flwr 2024-05-14 15:08:14,087 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-14 15:08:14,090 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-14 15:08:17,858 | Simulation.py:367 | Created 8 clients with resources 8 CPUs and 0.125 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-14 15:08:17,859 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-14 15:08:17,861 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-05-14 15:08:24,327	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-14 15:08:24,449	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-14 15:08:24,528	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_eae2a1aa967e3a95.zip' (1.09MiB) to Ray cluster...
2024-05-14 15:08:24,533	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_eae2a1aa967e3a95.zip'.
INFO flwr 2024-05-14 15:08:51,289 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 136987481088.0, 'node:__internal_head__': 1.0, 'object_store_memory': 62994634752.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-05-14 15:08:51,289 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-14 15:08:51,289 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-14 15:08:51,303 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-14 15:08:51,303 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-14 15:08:51,303 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-14 15:08:51,304 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-14 15:08:52,641 | server.py:94 | initial parameters (loss, other metrics): 0.00023044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-05-14 15:08:52,641 | server.py:104 | FL starting
DEBUG flwr 2024-05-14 15:08:52,646 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=651167)[0m 2024-05-14 15:08:59.906153: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=651167)[0m 2024-05-14 15:08:59.991923: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=651167)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=651165)[0m 2024-05-14 15:09:02.555002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=651167)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=651167)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=651163)[0m 2024-05-14 15:09:00.082585: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=651163)[0m 2024-05-14 15:09:00.203759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=651163)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=651163)[0m 2024-05-14 15:09:02.570224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
ERROR flwr 2024-05-14 15:09:24,132 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 92 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:24,147 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 92 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:25,324 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 85 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:25,324 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 85 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:25,329 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651168, ip=10.20.240.18, actor_id=e744e2a627c1f602d9470e4701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3cc93a6710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651168, ip=10.20.240.18, actor_id=e744e2a627c1f602d9470e4701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3cc93a6710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651168, ip=10.20.240.18, actor_id=e744e2a627c1f602d9470e4701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3cc93a6710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 21 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:25,330 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651167, ip=10.20.240.18, actor_id=cb122ebf28aab2677b7ae17101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f41f0246aa0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651167, ip=10.20.240.18, actor_id=cb122ebf28aab2677b7ae17101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f41f0246aa0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651167, ip=10.20.240.18, actor_id=cb122ebf28aab2677b7ae17101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f41f0246aa0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 80 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:25,330 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651168, ip=10.20.240.18, actor_id=e744e2a627c1f602d9470e4701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3cc93a6710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651168, ip=10.20.240.18, actor_id=e744e2a627c1f602d9470e4701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3cc93a6710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651168, ip=10.20.240.18, actor_id=e744e2a627c1f602d9470e4701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3cc93a6710>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 21 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:25,331 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651167, ip=10.20.240.18, actor_id=cb122ebf28aab2677b7ae17101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f41f0246aa0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651167, ip=10.20.240.18, actor_id=cb122ebf28aab2677b7ae17101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f41f0246aa0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651167, ip=10.20.240.18, actor_id=cb122ebf28aab2677b7ae17101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f41f0246aa0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 80 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:25,336 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:25,337 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651166, ip=10.20.240.18, actor_id=d183c77a9bd5cc1b63fbd4fb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f910da0aa70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651166, ip=10.20.240.18, actor_id=d183c77a9bd5cc1b63fbd4fb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f910da0aa70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651166, ip=10.20.240.18, actor_id=d183c77a9bd5cc1b63fbd4fb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f910da0aa70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 64 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:25,337 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651159, ip=10.20.240.18, actor_id=ef45f6a689f997187e7b553401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6299ada8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651159, ip=10.20.240.18, actor_id=ef45f6a689f997187e7b553401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6299ada8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651159, ip=10.20.240.18, actor_id=ef45f6a689f997187e7b553401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6299ada8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 89 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:25,338 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651163, ip=10.20.240.18, actor_id=94beff19a62a770324d907e201000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f412ba1e8f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:25,339 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651164, ip=10.20.240.18, actor_id=4a424ebe1ff8b6f379f1d03e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f03cf7d2a10>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651164, ip=10.20.240.18, actor_id=4a424ebe1ff8b6f379f1d03e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f03cf7d2a10>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651164, ip=10.20.240.18, actor_id=4a424ebe1ff8b6f379f1d03e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f03cf7d2a10>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 95 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:25,340 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651166, ip=10.20.240.18, actor_id=d183c77a9bd5cc1b63fbd4fb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f910da0aa70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651166, ip=10.20.240.18, actor_id=d183c77a9bd5cc1b63fbd4fb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f910da0aa70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651166, ip=10.20.240.18, actor_id=d183c77a9bd5cc1b63fbd4fb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f910da0aa70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 64 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:25,340 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651162, ip=10.20.240.18, actor_id=84db106b98fe9fb7bdec772501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbcca2da9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651162, ip=10.20.240.18, actor_id=84db106b98fe9fb7bdec772501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbcca2da9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651162, ip=10.20.240.18, actor_id=84db106b98fe9fb7bdec772501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbcca2da9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 49 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:25,341 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651159, ip=10.20.240.18, actor_id=ef45f6a689f997187e7b553401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6299ada8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651159, ip=10.20.240.18, actor_id=ef45f6a689f997187e7b553401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6299ada8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651159, ip=10.20.240.18, actor_id=ef45f6a689f997187e7b553401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f6299ada8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 89 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:25,342 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651164, ip=10.20.240.18, actor_id=4a424ebe1ff8b6f379f1d03e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f03cf7d2a10>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651164, ip=10.20.240.18, actor_id=4a424ebe1ff8b6f379f1d03e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f03cf7d2a10>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651164, ip=10.20.240.18, actor_id=4a424ebe1ff8b6f379f1d03e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f03cf7d2a10>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 95 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:25,342 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651162, ip=10.20.240.18, actor_id=84db106b98fe9fb7bdec772501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbcca2da9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651162, ip=10.20.240.18, actor_id=84db106b98fe9fb7bdec772501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbcca2da9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651162, ip=10.20.240.18, actor_id=84db106b98fe9fb7bdec772501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fbcca2da9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 49 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Process 651165 has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 15:09:26,010 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 15:09:26,010 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=651165, ip=10.20.240.18, actor_id=4cc6ec29dc0823934d01db8801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fddee3ca8c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_15-08-17_964200_643578/runtime_resources/working_dir_files/_ray_pkg_eae2a1aa967e3a95/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 643578 has 224.00 MiB memory in use. Process 651166 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 651163 has 1.31 GiB memory in use. Process 651159 has 1.48 GiB memory in use. Process 651162 has 1.48 GiB memory in use. Process 651164 has 1.31 GiB memory in use. Process 651167 has 1.48 GiB memory in use. Process 651168 has 1.48 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
DEBUG flwr 2024-05-14 15:09:26,011 | server.py:236 | fit_round 1 received 0 results and 10 failures
ERROR flwr 2024-05-14 15:09:26,013 | app.py:313 | list index out of range
ERROR flwr 2024-05-14 15:09:26,016 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_messages(results)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 141, in _capture_messages
    self._capture_variable(captured_results["parameters"], parameters_path)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 152, in _capture_variable
    message = list(messages.values())[0]
IndexError: list index out of range

ERROR flwr 2024-05-14 15:09:26,016 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-14 15:09:26,016 | Simulation.py:168 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0654
wandb:     loss 0.00023
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240514_150817-2w85w2o8
wandb: Find logs at: ./wandb/offline-run-20240514_150817-2w85w2o8/logs
[2m[36m(DefaultActor pid=651163)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=651163)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
