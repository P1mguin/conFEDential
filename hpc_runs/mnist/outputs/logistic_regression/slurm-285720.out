ctit088
2024-05-14 21:01:37.504534: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-14 21:02:17.142979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-14 21:03:41.972988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-14 21:08:49,220 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-14 21:08:49,358 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-14 21:08:56,294 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-14 21:08:56,295 | Simulation.py:242 | Found previously split dataloaders, loading them
INFO flwr 2024-05-14 21:09:09,008 | main.py:70 | Loaded 1 configs with name MNIST-LOGISTICREGRESSION-FEDNL, running...
INFO flwr 2024-05-14 21:09:09,008 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-14 21:09:09,062 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-14 21:09:29,529 | Simulation.py:367 | Created 8 clients with resources 8 CPUs and 0.125 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-14 21:09:29,530 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-14 21:09:29,532 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-05-14 21:09:43,129	ERROR node.py:605 -- Failed to connect to GCS. Please check `gcs_server.out` for more details.
2024-05-14 21:10:05,922	ERROR services.py:1207 -- Failed to start the dashboard 
2024-05-14 21:10:05,924	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-05-14 21:10:05,924	ERROR services.py:1276 -- 
The last 20 lines of /local/ray/session_2024-05-14_21-09-31_447163_740646/logs/dashboard.log (it contains the error message from the dashboard): 
2024-05-14 21:09:47,425	INFO utils.py:123 -- Module ray.dashboard.modules.actor.actor_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:09:51,531	INFO utils.py:123 -- Module ray.dashboard.modules.event.event_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:09:51,555	INFO utils.py:123 -- Module ray.dashboard.modules.healthz.healthz_agent cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:09:51,560	INFO utils.py:123 -- Module ray.dashboard.modules.healthz.healthz_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:03,690	INFO utils.py:123 -- Module ray.dashboard.modules.job.job_agent cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:03,758	INFO utils.py:123 -- Module ray.dashboard.modules.job.job_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:04,209	INFO utils.py:123 -- Module ray.dashboard.modules.log.log_agent cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:04,211	INFO utils.py:123 -- Module ray.dashboard.modules.log.log_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:05,159	INFO utils.py:123 -- Module ray.dashboard.modules.metrics.metrics_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:05,286	INFO utils.py:123 -- Module ray.dashboard.modules.node.node_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:05,389	INFO utils.py:123 -- Module ray.dashboard.modules.reporter.reporter_agent cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:05,407	INFO utils.py:123 -- Module ray.dashboard.modules.reporter.reporter_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:05,585	INFO utils.py:123 -- Module ray.dashboard.modules.serve.serve_agent cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:05,589	INFO utils.py:123 -- Module ray.dashboard.modules.serve.serve_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:05,801	INFO utils.py:123 -- Module ray.dashboard.modules.snapshot.snapshot_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-05-14 21:10:05,874	INFO utils.py:123 -- Module ray.dashboard.modules.state.state_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'

2024-05-14 21:10:09,412	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-14 21:10:24,929	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-14 21:10:26,150	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d4c826d6263ded02.zip' (1.08MiB) to Ray cluster...
2024-05-14 21:10:26,158	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d4c826d6263ded02.zip'.
INFO flwr 2024-05-14 21:10:40,401 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 137009045300.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 63003876556.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-05-14 21:10:40,401 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-14 21:10:40,401 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-14 21:10:40,416 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-14 21:10:40,416 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-14 21:10:40,417 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-14 21:10:40,417 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=744170)[0m 2024-05-14 21:10:46.837903: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=744170)[0m 2024-05-14 21:10:46.931848: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=744170)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=744170)[0m 2024-05-14 21:10:48.985095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-14 21:10:51,099 | server.py:94 | initial parameters (loss, other metrics): 0.00023044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-05-14 21:10:51,099 | server.py:104 | FL starting
DEBUG flwr 2024-05-14 21:10:51,100 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=744170)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=744170)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=744176)[0m 2024-05-14 21:10:46.837878: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=744180)[0m 2024-05-14 21:10:46.937549: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=744180)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=744176)[0m 2024-05-14 21:10:48.985061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
ERROR flwr 2024-05-14 21:12:25,263 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 64 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:25,279 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 64 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,208 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 80 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,209 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 80 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,210 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744184, ip=10.20.240.18, actor_id=284499b1665222591a5975bb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f06efe6e950>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744184, ip=10.20.240.18, actor_id=284499b1665222591a5975bb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f06efe6e950>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744184, ip=10.20.240.18, actor_id=284499b1665222591a5975bb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f06efe6e950>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 21 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,211 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744184, ip=10.20.240.18, actor_id=284499b1665222591a5975bb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f06efe6e950>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744184, ip=10.20.240.18, actor_id=284499b1665222591a5975bb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f06efe6e950>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744184, ip=10.20.240.18, actor_id=284499b1665222591a5975bb01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f06efe6e950>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 21 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,213 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744180, ip=10.20.240.18, actor_id=9725679a176d925ab320cc6c01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7eed2e5426e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744180, ip=10.20.240.18, actor_id=9725679a176d925ab320cc6c01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7eed2e5426e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744180, ip=10.20.240.18, actor_id=9725679a176d925ab320cc6c01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7eed2e5426e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 85 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,214 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744180, ip=10.20.240.18, actor_id=9725679a176d925ab320cc6c01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7eed2e5426e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744180, ip=10.20.240.18, actor_id=9725679a176d925ab320cc6c01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7eed2e5426e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744180, ip=10.20.240.18, actor_id=9725679a176d925ab320cc6c01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7eed2e5426e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 85 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,217 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744176, ip=10.20.240.18, actor_id=54d6a95129475f20612da4a401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f27eff6a590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744176, ip=10.20.240.18, actor_id=54d6a95129475f20612da4a401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f27eff6a590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744176, ip=10.20.240.18, actor_id=54d6a95129475f20612da4a401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f27eff6a590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 89 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,217 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744176, ip=10.20.240.18, actor_id=54d6a95129475f20612da4a401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f27eff6a590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744176, ip=10.20.240.18, actor_id=54d6a95129475f20612da4a401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f27eff6a590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744176, ip=10.20.240.18, actor_id=54d6a95129475f20612da4a401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f27eff6a590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 89 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,218 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744177, ip=10.20.240.18, actor_id=a003b00371ff5565983abc9001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ef014eee590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744177, ip=10.20.240.18, actor_id=a003b00371ff5565983abc9001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ef014eee590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744177, ip=10.20.240.18, actor_id=a003b00371ff5565983abc9001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ef014eee590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 95 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,219 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744177, ip=10.20.240.18, actor_id=a003b00371ff5565983abc9001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ef014eee590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744177, ip=10.20.240.18, actor_id=a003b00371ff5565983abc9001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ef014eee590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744177, ip=10.20.240.18, actor_id=a003b00371ff5565983abc9001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ef014eee590>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 95 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,219 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,222 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744182, ip=10.20.240.18, actor_id=3cef808f2362287e9c82655301000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9c5423e9e0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,222 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744174, ip=10.20.240.18, actor_id=188446f71de80fe79deb7ab901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7efa2ff868c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744174, ip=10.20.240.18, actor_id=188446f71de80fe79deb7ab901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7efa2ff868c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744174, ip=10.20.240.18, actor_id=188446f71de80fe79deb7ab901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7efa2ff868c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 49 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,223 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744170, ip=10.20.240.18, actor_id=0752e924b570fe4d73be423801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5d325828f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744170, ip=10.20.240.18, actor_id=0752e924b570fe4d73be423801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5d325828f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744170, ip=10.20.240.18, actor_id=0752e924b570fe4d73be423801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5d325828f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 92 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,223 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744174, ip=10.20.240.18, actor_id=188446f71de80fe79deb7ab901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7efa2ff868c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744174, ip=10.20.240.18, actor_id=188446f71de80fe79deb7ab901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7efa2ff868c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744174, ip=10.20.240.18, actor_id=188446f71de80fe79deb7ab901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7efa2ff868c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 49 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,224 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744170, ip=10.20.240.18, actor_id=0752e924b570fe4d73be423801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5d325828f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744170, ip=10.20.240.18, actor_id=0752e924b570fe4d73be423801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5d325828f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~ <--- HERE
        grad_output, dim=dim, keepdim=True
    )
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744170, ip=10.20.240.18, actor_id=0752e924b570fe4d73be423801000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f5d325828f0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 92 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~ <--- HERE\n        grad_output, dim=dim, keepdim=True\n    )\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Process 744183 has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Including non-PyTorch memory, this process has 1.31 GiB memory in use. Of the allocated memory 1009.33 MiB is allocated by PyTorch, and 148.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
ERROR flwr 2024-05-14 21:12:26,871 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)

ERROR flwr 2024-05-14 21:12:26,872 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit
    new_parameters, data_size, metrics = self.learning_method.train(
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train
    optimizer.step(get_gradient)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step
    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn
    results = vmap(push_jvp, randomness=randomness)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp
    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn
    flat_jacobians_per_input = compute_jacobian_stacked()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked
    chunked_result = vmap(vjp_fn)(basis)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped
    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl
    return _flat_vmap(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn
    return f(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper
    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad
    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: The following operation failed in the TorchScript interpreter.
[36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data
    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype
):
    grad_input = grad_output - torch.exp(output) * torch.sum(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        grad_output, dim=dim, keepdim=True
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    )
    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)
RuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=744183, ip=10.20.240.18, actor_id=6e401eab32c8bf8c4019b82601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f8059cdea70>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/Client.py", line 16, in fit\n    new_parameters, data_size, metrics = self.learning_method.train(\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 127, in train\n    optimizer.step(get_gradient)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper\n    out = func(*args, **kwargs)\n  File "/local/ray/session_2024-05-14_21-09-31_447163_740646/runtime_resources/working_dir_files/_ray_pkg_d4c826d6263ded02/src/training/learning_methods/FedNL.py", line 42, in step\n    hessians = torch.func.hessian(closure, argnums=tuple(range(len(model_parameters))))(*model_parameters)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1139, in wrapper_fn\n    results = vmap(push_jvp, randomness=randomness)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 1130, in push_jvp\n    output = _jvp_with_argnums(func, args, basis, argnums=argnums, has_aux=has_aux)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 976, in _jvp_with_argnums\n    result_duals = func(*duals)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 609, in wrapper_fn\n    flat_jacobians_per_input = compute_jacobian_stacked()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 540, in compute_jacobian_stacked\n    chunked_result = vmap(vjp_fn)(basis)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/apis.py", line 188, in wrapped\n    return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 278, in vmap_impl\n    return _flat_vmap(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 44, in fn\n    return f(*args, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/vmap.py", line 391, in _flat_vmap\n    batched_outputs = func(*batched_inputs, **kwargs)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 336, in wrapper\n    result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py", line 124, in _autograd_grad\n    grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad\n    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_decomp/decompositions.py", line 820, in aten::_log_softmax_backward_data\n    grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype\n):\n    grad_input = grad_output - torch.exp(output) * torch.sum(\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        grad_output, dim=dim, keepdim=True\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    )\n    return _cast_grad_to_input_dtype(grad_output, grad_input, input_dtype)\nRuntimeError: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 168.69 MiB is free. Process 740646 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Process 744184 has 1.31 GiB memory in use. Process 744182 has 1.48 GiB memory in use. Process 744180 has 1.48 GiB memory in use. Process 744177 has 1.48 GiB memory in use. Process 744176 has 1.48 GiB memory in use. Process 744174 has 1.48 GiB memory in use. Process 744170 has 1.31 GiB memory in use. Of the allocated memory 1.16 GiB is allocated by PyTorch, and 148.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n',)
DEBUG flwr 2024-05-14 21:12:26,873 | server.py:236 | fit_round 1 received 0 results and 10 failures
ERROR flwr 2024-05-14 21:12:26,874 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-05-14 21:12:27,007 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 79, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 99, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 150, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-05-14 21:12:27,007 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-14 21:12:27,008 | Simulation.py:168 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0654
wandb:     loss 0.00023
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240514_210911-3t2n1xgp
wandb: Find logs at: ./wandb/offline-run-20240514_210911-3t2n1xgp/logs
[2m[36m(DefaultActor pid=744176)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=744176)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
