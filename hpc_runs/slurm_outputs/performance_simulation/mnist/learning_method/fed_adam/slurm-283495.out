ctit090
2024-04-30 17:08:36.486913: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-30 17:08:39.837236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-30 17:08:44.188082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-30 17:09:02,352 | batch_run_simulation.py:80 | Loaded 108 configs with name MINST-LOGISTICREGRESSION-FEDADAM, running...
INFO flwr 2024-04-30 17:09:02,353 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:09:04,241 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-30 17:09:06,883	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:09:07,027	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:09:07,095	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:09:07,096	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip'.
INFO flwr 2024-04-30 17:09:16,779 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 55262184652.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:A40': 1.0, 'CPU': 64.0, 'node:10.20.240.20': 1.0, 'memory': 118945097524.0}
INFO flwr 2024-04-30 17:09:16,780 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:09:16,780 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:09:16,793 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:09:16,794 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:09:16,794 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:09:16,794 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:09:21,617 | server.py:94 | initial parameters (loss, other metrics): 2.3044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-04-30 17:09:21,618 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:09:21,618 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=934053)[0m 2024-04-30 17:09:22.194768: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=934053)[0m 2024-04-30 17:09:22.291290: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=934053)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=934053)[0m 2024-04-30 17:09:24.089153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=934058)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=934058)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=934058)[0m 2024-04-30 17:09:22.436386: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=934058)[0m 2024-04-30 17:09:22.529230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=934058)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=934057)[0m 2024-04-30 17:09:24.887154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:09:38,006 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:09:39,206 | server.py:125 | fit progress: (1, 1.9723724126815796, {'accuracy': 0.4844, 'data_size': 10000}, 17.587847304996103)
INFO flwr 2024-04-30 17:09:39,206 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:09:39,207 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:09:47,735 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:09:48,689 | server.py:125 | fit progress: (2, 1.7960888147354126, {'accuracy': 0.671, 'data_size': 10000}, 27.070746951998444)
INFO flwr 2024-04-30 17:09:48,689 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:09:48,689 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:09:56,493 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:09:57,671 | server.py:125 | fit progress: (3, 1.7721357345581055, {'accuracy': 0.6911, 'data_size': 10000}, 36.052999039995484)
INFO flwr 2024-04-30 17:09:57,671 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:09:57,671 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:10:04,813 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:10:05,986 | server.py:125 | fit progress: (4, 1.731767177581787, {'accuracy': 0.7327, 'data_size': 10000}, 44.368063280999195)
INFO flwr 2024-04-30 17:10:05,986 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:10:05,986 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:10:13,643 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:10:14,863 | server.py:125 | fit progress: (5, 1.694071650505066, {'accuracy': 0.7682, 'data_size': 10000}, 53.2454339889955)
INFO flwr 2024-04-30 17:10:14,864 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:10:14,864 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:10:22,351 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:10:23,329 | server.py:125 | fit progress: (6, 1.695738434791565, {'accuracy': 0.765, 'data_size': 10000}, 61.71071352099534)
INFO flwr 2024-04-30 17:10:23,329 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:10:23,329 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:10:30,918 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:10:32,105 | server.py:125 | fit progress: (7, 1.6844971179962158, {'accuracy': 0.776, 'data_size': 10000}, 70.4873271539982)
INFO flwr 2024-04-30 17:10:32,106 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:10:32,106 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:10:40,475 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:10:41,695 | server.py:125 | fit progress: (8, 1.6461924314498901, {'accuracy': 0.815, 'data_size': 10000}, 80.07703279200359)
INFO flwr 2024-04-30 17:10:41,695 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:10:41,695 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:10:49,889 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:10:51,073 | server.py:125 | fit progress: (9, 1.644797682762146, {'accuracy': 0.8159, 'data_size': 10000}, 89.45462657499593)
INFO flwr 2024-04-30 17:10:51,073 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:10:51,073 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:10:58,722 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:10:59,925 | server.py:125 | fit progress: (10, 1.612777590751648, {'accuracy': 0.8484, 'data_size': 10000}, 98.3073651099985)
INFO flwr 2024-04-30 17:10:59,926 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:10:59,926 | server.py:153 | FL finished in 98.30773474600574
INFO flwr 2024-04-30 17:10:59,926 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:10:59,926 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:10:59,926 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:10:59,926 | app.py:229 | app_fit: losses_centralized [(0, 2.3044753074645996), (1, 1.9723724126815796), (2, 1.7960888147354126), (3, 1.7721357345581055), (4, 1.731767177581787), (5, 1.694071650505066), (6, 1.695738434791565), (7, 1.6844971179962158), (8, 1.6461924314498901), (9, 1.644797682762146), (10, 1.612777590751648)]
INFO flwr 2024-04-30 17:10:59,926 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0654), (1, 0.4844), (2, 0.671), (3, 0.6911), (4, 0.7327), (5, 0.7682), (6, 0.765), (7, 0.776), (8, 0.815), (9, 0.8159), (10, 0.8484)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8484
wandb:     loss 1.61278
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_170903-6ttkecwv
wandb: Find logs at: ./wandb/offline-run-20240430_170903-6ttkecwv/logs
INFO flwr 2024-04-30 17:11:03,426 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:11:04,079 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=934052)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=934052)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:11:09,156	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:11:09,232	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:11:09,296	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:11:09,297	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip'.
INFO flwr 2024-04-30 17:11:18,939 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 54508867584.0, 'node:10.20.240.20': 1.0, 'CPU': 64.0, 'memory': 117187357696.0, 'accelerator_type:A40': 1.0}
INFO flwr 2024-04-30 17:11:18,939 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:11:18,939 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:11:18,952 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:11:18,952 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:11:18,953 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:11:18,953 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:11:20,774 | server.py:94 | initial parameters (loss, other metrics): 2.3022866249084473, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-30 17:11:20,775 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:11:20,775 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=937174)[0m 2024-04-30 17:11:24.383575: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=937174)[0m 2024-04-30 17:11:24.470991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=937174)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=937180)[0m 2024-04-30 17:11:26.676043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=937177)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=937177)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=937177)[0m 2024-04-30 17:11:24.515579: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=937177)[0m 2024-04-30 17:11:24.601944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=937177)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=937177)[0m 2024-04-30 17:11:26.735778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:11:42,535 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:11:43,744 | server.py:125 | fit progress: (1, 1.8622398376464844, {'accuracy': 0.6459, 'data_size': 10000}, 22.969743991998257)
INFO flwr 2024-04-30 17:11:43,745 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:11:43,745 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:11:52,077 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:11:53,257 | server.py:125 | fit progress: (2, 1.6830966472625732, {'accuracy': 0.793, 'data_size': 10000}, 32.48263119999319)
INFO flwr 2024-04-30 17:11:53,258 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:11:53,258 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:12:01,178 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:12:02,412 | server.py:125 | fit progress: (3, 1.6593703031539917, {'accuracy': 0.8042, 'data_size': 10000}, 41.63735396999982)
INFO flwr 2024-04-30 17:12:02,412 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:12:02,413 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:12:09,735 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:12:10,736 | server.py:125 | fit progress: (4, 1.5941450595855713, {'accuracy': 0.8733, 'data_size': 10000}, 49.96096738000051)
INFO flwr 2024-04-30 17:12:10,736 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:12:10,736 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:12:18,607 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:12:19,650 | server.py:125 | fit progress: (5, 1.5920077562332153, {'accuracy': 0.8721, 'data_size': 10000}, 58.874998707004124)
INFO flwr 2024-04-30 17:12:19,650 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:12:19,650 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:12:27,300 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:12:28,299 | server.py:125 | fit progress: (6, 1.5767263174057007, {'accuracy': 0.8875, 'data_size': 10000}, 67.52474548399914)
INFO flwr 2024-04-30 17:12:28,300 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:12:28,300 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:12:36,056 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:12:37,257 | server.py:125 | fit progress: (7, 1.5839424133300781, {'accuracy': 0.8791, 'data_size': 10000}, 76.4818994209927)
INFO flwr 2024-04-30 17:12:37,257 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:12:37,257 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:12:45,046 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:12:46,260 | server.py:125 | fit progress: (8, 1.5696886777877808, {'accuracy': 0.8948, 'data_size': 10000}, 85.48500558000524)
INFO flwr 2024-04-30 17:12:46,260 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:12:46,260 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:12:54,137 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:12:55,308 | server.py:125 | fit progress: (9, 1.560347318649292, {'accuracy': 0.9027, 'data_size': 10000}, 94.53365199000109)
INFO flwr 2024-04-30 17:12:55,309 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:12:55,309 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:13:03,267 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:13:04,490 | server.py:125 | fit progress: (10, 1.5731120109558105, {'accuracy': 0.8882, 'data_size': 10000}, 103.71504128600645)
INFO flwr 2024-04-30 17:13:04,490 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:13:04,490 | server.py:153 | FL finished in 103.71548825000355
INFO flwr 2024-04-30 17:13:04,490 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:13:04,490 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:13:04,491 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:13:04,491 | app.py:229 | app_fit: losses_centralized [(0, 2.3022866249084473), (1, 1.8622398376464844), (2, 1.6830966472625732), (3, 1.6593703031539917), (4, 1.5941450595855713), (5, 1.5920077562332153), (6, 1.5767263174057007), (7, 1.5839424133300781), (8, 1.5696886777877808), (9, 1.560347318649292), (10, 1.5731120109558105)]
INFO flwr 2024-04-30 17:13:04,491 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.6459), (2, 0.793), (3, 0.8042), (4, 0.8733), (5, 0.8721), (6, 0.8875), (7, 0.8791), (8, 0.8948), (9, 0.9027), (10, 0.8882)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8882
wandb:     loss 1.57311
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_171103-a1iea8fs
wandb: Find logs at: ./wandb/offline-run-20240430_171103-a1iea8fs/logs
INFO flwr 2024-04-30 17:13:07,977 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:13:10,973 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=937176)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=937176)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:13:15,362	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:13:15,454	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:13:15,560	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:13:15,561	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip'.
INFO flwr 2024-04-30 17:13:25,145 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:A40': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 54046970265.0, 'CPU': 64.0, 'node:10.20.240.20': 1.0, 'memory': 116109597287.0, 'GPU': 1.0}
INFO flwr 2024-04-30 17:13:25,145 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:13:25,145 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:13:25,158 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:13:25,159 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:13:25,159 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:13:25,159 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:13:27,296 | server.py:94 | initial parameters (loss, other metrics): 2.302816867828369, {'accuracy': 0.0734, 'data_size': 10000}
INFO flwr 2024-04-30 17:13:27,297 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:13:27,298 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=940259)[0m 2024-04-30 17:13:30.586490: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=940259)[0m 2024-04-30 17:13:30.675569: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=940259)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=940259)[0m 2024-04-30 17:13:32.539657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=940259)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=940259)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=940258)[0m 2024-04-30 17:13:30.876656: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=940260)[0m 2024-04-30 17:13:30.984457: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=940260)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=940258)[0m 2024-04-30 17:13:32.805197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:13:47,013 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:13:48,289 | server.py:125 | fit progress: (1, 1.838166356086731, {'accuracy': 0.6681, 'data_size': 10000}, 20.99177282299206)
INFO flwr 2024-04-30 17:13:48,289 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:13:48,290 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:13:57,032 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:13:58,194 | server.py:125 | fit progress: (2, 1.6658216714859009, {'accuracy': 0.814, 'data_size': 10000}, 30.896359966995078)
INFO flwr 2024-04-30 17:13:58,194 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:13:58,194 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:14:05,826 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:14:07,031 | server.py:125 | fit progress: (3, 1.6092690229415894, {'accuracy': 0.8595, 'data_size': 10000}, 39.73354696300521)
INFO flwr 2024-04-30 17:14:07,031 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:14:07,031 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:14:14,378 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:14:15,600 | server.py:125 | fit progress: (4, 1.590683102607727, {'accuracy': 0.8728, 'data_size': 10000}, 48.302996430997155)
INFO flwr 2024-04-30 17:14:15,601 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:14:15,601 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:14:23,148 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:14:24,368 | server.py:125 | fit progress: (5, 1.595191240310669, {'accuracy': 0.8688, 'data_size': 10000}, 57.07095642699278)
INFO flwr 2024-04-30 17:14:24,369 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:14:24,369 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:14:32,029 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:14:33,019 | server.py:125 | fit progress: (6, 1.587022066116333, {'accuracy': 0.8758, 'data_size': 10000}, 65.72172222800145)
INFO flwr 2024-04-30 17:14:33,019 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:14:33,020 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:14:40,698 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:14:41,904 | server.py:125 | fit progress: (7, 1.5707765817642212, {'accuracy': 0.891, 'data_size': 10000}, 74.60663244199532)
INFO flwr 2024-04-30 17:14:41,904 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:14:41,904 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:14:49,756 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:14:50,960 | server.py:125 | fit progress: (8, 1.5597193241119385, {'accuracy': 0.9028, 'data_size': 10000}, 83.66264177999983)
INFO flwr 2024-04-30 17:14:50,960 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:14:50,961 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:14:58,126 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:14:59,319 | server.py:125 | fit progress: (9, 1.5574077367782593, {'accuracy': 0.9047, 'data_size': 10000}, 92.02151758599211)
INFO flwr 2024-04-30 17:14:59,319 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:14:59,319 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:15:07,065 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:15:08,294 | server.py:125 | fit progress: (10, 1.5579249858856201, {'accuracy': 0.9035, 'data_size': 10000}, 100.99666219299252)
INFO flwr 2024-04-30 17:15:08,294 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:15:08,294 | server.py:153 | FL finished in 100.99700407699856
INFO flwr 2024-04-30 17:15:08,295 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:15:08,295 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:15:08,295 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:15:08,295 | app.py:229 | app_fit: losses_centralized [(0, 2.302816867828369), (1, 1.838166356086731), (2, 1.6658216714859009), (3, 1.6092690229415894), (4, 1.590683102607727), (5, 1.595191240310669), (6, 1.587022066116333), (7, 1.5707765817642212), (8, 1.5597193241119385), (9, 1.5574077367782593), (10, 1.5579249858856201)]
INFO flwr 2024-04-30 17:15:08,295 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0734), (1, 0.6681), (2, 0.814), (3, 0.8595), (4, 0.8728), (5, 0.8688), (6, 0.8758), (7, 0.891), (8, 0.9028), (9, 0.9047), (10, 0.9035)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9035
wandb:     loss 1.55792
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_171308-sm840dky
wandb: Find logs at: ./wandb/offline-run-20240430_171308-sm840dky/logs
INFO flwr 2024-04-30 17:15:11,789 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:15:12,428 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=940258)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=940258)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:15:16,889	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:15:16,995	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:15:17,068	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:15:17,070	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip'.
INFO flwr 2024-04-30 17:15:27,414 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:A40': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'memory': 114111551693.0, 'object_store_memory': 53190665011.0, 'node:10.20.240.20': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 17:15:27,414 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:15:27,414 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:15:27,428 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:15:27,430 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:15:27,430 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:15:27,430 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:15:30,086 | server.py:94 | initial parameters (loss, other metrics): 2.304152250289917, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-30 17:15:30,087 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:15:30,087 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=943927)[0m 2024-04-30 17:15:32.639917: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=943927)[0m 2024-04-30 17:15:32.732091: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=943927)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=943927)[0m 2024-04-30 17:15:34.619206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=943923)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=943923)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=943915)[0m 2024-04-30 17:15:32.864646: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=943915)[0m 2024-04-30 17:15:32.951204: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=943915)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=943915)[0m 2024-04-30 17:15:34.744296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:15:50,541 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:15:51,767 | server.py:125 | fit progress: (1, 1.9148750305175781, {'accuracy': 0.5761, 'data_size': 10000}, 21.680059815000277)
INFO flwr 2024-04-30 17:15:51,767 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:15:51,768 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:16:01,081 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:16:02,086 | server.py:125 | fit progress: (2, 1.7685304880142212, {'accuracy': 0.7087, 'data_size': 10000}, 31.998841960012214)
INFO flwr 2024-04-30 17:16:02,086 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:16:02,086 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:16:11,321 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:16:12,336 | server.py:125 | fit progress: (3, 1.7030112743377686, {'accuracy': 0.764, 'data_size': 10000}, 42.24890146301186)
INFO flwr 2024-04-30 17:16:12,336 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:16:12,336 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:16:21,233 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:16:22,448 | server.py:125 | fit progress: (4, 1.7375408411026, {'accuracy': 0.7256, 'data_size': 10000}, 52.36065839100047)
INFO flwr 2024-04-30 17:16:22,448 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:16:22,448 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:16:30,845 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:16:32,057 | server.py:125 | fit progress: (5, 1.7286404371261597, {'accuracy': 0.7299, 'data_size': 10000}, 61.969578621006804)
INFO flwr 2024-04-30 17:16:32,057 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:16:32,057 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:16:41,037 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:16:42,263 | server.py:125 | fit progress: (6, 1.6218650341033936, {'accuracy': 0.8422, 'data_size': 10000}, 72.17579961201409)
INFO flwr 2024-04-30 17:16:42,263 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:16:42,263 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:16:50,746 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:16:51,744 | server.py:125 | fit progress: (7, 1.6069884300231934, {'accuracy': 0.8554, 'data_size': 10000}, 81.65723745099967)
INFO flwr 2024-04-30 17:16:51,745 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:16:51,745 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:17:00,859 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:17:01,864 | server.py:125 | fit progress: (8, 1.597864031791687, {'accuracy': 0.8631, 'data_size': 10000}, 91.77672575600445)
INFO flwr 2024-04-30 17:17:01,864 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:17:01,864 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:17:10,803 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:17:11,797 | server.py:125 | fit progress: (9, 1.5863783359527588, {'accuracy': 0.8757, 'data_size': 10000}, 101.71010330400895)
INFO flwr 2024-04-30 17:17:11,797 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:17:11,798 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:17:20,674 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:17:21,872 | server.py:125 | fit progress: (10, 1.5866062641143799, {'accuracy': 0.874, 'data_size': 10000}, 111.78513850801392)
INFO flwr 2024-04-30 17:17:21,872 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:17:21,873 | server.py:153 | FL finished in 111.78549713001121
INFO flwr 2024-04-30 17:17:21,873 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:17:21,873 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:17:21,873 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:17:21,873 | app.py:229 | app_fit: losses_centralized [(0, 2.304152250289917), (1, 1.9148750305175781), (2, 1.7685304880142212), (3, 1.7030112743377686), (4, 1.7375408411026), (5, 1.7286404371261597), (6, 1.6218650341033936), (7, 1.6069884300231934), (8, 1.597864031791687), (9, 1.5863783359527588), (10, 1.5866062641143799)]
INFO flwr 2024-04-30 17:17:21,873 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.5761), (2, 0.7087), (3, 0.764), (4, 0.7256), (5, 0.7299), (6, 0.8422), (7, 0.8554), (8, 0.8631), (9, 0.8757), (10, 0.874)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.874
wandb:     loss 1.58661
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_171512-gh4qrj6t
wandb: Find logs at: ./wandb/offline-run-20240430_171512-gh4qrj6t/logs
INFO flwr 2024-04-30 17:17:25,349 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:17:26,038 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=943913)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=943913)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:17:30,193	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:17:30,276	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:17:30,345	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:17:30,346	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip'.
INFO flwr 2024-04-30 17:17:39,966 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 116837295514.0, 'accelerator_type:A40': 1.0, 'object_store_memory': 54358840934.0, 'CPU': 64.0, 'node:10.20.240.20': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-30 17:17:39,966 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:17:39,966 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:17:39,984 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:17:39,985 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:17:39,986 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:17:39,986 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:17:42,096 | server.py:94 | initial parameters (loss, other metrics): 2.3035645484924316, {'accuracy': 0.0768, 'data_size': 10000}
INFO flwr 2024-04-30 17:17:42,096 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:17:42,097 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=947023)[0m 2024-04-30 17:17:46.573769: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=947023)[0m 2024-04-30 17:17:46.667368: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=947023)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=947023)[0m 2024-04-30 17:17:48.860877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=947019)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=947019)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=947019)[0m 2024-04-30 17:17:46.749378: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=947019)[0m 2024-04-30 17:17:46.841827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=947019)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=947019)[0m 2024-04-30 17:17:49.044909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:18:05,214 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:18:06,422 | server.py:125 | fit progress: (1, 1.8622902631759644, {'accuracy': 0.6667, 'data_size': 10000}, 24.32521829500911)
INFO flwr 2024-04-30 17:18:06,422 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:18:06,422 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:18:15,967 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:18:17,170 | server.py:125 | fit progress: (2, 1.6687898635864258, {'accuracy': 0.8134, 'data_size': 10000}, 35.073912324005505)
INFO flwr 2024-04-30 17:18:17,171 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:18:17,171 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:18:25,770 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:18:27,047 | server.py:125 | fit progress: (3, 1.6070761680603027, {'accuracy': 0.8652, 'data_size': 10000}, 44.950605304999044)
INFO flwr 2024-04-30 17:18:27,047 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:18:27,048 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:18:36,299 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:18:37,299 | server.py:125 | fit progress: (4, 1.6091793775558472, {'accuracy': 0.8559, 'data_size': 10000}, 55.2021821810049)
INFO flwr 2024-04-30 17:18:37,299 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:18:37,299 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:18:46,159 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:18:47,151 | server.py:125 | fit progress: (5, 1.576001763343811, {'accuracy': 0.8881, 'data_size': 10000}, 65.05403024100815)
INFO flwr 2024-04-30 17:18:47,151 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:18:47,151 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:18:55,719 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:18:56,713 | server.py:125 | fit progress: (6, 1.5752437114715576, {'accuracy': 0.8867, 'data_size': 10000}, 74.61639653200109)
INFO flwr 2024-04-30 17:18:56,713 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:18:56,714 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:19:05,636 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:19:06,860 | server.py:125 | fit progress: (7, 1.5666403770446777, {'accuracy': 0.8959, 'data_size': 10000}, 84.76370563700038)
INFO flwr 2024-04-30 17:19:06,860 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:19:06,861 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:19:15,721 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:19:16,947 | server.py:125 | fit progress: (8, 1.5969516038894653, {'accuracy': 0.8655, 'data_size': 10000}, 94.85062243900029)
INFO flwr 2024-04-30 17:19:16,947 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:19:16,948 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:19:25,665 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:19:26,871 | server.py:125 | fit progress: (9, 1.5677231550216675, {'accuracy': 0.8939, 'data_size': 10000}, 104.77399456501007)
INFO flwr 2024-04-30 17:19:26,871 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:19:26,871 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:19:35,903 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:19:36,908 | server.py:125 | fit progress: (10, 1.555010199546814, {'accuracy': 0.9058, 'data_size': 10000}, 114.81157652499678)
INFO flwr 2024-04-30 17:19:36,908 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:19:36,909 | server.py:153 | FL finished in 114.81203110399656
INFO flwr 2024-04-30 17:19:36,909 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:19:36,909 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:19:36,909 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:19:36,909 | app.py:229 | app_fit: losses_centralized [(0, 2.3035645484924316), (1, 1.8622902631759644), (2, 1.6687898635864258), (3, 1.6070761680603027), (4, 1.6091793775558472), (5, 1.576001763343811), (6, 1.5752437114715576), (7, 1.5666403770446777), (8, 1.5969516038894653), (9, 1.5677231550216675), (10, 1.555010199546814)]
INFO flwr 2024-04-30 17:19:36,909 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0768), (1, 0.6667), (2, 0.8134), (3, 0.8652), (4, 0.8559), (5, 0.8881), (6, 0.8867), (7, 0.8959), (8, 0.8655), (9, 0.8939), (10, 0.9058)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9058
wandb:     loss 1.55501
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_171725-2qt9st2l
wandb: Find logs at: ./wandb/offline-run-20240430_171725-2qt9st2l/logs
INFO flwr 2024-04-30 17:19:40,395 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 2
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:19:41,030 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=947020)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=947020)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:19:45,347	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:19:45,430	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:19:45,500	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:19:45,501	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip'.
INFO flwr 2024-04-30 17:20:02,206 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 54691282944.0, 'accelerator_type:A40': 1.0, 'node:10.20.240.20': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 117612993536.0}
INFO flwr 2024-04-30 17:20:02,207 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:20:02,207 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:20:02,219 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:20:02,221 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:20:02,221 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:20:02,221 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:20:04,649 | server.py:94 | initial parameters (loss, other metrics): 2.304069757461548, {'accuracy': 0.0882, 'data_size': 10000}
INFO flwr 2024-04-30 17:20:04,650 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:20:04,650 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=950715)[0m 2024-04-30 17:20:07.620142: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=950715)[0m 2024-04-30 17:20:07.714345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=950715)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=950715)[0m 2024-04-30 17:20:09.591984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=950715)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=950715)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=950713)[0m 2024-04-30 17:20:07.799433: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=950713)[0m 2024-04-30 17:20:07.890051: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=950713)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=950707)[0m 2024-04-30 17:20:09.740979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:20:25,549 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:20:26,756 | server.py:125 | fit progress: (1, 1.9621695280075073, {'accuracy': 0.5061, 'data_size': 10000}, 22.10580751299858)
INFO flwr 2024-04-30 17:20:26,756 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:20:26,756 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:20:36,355 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:20:37,573 | server.py:125 | fit progress: (2, 1.6873443126678467, {'accuracy': 0.7886, 'data_size': 10000}, 32.92344509100076)
INFO flwr 2024-04-30 17:20:37,574 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:20:37,574 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:20:46,640 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:20:47,849 | server.py:125 | fit progress: (3, 1.5881364345550537, {'accuracy': 0.8833, 'data_size': 10000}, 43.199361275997944)
INFO flwr 2024-04-30 17:20:47,849 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:20:47,850 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:20:56,767 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:20:57,771 | server.py:125 | fit progress: (4, 1.5844272375106812, {'accuracy': 0.8815, 'data_size': 10000}, 53.12099155699252)
INFO flwr 2024-04-30 17:20:57,771 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:20:57,771 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:21:06,848 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:21:08,046 | server.py:125 | fit progress: (5, 1.5973820686340332, {'accuracy': 0.8649, 'data_size': 10000}, 63.396406845989986)
INFO flwr 2024-04-30 17:21:08,047 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:21:08,047 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:21:17,464 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:21:18,666 | server.py:125 | fit progress: (6, 1.5785940885543823, {'accuracy': 0.8839, 'data_size': 10000}, 74.01647668899386)
INFO flwr 2024-04-30 17:21:18,667 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:21:18,667 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:21:27,479 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:21:28,745 | server.py:125 | fit progress: (7, 1.5725992918014526, {'accuracy': 0.8897, 'data_size': 10000}, 84.09521588899952)
INFO flwr 2024-04-30 17:21:28,745 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:21:28,746 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:21:37,479 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:21:38,711 | server.py:125 | fit progress: (8, 1.5629640817642212, {'accuracy': 0.8993, 'data_size': 10000}, 94.06109242599632)
INFO flwr 2024-04-30 17:21:38,711 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:21:38,711 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:21:47,803 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:21:49,014 | server.py:125 | fit progress: (9, 1.553744912147522, {'accuracy': 0.9083, 'data_size': 10000}, 104.36392698099371)
INFO flwr 2024-04-30 17:21:49,014 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:21:49,014 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:21:58,115 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:21:59,327 | server.py:125 | fit progress: (10, 1.574099063873291, {'accuracy': 0.8873, 'data_size': 10000}, 114.67754952100222)
INFO flwr 2024-04-30 17:21:59,328 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:21:59,328 | server.py:153 | FL finished in 114.67788330599433
INFO flwr 2024-04-30 17:21:59,328 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:21:59,328 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:21:59,328 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:21:59,328 | app.py:229 | app_fit: losses_centralized [(0, 2.304069757461548), (1, 1.9621695280075073), (2, 1.6873443126678467), (3, 1.5881364345550537), (4, 1.5844272375106812), (5, 1.5973820686340332), (6, 1.5785940885543823), (7, 1.5725992918014526), (8, 1.5629640817642212), (9, 1.553744912147522), (10, 1.574099063873291)]
INFO flwr 2024-04-30 17:21:59,328 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0882), (1, 0.5061), (2, 0.7886), (3, 0.8833), (4, 0.8815), (5, 0.8649), (6, 0.8839), (7, 0.8897), (8, 0.8993), (9, 0.9083), (10, 0.8873)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8873
wandb:     loss 1.5741
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_171940-fmv24hyo
wandb: Find logs at: ./wandb/offline-run-20240430_171940-fmv24hyo/logs
INFO flwr 2024-04-30 17:22:02,801 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:22:03,540 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=950710)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=950710)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:22:07,892	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:22:08,005	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:22:08,087	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:22:08,088	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip'.
INFO flwr 2024-04-30 17:22:17,901 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 53415634944.0, 'GPU': 1.0, 'accelerator_type:A40': 1.0, 'memory': 114636481536.0, 'node:__internal_head__': 1.0, 'node:10.20.240.20': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-30 17:22:17,901 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:22:17,902 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:22:17,914 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:22:17,914 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:22:17,915 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:22:17,915 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:22:19,873 | server.py:94 | initial parameters (loss, other metrics): 2.299149513244629, {'accuracy': 0.1618, 'data_size': 10000}
INFO flwr 2024-04-30 17:22:19,880 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:22:19,881 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=953805)[0m 2024-04-30 17:22:23.251433: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=953806)[0m 2024-04-30 17:22:23.282725: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=953806)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=953805)[0m 2024-04-30 17:22:25.170453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=953803)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=953803)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=953804)[0m 2024-04-30 17:22:23.300934: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=953802)[0m 2024-04-30 17:22:23.423517: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=953802)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=953804)[0m 2024-04-30 17:22:25.303150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:22:46,418 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:22:47,400 | server.py:125 | fit progress: (1, 1.902317762374878, {'accuracy': 0.6091, 'data_size': 10000}, 27.51990760699846)
INFO flwr 2024-04-30 17:22:47,401 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:22:47,401 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:23:01,763 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:23:03,039 | server.py:125 | fit progress: (2, 1.6637824773788452, {'accuracy': 0.8232, 'data_size': 10000}, 43.1582724339969)
INFO flwr 2024-04-30 17:23:03,039 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:23:03,039 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:23:14,827 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:23:16,047 | server.py:125 | fit progress: (3, 1.6788042783737183, {'accuracy': 0.7902, 'data_size': 10000}, 56.16627412500384)
INFO flwr 2024-04-30 17:23:16,047 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:23:16,047 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:23:27,360 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:23:28,607 | server.py:125 | fit progress: (4, 1.6219561100006104, {'accuracy': 0.8449, 'data_size': 10000}, 68.72664158399857)
INFO flwr 2024-04-30 17:23:28,607 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:23:28,607 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:23:40,730 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:23:41,957 | server.py:125 | fit progress: (5, 1.5821611881256104, {'accuracy': 0.8816, 'data_size': 10000}, 82.07622399099637)
INFO flwr 2024-04-30 17:23:41,957 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:23:41,957 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:23:52,597 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:23:53,821 | server.py:125 | fit progress: (6, 1.5914446115493774, {'accuracy': 0.8727, 'data_size': 10000}, 93.9402547589998)
INFO flwr 2024-04-30 17:23:53,821 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:23:53,821 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:24:04,900 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:24:05,900 | server.py:125 | fit progress: (7, 1.5953645706176758, {'accuracy': 0.8675, 'data_size': 10000}, 106.0196247260028)
INFO flwr 2024-04-30 17:24:05,900 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:24:05,900 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:24:16,721 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:24:17,945 | server.py:125 | fit progress: (8, 1.5683850049972534, {'accuracy': 0.8952, 'data_size': 10000}, 118.06417407499976)
INFO flwr 2024-04-30 17:24:17,945 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:24:17,945 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:24:30,445 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:24:31,642 | server.py:125 | fit progress: (9, 1.567171573638916, {'accuracy': 0.895, 'data_size': 10000}, 131.76193312299438)
INFO flwr 2024-04-30 17:24:31,643 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:24:31,643 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:24:43,564 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:24:44,786 | server.py:125 | fit progress: (10, 1.56936776638031, {'accuracy': 0.8921, 'data_size': 10000}, 144.9059095769917)
INFO flwr 2024-04-30 17:24:44,787 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:24:44,787 | server.py:153 | FL finished in 144.90628760200343
INFO flwr 2024-04-30 17:24:44,787 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:24:44,787 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:24:44,787 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:24:44,787 | app.py:229 | app_fit: losses_centralized [(0, 2.299149513244629), (1, 1.902317762374878), (2, 1.6637824773788452), (3, 1.6788042783737183), (4, 1.6219561100006104), (5, 1.5821611881256104), (6, 1.5914446115493774), (7, 1.5953645706176758), (8, 1.5683850049972534), (9, 1.567171573638916), (10, 1.56936776638031)]
INFO flwr 2024-04-30 17:24:44,787 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1618), (1, 0.6091), (2, 0.8232), (3, 0.7902), (4, 0.8449), (5, 0.8816), (6, 0.8727), (7, 0.8675), (8, 0.8952), (9, 0.895), (10, 0.8921)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8921
wandb:     loss 1.56937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_172203-ghzonimr
wandb: Find logs at: ./wandb/offline-run-20240430_172203-ghzonimr/logs
INFO flwr 2024-04-30 17:24:48,302 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:24:48,989 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=953800)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=953800)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:24:54,128	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:24:54,225	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:24:54,327	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:24:54,328	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d2bf7a99cede715.zip'.
INFO flwr 2024-04-30 17:25:04,027 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:A40': 1.0, 'object_store_memory': 54915221913.0, 'CPU': 64.0, 'node:10.20.240.20': 1.0, 'memory': 118135517799.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-30 17:25:04,027 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:25:04,027 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:25:04,039 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:25:04,041 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:25:04,041 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:25:04,041 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:25:06,633 | server.py:94 | initial parameters (loss, other metrics): 2.3048603534698486, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-30 17:25:06,634 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:25:06,634 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=957471)[0m 2024-04-30 17:25:09.374082: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=957471)[0m 2024-04-30 17:25:09.471318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=957471)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=957468)[0m 2024-04-30 17:25:11.379050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=957473)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=957473)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=957473)[0m 2024-04-30 17:25:09.519432: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=957474)[0m 2024-04-30 17:25:09.634835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=957474)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=957473)[0m 2024-04-30 17:25:11.483994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:25:32,167 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:25:33,166 | server.py:125 | fit progress: (1, 1.8883929252624512, {'accuracy': 0.6391, 'data_size': 10000}, 26.531964647001587)
INFO flwr 2024-04-30 17:25:33,166 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:25:33,166 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:25:44,862 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:25:45,855 | server.py:125 | fit progress: (2, 1.6319048404693604, {'accuracy': 0.856, 'data_size': 10000}, 39.22102449099475)
INFO flwr 2024-04-30 17:25:45,855 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:25:45,855 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:25:56,488 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:25:57,485 | server.py:125 | fit progress: (3, 1.58745539188385, {'accuracy': 0.8849, 'data_size': 10000}, 50.851381337997736)
INFO flwr 2024-04-30 17:25:57,485 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:25:57,486 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:26:08,983 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:26:10,258 | server.py:125 | fit progress: (4, 1.5881602764129639, {'accuracy': 0.8792, 'data_size': 10000}, 63.62399173399899)
INFO flwr 2024-04-30 17:26:10,258 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:26:10,258 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:26:21,758 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:26:22,971 | server.py:125 | fit progress: (5, 1.5678578615188599, {'accuracy': 0.8971, 'data_size': 10000}, 76.336827755993)
INFO flwr 2024-04-30 17:26:22,971 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:26:22,971 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:26:34,595 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:26:35,823 | server.py:125 | fit progress: (6, 1.5615932941436768, {'accuracy': 0.9029, 'data_size': 10000}, 89.18872940300207)
INFO flwr 2024-04-30 17:26:35,823 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:26:35,823 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:26:47,025 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:26:48,035 | server.py:125 | fit progress: (7, 1.5552122592926025, {'accuracy': 0.9073, 'data_size': 10000}, 101.40082034999796)
INFO flwr 2024-04-30 17:26:48,035 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:26:48,035 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:27:00,175 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:27:01,387 | server.py:125 | fit progress: (8, 1.5547003746032715, {'accuracy': 0.9069, 'data_size': 10000}, 114.75363115499204)
INFO flwr 2024-04-30 17:27:01,388 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:27:01,388 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:27:13,032 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:27:14,278 | server.py:125 | fit progress: (9, 1.5501608848571777, {'accuracy': 0.9126, 'data_size': 10000}, 127.6438532799948)
INFO flwr 2024-04-30 17:27:14,278 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:27:14,278 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:27:26,204 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:27:27,427 | server.py:125 | fit progress: (10, 1.5485140085220337, {'accuracy': 0.9144, 'data_size': 10000}, 140.7934234639979)
INFO flwr 2024-04-30 17:27:27,427 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:27:27,428 | server.py:153 | FL finished in 140.7937707079982
INFO flwr 2024-04-30 17:27:27,428 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:27:27,428 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:27:27,428 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:27:27,428 | app.py:229 | app_fit: losses_centralized [(0, 2.3048603534698486), (1, 1.8883929252624512), (2, 1.6319048404693604), (3, 1.58745539188385), (4, 1.5881602764129639), (5, 1.5678578615188599), (6, 1.5615932941436768), (7, 1.5552122592926025), (8, 1.5547003746032715), (9, 1.5501608848571777), (10, 1.5485140085220337)]
INFO flwr 2024-04-30 17:27:27,428 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.6391), (2, 0.856), (3, 0.8849), (4, 0.8792), (5, 0.8971), (6, 0.9029), (7, 0.9073), (8, 0.9069), (9, 0.9126), (10, 0.9144)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9144
wandb:     loss 1.54851
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_172448-ui1htqmb
wandb: Find logs at: ./wandb/offline-run-20240430_172448-ui1htqmb/logs
INFO flwr 2024-04-30 17:27:30,913 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 4
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:27:31,561 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=957468)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=957468)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:27:35,884	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:27:35,994	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:27:36,096	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:27:36,097	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 17:27:46,080 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 53655845683.0, 'CPU': 64.0, 'node:10.20.240.20': 1.0, 'accelerator_type:A40': 1.0, 'node:__internal_head__': 1.0, 'memory': 115196973261.0, 'GPU': 1.0}
INFO flwr 2024-04-30 17:27:46,080 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:27:46,080 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:27:46,095 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:27:46,096 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:27:46,096 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:27:46,097 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:27:48,904 | server.py:94 | initial parameters (loss, other metrics): 2.3016185760498047, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-30 17:27:48,905 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:27:48,905 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=960552)[0m 2024-04-30 17:27:51.510734: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=960552)[0m 2024-04-30 17:27:51.604602: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=960552)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=960553)[0m 2024-04-30 17:27:53.573005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=960559)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=960559)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=960556)[0m 2024-04-30 17:27:51.585585: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=960556)[0m 2024-04-30 17:27:51.687684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=960556)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=960555)[0m 2024-04-30 17:27:53.600726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:28:13,354 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:28:14,340 | server.py:125 | fit progress: (1, 1.913339614868164, {'accuracy': 0.6081, 'data_size': 10000}, 25.435017882002285)
INFO flwr 2024-04-30 17:28:14,340 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:28:14,340 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:28:26,879 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:28:27,907 | server.py:125 | fit progress: (2, 1.7129870653152466, {'accuracy': 0.761, 'data_size': 10000}, 39.00204132801446)
INFO flwr 2024-04-30 17:28:27,907 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:28:27,907 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:28:40,048 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:28:41,048 | server.py:125 | fit progress: (3, 1.631222128868103, {'accuracy': 0.8347, 'data_size': 10000}, 52.143610386003274)
INFO flwr 2024-04-30 17:28:41,049 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:28:41,049 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:28:54,134 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:28:55,334 | server.py:125 | fit progress: (4, 1.6031996011734009, {'accuracy': 0.8638, 'data_size': 10000}, 66.4294999140111)
INFO flwr 2024-04-30 17:28:55,335 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:28:55,335 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:29:07,092 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:29:08,297 | server.py:125 | fit progress: (5, 1.6240276098251343, {'accuracy': 0.8399, 'data_size': 10000}, 79.39195285300957)
INFO flwr 2024-04-30 17:29:08,297 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:29:08,297 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:29:20,290 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:29:21,518 | server.py:125 | fit progress: (6, 1.5759786367416382, {'accuracy': 0.8887, 'data_size': 10000}, 92.61349944901303)
INFO flwr 2024-04-30 17:29:21,519 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:29:21,519 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:29:32,408 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:29:33,399 | server.py:125 | fit progress: (7, 1.5631872415542603, {'accuracy': 0.9, 'data_size': 10000}, 104.49387488300272)
INFO flwr 2024-04-30 17:29:33,399 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:29:33,399 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:29:45,121 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:29:46,326 | server.py:125 | fit progress: (8, 1.5600402355194092, {'accuracy': 0.9022, 'data_size': 10000}, 117.42133986600675)
INFO flwr 2024-04-30 17:29:46,326 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:29:46,327 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:29:57,577 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:29:58,791 | server.py:125 | fit progress: (9, 1.557528018951416, {'accuracy': 0.905, 'data_size': 10000}, 129.886265277004)
INFO flwr 2024-04-30 17:29:58,791 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:29:58,791 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:30:10,064 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:30:11,292 | server.py:125 | fit progress: (10, 1.5569672584533691, {'accuracy': 0.906, 'data_size': 10000}, 142.3876045620127)
INFO flwr 2024-04-30 17:30:11,293 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:30:11,293 | server.py:153 | FL finished in 142.3880051610031
INFO flwr 2024-04-30 17:30:11,293 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:30:11,293 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:30:11,293 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:30:11,293 | app.py:229 | app_fit: losses_centralized [(0, 2.3016185760498047), (1, 1.913339614868164), (2, 1.7129870653152466), (3, 1.631222128868103), (4, 1.6031996011734009), (5, 1.6240276098251343), (6, 1.5759786367416382), (7, 1.5631872415542603), (8, 1.5600402355194092), (9, 1.557528018951416), (10, 1.5569672584533691)]
INFO flwr 2024-04-30 17:30:11,293 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.6081), (2, 0.761), (3, 0.8347), (4, 0.8638), (5, 0.8399), (6, 0.8887), (7, 0.9), (8, 0.9022), (9, 0.905), (10, 0.906)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.906
wandb:     loss 1.55697
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_172731-ikbzdpsj
wandb: Find logs at: ./wandb/offline-run-20240430_172731-ikbzdpsj/logs
INFO flwr 2024-04-30 17:30:14,769 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.01}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:30:15,410 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=960552)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=960552)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:30:20,439	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:30:20,516	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:30:20,608	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:30:20,610	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 17:30:30,957 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:A40': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 117234302362.0, 'CPU': 64.0, 'node:10.20.240.20': 1.0, 'object_store_memory': 54528986726.0}
INFO flwr 2024-04-30 17:30:30,958 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:30:30,958 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:30:30,972 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:30:30,973 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:30:30,973 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:30:30,974 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:30:33,567 | server.py:94 | initial parameters (loss, other metrics): 2.3034725189208984, {'accuracy': 0.077, 'data_size': 10000}
INFO flwr 2024-04-30 17:30:33,567 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:30:33,568 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=964486)[0m 2024-04-30 17:30:36.388779: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=964486)[0m 2024-04-30 17:30:36.511338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=964486)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=964486)[0m 2024-04-30 17:30:38.421023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=964490)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=964490)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=964485)[0m 2024-04-30 17:30:36.598818: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=964485)[0m 2024-04-30 17:30:36.689809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=964485)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=964485)[0m 2024-04-30 17:30:38.539473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:31:06,894 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:31:07,921 | server.py:125 | fit progress: (1, 1.9303932189941406, {'accuracy': 0.5549, 'data_size': 10000}, 34.35359625599813)
INFO flwr 2024-04-30 17:31:07,922 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:31:07,922 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:31:28,953 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:31:29,971 | server.py:125 | fit progress: (2, 1.775592565536499, {'accuracy': 0.7005, 'data_size': 10000}, 56.40272685298987)
INFO flwr 2024-04-30 17:31:29,971 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:31:29,971 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:31:48,168 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:31:49,167 | server.py:125 | fit progress: (3, 1.8155593872070312, {'accuracy': 0.6463, 'data_size': 10000}, 75.59899849699286)
INFO flwr 2024-04-30 17:31:49,167 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:31:49,167 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:32:08,870 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:32:10,103 | server.py:125 | fit progress: (4, 1.7431994676589966, {'accuracy': 0.7225, 'data_size': 10000}, 96.53476992699143)
INFO flwr 2024-04-30 17:32:10,103 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:32:10,103 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:32:29,081 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:32:30,320 | server.py:125 | fit progress: (5, 1.7271229028701782, {'accuracy': 0.7349, 'data_size': 10000}, 116.75183767799172)
INFO flwr 2024-04-30 17:32:30,320 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:32:30,320 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:32:49,725 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:32:50,941 | server.py:125 | fit progress: (6, 1.7226449251174927, {'accuracy': 0.7399, 'data_size': 10000}, 137.37292293499922)
INFO flwr 2024-04-30 17:32:50,941 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:32:50,941 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:33:09,705 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:33:10,718 | server.py:125 | fit progress: (7, 1.716322898864746, {'accuracy': 0.7444, 'data_size': 10000}, 157.14981715400063)
INFO flwr 2024-04-30 17:33:10,718 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:33:10,718 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:33:29,068 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:33:30,320 | server.py:125 | fit progress: (8, 1.723702311515808, {'accuracy': 0.7377, 'data_size': 10000}, 176.7523712569964)
INFO flwr 2024-04-30 17:33:30,320 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:33:30,321 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:33:50,832 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:33:52,109 | server.py:125 | fit progress: (9, 1.7160595655441284, {'accuracy': 0.7445, 'data_size': 10000}, 198.54077696299646)
INFO flwr 2024-04-30 17:33:52,109 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:33:52,109 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:34:10,750 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:34:11,957 | server.py:125 | fit progress: (10, 1.7117646932601929, {'accuracy': 0.7488, 'data_size': 10000}, 218.38923843098746)
INFO flwr 2024-04-30 17:34:11,957 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:34:11,957 | server.py:153 | FL finished in 218.38964036299149
INFO flwr 2024-04-30 17:34:11,958 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:34:11,958 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:34:11,958 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:34:11,958 | app.py:229 | app_fit: losses_centralized [(0, 2.3034725189208984), (1, 1.9303932189941406), (2, 1.775592565536499), (3, 1.8155593872070312), (4, 1.7431994676589966), (5, 1.7271229028701782), (6, 1.7226449251174927), (7, 1.716322898864746), (8, 1.723702311515808), (9, 1.7160595655441284), (10, 1.7117646932601929)]
INFO flwr 2024-04-30 17:34:11,958 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.077), (1, 0.5549), (2, 0.7005), (3, 0.6463), (4, 0.7225), (5, 0.7349), (6, 0.7399), (7, 0.7444), (8, 0.7377), (9, 0.7445), (10, 0.7488)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7488
wandb:     loss 1.71176
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_173015-vo4zejaf
wandb: Find logs at: ./wandb/offline-run-20240430_173015-vo4zejaf/logs
INFO flwr 2024-04-30 17:34:15,448 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:34:16,118 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=964480)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=964480)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:34:20,703	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:34:20,778	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:34:20,848	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:34:20,849	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
INFO flwr 2024-04-30 17:34:30,573 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 53448287846.0, 'GPU': 1.0, 'memory': 114712671642.0, 'accelerator_type:A40': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.20': 1.0}
INFO flwr 2024-04-30 17:34:30,573 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-30 17:34:30,573 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-30 17:34:30,586 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-30 17:34:30,587 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-30 17:34:30,587 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-30 17:34:30,587 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-30 17:34:33,068 | server.py:94 | initial parameters (loss, other metrics): 2.30568265914917, {'accuracy': 0.0528, 'data_size': 10000}
INFO flwr 2024-04-30 17:34:33,069 | server.py:104 | FL starting
DEBUG flwr 2024-04-30 17:34:33,069 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=968689)[0m 2024-04-30 17:34:35.941767: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=968689)[0m 2024-04-30 17:34:36.029740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=968689)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=968691)[0m 2024-04-30 17:34:37.875090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=968689)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=968689)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=968694)[0m 2024-04-30 17:34:36.304177: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=968694)[0m 2024-04-30 17:34:36.396917: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=968694)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=968694)[0m 2024-04-30 17:34:38.291588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-30 17:35:02,924 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-30 17:35:03,932 | server.py:125 | fit progress: (1, 1.882944107055664, {'accuracy': 0.6296, 'data_size': 10000}, 30.863028973006294)
INFO flwr 2024-04-30 17:35:03,932 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-30 17:35:03,933 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:35:22,881 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-30 17:35:23,913 | server.py:125 | fit progress: (2, 1.7302616834640503, {'accuracy': 0.7361, 'data_size': 10000}, 50.843445991995395)
INFO flwr 2024-04-30 17:35:23,913 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-30 17:35:23,913 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:35:43,038 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-30 17:35:44,255 | server.py:125 | fit progress: (3, 1.590396523475647, {'accuracy': 0.8825, 'data_size': 10000}, 71.18547419000242)
INFO flwr 2024-04-30 17:35:44,255 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-30 17:35:44,255 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:36:03,774 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-30 17:36:04,796 | server.py:125 | fit progress: (4, 1.5810019969940186, {'accuracy': 0.885, 'data_size': 10000}, 91.72705635000602)
INFO flwr 2024-04-30 17:36:04,796 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-30 17:36:04,797 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:36:23,037 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-30 17:36:24,047 | server.py:125 | fit progress: (5, 1.5641460418701172, {'accuracy': 0.901, 'data_size': 10000}, 110.9774647019949)
INFO flwr 2024-04-30 17:36:24,047 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-30 17:36:24,047 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:36:41,266 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-30 17:36:42,273 | server.py:125 | fit progress: (6, 1.5647306442260742, {'accuracy': 0.8993, 'data_size': 10000}, 129.2033637449931)
INFO flwr 2024-04-30 17:36:42,273 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-30 17:36:42,273 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:37:01,596 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-30 17:37:02,817 | server.py:125 | fit progress: (7, 1.568623661994934, {'accuracy': 0.8939, 'data_size': 10000}, 149.7481838560052)
INFO flwr 2024-04-30 17:37:02,818 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-30 17:37:02,818 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:37:21,431 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-30 17:37:22,643 | server.py:125 | fit progress: (8, 1.5589723587036133, {'accuracy': 0.9026, 'data_size': 10000}, 169.574248171004)
INFO flwr 2024-04-30 17:37:22,644 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-30 17:37:22,644 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:37:41,376 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-30 17:37:42,600 | server.py:125 | fit progress: (9, 1.5550044775009155, {'accuracy': 0.9079, 'data_size': 10000}, 189.53051351899921)
INFO flwr 2024-04-30 17:37:42,600 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-30 17:37:42,600 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-30 17:38:01,717 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-30 17:38:02,729 | server.py:125 | fit progress: (10, 1.5571200847625732, {'accuracy': 0.9054, 'data_size': 10000}, 209.6602932639944)
INFO flwr 2024-04-30 17:38:02,730 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-30 17:38:02,730 | server.py:153 | FL finished in 209.6607061179966
INFO flwr 2024-04-30 17:38:02,730 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-30 17:38:02,730 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-30 17:38:02,730 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-30 17:38:02,730 | app.py:229 | app_fit: losses_centralized [(0, 2.30568265914917), (1, 1.882944107055664), (2, 1.7302616834640503), (3, 1.590396523475647), (4, 1.5810019969940186), (5, 1.5641460418701172), (6, 1.5647306442260742), (7, 1.568623661994934), (8, 1.5589723587036133), (9, 1.5550044775009155), (10, 1.5571200847625732)]
INFO flwr 2024-04-30 17:38:02,730 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0528), (1, 0.6296), (2, 0.7361), (3, 0.8825), (4, 0.885), (5, 0.901), (6, 0.8993), (7, 0.8939), (8, 0.9026), (9, 0.9079), (10, 0.9054)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9054
wandb:     loss 1.55712
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240430_173415-4128tewe
wandb: Find logs at: ./wandb/offline-run-20240430_173415-4128tewe/logs
INFO flwr 2024-04-30 17:38:06,237 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 8
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-30 17:38:09,695 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=968693)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=968693)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-30 17:38:24,081	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-30 17:38:24,862	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-30 17:38:25,445	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip' (0.20MiB) to Ray cluster...
2024-04-30 17:38:25,447	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_687c1ab3c62d2abc.zip'.
[2024-04-30 17:38:54,188 E 931950 931950] core_worker.cc:201: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory
srun: error: ctit090: task 0: Exited with exit code 1
