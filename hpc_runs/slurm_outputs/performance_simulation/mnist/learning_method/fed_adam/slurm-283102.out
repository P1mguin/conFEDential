ctit082
2024-04-29 10:36:28.559372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-29 10:36:50.708843: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-29 10:38:52,146 | batch_run_simulation.py:80 | Loaded 6 configs with name MINST-CNN-FEDADAM, running...
INFO flwr 2024-04-29 10:38:52,147 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 10:46:24,848 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-29 10:46:27,856	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 10:46:28,748	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 10:46:29,099	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 10:46:29,101	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 10:46:40,157 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170819443303.0, 'object_store_memory': 77494047129.0}
INFO flwr 2024-04-29 10:46:40,157 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 10:46:40,158 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 10:46:40,174 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 10:46:40,175 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 10:46:40,175 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 10:46:40,175 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=118392)[0m 2024-04-29 10:46:46.427685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=118392)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=118392)[0m 2024-04-29 10:46:48.805718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-29 10:46:52,750 | server.py:94 | initial parameters (loss, other metrics): 2.302238702774048, {'accuracy': 0.1222, 'data_size': 10000}
INFO flwr 2024-04-29 10:46:52,750 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 10:46:52,750 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=118404)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=118404)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=118402)[0m 2024-04-29 10:46:46.628886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=118402)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=118389)[0m 2024-04-29 10:46:48.926143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=118395)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=118395)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-29 10:47:16,524 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 10:47:21,224 | server.py:125 | fit progress: (1, 1.8856841325759888, {'accuracy': 0.5657, 'data_size': 10000}, 28.47399379000126)
INFO flwr 2024-04-29 10:47:21,225 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 10:47:21,225 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:47:39,439 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 10:47:44,062 | server.py:125 | fit progress: (2, 1.544062852859497, {'accuracy': 0.9212, 'data_size': 10000}, 51.31208842700289)
INFO flwr 2024-04-29 10:47:44,063 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 10:47:44,063 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:47:59,725 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 10:48:04,396 | server.py:125 | fit progress: (3, 1.514958143234253, {'accuracy': 0.9457, 'data_size': 10000}, 71.64531100999739)
INFO flwr 2024-04-29 10:48:04,396 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 10:48:04,396 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:48:21,064 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 10:48:25,754 | server.py:125 | fit progress: (4, 1.4984248876571655, {'accuracy': 0.9629, 'data_size': 10000}, 93.00369872499869)
INFO flwr 2024-04-29 10:48:25,754 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 10:48:25,754 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:48:43,733 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 10:48:48,137 | server.py:125 | fit progress: (5, 1.4916508197784424, {'accuracy': 0.9696, 'data_size': 10000}, 115.38642830299796)
INFO flwr 2024-04-29 10:48:48,137 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 10:48:48,137 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:49:04,449 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 10:49:08,869 | server.py:125 | fit progress: (6, 1.4894713163375854, {'accuracy': 0.9717, 'data_size': 10000}, 136.11861340599717)
INFO flwr 2024-04-29 10:49:08,869 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 10:49:08,869 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:49:25,008 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 10:49:29,641 | server.py:125 | fit progress: (7, 1.4886022806167603, {'accuracy': 0.9722, 'data_size': 10000}, 156.89084055700368)
INFO flwr 2024-04-29 10:49:29,641 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 10:49:29,642 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:49:52,322 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 10:49:56,965 | server.py:125 | fit progress: (8, 1.4882680177688599, {'accuracy': 0.9726, 'data_size': 10000}, 184.21492086200305)
INFO flwr 2024-04-29 10:49:56,966 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 10:49:56,966 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:50:14,685 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 10:50:19,309 | server.py:125 | fit progress: (9, 1.4915664196014404, {'accuracy': 0.9695, 'data_size': 10000}, 206.5589634459975)
INFO flwr 2024-04-29 10:50:19,310 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 10:50:19,310 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:50:36,585 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 10:50:41,236 | server.py:125 | fit progress: (10, 1.4898687601089478, {'accuracy': 0.9713, 'data_size': 10000}, 228.48586952400365)
INFO flwr 2024-04-29 10:50:41,236 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 10:50:41,237 | server.py:153 | FL finished in 228.48630524000328
INFO flwr 2024-04-29 10:50:41,244 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 10:50:41,244 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 10:50:41,244 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 10:50:41,245 | app.py:229 | app_fit: losses_centralized [(0, 2.302238702774048), (1, 1.8856841325759888), (2, 1.544062852859497), (3, 1.514958143234253), (4, 1.4984248876571655), (5, 1.4916508197784424), (6, 1.4894713163375854), (7, 1.4886022806167603), (8, 1.4882680177688599), (9, 1.4915664196014404), (10, 1.4898687601089478)]
INFO flwr 2024-04-29 10:50:41,245 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1222), (1, 0.5657), (2, 0.9212), (3, 0.9457), (4, 0.9629), (5, 0.9696), (6, 0.9717), (7, 0.9722), (8, 0.9726), (9, 0.9695), (10, 0.9713)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9713
wandb:     loss 1.48987
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_104624-ssyxvbbx
wandb: Find logs at: ./wandb/offline-run-20240429_104624-ssyxvbbx/logs
INFO flwr 2024-04-29 10:50:44,889 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 10:50:46,099 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=118389)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=118389)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-29 10:50:57,431	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 10:50:58,479	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 10:50:58,839	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 10:50:58,841	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 10:51:09,898 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:TITAN': 1.0, 'memory': 169029762868.0, 'object_store_memory': 76727041228.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'GPU': 1.0}
INFO flwr 2024-04-29 10:51:09,899 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 10:51:09,899 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 10:51:09,914 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 10:51:09,915 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 10:51:09,915 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 10:51:09,915 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-29 10:51:17,525 | server.py:94 | initial parameters (loss, other metrics): 2.3024935722351074, {'accuracy': 0.1005, 'data_size': 10000}
INFO flwr 2024-04-29 10:51:17,525 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 10:51:17,525 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=123072)[0m 2024-04-29 10:51:18.240688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=123072)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=123072)[0m 2024-04-29 10:51:31.673494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=123070)[0m 2024-04-29 10:51:18.418792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=123070)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=123072)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=123072)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=123085)[0m 2024-04-29 10:51:31.673500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-29 10:52:30,778 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 10:52:35,429 | server.py:125 | fit progress: (1, 1.7877026796340942, {'accuracy': 0.7987, 'data_size': 10000}, 77.9035050129969)
INFO flwr 2024-04-29 10:52:35,429 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 10:52:35,429 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:52:53,204 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 10:52:57,826 | server.py:125 | fit progress: (2, 1.6110414266586304, {'accuracy': 0.8507, 'data_size': 10000}, 100.30102706999605)
INFO flwr 2024-04-29 10:52:57,827 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 10:52:57,827 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:53:12,852 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 10:53:17,451 | server.py:125 | fit progress: (3, 1.5098787546157837, {'accuracy': 0.9517, 'data_size': 10000}, 119.92609371799335)
INFO flwr 2024-04-29 10:53:17,451 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 10:53:17,452 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:53:34,463 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 10:53:38,821 | server.py:125 | fit progress: (4, 1.497300148010254, {'accuracy': 0.9642, 'data_size': 10000}, 141.29632585099898)
INFO flwr 2024-04-29 10:53:38,822 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 10:53:38,822 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:53:53,990 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 10:53:58,377 | server.py:125 | fit progress: (5, 1.4912023544311523, {'accuracy': 0.9701, 'data_size': 10000}, 160.851425421999)
INFO flwr 2024-04-29 10:53:58,377 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 10:53:58,377 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:54:14,102 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 10:54:18,708 | server.py:125 | fit progress: (6, 1.4883008003234863, {'accuracy': 0.9729, 'data_size': 10000}, 181.18311883899878)
INFO flwr 2024-04-29 10:54:18,708 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 10:54:18,709 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:54:35,643 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 10:54:40,209 | server.py:125 | fit progress: (7, 1.4897717237472534, {'accuracy': 0.9713, 'data_size': 10000}, 202.6838103859991)
INFO flwr 2024-04-29 10:54:40,210 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 10:54:40,210 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:54:57,246 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 10:55:01,855 | server.py:125 | fit progress: (8, 1.4883928298950195, {'accuracy': 0.9728, 'data_size': 10000}, 224.329884767998)
INFO flwr 2024-04-29 10:55:01,855 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 10:55:01,856 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:55:19,543 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 10:55:24,141 | server.py:125 | fit progress: (9, 1.4882327318191528, {'accuracy': 0.973, 'data_size': 10000}, 246.6161022919987)
INFO flwr 2024-04-29 10:55:24,142 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 10:55:24,142 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:55:39,566 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 10:55:44,201 | server.py:125 | fit progress: (10, 1.4900925159454346, {'accuracy': 0.971, 'data_size': 10000}, 266.6761722689989)
INFO flwr 2024-04-29 10:55:44,202 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 10:55:44,202 | server.py:153 | FL finished in 266.67661447899445
INFO flwr 2024-04-29 10:55:44,209 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 10:55:44,209 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 10:55:44,209 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 10:55:44,209 | app.py:229 | app_fit: losses_centralized [(0, 2.3024935722351074), (1, 1.7877026796340942), (2, 1.6110414266586304), (3, 1.5098787546157837), (4, 1.497300148010254), (5, 1.4912023544311523), (6, 1.4883008003234863), (7, 1.4897717237472534), (8, 1.4883928298950195), (9, 1.4882327318191528), (10, 1.4900925159454346)]
INFO flwr 2024-04-29 10:55:44,209 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1005), (1, 0.7987), (2, 0.8507), (3, 0.9517), (4, 0.9642), (5, 0.9701), (6, 0.9729), (7, 0.9713), (8, 0.9728), (9, 0.973), (10, 0.971)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.971
wandb:     loss 1.49009
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_105045-tenddxn8
wandb: Find logs at: ./wandb/offline-run-20240429_105045-tenddxn8/logs
INFO flwr 2024-04-29 10:55:47,840 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 4
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 10:55:48,878 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=123085)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=123085)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-29 10:55:55,382	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 10:55:56,242	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 10:55:56,609	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 10:55:56,611	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 10:56:07,724 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77460771225.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'memory': 170741799527.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-29 10:56:07,724 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 10:56:07,724 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 10:56:07,739 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 10:56:07,740 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 10:56:07,740 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 10:56:07,740 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=127273)[0m 2024-04-29 10:56:14.188704: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=127273)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-29 10:56:16,696 | server.py:94 | initial parameters (loss, other metrics): 2.3026363849639893, {'accuracy': 0.096, 'data_size': 10000}
INFO flwr 2024-04-29 10:56:16,697 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 10:56:16,697 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=127273)[0m 2024-04-29 10:56:16.841886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=127280)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=127280)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=127271)[0m 2024-04-29 10:56:14.385055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=127271)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=127271)[0m 2024-04-29 10:56:16.841886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-29 10:56:47,792 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 10:56:52,550 | server.py:125 | fit progress: (1, 2.0597760677337646, {'accuracy': 0.7108, 'data_size': 10000}, 35.85357657499844)
INFO flwr 2024-04-29 10:56:52,551 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 10:56:52,551 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:57:10,252 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 10:57:14,695 | server.py:125 | fit progress: (2, 1.6151460409164429, {'accuracy': 0.9172, 'data_size': 10000}, 57.99841873299738)
INFO flwr 2024-04-29 10:57:14,696 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 10:57:14,696 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:57:31,675 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 10:57:36,311 | server.py:125 | fit progress: (3, 1.516568660736084, {'accuracy': 0.9476, 'data_size': 10000}, 79.61392407099629)
INFO flwr 2024-04-29 10:57:36,311 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 10:57:36,311 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:57:51,824 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 10:57:56,482 | server.py:125 | fit progress: (4, 1.5009626150131226, {'accuracy': 0.96, 'data_size': 10000}, 99.78495389899763)
INFO flwr 2024-04-29 10:57:56,482 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 10:57:56,482 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:58:12,100 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 10:58:16,798 | server.py:125 | fit progress: (5, 1.4944603443145752, {'accuracy': 0.9668, 'data_size': 10000}, 120.10139683099987)
INFO flwr 2024-04-29 10:58:16,798 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 10:58:16,799 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:58:33,691 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 10:58:38,173 | server.py:125 | fit progress: (6, 1.4980030059814453, {'accuracy': 0.9626, 'data_size': 10000}, 141.47673442999803)
INFO flwr 2024-04-29 10:58:38,174 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 10:58:38,174 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:58:56,812 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 10:59:01,297 | server.py:125 | fit progress: (7, 1.5017887353897095, {'accuracy': 0.9593, 'data_size': 10000}, 164.60048623899638)
INFO flwr 2024-04-29 10:59:01,298 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 10:59:01,298 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:59:19,167 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 10:59:23,585 | server.py:125 | fit progress: (8, 1.5024374723434448, {'accuracy': 0.9588, 'data_size': 10000}, 186.8884739119967)
INFO flwr 2024-04-29 10:59:23,586 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 10:59:23,586 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 10:59:39,846 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 10:59:44,472 | server.py:125 | fit progress: (9, 1.502370834350586, {'accuracy': 0.9589, 'data_size': 10000}, 207.77499042300042)
INFO flwr 2024-04-29 10:59:44,472 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 10:59:44,473 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:00:01,731 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 11:00:06,393 | server.py:125 | fit progress: (10, 1.505384087562561, {'accuracy': 0.9557, 'data_size': 10000}, 229.69640256500134)
INFO flwr 2024-04-29 11:00:06,393 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 11:00:06,394 | server.py:153 | FL finished in 229.69687521599553
INFO flwr 2024-04-29 11:00:06,400 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 11:00:06,400 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 11:00:06,401 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 11:00:06,401 | app.py:229 | app_fit: losses_centralized [(0, 2.3026363849639893), (1, 2.0597760677337646), (2, 1.6151460409164429), (3, 1.516568660736084), (4, 1.5009626150131226), (5, 1.4944603443145752), (6, 1.4980030059814453), (7, 1.5017887353897095), (8, 1.5024374723434448), (9, 1.502370834350586), (10, 1.505384087562561)]
INFO flwr 2024-04-29 11:00:06,401 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.096), (1, 0.7108), (2, 0.9172), (3, 0.9476), (4, 0.96), (5, 0.9668), (6, 0.9626), (7, 0.9593), (8, 0.9588), (9, 0.9589), (10, 0.9557)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9557
wandb:     loss 1.50538
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_105548-9jkg28qe
wandb: Find logs at: ./wandb/offline-run-20240429_105548-9jkg28qe/logs
INFO flwr 2024-04-29 11:00:10,420 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 11:00:15,401 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=127271)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=127271)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-29 11:00:20,415	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 11:00:20,796	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 11:00:21,235	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 11:00:21,237	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 11:00:32,386 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 76960594329.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169574720103.0}
INFO flwr 2024-04-29 11:00:32,387 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 11:00:32,387 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 11:00:32,402 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 11:00:32,402 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 11:00:32,403 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 11:00:32,403 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=132390)[0m 2024-04-29 11:00:38.592452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=132390)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=132390)[0m 2024-04-29 11:00:40.927228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-29 11:00:42,627 | server.py:94 | initial parameters (loss, other metrics): 2.302727460861206, {'accuracy': 0.1025, 'data_size': 10000}
INFO flwr 2024-04-29 11:00:42,628 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 11:00:42,628 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=132397)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=132397)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=132392)[0m 2024-04-29 11:00:38.915530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=132392)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=132392)[0m 2024-04-29 11:00:41.077132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=132390)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=132390)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-29 11:01:02,502 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 11:01:06,954 | server.py:125 | fit progress: (1, 2.2355270385742188, {'accuracy': 0.2182, 'data_size': 10000}, 24.326514387001225)
INFO flwr 2024-04-29 11:01:06,955 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 11:01:06,955 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:01:20,131 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 11:01:24,522 | server.py:125 | fit progress: (2, 1.784059762954712, {'accuracy': 0.6763, 'data_size': 10000}, 41.894276498998806)
INFO flwr 2024-04-29 11:01:24,523 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 11:01:24,523 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:01:37,556 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 11:01:42,129 | server.py:125 | fit progress: (3, 1.612542748451233, {'accuracy': 0.848, 'data_size': 10000}, 59.5007052860019)
INFO flwr 2024-04-29 11:01:42,129 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 11:01:42,129 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:01:53,642 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 11:01:58,214 | server.py:125 | fit progress: (4, 1.609192132949829, {'accuracy': 0.8517, 'data_size': 10000}, 75.58620666900242)
INFO flwr 2024-04-29 11:01:58,214 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 11:01:58,215 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:02:10,269 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 11:02:14,962 | server.py:125 | fit progress: (5, 1.6029404401779175, {'accuracy': 0.8581, 'data_size': 10000}, 92.33386977099872)
INFO flwr 2024-04-29 11:02:14,962 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 11:02:14,962 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:02:27,196 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 11:02:31,806 | server.py:125 | fit progress: (6, 1.5901719331741333, {'accuracy': 0.8708, 'data_size': 10000}, 109.17778150400409)
INFO flwr 2024-04-29 11:02:31,806 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 11:02:31,806 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:02:44,159 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 11:02:48,762 | server.py:125 | fit progress: (7, 1.5873339176177979, {'accuracy': 0.8734, 'data_size': 10000}, 126.13408548900043)
INFO flwr 2024-04-29 11:02:48,762 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 11:02:48,763 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:03:01,214 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 11:03:05,867 | server.py:125 | fit progress: (8, 1.590633749961853, {'accuracy': 0.8704, 'data_size': 10000}, 143.2391350609978)
INFO flwr 2024-04-29 11:03:05,867 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 11:03:05,867 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:03:17,005 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 11:03:21,634 | server.py:125 | fit progress: (9, 1.5846935510635376, {'accuracy': 0.8763, 'data_size': 10000}, 159.00639053100167)
INFO flwr 2024-04-29 11:03:21,635 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 11:03:21,635 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:03:34,809 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 11:03:39,192 | server.py:125 | fit progress: (10, 1.5860880613327026, {'accuracy': 0.875, 'data_size': 10000}, 176.56386383099743)
INFO flwr 2024-04-29 11:03:39,192 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 11:03:39,192 | server.py:153 | FL finished in 176.56444694600214
INFO flwr 2024-04-29 11:03:39,198 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 11:03:39,199 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 11:03:39,199 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 11:03:39,199 | app.py:229 | app_fit: losses_centralized [(0, 2.302727460861206), (1, 2.2355270385742188), (2, 1.784059762954712), (3, 1.612542748451233), (4, 1.609192132949829), (5, 1.6029404401779175), (6, 1.5901719331741333), (7, 1.5873339176177979), (8, 1.590633749961853), (9, 1.5846935510635376), (10, 1.5860880613327026)]
INFO flwr 2024-04-29 11:03:39,199 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1025), (1, 0.2182), (2, 0.6763), (3, 0.848), (4, 0.8517), (5, 0.8581), (6, 0.8708), (7, 0.8734), (8, 0.8704), (9, 0.8763), (10, 0.875)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.875
wandb:     loss 1.58609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_110015-cdu87g64
wandb: Find logs at: ./wandb/offline-run-20240429_110015-cdu87g64/logs
INFO flwr 2024-04-29 11:03:42,857 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 11:03:43,495 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=132388)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=132388)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-29 11:03:48,543	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 11:03:48,913	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 11:03:49,258	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 11:03:49,260	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 11:04:00,475 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 76991422464.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169646652416.0}
INFO flwr 2024-04-29 11:04:00,475 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 11:04:00,475 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 11:04:00,490 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 11:04:00,491 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 11:04:00,492 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 11:04:00,492 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=137458)[0m 2024-04-29 11:04:06.709010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=137458)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=137459)[0m 2024-04-29 11:04:09.060958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-29 11:04:10,260 | server.py:94 | initial parameters (loss, other metrics): 2.302785634994507, {'accuracy': 0.0847, 'data_size': 10000}
INFO flwr 2024-04-29 11:04:10,260 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 11:04:10,261 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=137467)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=137467)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=137465)[0m 2024-04-29 11:04:06.898150: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=137465)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=137465)[0m 2024-04-29 11:04:09.236648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=137459)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=137459)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-29 11:04:30,755 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 11:04:35,436 | server.py:125 | fit progress: (1, 1.8408540487289429, {'accuracy': 0.6412, 'data_size': 10000}, 25.175573458000144)
INFO flwr 2024-04-29 11:04:35,437 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 11:04:35,437 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:04:48,603 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 11:04:53,245 | server.py:125 | fit progress: (2, 1.551302194595337, {'accuracy': 0.9126, 'data_size': 10000}, 42.98383094299788)
INFO flwr 2024-04-29 11:04:53,245 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 11:04:53,245 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:05:04,727 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 11:05:09,316 | server.py:125 | fit progress: (3, 1.5027425289154053, {'accuracy': 0.9589, 'data_size': 10000}, 59.05538406800042)
INFO flwr 2024-04-29 11:05:09,316 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 11:05:09,317 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:05:21,703 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 11:05:26,327 | server.py:125 | fit progress: (4, 1.4977376461029053, {'accuracy': 0.9629, 'data_size': 10000}, 76.0665196139962)
INFO flwr 2024-04-29 11:05:26,328 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 11:05:26,328 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:05:37,932 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 11:05:42,533 | server.py:125 | fit progress: (5, 1.4939124584197998, {'accuracy': 0.967, 'data_size': 10000}, 92.27235722100158)
INFO flwr 2024-04-29 11:05:42,534 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 11:05:42,534 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:05:53,740 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 11:05:58,447 | server.py:125 | fit progress: (6, 1.4891316890716553, {'accuracy': 0.9723, 'data_size': 10000}, 108.18668340099975)
INFO flwr 2024-04-29 11:05:58,448 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 11:05:58,448 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:06:10,089 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 11:06:14,684 | server.py:125 | fit progress: (7, 1.495963215827942, {'accuracy': 0.9652, 'data_size': 10000}, 124.42312495700025)
INFO flwr 2024-04-29 11:06:14,684 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 11:06:14,684 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:06:27,351 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 11:06:31,987 | server.py:125 | fit progress: (8, 1.4876279830932617, {'accuracy': 0.9732, 'data_size': 10000}, 141.72670651500084)
INFO flwr 2024-04-29 11:06:31,988 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 11:06:31,988 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:06:44,435 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 11:06:49,064 | server.py:125 | fit progress: (9, 1.4947447776794434, {'accuracy': 0.9664, 'data_size': 10000}, 158.8031383310008)
INFO flwr 2024-04-29 11:06:49,064 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 11:06:49,064 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:07:01,502 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 11:07:06,102 | server.py:125 | fit progress: (10, 1.4950644969940186, {'accuracy': 0.966, 'data_size': 10000}, 175.84127798899863)
INFO flwr 2024-04-29 11:07:06,102 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 11:07:06,102 | server.py:153 | FL finished in 175.8417363289991
INFO flwr 2024-04-29 11:07:06,106 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 11:07:06,106 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 11:07:06,106 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 11:07:06,106 | app.py:229 | app_fit: losses_centralized [(0, 2.302785634994507), (1, 1.8408540487289429), (2, 1.551302194595337), (3, 1.5027425289154053), (4, 1.4977376461029053), (5, 1.4939124584197998), (6, 1.4891316890716553), (7, 1.495963215827942), (8, 1.4876279830932617), (9, 1.4947447776794434), (10, 1.4950644969940186)]
INFO flwr 2024-04-29 11:07:06,106 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0847), (1, 0.6412), (2, 0.9126), (3, 0.9589), (4, 0.9629), (5, 0.967), (6, 0.9723), (7, 0.9652), (8, 0.9732), (9, 0.9664), (10, 0.966)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.966
wandb:     loss 1.49506
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_110343-ktq0qy2l
wandb: Find logs at: ./wandb/offline-run-20240429_110343-ktq0qy2l/logs
INFO flwr 2024-04-29 11:07:09,744 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 11:07:10,421 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=137455)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=137455)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-29 11:07:15,326	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 11:07:15,828	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 11:07:16,175	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 11:07:16,176	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 11:07:27,383 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'object_store_memory': 77239223500.0, 'accelerator_type:TITAN': 1.0, 'memory': 170224854836.0}
INFO flwr 2024-04-29 11:07:27,384 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 11:07:27,384 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 11:07:27,399 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 11:07:27,400 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 11:07:27,400 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 11:07:27,400 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=140807)[0m 2024-04-29 11:07:33.626273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=140807)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=140807)[0m 2024-04-29 11:07:35.942913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-29 11:07:36,269 | server.py:94 | initial parameters (loss, other metrics): 2.302922487258911, {'accuracy': 0.1152, 'data_size': 10000}
INFO flwr 2024-04-29 11:07:36,269 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 11:07:36,270 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=140828)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=140828)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=140803)[0m 2024-04-29 11:07:33.983649: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=140803)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=140803)[0m 2024-04-29 11:07:36.384657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=140814)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=140814)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-29 11:07:56,804 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 11:08:01,437 | server.py:125 | fit progress: (1, 1.8713759183883667, {'accuracy': 0.6805, 'data_size': 10000}, 25.167537204994005)
INFO flwr 2024-04-29 11:08:01,438 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 11:08:01,438 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:08:14,013 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 11:08:18,625 | server.py:125 | fit progress: (2, 1.5550236701965332, {'accuracy': 0.916, 'data_size': 10000}, 42.35496749699814)
INFO flwr 2024-04-29 11:08:18,625 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 11:08:18,625 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:08:30,463 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 11:08:35,086 | server.py:125 | fit progress: (3, 1.5001256465911865, {'accuracy': 0.961, 'data_size': 10000}, 58.81683712999802)
INFO flwr 2024-04-29 11:08:35,087 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 11:08:35,087 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:08:46,053 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 11:08:50,633 | server.py:125 | fit progress: (4, 1.4937244653701782, {'accuracy': 0.9676, 'data_size': 10000}, 74.36350326299726)
INFO flwr 2024-04-29 11:08:50,633 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 11:08:50,634 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:09:02,274 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 11:09:06,630 | server.py:125 | fit progress: (5, 1.4954760074615479, {'accuracy': 0.966, 'data_size': 10000}, 90.36062687399681)
INFO flwr 2024-04-29 11:09:06,631 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 11:09:06,631 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:09:18,401 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 11:09:22,818 | server.py:125 | fit progress: (6, 1.4900765419006348, {'accuracy': 0.9709, 'data_size': 10000}, 106.5478983859939)
INFO flwr 2024-04-29 11:09:22,818 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 11:09:22,818 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:09:35,490 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 11:09:39,843 | server.py:125 | fit progress: (7, 1.4863871335983276, {'accuracy': 0.9745, 'data_size': 10000}, 123.57353297899681)
INFO flwr 2024-04-29 11:09:39,843 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 11:09:39,844 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:09:52,894 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 11:09:57,289 | server.py:125 | fit progress: (8, 1.4880472421646118, {'accuracy': 0.9729, 'data_size': 10000}, 141.019399978999)
INFO flwr 2024-04-29 11:09:57,290 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 11:09:57,290 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:10:09,838 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 11:10:14,440 | server.py:125 | fit progress: (9, 1.4866347312927246, {'accuracy': 0.9744, 'data_size': 10000}, 158.17081671199412)
INFO flwr 2024-04-29 11:10:14,441 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 11:10:14,441 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:10:26,898 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 11:10:31,476 | server.py:125 | fit progress: (10, 1.4873007535934448, {'accuracy': 0.9739, 'data_size': 10000}, 175.20646202100033)
INFO flwr 2024-04-29 11:10:31,476 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 11:10:31,477 | server.py:153 | FL finished in 175.20703495400085
INFO flwr 2024-04-29 11:10:31,480 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 11:10:31,480 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 11:10:31,480 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 11:10:31,481 | app.py:229 | app_fit: losses_centralized [(0, 2.302922487258911), (1, 1.8713759183883667), (2, 1.5550236701965332), (3, 1.5001256465911865), (4, 1.4937244653701782), (5, 1.4954760074615479), (6, 1.4900765419006348), (7, 1.4863871335983276), (8, 1.4880472421646118), (9, 1.4866347312927246), (10, 1.4873007535934448)]
INFO flwr 2024-04-29 11:10:31,481 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1152), (1, 0.6805), (2, 0.916), (3, 0.961), (4, 0.9676), (5, 0.966), (6, 0.9709), (7, 0.9745), (8, 0.9729), (9, 0.9744), (10, 0.9739)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9739
wandb:     loss 1.4873
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_110710-dt677rop
wandb: Find logs at: ./wandb/offline-run-20240429_110710-dt677rop/logs
[2m[36m(DefaultActor pid=140803)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=140803)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-29 11:11:39.519241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-29 11:11:40.805582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-29 11:11:51,003 | batch_run_simulation.py:80 | Loaded 4 configs with name MINST-LOGISTICREGRESSION-FEDADAM, running...
INFO flwr 2024-04-29 11:11:51,004 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 11:11:54,146 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-29 11:11:56,702	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 11:11:57,147	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 11:11:57,598	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 11:11:57,600	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 11:12:08,731 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:TITAN': 1.0, 'memory': 171000386560.0, 'object_store_memory': 77571594240.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-29 11:12:08,731 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 11:12:08,731 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 11:12:08,747 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 11:12:08,748 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 11:12:08,748 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 11:12:08,748 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-29 11:12:13,211 | server.py:94 | initial parameters (loss, other metrics): 2.3044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-04-29 11:12:13,212 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 11:12:13,212 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=144372)[0m 2024-04-29 11:12:14.804689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=144372)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=144372)[0m 2024-04-29 11:12:17.126883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=144377)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=144377)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=144379)[0m 2024-04-29 11:12:15.016622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=144379)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=144370)[0m 2024-04-29 11:12:17.370762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-29 11:12:31,590 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 11:12:33,149 | server.py:125 | fit progress: (1, 2.2434842586517334, {'accuracy': 0.5552, 'data_size': 10000}, 19.937239230006526)
INFO flwr 2024-04-29 11:12:33,150 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 11:12:33,150 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:12:41,913 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 11:12:43,463 | server.py:125 | fit progress: (2, 2.1380836963653564, {'accuracy': 0.6376, 'data_size': 10000}, 30.250548823001736)
INFO flwr 2024-04-29 11:12:43,463 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 11:12:43,463 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:12:51,379 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 11:12:52,938 | server.py:125 | fit progress: (3, 2.0252139568328857, {'accuracy': 0.6737, 'data_size': 10000}, 39.726135165001324)
INFO flwr 2024-04-29 11:12:52,939 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 11:12:52,939 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:13:00,613 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 11:13:02,135 | server.py:125 | fit progress: (4, 1.9273535013198853, {'accuracy': 0.7076, 'data_size': 10000}, 48.923187752006925)
INFO flwr 2024-04-29 11:13:02,136 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 11:13:02,136 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:13:09,943 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 11:13:11,542 | server.py:125 | fit progress: (5, 1.842438817024231, {'accuracy': 0.7546, 'data_size': 10000}, 58.33021369900234)
INFO flwr 2024-04-29 11:13:11,543 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 11:13:11,543 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:13:19,379 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 11:13:20,696 | server.py:125 | fit progress: (6, 1.7755934000015259, {'accuracy': 0.7919, 'data_size': 10000}, 67.48383152200404)
INFO flwr 2024-04-29 11:13:20,696 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 11:13:20,697 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:13:28,901 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 11:13:30,502 | server.py:125 | fit progress: (7, 1.724334955215454, {'accuracy': 0.826, 'data_size': 10000}, 77.28940394200617)
INFO flwr 2024-04-29 11:13:30,502 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 11:13:30,502 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:13:38,510 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 11:13:39,867 | server.py:125 | fit progress: (8, 1.6891756057739258, {'accuracy': 0.8463, 'data_size': 10000}, 86.65480441800173)
INFO flwr 2024-04-29 11:13:39,867 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 11:13:39,868 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:13:47,825 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 11:13:49,144 | server.py:125 | fit progress: (9, 1.6669306755065918, {'accuracy': 0.8584, 'data_size': 10000}, 95.93186504400364)
INFO flwr 2024-04-29 11:13:49,144 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 11:13:49,145 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:13:57,453 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 11:13:58,799 | server.py:125 | fit progress: (10, 1.652496337890625, {'accuracy': 0.8607, 'data_size': 10000}, 105.58636540300358)
INFO flwr 2024-04-29 11:13:58,799 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 11:13:58,799 | server.py:153 | FL finished in 105.5868288250058
INFO flwr 2024-04-29 11:13:58,800 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 11:13:58,800 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 11:13:58,800 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 11:13:58,800 | app.py:229 | app_fit: losses_centralized [(0, 2.3044753074645996), (1, 2.2434842586517334), (2, 2.1380836963653564), (3, 2.0252139568328857), (4, 1.9273535013198853), (5, 1.842438817024231), (6, 1.7755934000015259), (7, 1.724334955215454), (8, 1.6891756057739258), (9, 1.6669306755065918), (10, 1.652496337890625)]
INFO flwr 2024-04-29 11:13:58,800 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0654), (1, 0.5552), (2, 0.6376), (3, 0.6737), (4, 0.7076), (5, 0.7546), (6, 0.7919), (7, 0.826), (8, 0.8463), (9, 0.8584), (10, 0.8607)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8607
wandb:     loss 1.6525
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_111153-bpidn7wl
wandb: Find logs at: ./wandb/offline-run-20240429_111153-bpidn7wl/logs
INFO flwr 2024-04-29 11:14:02,453 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 11:14:03,127 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=144369)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=144369)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-29 11:14:08,089	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 11:14:08,530	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 11:14:08,888	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 11:14:08,889	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 11:14:20,058 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77313752678.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170398756250.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-29 11:14:20,058 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 11:14:20,059 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 11:14:20,074 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 11:14:20,075 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 11:14:20,075 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 11:14:20,076 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-29 11:14:23,970 | server.py:94 | initial parameters (loss, other metrics): 2.3022866249084473, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-29 11:14:23,971 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 11:14:23,971 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=148094)[0m 2024-04-29 11:14:26.278748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=148094)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=148094)[0m 2024-04-29 11:14:28.635806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=148101)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=148101)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=148101)[0m 2024-04-29 11:14:26.545488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=148101)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=148101)[0m 2024-04-29 11:14:28.932009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-29 11:14:44,383 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 11:14:45,936 | server.py:125 | fit progress: (1, 2.2447190284729004, {'accuracy': 0.4655, 'data_size': 10000}, 21.964852422999684)
INFO flwr 2024-04-29 11:14:45,936 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 11:14:45,937 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:14:55,181 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 11:14:56,716 | server.py:125 | fit progress: (2, 2.150257110595703, {'accuracy': 0.5405, 'data_size': 10000}, 32.74482116699801)
INFO flwr 2024-04-29 11:14:56,716 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 11:14:56,717 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:15:05,268 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 11:15:06,808 | server.py:125 | fit progress: (3, 2.0479624271392822, {'accuracy': 0.6099, 'data_size': 10000}, 42.836878932997934)
INFO flwr 2024-04-29 11:15:06,808 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 11:15:06,809 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:15:15,331 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 11:15:16,909 | server.py:125 | fit progress: (4, 1.9486359357833862, {'accuracy': 0.6797, 'data_size': 10000}, 52.938070352996874)
INFO flwr 2024-04-29 11:15:16,910 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 11:15:16,910 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:15:25,238 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 11:15:26,781 | server.py:125 | fit progress: (5, 1.8560659885406494, {'accuracy': 0.7445, 'data_size': 10000}, 62.809787762998894)
INFO flwr 2024-04-29 11:15:26,781 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 11:15:26,782 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:15:35,258 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 11:15:36,858 | server.py:125 | fit progress: (6, 1.7854827642440796, {'accuracy': 0.7869, 'data_size': 10000}, 72.88714369799709)
INFO flwr 2024-04-29 11:15:36,859 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 11:15:36,859 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:15:45,303 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 11:15:46,842 | server.py:125 | fit progress: (7, 1.7315270900726318, {'accuracy': 0.8206, 'data_size': 10000}, 82.87043112199899)
INFO flwr 2024-04-29 11:15:46,842 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 11:15:46,842 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:15:55,184 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 11:15:56,782 | server.py:125 | fit progress: (8, 1.6939339637756348, {'accuracy': 0.8451, 'data_size': 10000}, 92.81099934499798)
INFO flwr 2024-04-29 11:15:56,783 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 11:15:56,783 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:16:05,351 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 11:16:06,923 | server.py:125 | fit progress: (9, 1.6667358875274658, {'accuracy': 0.8611, 'data_size': 10000}, 102.9522439709981)
INFO flwr 2024-04-29 11:16:06,924 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 11:16:06,924 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:16:15,425 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 11:16:16,987 | server.py:125 | fit progress: (10, 1.6457152366638184, {'accuracy': 0.8728, 'data_size': 10000}, 113.01592810700095)
INFO flwr 2024-04-29 11:16:16,987 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 11:16:16,988 | server.py:153 | FL finished in 113.01636212300218
INFO flwr 2024-04-29 11:16:16,988 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 11:16:16,988 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 11:16:16,988 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 11:16:16,988 | app.py:229 | app_fit: losses_centralized [(0, 2.3022866249084473), (1, 2.2447190284729004), (2, 2.150257110595703), (3, 2.0479624271392822), (4, 1.9486359357833862), (5, 1.8560659885406494), (6, 1.7854827642440796), (7, 1.7315270900726318), (8, 1.6939339637756348), (9, 1.6667358875274658), (10, 1.6457152366638184)]
INFO flwr 2024-04-29 11:16:16,988 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.4655), (2, 0.5405), (3, 0.6099), (4, 0.6797), (5, 0.7445), (6, 0.7869), (7, 0.8206), (8, 0.8451), (9, 0.8611), (10, 0.8728)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8728
wandb:     loss 1.64572
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_111402-gxc7mxu3
wandb: Find logs at: ./wandb/offline-run-20240429_111402-gxc7mxu3/logs
INFO flwr 2024-04-29 11:16:20,632 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 11:16:22,190 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=148094)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=148094)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-29 11:16:28,712	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 11:16:29,583	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 11:16:29,948	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 11:16:29,949	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 11:16:41,006 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77536902758.0, 'memory': 170919439770.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-29 11:16:41,006 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 11:16:41,007 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 11:16:41,027 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 11:16:41,027 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 11:16:41,028 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 11:16:41,028 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-29 11:16:44,504 | server.py:94 | initial parameters (loss, other metrics): 2.302816867828369, {'accuracy': 0.0734, 'data_size': 10000}
INFO flwr 2024-04-29 11:16:44,504 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 11:16:44,504 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=151229)[0m 2024-04-29 11:16:47.945500: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=151229)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=151229)[0m 2024-04-29 11:16:51.829853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=151229)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=151229)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=151295)[0m 2024-04-29 11:16:48.012475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=151295)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=151295)[0m 2024-04-29 11:16:51.828265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-29 11:17:10,759 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 11:17:12,343 | server.py:125 | fit progress: (1, 2.2420578002929688, {'accuracy': 0.423, 'data_size': 10000}, 27.838853324996307)
INFO flwr 2024-04-29 11:17:12,344 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 11:17:12,344 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:17:21,145 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 11:17:22,478 | server.py:125 | fit progress: (2, 2.1329386234283447, {'accuracy': 0.5867, 'data_size': 10000}, 37.97395590799715)
INFO flwr 2024-04-29 11:17:22,479 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 11:17:22,479 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:17:30,726 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 11:17:32,066 | server.py:125 | fit progress: (3, 2.0129952430725098, {'accuracy': 0.6598, 'data_size': 10000}, 47.56123540500266)
INFO flwr 2024-04-29 11:17:32,066 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 11:17:32,066 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:17:40,162 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 11:17:41,490 | server.py:125 | fit progress: (4, 1.911939263343811, {'accuracy': 0.7111, 'data_size': 10000}, 56.98613828999805)
INFO flwr 2024-04-29 11:17:41,491 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 11:17:41,491 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:17:49,382 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 11:17:50,692 | server.py:125 | fit progress: (5, 1.8347878456115723, {'accuracy': 0.7472, 'data_size': 10000}, 66.18724858399946)
INFO flwr 2024-04-29 11:17:50,692 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 11:17:50,692 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:17:58,680 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 11:18:00,290 | server.py:125 | fit progress: (6, 1.7756047248840332, {'accuracy': 0.7835, 'data_size': 10000}, 75.78610587200092)
INFO flwr 2024-04-29 11:18:00,291 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 11:18:00,291 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:18:08,266 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 11:18:09,787 | server.py:125 | fit progress: (7, 1.7305501699447632, {'accuracy': 0.8091, 'data_size': 10000}, 85.2825567789987)
INFO flwr 2024-04-29 11:18:09,787 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 11:18:09,788 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:18:17,830 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 11:18:19,441 | server.py:125 | fit progress: (8, 1.6979857683181763, {'accuracy': 0.8286, 'data_size': 10000}, 94.93692573699809)
INFO flwr 2024-04-29 11:18:19,442 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 11:18:19,442 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:18:26,945 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 11:18:28,593 | server.py:125 | fit progress: (9, 1.6779179573059082, {'accuracy': 0.8401, 'data_size': 10000}, 104.08875012300268)
INFO flwr 2024-04-29 11:18:28,593 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 11:18:28,594 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:18:36,491 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 11:18:38,036 | server.py:125 | fit progress: (10, 1.6616240739822388, {'accuracy': 0.8509, 'data_size': 10000}, 113.53170133799722)
INFO flwr 2024-04-29 11:18:38,036 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 11:18:38,037 | server.py:153 | FL finished in 113.53219979500136
INFO flwr 2024-04-29 11:18:38,037 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 11:18:38,037 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 11:18:38,037 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 11:18:38,037 | app.py:229 | app_fit: losses_centralized [(0, 2.302816867828369), (1, 2.2420578002929688), (2, 2.1329386234283447), (3, 2.0129952430725098), (4, 1.911939263343811), (5, 1.8347878456115723), (6, 1.7756047248840332), (7, 1.7305501699447632), (8, 1.6979857683181763), (9, 1.6779179573059082), (10, 1.6616240739822388)]
INFO flwr 2024-04-29 11:18:38,037 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0734), (1, 0.423), (2, 0.5867), (3, 0.6598), (4, 0.7111), (5, 0.7472), (6, 0.7835), (7, 0.8091), (8, 0.8286), (9, 0.8401), (10, 0.8509)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8509
wandb:     loss 1.66162
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_111621-9d6wjeuv
wandb: Find logs at: ./wandb/offline-run-20240429_111621-9d6wjeuv/logs
INFO flwr 2024-04-29 11:18:41,688 | run_simulation.py:150 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-29 11:18:43,199 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=151295)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=151295)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-29 11:18:49,325	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-29 11:18:50,472	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-29 11:18:50,841	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip' (0.20MiB) to Ray cluster...
2024-04-29 11:18:50,843	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2e1e36fef891c84.zip'.
INFO flwr 2024-04-29 11:19:07,987 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77459670220.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170739230516.0}
INFO flwr 2024-04-29 11:19:07,987 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-29 11:19:07,987 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-29 11:19:08,002 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-29 11:19:08,004 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-29 11:19:08,004 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-29 11:19:08,004 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-29 11:19:11,361 | server.py:94 | initial parameters (loss, other metrics): 2.304152250289917, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-29 11:19:11,361 | server.py:104 | FL starting
DEBUG flwr 2024-04-29 11:19:11,362 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=154973)[0m 2024-04-29 11:19:15.361743: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=154973)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=154974)[0m 2024-04-29 11:19:18.081285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=154974)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=154974)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=154972)[0m 2024-04-29 11:19:15.485390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=154972)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=154972)[0m 2024-04-29 11:19:18.081256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-29 11:19:38,025 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-29 11:19:39,563 | server.py:125 | fit progress: (1, 2.2443885803222656, {'accuracy': 0.5071, 'data_size': 10000}, 28.201670723996358)
INFO flwr 2024-04-29 11:19:39,564 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-29 11:19:39,564 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:19:48,476 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-29 11:19:50,056 | server.py:125 | fit progress: (2, 2.137441396713257, {'accuracy': 0.6573, 'data_size': 10000}, 38.69382217800012)
INFO flwr 2024-04-29 11:19:50,056 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-29 11:19:50,056 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:19:58,204 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-29 11:19:59,774 | server.py:125 | fit progress: (3, 2.016371011734009, {'accuracy': 0.7336, 'data_size': 10000}, 48.41217439100001)
INFO flwr 2024-04-29 11:19:59,774 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-29 11:19:59,774 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:20:07,447 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-29 11:20:08,994 | server.py:125 | fit progress: (4, 1.9052636623382568, {'accuracy': 0.7862, 'data_size': 10000}, 57.6323060959985)
INFO flwr 2024-04-29 11:20:08,994 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-29 11:20:08,995 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:20:17,075 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-29 11:20:18,625 | server.py:125 | fit progress: (5, 1.818367600440979, {'accuracy': 0.8162, 'data_size': 10000}, 67.26353989700146)
INFO flwr 2024-04-29 11:20:18,626 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-29 11:20:18,626 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:20:26,569 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-29 11:20:28,112 | server.py:125 | fit progress: (6, 1.7586219310760498, {'accuracy': 0.8282, 'data_size': 10000}, 76.75047244600137)
INFO flwr 2024-04-29 11:20:28,113 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-29 11:20:28,113 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:20:36,053 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-29 11:20:37,630 | server.py:125 | fit progress: (7, 1.7169685363769531, {'accuracy': 0.841, 'data_size': 10000}, 86.26815926800191)
INFO flwr 2024-04-29 11:20:37,630 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-29 11:20:37,631 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:20:45,434 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-29 11:20:46,963 | server.py:125 | fit progress: (8, 1.6860464811325073, {'accuracy': 0.8526, 'data_size': 10000}, 95.60073291699518)
INFO flwr 2024-04-29 11:20:46,963 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-29 11:20:46,963 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:20:54,766 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-29 11:20:56,349 | server.py:125 | fit progress: (9, 1.6639156341552734, {'accuracy': 0.8616, 'data_size': 10000}, 104.98681949899765)
INFO flwr 2024-04-29 11:20:56,349 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-29 11:20:56,349 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-29 11:21:04,570 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-29 11:21:06,182 | server.py:125 | fit progress: (10, 1.646163821220398, {'accuracy': 0.8694, 'data_size': 10000}, 114.81981950399495)
INFO flwr 2024-04-29 11:21:06,182 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-29 11:21:06,182 | server.py:153 | FL finished in 114.82039308000094
INFO flwr 2024-04-29 11:21:06,182 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-29 11:21:06,183 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-29 11:21:06,183 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-29 11:21:06,183 | app.py:229 | app_fit: losses_centralized [(0, 2.304152250289917), (1, 2.2443885803222656), (2, 2.137441396713257), (3, 2.016371011734009), (4, 1.9052636623382568), (5, 1.818367600440979), (6, 1.7586219310760498), (7, 1.7169685363769531), (8, 1.6860464811325073), (9, 1.6639156341552734), (10, 1.646163821220398)]
INFO flwr 2024-04-29 11:21:06,183 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.5071), (2, 0.6573), (3, 0.7336), (4, 0.7862), (5, 0.8162), (6, 0.8282), (7, 0.841), (8, 0.8526), (9, 0.8616), (10, 0.8694)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8694
wandb:     loss 1.64616
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240429_111842-3easurws
wandb: Find logs at: ./wandb/offline-run-20240429_111842-3easurws/logs
[2m[36m(DefaultActor pid=154972)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=154972)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
