ctit082
	Adding python 3.10.7 (ubuntu 20.04) to your environment
2024-04-16 19:23:37.577298: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-16 19:24:02.742905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-16 19:26:07,673 | batch_run_simulation.py:80 | Loaded 180 configs with name MINST-CNN-FEDADAM, running...
INFO flwr 2024-04-16 19:26:07,674 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 19:33:29,236 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-16 19:33:39,162	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 19:34:15,801	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 19:34:16,294	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 19:34:16,666	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6733a248293d9658.zip' (36.21MiB) to Ray cluster...
2024-04-16 19:34:16,781	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6733a248293d9658.zip'.
INFO flwr 2024-04-16 19:34:28,072 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 70860791808.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 155341847552.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-16 19:34:28,072 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 19:34:28,073 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 19:34:28,098 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 19:34:28,101 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 19:34:28,101 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 19:34:28,101 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=104209)[0m 2024-04-16 19:34:34.084116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=104209)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=104209)[0m 2024-04-16 19:34:36.139331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-16 19:34:37,007 | server.py:94 | initial parameters (loss, other metrics): 2.302842617034912, {'accuracy': 0.0938, 'data_size': 10000}
INFO flwr 2024-04-16 19:34:37,009 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 19:34:37,009 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=104209)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=104209)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=104212)[0m 2024-04-16 19:34:34.153366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=104212)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=104212)[0m 2024-04-16 19:34:36.162043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 19:35:08,279 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 19:35:09,538 | server.py:125 | fit progress: (1, 2.1576855182647705, {'accuracy': 0.3033, 'data_size': 10000}, 32.529173898998124)
INFO flwr 2024-04-16 19:35:09,539 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 19:35:09,539 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:35:20,914 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 19:35:22,391 | server.py:125 | fit progress: (2, 2.170433282852173, {'accuracy': 0.2907, 'data_size': 10000}, 45.382011309997324)
INFO flwr 2024-04-16 19:35:22,391 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 19:35:22,392 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:35:32,666 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 19:35:33,900 | server.py:125 | fit progress: (3, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 56.8907320119979)
INFO flwr 2024-04-16 19:35:33,900 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 19:35:33,900 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:35:44,658 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 19:35:46,119 | server.py:125 | fit progress: (4, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 69.1098997199988)
INFO flwr 2024-04-16 19:35:46,120 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 19:35:46,120 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:35:56,809 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 19:35:58,042 | server.py:125 | fit progress: (5, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 81.03265761199873)
INFO flwr 2024-04-16 19:35:58,042 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 19:35:58,042 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:36:08,288 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 19:36:09,508 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 92.49926248599877)
INFO flwr 2024-04-16 19:36:09,509 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 19:36:09,509 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:36:20,272 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 19:36:21,702 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 104.69273281599817)
INFO flwr 2024-04-16 19:36:21,702 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 19:36:21,702 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:36:33,041 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 19:36:34,499 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 117.48996862099739)
INFO flwr 2024-04-16 19:36:34,499 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 19:36:34,500 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:36:45,241 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 19:36:46,693 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 129.68370409199997)
INFO flwr 2024-04-16 19:36:46,693 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 19:36:46,693 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:36:57,346 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 19:36:58,789 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 141.77971240999977)
INFO flwr 2024-04-16 19:36:58,789 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 19:36:58,789 | server.py:153 | FL finished in 141.78015808399869
INFO flwr 2024-04-16 19:36:58,790 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 19:36:58,790 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 19:36:58,790 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 19:36:58,790 | app.py:229 | app_fit: losses_centralized [(0, 2.302842617034912), (1, 2.1576855182647705), (2, 2.170433282852173), (3, 2.358342170715332), (4, 2.358342170715332), (5, 2.358342170715332), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-16 19:36:58,790 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0938), (1, 0.3033), (2, 0.2907), (3, 0.1028), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_193326-u6g1twze
wandb: Find logs at: ./wandb/offline-run-20240416_193326-u6g1twze/logs
INFO flwr 2024-04-16 19:37:02,336 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 19:44:11,008 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=104212)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=104212)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 19:44:15,442	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 19:44:16,265	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 19:44:16,734	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 19:44:17,094	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_084585c3447eceff.zip' (36.22MiB) to Ray cluster...
2024-04-16 19:44:17,204	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_084585c3447eceff.zip'.
INFO flwr 2024-04-16 19:44:28,450 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 70351989964.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 154154643252.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-16 19:44:28,451 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 19:44:28,451 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 19:44:28,472 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 19:44:28,474 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 19:44:28,474 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 19:44:28,474 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 19:44:31,188 | server.py:94 | initial parameters (loss, other metrics): 2.302441120147705, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-16 19:44:31,188 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 19:44:31,189 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=108980)[0m 2024-04-16 19:44:34.527570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=108980)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=108980)[0m 2024-04-16 19:44:36.829870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=108982)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=108982)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=108986)[0m 2024-04-16 19:44:34.900554: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=108986)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=108986)[0m 2024-04-16 19:44:37.274880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 19:44:55,408 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 19:44:56,847 | server.py:125 | fit progress: (1, 2.1861283779144287, {'accuracy': 0.6007, 'data_size': 10000}, 25.65789816599863)
INFO flwr 2024-04-16 19:44:56,847 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 19:44:56,847 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:45:07,681 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 19:45:09,145 | server.py:125 | fit progress: (2, 1.8174160718917847, {'accuracy': 0.655, 'data_size': 10000}, 37.95676567400005)
INFO flwr 2024-04-16 19:45:09,146 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 19:45:09,146 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:45:20,190 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 19:45:21,382 | server.py:125 | fit progress: (3, 1.7248265743255615, {'accuracy': 0.7328, 'data_size': 10000}, 50.19287220800106)
INFO flwr 2024-04-16 19:45:21,382 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 19:45:21,382 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:45:31,475 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 19:45:32,924 | server.py:125 | fit progress: (4, 1.6153664588928223, {'accuracy': 0.8456, 'data_size': 10000}, 61.735689488999924)
INFO flwr 2024-04-16 19:45:32,925 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 19:45:32,925 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:45:44,207 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 19:45:45,456 | server.py:125 | fit progress: (5, 1.572585940361023, {'accuracy': 0.8881, 'data_size': 10000}, 74.26782581500083)
INFO flwr 2024-04-16 19:45:45,457 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 19:45:45,457 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:45:56,257 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 19:45:57,746 | server.py:125 | fit progress: (6, 1.5539275407791138, {'accuracy': 0.9065, 'data_size': 10000}, 86.55748871499964)
INFO flwr 2024-04-16 19:45:57,746 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 19:45:57,747 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:46:09,053 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 19:46:10,304 | server.py:125 | fit progress: (7, 1.546348214149475, {'accuracy': 0.9143, 'data_size': 10000}, 99.11534327700065)
INFO flwr 2024-04-16 19:46:10,304 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 19:46:10,305 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:46:21,522 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 19:46:22,983 | server.py:125 | fit progress: (8, 1.539441466331482, {'accuracy': 0.9211, 'data_size': 10000}, 111.79478891799954)
INFO flwr 2024-04-16 19:46:22,984 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 19:46:22,984 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:46:34,095 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 19:46:35,571 | server.py:125 | fit progress: (9, 1.5378421545028687, {'accuracy': 0.9232, 'data_size': 10000}, 124.38259185800052)
INFO flwr 2024-04-16 19:46:35,572 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 19:46:35,572 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:46:46,854 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 19:46:48,332 | server.py:125 | fit progress: (10, 1.5341113805770874, {'accuracy': 0.9271, 'data_size': 10000}, 137.1430358969992)
INFO flwr 2024-04-16 19:46:48,332 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 19:46:48,332 | server.py:153 | FL finished in 137.1435200679989
INFO flwr 2024-04-16 19:46:48,332 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 19:46:48,332 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 19:46:48,333 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 19:46:48,333 | app.py:229 | app_fit: losses_centralized [(0, 2.302441120147705), (1, 2.1861283779144287), (2, 1.8174160718917847), (3, 1.7248265743255615), (4, 1.6153664588928223), (5, 1.572585940361023), (6, 1.5539275407791138), (7, 1.546348214149475), (8, 1.539441466331482), (9, 1.5378421545028687), (10, 1.5341113805770874)]
INFO flwr 2024-04-16 19:46:48,333 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.6007), (2, 0.655), (3, 0.7328), (4, 0.8456), (5, 0.8881), (6, 0.9065), (7, 0.9143), (8, 0.9211), (9, 0.9232), (10, 0.9271)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9271
wandb:     loss 1.53411
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_194410-e60ms3dn
wandb: Find logs at: ./wandb/offline-run-20240416_194410-e60ms3dn/logs
INFO flwr 2024-04-16 19:46:51,958 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 19:54:01,256 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=108976)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=108976)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 19:54:06,145	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 19:54:06,924	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 19:54:07,353	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 19:54:07,708	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ebe7840870fddc9f.zip' (36.24MiB) to Ray cluster...
2024-04-16 19:54:07,826	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ebe7840870fddc9f.zip'.
INFO flwr 2024-04-16 19:54:19,640 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 70179643392.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 153752501248.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-16 19:54:19,641 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 19:54:19,641 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 19:54:19,665 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 19:54:19,666 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 19:54:19,667 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 19:54:19,667 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 19:54:22,859 | server.py:94 | initial parameters (loss, other metrics): 2.3025121688842773, {'accuracy': 0.0851, 'data_size': 10000}
INFO flwr 2024-04-16 19:54:22,860 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 19:54:22,860 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=113343)[0m 2024-04-16 19:54:25.951317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=113343)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=113343)[0m 2024-04-16 19:54:28.268752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=113346)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=113346)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=113349)[0m 2024-04-16 19:54:26.213609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=113349)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=113349)[0m 2024-04-16 19:54:28.797934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 19:54:46,084 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 19:54:47,595 | server.py:125 | fit progress: (1, 2.3014237880706787, {'accuracy': 0.2035, 'data_size': 10000}, 24.734739008999895)
INFO flwr 2024-04-16 19:54:47,595 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 19:54:47,595 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:54:59,355 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 19:55:00,611 | server.py:125 | fit progress: (2, 2.298325300216675, {'accuracy': 0.2845, 'data_size': 10000}, 37.75143262700294)
INFO flwr 2024-04-16 19:55:00,612 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 19:55:00,612 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:55:11,402 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 19:55:12,935 | server.py:125 | fit progress: (3, 2.291161060333252, {'accuracy': 0.4244, 'data_size': 10000}, 50.07526986800076)
INFO flwr 2024-04-16 19:55:12,935 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 19:55:12,936 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:55:23,648 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 19:55:25,132 | server.py:125 | fit progress: (4, 2.2765579223632812, {'accuracy': 0.5746, 'data_size': 10000}, 62.271655290001945)
INFO flwr 2024-04-16 19:55:25,132 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 19:55:25,132 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:55:36,642 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 19:55:38,106 | server.py:125 | fit progress: (5, 2.249241590499878, {'accuracy': 0.6492, 'data_size': 10000}, 75.24610528700214)
INFO flwr 2024-04-16 19:55:38,106 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 19:55:38,107 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:55:49,524 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 19:55:50,787 | server.py:125 | fit progress: (6, 2.2013535499572754, {'accuracy': 0.6887, 'data_size': 10000}, 87.92761895100193)
INFO flwr 2024-04-16 19:55:50,788 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 19:55:50,788 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:56:02,172 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 19:56:03,674 | server.py:125 | fit progress: (7, 2.1264474391937256, {'accuracy': 0.7163, 'data_size': 10000}, 100.81444318300055)
INFO flwr 2024-04-16 19:56:03,675 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 19:56:03,675 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:56:15,116 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 19:56:16,369 | server.py:125 | fit progress: (8, 2.0291731357574463, {'accuracy': 0.7383, 'data_size': 10000}, 113.50950499700048)
INFO flwr 2024-04-16 19:56:16,370 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 19:56:16,370 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:56:28,924 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 19:56:30,472 | server.py:125 | fit progress: (9, 1.9263380765914917, {'accuracy': 0.7605, 'data_size': 10000}, 127.61213235300238)
INFO flwr 2024-04-16 19:56:30,472 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 19:56:30,473 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 19:56:41,766 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 19:56:43,036 | server.py:125 | fit progress: (10, 1.8358476161956787, {'accuracy': 0.7834, 'data_size': 10000}, 140.17620924400035)
INFO flwr 2024-04-16 19:56:43,036 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 19:56:43,037 | server.py:153 | FL finished in 140.17676363300052
INFO flwr 2024-04-16 19:56:43,037 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 19:56:43,037 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 19:56:43,037 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 19:56:43,037 | app.py:229 | app_fit: losses_centralized [(0, 2.3025121688842773), (1, 2.3014237880706787), (2, 2.298325300216675), (3, 2.291161060333252), (4, 2.2765579223632812), (5, 2.249241590499878), (6, 2.2013535499572754), (7, 2.1264474391937256), (8, 2.0291731357574463), (9, 1.9263380765914917), (10, 1.8358476161956787)]
INFO flwr 2024-04-16 19:56:43,037 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0851), (1, 0.2035), (2, 0.2845), (3, 0.4244), (4, 0.5746), (5, 0.6492), (6, 0.6887), (7, 0.7163), (8, 0.7383), (9, 0.7605), (10, 0.7834)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7834
wandb:     loss 1.83585
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_195400-oy4hz2gk
wandb: Find logs at: ./wandb/offline-run-20240416_195400-oy4hz2gk/logs
INFO flwr 2024-04-16 19:56:46,696 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 20:03:56,324 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=113342)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=113342)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 20:04:01,074	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 20:04:01,817	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 20:04:02,225	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 20:04:02,580	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f2e41d53be03b30b.zip' (36.25MiB) to Ray cluster...
2024-04-16 20:04:02,688	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f2e41d53be03b30b.zip'.
INFO flwr 2024-04-16 20:04:13,951 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 70128428236.0, 'memory': 153632999220.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 20:04:13,952 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 20:04:13,952 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 20:04:13,978 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 20:04:13,979 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 20:04:13,979 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 20:04:13,980 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 20:04:17,864 | server.py:94 | initial parameters (loss, other metrics): 2.3025853633880615, {'accuracy': 0.0866, 'data_size': 10000}
INFO flwr 2024-04-16 20:04:17,866 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 20:04:17,869 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=117733)[0m 2024-04-16 20:04:19.971556: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=117733)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=117733)[0m 2024-04-16 20:04:22.263065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=117733)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=117733)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=117731)[0m 2024-04-16 20:04:20.355975: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=117731)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=117731)[0m 2024-04-16 20:04:22.726127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 20:04:39,517 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 20:04:40,994 | server.py:125 | fit progress: (1, 2.3025102615356445, {'accuracy': 0.0884, 'data_size': 10000}, 23.127115532999596)
INFO flwr 2024-04-16 20:04:40,995 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 20:04:40,995 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:04:52,362 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 20:04:53,805 | server.py:125 | fit progress: (2, 2.302398920059204, {'accuracy': 0.0918, 'data_size': 10000}, 35.9374527689979)
INFO flwr 2024-04-16 20:04:53,805 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 20:04:53,805 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:05:04,712 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 20:05:06,174 | server.py:125 | fit progress: (3, 2.302255392074585, {'accuracy': 0.0983, 'data_size': 10000}, 48.30678225099837)
INFO flwr 2024-04-16 20:05:06,174 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 20:05:06,175 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:05:16,991 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 20:05:18,191 | server.py:125 | fit progress: (4, 2.302072525024414, {'accuracy': 0.1043, 'data_size': 10000}, 60.32370598000125)
INFO flwr 2024-04-16 20:05:18,191 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 20:05:18,192 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:05:28,607 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 20:05:30,048 | server.py:125 | fit progress: (5, 2.301854133605957, {'accuracy': 0.1128, 'data_size': 10000}, 72.18113870099842)
INFO flwr 2024-04-16 20:05:30,049 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 20:05:30,049 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:05:41,561 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 20:05:42,807 | server.py:125 | fit progress: (6, 2.3015918731689453, {'accuracy': 0.1267, 'data_size': 10000}, 84.93982707000032)
INFO flwr 2024-04-16 20:05:42,807 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 20:05:42,808 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:05:53,308 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 20:05:54,733 | server.py:125 | fit progress: (7, 2.301283836364746, {'accuracy': 0.1495, 'data_size': 10000}, 96.86584551499982)
INFO flwr 2024-04-16 20:05:54,733 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 20:05:54,734 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:06:05,126 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 20:06:06,577 | server.py:125 | fit progress: (8, 2.3009209632873535, {'accuracy': 0.1773, 'data_size': 10000}, 108.70921894999992)
INFO flwr 2024-04-16 20:06:06,577 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 20:06:06,577 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:06:17,393 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 20:06:18,842 | server.py:125 | fit progress: (9, 2.3005027770996094, {'accuracy': 0.2111, 'data_size': 10000}, 120.97424470899932)
INFO flwr 2024-04-16 20:06:18,842 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 20:06:18,842 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:06:30,993 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 20:06:32,272 | server.py:125 | fit progress: (10, 2.3000240325927734, {'accuracy': 0.2468, 'data_size': 10000}, 134.40435159700064)
INFO flwr 2024-04-16 20:06:32,272 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 20:06:32,272 | server.py:153 | FL finished in 134.40476910099824
INFO flwr 2024-04-16 20:06:32,272 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 20:06:32,272 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 20:06:32,273 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 20:06:32,273 | app.py:229 | app_fit: losses_centralized [(0, 2.3025853633880615), (1, 2.3025102615356445), (2, 2.302398920059204), (3, 2.302255392074585), (4, 2.302072525024414), (5, 2.301854133605957), (6, 2.3015918731689453), (7, 2.301283836364746), (8, 2.3009209632873535), (9, 2.3005027770996094), (10, 2.3000240325927734)]
INFO flwr 2024-04-16 20:06:32,273 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0866), (1, 0.0884), (2, 0.0918), (3, 0.0983), (4, 0.1043), (5, 0.1128), (6, 0.1267), (7, 0.1495), (8, 0.1773), (9, 0.2111), (10, 0.2468)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2468
wandb:     loss 2.30002
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_200355-tjxi2nvt
wandb: Find logs at: ./wandb/offline-run-20240416_200355-tjxi2nvt/logs
INFO flwr 2024-04-16 20:06:35,878 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 20:13:47,195 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=117724)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=117724)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 20:13:52,006	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 20:13:52,831	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 20:13:53,260	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 20:13:53,619	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_87e9f194952df20b.zip' (36.26MiB) to Ray cluster...
2024-04-16 20:13:53,728	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_87e9f194952df20b.zip'.
INFO flwr 2024-04-16 20:14:04,813 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 70086795264.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 153535855616.0, 'GPU': 1.0}
INFO flwr 2024-04-16 20:14:04,813 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 20:14:04,813 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 20:14:04,833 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 20:14:04,834 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 20:14:04,834 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 20:14:04,834 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 20:14:08,440 | server.py:94 | initial parameters (loss, other metrics): 2.3025784492492676, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-16 20:14:08,440 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 20:14:08,441 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=122458)[0m 2024-04-16 20:14:10.972284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=122458)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=122456)[0m 2024-04-16 20:14:13.221846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=122464)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=122464)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=122459)[0m 2024-04-16 20:14:11.165112: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=122459)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=122459)[0m 2024-04-16 20:14:13.468117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 20:14:31,352 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 20:14:32,814 | server.py:125 | fit progress: (1, 2.3025708198547363, {'accuracy': 0.1135, 'data_size': 10000}, 24.373477594999713)
INFO flwr 2024-04-16 20:14:32,815 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 20:14:32,815 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:14:44,612 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 20:14:45,856 | server.py:125 | fit progress: (2, 2.302560567855835, {'accuracy': 0.1135, 'data_size': 10000}, 37.41560278599718)
INFO flwr 2024-04-16 20:14:45,857 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 20:14:45,857 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:14:56,453 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 20:14:57,893 | server.py:125 | fit progress: (3, 2.302548885345459, {'accuracy': 0.1135, 'data_size': 10000}, 49.45190641099907)
INFO flwr 2024-04-16 20:14:57,893 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 20:14:57,893 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:15:09,014 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 20:15:10,220 | server.py:125 | fit progress: (4, 2.30253529548645, {'accuracy': 0.1135, 'data_size': 10000}, 61.779486199997336)
INFO flwr 2024-04-16 20:15:10,221 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 20:15:10,221 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:15:20,223 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 20:15:21,677 | server.py:125 | fit progress: (5, 2.302521228790283, {'accuracy': 0.1135, 'data_size': 10000}, 73.23647280200021)
INFO flwr 2024-04-16 20:15:21,678 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 20:15:21,678 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:15:33,159 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 20:15:34,380 | server.py:125 | fit progress: (6, 2.3025062084198, {'accuracy': 0.1135, 'data_size': 10000}, 85.93884329999855)
INFO flwr 2024-04-16 20:15:34,380 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 20:15:34,380 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:15:45,409 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 20:15:46,844 | server.py:125 | fit progress: (7, 2.302490711212158, {'accuracy': 0.1135, 'data_size': 10000}, 98.40319430199816)
INFO flwr 2024-04-16 20:15:46,844 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 20:15:46,844 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:15:57,518 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 20:15:58,745 | server.py:125 | fit progress: (8, 2.302474021911621, {'accuracy': 0.1135, 'data_size': 10000}, 110.30447296000057)
INFO flwr 2024-04-16 20:15:58,746 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 20:15:58,746 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:16:09,465 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 20:16:10,900 | server.py:125 | fit progress: (9, 2.3024566173553467, {'accuracy': 0.1135, 'data_size': 10000}, 122.45883771199806)
INFO flwr 2024-04-16 20:16:10,900 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 20:16:10,900 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:16:21,281 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 20:16:22,510 | server.py:125 | fit progress: (10, 2.3024394512176514, {'accuracy': 0.1135, 'data_size': 10000}, 134.06909039699894)
INFO flwr 2024-04-16 20:16:22,510 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 20:16:22,510 | server.py:153 | FL finished in 134.06962760699753
INFO flwr 2024-04-16 20:16:22,511 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 20:16:22,511 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 20:16:22,511 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 20:16:22,511 | app.py:229 | app_fit: losses_centralized [(0, 2.3025784492492676), (1, 2.3025708198547363), (2, 2.302560567855835), (3, 2.302548885345459), (4, 2.30253529548645), (5, 2.302521228790283), (6, 2.3025062084198), (7, 2.302490711212158), (8, 2.302474021911621), (9, 2.3024566173553467), (10, 2.3024394512176514)]
INFO flwr 2024-04-16 20:16:22,511 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1135), (2, 0.1135), (3, 0.1135), (4, 0.1135), (5, 0.1135), (6, 0.1135), (7, 0.1135), (8, 0.1135), (9, 0.1135), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.30244
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_201346-akmq4y21
wandb: Find logs at: ./wandb/offline-run-20240416_201346-akmq4y21/logs
INFO flwr 2024-04-16 20:16:26,054 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 20:23:36,363 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=122454)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=122454)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 20:23:41,299	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 20:23:42,064	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 20:23:42,501	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 20:23:42,866	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ea5fee5e6a1dfa74.zip' (36.27MiB) to Ray cluster...
2024-04-16 20:23:42,979	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ea5fee5e6a1dfa74.zip'.
INFO flwr 2024-04-16 20:23:54,877 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 153007098061.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 69860184883.0}
INFO flwr 2024-04-16 20:23:54,878 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 20:23:54,878 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 20:23:54,895 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 20:23:54,897 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 20:23:54,897 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 20:23:54,897 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 20:23:58,402 | server.py:94 | initial parameters (loss, other metrics): 2.3024845123291016, {'accuracy': 0.1249, 'data_size': 10000}
INFO flwr 2024-04-16 20:23:58,402 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 20:23:58,403 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=126847)[0m 2024-04-16 20:24:01.148714: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=126847)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=126847)[0m 2024-04-16 20:24:03.631082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=126847)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=126847)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=126849)[0m 2024-04-16 20:24:01.634362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=126849)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=126849)[0m 2024-04-16 20:24:03.889732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 20:24:22,271 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 20:24:23,493 | server.py:125 | fit progress: (1, 1.9490042924880981, {'accuracy': 0.512, 'data_size': 10000}, 25.090296165995824)
INFO flwr 2024-04-16 20:24:23,493 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 20:24:23,493 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:24:35,015 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 20:24:36,535 | server.py:125 | fit progress: (2, 2.2951266765594482, {'accuracy': 0.166, 'data_size': 10000}, 38.13237158699485)
INFO flwr 2024-04-16 20:24:36,535 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 20:24:36,536 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:24:47,599 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 20:24:48,854 | server.py:125 | fit progress: (3, 2.302440881729126, {'accuracy': 0.1587, 'data_size': 10000}, 50.451798868998594)
INFO flwr 2024-04-16 20:24:48,855 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 20:24:48,855 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:25:00,091 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 20:25:01,582 | server.py:125 | fit progress: (4, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 63.17916395499924)
INFO flwr 2024-04-16 20:25:01,582 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 20:25:01,582 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:25:12,196 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 20:25:13,444 | server.py:125 | fit progress: (5, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 75.04175613699772)
INFO flwr 2024-04-16 20:25:13,445 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 20:25:13,445 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:25:25,215 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 20:25:26,738 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 88.33521701699647)
INFO flwr 2024-04-16 20:25:26,738 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 20:25:26,738 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:25:37,299 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 20:25:38,544 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 100.14164482599881)
INFO flwr 2024-04-16 20:25:38,545 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 20:25:38,545 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:25:48,924 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 20:25:50,377 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 111.97400552999898)
INFO flwr 2024-04-16 20:25:50,377 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 20:25:50,377 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:26:00,802 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 20:26:02,048 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 123.64566638299584)
INFO flwr 2024-04-16 20:26:02,049 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 20:26:02,049 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:26:12,944 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 20:26:14,424 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 136.02114352999342)
INFO flwr 2024-04-16 20:26:14,424 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 20:26:14,424 | server.py:153 | FL finished in 136.02156098399428
INFO flwr 2024-04-16 20:26:14,424 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 20:26:14,424 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 20:26:14,425 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 20:26:14,425 | app.py:229 | app_fit: losses_centralized [(0, 2.3024845123291016), (1, 1.9490042924880981), (2, 2.2951266765594482), (3, 2.302440881729126), (4, 2.358342170715332), (5, 2.358342170715332), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-16 20:26:14,425 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1249), (1, 0.512), (2, 0.166), (3, 0.1587), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_202336-ferrdqun
wandb: Find logs at: ./wandb/offline-run-20240416_202336-ferrdqun/logs
INFO flwr 2024-04-16 20:26:17,999 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 20:33:27,048 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=126839)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=126839)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 20:33:31,864	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 20:33:32,612	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 20:33:33,118	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 20:33:33,478	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a88abc04e8a7a62c.zip' (36.28MiB) to Ray cluster...
2024-04-16 20:33:33,585	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a88abc04e8a7a62c.zip'.
INFO flwr 2024-04-16 20:33:45,186 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 69927088128.0, 'accelerator_type:TITAN': 1.0, 'memory': 153163205632.0}
INFO flwr 2024-04-16 20:33:45,186 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 20:33:45,186 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 20:33:45,211 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 20:33:45,212 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 20:33:45,213 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 20:33:45,213 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 20:33:48,596 | server.py:94 | initial parameters (loss, other metrics): 2.3026721477508545, {'accuracy': 0.0954, 'data_size': 10000}
INFO flwr 2024-04-16 20:33:48,597 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 20:33:48,597 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=131203)[0m 2024-04-16 20:33:51.652178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=131203)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=131203)[0m 2024-04-16 20:33:53.966018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=131204)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=131204)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=131207)[0m 2024-04-16 20:33:51.687524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=131207)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=131207)[0m 2024-04-16 20:33:54.201923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 20:34:12,158 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 20:34:13,436 | server.py:125 | fit progress: (1, 2.200263500213623, {'accuracy': 0.6143, 'data_size': 10000}, 24.83888116599701)
INFO flwr 2024-04-16 20:34:13,436 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 20:34:13,436 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:34:24,201 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 20:34:25,712 | server.py:125 | fit progress: (2, 1.821509599685669, {'accuracy': 0.6501, 'data_size': 10000}, 37.11512290799874)
INFO flwr 2024-04-16 20:34:25,712 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 20:34:25,713 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:34:35,909 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 20:34:37,153 | server.py:125 | fit progress: (3, 1.6701830625534058, {'accuracy': 0.7937, 'data_size': 10000}, 48.556116218998795)
INFO flwr 2024-04-16 20:34:37,153 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 20:34:37,154 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:34:47,265 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 20:34:48,741 | server.py:125 | fit progress: (4, 1.5919402837753296, {'accuracy': 0.8693, 'data_size': 10000}, 60.143789207002555)
INFO flwr 2024-04-16 20:34:48,741 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 20:34:48,741 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:34:59,728 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 20:35:00,951 | server.py:125 | fit progress: (5, 1.5563045740127563, {'accuracy': 0.9052, 'data_size': 10000}, 72.35373141000309)
INFO flwr 2024-04-16 20:35:00,951 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 20:35:00,951 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:35:11,285 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 20:35:12,764 | server.py:125 | fit progress: (6, 1.5469629764556885, {'accuracy': 0.9144, 'data_size': 10000}, 84.16711992199998)
INFO flwr 2024-04-16 20:35:12,764 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 20:35:12,764 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:35:23,820 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 20:35:25,055 | server.py:125 | fit progress: (7, 1.5431402921676636, {'accuracy': 0.9177, 'data_size': 10000}, 96.45824631099822)
INFO flwr 2024-04-16 20:35:25,055 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 20:35:25,056 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:35:36,257 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 20:35:37,754 | server.py:125 | fit progress: (8, 1.5382030010223389, {'accuracy': 0.9226, 'data_size': 10000}, 109.15709376199811)
INFO flwr 2024-04-16 20:35:37,754 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 20:35:37,754 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:35:48,612 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 20:35:50,087 | server.py:125 | fit progress: (9, 1.5335772037506104, {'accuracy': 0.9275, 'data_size': 10000}, 121.48977923199709)
INFO flwr 2024-04-16 20:35:50,087 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 20:35:50,087 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:36:00,889 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 20:36:02,356 | server.py:125 | fit progress: (10, 1.5336143970489502, {'accuracy': 0.9272, 'data_size': 10000}, 133.75927220899757)
INFO flwr 2024-04-16 20:36:02,356 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 20:36:02,357 | server.py:153 | FL finished in 133.75971311500325
INFO flwr 2024-04-16 20:36:02,357 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 20:36:02,357 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 20:36:02,357 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 20:36:02,357 | app.py:229 | app_fit: losses_centralized [(0, 2.3026721477508545), (1, 2.200263500213623), (2, 1.821509599685669), (3, 1.6701830625534058), (4, 1.5919402837753296), (5, 1.5563045740127563), (6, 1.5469629764556885), (7, 1.5431402921676636), (8, 1.5382030010223389), (9, 1.5335772037506104), (10, 1.5336143970489502)]
INFO flwr 2024-04-16 20:36:02,357 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0954), (1, 0.6143), (2, 0.6501), (3, 0.7937), (4, 0.8693), (5, 0.9052), (6, 0.9144), (7, 0.9177), (8, 0.9226), (9, 0.9275), (10, 0.9272)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9272
wandb:     loss 1.53361
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_203326-rgku6m9p
wandb: Find logs at: ./wandb/offline-run-20240416_203326-rgku6m9p/logs
INFO flwr 2024-04-16 20:36:05,918 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 20:43:15,348 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=131195)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=131195)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 20:43:21,059	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 20:43:21,818	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 20:43:22,263	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 20:43:22,627	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a0ed5910dc6811ff.zip' (36.29MiB) to Ray cluster...
2024-04-16 20:43:22,736	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a0ed5910dc6811ff.zip'.
INFO flwr 2024-04-16 20:43:33,820 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 153316219495.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'object_store_memory': 69992665497.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 20:43:33,821 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 20:43:33,821 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 20:43:33,840 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 20:43:33,841 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 20:43:33,841 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 20:43:33,841 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 20:43:36,586 | server.py:94 | initial parameters (loss, other metrics): 2.30263352394104, {'accuracy': 0.1007, 'data_size': 10000}
INFO flwr 2024-04-16 20:43:36,586 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 20:43:36,587 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=135940)[0m 2024-04-16 20:43:39.927319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=135940)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=135940)[0m 2024-04-16 20:43:42.283828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=135943)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=135943)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=135935)[0m 2024-04-16 20:43:40.212996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=135935)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=135945)[0m 2024-04-16 20:43:42.396358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 20:44:00,595 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 20:44:02,068 | server.py:125 | fit progress: (1, 2.301515817642212, {'accuracy': 0.1407, 'data_size': 10000}, 25.481512044003466)
INFO flwr 2024-04-16 20:44:02,069 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 20:44:02,069 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:44:13,907 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 20:44:15,160 | server.py:125 | fit progress: (2, 2.298637628555298, {'accuracy': 0.3115, 'data_size': 10000}, 38.573441906999506)
INFO flwr 2024-04-16 20:44:15,161 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 20:44:15,161 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:44:26,465 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 20:44:27,880 | server.py:125 | fit progress: (3, 2.2923789024353027, {'accuracy': 0.6052, 'data_size': 10000}, 51.29347979099839)
INFO flwr 2024-04-16 20:44:27,881 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 20:44:27,881 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:44:38,393 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 20:44:39,823 | server.py:125 | fit progress: (4, 2.280296564102173, {'accuracy': 0.7284, 'data_size': 10000}, 63.23635201500292)
INFO flwr 2024-04-16 20:44:39,824 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 20:44:39,824 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:44:50,865 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 20:44:52,311 | server.py:125 | fit progress: (5, 2.258410930633545, {'accuracy': 0.7643, 'data_size': 10000}, 75.72452815400175)
INFO flwr 2024-04-16 20:44:52,312 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 20:44:52,312 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:45:02,950 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 20:45:04,195 | server.py:125 | fit progress: (6, 2.2197368144989014, {'accuracy': 0.781, 'data_size': 10000}, 87.60834498100303)
INFO flwr 2024-04-16 20:45:04,195 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 20:45:04,196 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:45:14,698 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 20:45:16,176 | server.py:125 | fit progress: (7, 2.1559841632843018, {'accuracy': 0.7996, 'data_size': 10000}, 99.58893525999883)
INFO flwr 2024-04-16 20:45:16,176 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 20:45:16,176 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:45:26,825 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 20:45:28,054 | server.py:125 | fit progress: (8, 2.063767433166504, {'accuracy': 0.8037, 'data_size': 10000}, 111.46738945499965)
INFO flwr 2024-04-16 20:45:28,054 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 20:45:28,055 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:45:38,612 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 20:45:40,104 | server.py:125 | fit progress: (9, 1.9554991722106934, {'accuracy': 0.8087, 'data_size': 10000}, 123.51717742800247)
INFO flwr 2024-04-16 20:45:40,104 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 20:45:40,105 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:45:50,682 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 20:45:51,922 | server.py:125 | fit progress: (10, 1.8540470600128174, {'accuracy': 0.8158, 'data_size': 10000}, 135.33508437500132)
INFO flwr 2024-04-16 20:45:51,922 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 20:45:51,922 | server.py:153 | FL finished in 135.3356267449999
INFO flwr 2024-04-16 20:45:51,923 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 20:45:51,923 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 20:45:51,923 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 20:45:51,923 | app.py:229 | app_fit: losses_centralized [(0, 2.30263352394104), (1, 2.301515817642212), (2, 2.298637628555298), (3, 2.2923789024353027), (4, 2.280296564102173), (5, 2.258410930633545), (6, 2.2197368144989014), (7, 2.1559841632843018), (8, 2.063767433166504), (9, 1.9554991722106934), (10, 1.8540470600128174)]
INFO flwr 2024-04-16 20:45:51,923 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1007), (1, 0.1407), (2, 0.3115), (3, 0.6052), (4, 0.7284), (5, 0.7643), (6, 0.781), (7, 0.7996), (8, 0.8037), (9, 0.8087), (10, 0.8158)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8158
wandb:     loss 1.85405
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_204314-5a2hprdv
wandb: Find logs at: ./wandb/offline-run-20240416_204314-5a2hprdv/logs
INFO flwr 2024-04-16 20:45:55,506 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 20:53:04,445 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=135935)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=135935)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 20:53:10,414	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 20:53:11,193	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 20:53:11,707	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 20:53:12,073	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b5902364464a0912.zip' (36.31MiB) to Ray cluster...
2024-04-16 20:53:12,186	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b5902364464a0912.zip'.
INFO flwr 2024-04-16 20:53:25,047 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 153242681549.0, 'CPU': 64.0, 'object_store_memory': 69961149235.0}
INFO flwr 2024-04-16 20:53:25,048 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 20:53:25,048 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 20:53:25,068 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 20:53:25,070 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 20:53:25,071 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 20:53:25,071 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 20:53:28,653 | server.py:94 | initial parameters (loss, other metrics): 2.3025307655334473, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-16 20:53:28,653 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 20:53:28,653 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=140344)[0m 2024-04-16 20:53:31.411679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=140344)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=140344)[0m 2024-04-16 20:53:34.314076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=140308)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=140308)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=140305)[0m 2024-04-16 20:53:31.690218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=140305)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=140304)[0m 2024-04-16 20:53:34.468582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 20:53:55,818 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 20:53:57,075 | server.py:125 | fit progress: (1, 2.302457809448242, {'accuracy': 0.0974, 'data_size': 10000}, 28.42198399300105)
INFO flwr 2024-04-16 20:53:57,075 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 20:53:57,076 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:54:09,294 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 20:54:10,803 | server.py:125 | fit progress: (2, 2.30234956741333, {'accuracy': 0.0974, 'data_size': 10000}, 42.15004943400709)
INFO flwr 2024-04-16 20:54:10,803 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 20:54:10,804 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:54:21,969 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 20:54:23,222 | server.py:125 | fit progress: (3, 2.3022067546844482, {'accuracy': 0.0974, 'data_size': 10000}, 54.56900651500473)
INFO flwr 2024-04-16 20:54:23,223 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 20:54:23,223 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:54:33,738 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 20:54:35,227 | server.py:125 | fit progress: (4, 2.3020272254943848, {'accuracy': 0.0974, 'data_size': 10000}, 66.57364998700359)
INFO flwr 2024-04-16 20:54:35,227 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 20:54:35,227 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:54:46,552 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 20:54:47,837 | server.py:125 | fit progress: (5, 2.3018126487731934, {'accuracy': 0.0974, 'data_size': 10000}, 79.18347222500597)
INFO flwr 2024-04-16 20:54:47,837 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 20:54:47,837 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:54:58,163 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 20:54:59,703 | server.py:125 | fit progress: (6, 2.3015594482421875, {'accuracy': 0.0975, 'data_size': 10000}, 91.04984061300638)
INFO flwr 2024-04-16 20:54:59,703 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 20:54:59,703 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:55:10,771 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 20:55:12,052 | server.py:125 | fit progress: (7, 2.3012630939483643, {'accuracy': 0.098, 'data_size': 10000}, 103.39868088500225)
INFO flwr 2024-04-16 20:55:12,052 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 20:55:12,052 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:55:23,560 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 20:55:25,053 | server.py:125 | fit progress: (8, 2.3009181022644043, {'accuracy': 0.0992, 'data_size': 10000}, 116.39981295500183)
INFO flwr 2024-04-16 20:55:25,053 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 20:55:25,054 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:55:36,258 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 20:55:37,528 | server.py:125 | fit progress: (9, 2.300525188446045, {'accuracy': 0.1028, 'data_size': 10000}, 128.87513723800657)
INFO flwr 2024-04-16 20:55:37,529 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 20:55:37,529 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 20:55:48,925 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 20:55:50,401 | server.py:125 | fit progress: (10, 2.300079584121704, {'accuracy': 0.1078, 'data_size': 10000}, 141.74783499700425)
INFO flwr 2024-04-16 20:55:50,401 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 20:55:50,401 | server.py:153 | FL finished in 141.74828783400153
INFO flwr 2024-04-16 20:55:50,402 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 20:55:50,402 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 20:55:50,402 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 20:55:50,402 | app.py:229 | app_fit: losses_centralized [(0, 2.3025307655334473), (1, 2.302457809448242), (2, 2.30234956741333), (3, 2.3022067546844482), (4, 2.3020272254943848), (5, 2.3018126487731934), (6, 2.3015594482421875), (7, 2.3012630939483643), (8, 2.3009181022644043), (9, 2.300525188446045), (10, 2.300079584121704)]
INFO flwr 2024-04-16 20:55:50,402 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.0974), (2, 0.0974), (3, 0.0974), (4, 0.0974), (5, 0.0974), (6, 0.0975), (7, 0.098), (8, 0.0992), (9, 0.1028), (10, 0.1078)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1078
wandb:     loss 2.30008
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_205304-niuogezx
wandb: Find logs at: ./wandb/offline-run-20240416_205304-niuogezx/logs
INFO flwr 2024-04-16 20:55:53,969 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 21:03:03,320 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=140345)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=140345)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 21:03:08,228	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 21:03:08,970	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 21:03:09,430	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 21:03:09,785	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5496cf6120255840.zip' (36.32MiB) to Ray cluster...
2024-04-16 21:03:09,892	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5496cf6120255840.zip'.
INFO flwr 2024-04-16 21:03:21,085 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 70004672102.0, 'accelerator_type:TITAN': 1.0, 'memory': 153344234906.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 21:03:21,085 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 21:03:21,085 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 21:03:21,103 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 21:03:21,104 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 21:03:21,104 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 21:03:21,104 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 21:03:24,068 | server.py:94 | initial parameters (loss, other metrics): 2.3024537563323975, {'accuracy': 0.1429, 'data_size': 10000}
INFO flwr 2024-04-16 21:03:24,070 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 21:03:24,072 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=144329)[0m 2024-04-16 21:03:27.269161: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=144329)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=144329)[0m 2024-04-16 21:03:29.515023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=144328)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=144328)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=144322)[0m 2024-04-16 21:03:27.538792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=144322)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=144322)[0m 2024-04-16 21:03:29.725939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 21:03:47,755 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 21:03:49,222 | server.py:125 | fit progress: (1, 2.3024473190307617, {'accuracy': 0.1435, 'data_size': 10000}, 25.150659320999694)
INFO flwr 2024-04-16 21:03:49,222 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 21:03:49,222 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:04:00,584 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 21:04:02,052 | server.py:125 | fit progress: (2, 2.3024373054504395, {'accuracy': 0.1436, 'data_size': 10000}, 37.98075871600304)
INFO flwr 2024-04-16 21:04:02,052 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 21:04:02,053 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:04:12,762 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 21:04:14,254 | server.py:125 | fit progress: (3, 2.302427053451538, {'accuracy': 0.1441, 'data_size': 10000}, 50.182270467004855)
INFO flwr 2024-04-16 21:04:14,254 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 21:04:14,254 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:04:24,805 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 21:04:26,268 | server.py:125 | fit progress: (4, 2.302414655685425, {'accuracy': 0.1446, 'data_size': 10000}, 62.19652310100355)
INFO flwr 2024-04-16 21:04:26,268 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 21:04:26,268 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:04:36,703 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 21:04:37,949 | server.py:125 | fit progress: (5, 2.302401304244995, {'accuracy': 0.1446, 'data_size': 10000}, 73.87739305600553)
INFO flwr 2024-04-16 21:04:37,949 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 21:04:37,949 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:04:48,177 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 21:04:49,622 | server.py:125 | fit progress: (6, 2.302386999130249, {'accuracy': 0.1454, 'data_size': 10000}, 85.55086824200407)
INFO flwr 2024-04-16 21:04:49,622 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 21:04:49,623 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:05:00,767 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 21:05:01,993 | server.py:125 | fit progress: (7, 2.3023722171783447, {'accuracy': 0.1457, 'data_size': 10000}, 97.92162521400314)
INFO flwr 2024-04-16 21:05:01,993 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 21:05:01,993 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:05:12,615 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 21:05:14,113 | server.py:125 | fit progress: (8, 2.3023579120635986, {'accuracy': 0.1456, 'data_size': 10000}, 110.04190809800639)
INFO flwr 2024-04-16 21:05:14,113 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 21:05:14,114 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:05:24,662 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 21:05:25,892 | server.py:125 | fit progress: (9, 2.3023412227630615, {'accuracy': 0.1456, 'data_size': 10000}, 121.8204621860059)
INFO flwr 2024-04-16 21:05:25,892 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 21:05:25,892 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:05:36,819 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 21:05:38,302 | server.py:125 | fit progress: (10, 2.302325487136841, {'accuracy': 0.1458, 'data_size': 10000}, 134.2305572450059)
INFO flwr 2024-04-16 21:05:38,302 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 21:05:38,302 | server.py:153 | FL finished in 134.23096555400116
INFO flwr 2024-04-16 21:05:38,302 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 21:05:38,303 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 21:05:38,303 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 21:05:38,303 | app.py:229 | app_fit: losses_centralized [(0, 2.3024537563323975), (1, 2.3024473190307617), (2, 2.3024373054504395), (3, 2.302427053451538), (4, 2.302414655685425), (5, 2.302401304244995), (6, 2.302386999130249), (7, 2.3023722171783447), (8, 2.3023579120635986), (9, 2.3023412227630615), (10, 2.302325487136841)]
INFO flwr 2024-04-16 21:05:38,303 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1429), (1, 0.1435), (2, 0.1436), (3, 0.1441), (4, 0.1446), (5, 0.1446), (6, 0.1454), (7, 0.1457), (8, 0.1456), (9, 0.1456), (10, 0.1458)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1458
wandb:     loss 2.30233
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_210302-f6vh4mkt
wandb: Find logs at: ./wandb/offline-run-20240416_210302-f6vh4mkt/logs
INFO flwr 2024-04-16 21:05:41,478 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 21:12:50,467 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=144316)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=144316)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 21:12:55,124	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 21:12:55,875	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 21:12:56,315	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 21:12:56,677	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ac36e028945bd650.zip' (36.33MiB) to Ray cluster...
2024-04-16 21:12:56,782	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ac36e028945bd650.zip'.
INFO flwr 2024-04-16 21:13:07,927 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 69999613132.0, 'memory': 153332430644.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 21:13:07,928 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 21:13:07,928 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 21:13:07,948 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 21:13:07,950 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 21:13:07,950 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 21:13:07,950 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 21:13:11,432 | server.py:94 | initial parameters (loss, other metrics): 2.3025310039520264, {'accuracy': 0.1006, 'data_size': 10000}
INFO flwr 2024-04-16 21:13:11,432 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 21:13:11,433 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=148794)[0m 2024-04-16 21:13:13.975151: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=148794)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=148794)[0m 2024-04-16 21:13:16.332817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=148798)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=148798)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=148807)[0m 2024-04-16 21:13:14.310772: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=148807)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=148792)[0m 2024-04-16 21:13:16.476531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 21:13:34,209 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 21:13:35,528 | server.py:125 | fit progress: (1, 1.9653599262237549, {'accuracy': 0.4957, 'data_size': 10000}, 24.095691431000887)
INFO flwr 2024-04-16 21:13:35,528 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 21:13:35,529 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:13:47,231 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 21:13:48,688 | server.py:125 | fit progress: (2, 2.2637314796447754, {'accuracy': 0.1974, 'data_size': 10000}, 37.25578882500122)
INFO flwr 2024-04-16 21:13:48,689 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 21:13:48,689 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:13:59,275 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 21:14:00,693 | server.py:125 | fit progress: (3, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 49.260733671995695)
INFO flwr 2024-04-16 21:14:00,693 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 21:14:00,694 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:14:11,303 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 21:14:12,561 | server.py:125 | fit progress: (4, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 61.12834928400116)
INFO flwr 2024-04-16 21:14:12,561 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 21:14:12,561 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:14:23,151 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 21:14:24,660 | server.py:125 | fit progress: (5, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 73.22707783599617)
INFO flwr 2024-04-16 21:14:24,660 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 21:14:24,660 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:14:35,468 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 21:14:36,734 | server.py:125 | fit progress: (6, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 85.30136039899662)
INFO flwr 2024-04-16 21:14:36,734 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 21:14:36,735 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:14:47,437 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 21:14:48,903 | server.py:125 | fit progress: (7, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 97.47020790399984)
INFO flwr 2024-04-16 21:14:48,903 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 21:14:48,903 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:14:59,683 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 21:15:01,136 | server.py:125 | fit progress: (8, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 109.7038955120006)
INFO flwr 2024-04-16 21:15:01,137 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 21:15:01,137 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:15:12,091 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 21:15:13,540 | server.py:125 | fit progress: (9, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 122.10725015799835)
INFO flwr 2024-04-16 21:15:13,540 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 21:15:13,540 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:15:24,652 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 21:15:25,910 | server.py:125 | fit progress: (10, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 134.47747235000134)
INFO flwr 2024-04-16 21:15:25,910 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 21:15:25,910 | server.py:153 | FL finished in 134.47794584599615
INFO flwr 2024-04-16 21:15:25,911 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 21:15:25,911 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 21:15:25,911 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 21:15:25,911 | app.py:229 | app_fit: losses_centralized [(0, 2.3025310039520264), (1, 1.9653599262237549), (2, 2.2637314796447754), (3, 2.347642183303833), (4, 2.347642183303833), (5, 2.347642183303833), (6, 2.347642183303833), (7, 2.347642183303833), (8, 2.347642183303833), (9, 2.347642183303833), (10, 2.347642183303833)]
INFO flwr 2024-04-16 21:15:25,911 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1006), (1, 0.4957), (2, 0.1974), (3, 0.1135), (4, 0.1135), (5, 0.1135), (6, 0.1135), (7, 0.1135), (8, 0.1135), (9, 0.1135), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.34764
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_211250-2osvksph
wandb: Find logs at: ./wandb/offline-run-20240416_211250-2osvksph/logs
INFO flwr 2024-04-16 21:15:29,487 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 21:22:38,564 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=148792)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=148792)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 21:22:44,252	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 21:22:44,996	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 21:22:45,394	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 21:22:45,730	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8d0c62c885c223ad.zip' (36.34MiB) to Ray cluster...
2024-04-16 21:22:45,842	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8d0c62c885c223ad.zip'.
INFO flwr 2024-04-16 21:22:57,677 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 69995341824.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 153322464256.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 21:22:57,677 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 21:22:57,677 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 21:22:57,695 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 21:22:57,697 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 21:22:57,697 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 21:22:57,697 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 21:23:01,269 | server.py:94 | initial parameters (loss, other metrics): 2.302619457244873, {'accuracy': 0.1031, 'data_size': 10000}
INFO flwr 2024-04-16 21:23:01,271 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 21:23:01,273 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=153187)[0m 2024-04-16 21:23:04.066026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=153187)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=153187)[0m 2024-04-16 21:23:06.401362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=153196)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=153196)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=153193)[0m 2024-04-16 21:23:04.419899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=153193)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=153193)[0m 2024-04-16 21:23:06.910553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 21:23:25,467 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 21:23:26,730 | server.py:125 | fit progress: (1, 2.231842517852783, {'accuracy': 0.7392, 'data_size': 10000}, 25.457212786997843)
INFO flwr 2024-04-16 21:23:26,730 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 21:23:26,730 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:23:39,211 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 21:23:40,792 | server.py:125 | fit progress: (2, 1.7687933444976807, {'accuracy': 0.7813, 'data_size': 10000}, 39.51986282799771)
INFO flwr 2024-04-16 21:23:40,792 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 21:23:40,793 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:23:51,797 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 21:23:53,157 | server.py:125 | fit progress: (3, 1.6201692819595337, {'accuracy': 0.8554, 'data_size': 10000}, 51.88463353499537)
INFO flwr 2024-04-16 21:23:53,157 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 21:23:53,157 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:24:03,865 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 21:24:05,303 | server.py:125 | fit progress: (4, 1.5689185857772827, {'accuracy': 0.8949, 'data_size': 10000}, 64.03111989299941)
INFO flwr 2024-04-16 21:24:05,304 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 21:24:05,304 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:24:16,207 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 21:24:17,450 | server.py:125 | fit progress: (5, 1.5515506267547607, {'accuracy': 0.9103, 'data_size': 10000}, 76.17807619999803)
INFO flwr 2024-04-16 21:24:17,451 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 21:24:17,451 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:24:28,557 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 21:24:30,079 | server.py:125 | fit progress: (6, 1.545910358428955, {'accuracy': 0.9156, 'data_size': 10000}, 88.80688557699614)
INFO flwr 2024-04-16 21:24:30,080 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 21:24:30,080 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:24:41,424 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 21:24:42,649 | server.py:125 | fit progress: (7, 1.5498833656311035, {'accuracy': 0.9107, 'data_size': 10000}, 101.37647409399506)
INFO flwr 2024-04-16 21:24:42,649 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 21:24:42,649 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:24:53,884 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 21:24:55,344 | server.py:125 | fit progress: (8, 1.553769826889038, {'accuracy': 0.9069, 'data_size': 10000}, 114.07200609699794)
INFO flwr 2024-04-16 21:24:55,345 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 21:24:55,345 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:25:06,316 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 21:25:07,607 | server.py:125 | fit progress: (9, 1.544113039970398, {'accuracy': 0.9172, 'data_size': 10000}, 126.33415881199471)
INFO flwr 2024-04-16 21:25:07,607 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 21:25:07,607 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:25:18,467 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 21:25:19,949 | server.py:125 | fit progress: (10, 1.5366510152816772, {'accuracy': 0.9245, 'data_size': 10000}, 138.67672204599512)
INFO flwr 2024-04-16 21:25:19,949 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 21:25:19,950 | server.py:153 | FL finished in 138.6771884319969
INFO flwr 2024-04-16 21:25:19,950 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 21:25:19,950 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 21:25:19,950 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 21:25:19,950 | app.py:229 | app_fit: losses_centralized [(0, 2.302619457244873), (1, 2.231842517852783), (2, 1.7687933444976807), (3, 1.6201692819595337), (4, 1.5689185857772827), (5, 1.5515506267547607), (6, 1.545910358428955), (7, 1.5498833656311035), (8, 1.553769826889038), (9, 1.544113039970398), (10, 1.5366510152816772)]
INFO flwr 2024-04-16 21:25:19,950 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1031), (1, 0.7392), (2, 0.7813), (3, 0.8554), (4, 0.8949), (5, 0.9103), (6, 0.9156), (7, 0.9107), (8, 0.9069), (9, 0.9172), (10, 0.9245)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9245
wandb:     loss 1.53665
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_212238-e4p6qmdk
wandb: Find logs at: ./wandb/offline-run-20240416_212238-e4p6qmdk/logs
INFO flwr 2024-04-16 21:25:23,564 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 21:32:33,059 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=153187)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=153187)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 21:32:37,659	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 21:32:38,519	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 21:32:38,939	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 21:32:39,293	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fc8c9dc643ff8545.zip' (36.35MiB) to Ray cluster...
2024-04-16 21:32:39,415	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fc8c9dc643ff8545.zip'.
INFO flwr 2024-04-16 21:32:50,534 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 69981060710.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 153289141658.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-16 21:32:50,535 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 21:32:50,535 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 21:32:50,552 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 21:32:50,552 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 21:32:50,553 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 21:32:50,553 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 21:32:53,729 | server.py:94 | initial parameters (loss, other metrics): 2.3024065494537354, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-16 21:32:53,729 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 21:32:53,729 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=157539)[0m 2024-04-16 21:32:56.699136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=157539)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=157545)[0m 2024-04-16 21:32:58.977847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=157541)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=157541)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=157536)[0m 2024-04-16 21:32:56.760538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=157536)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=157536)[0m 2024-04-16 21:32:59.084122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 21:33:16,659 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 21:33:17,936 | server.py:125 | fit progress: (1, 2.3013806343078613, {'accuracy': 0.0986, 'data_size': 10000}, 24.20669303500472)
INFO flwr 2024-04-16 21:33:17,936 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 21:33:17,937 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:33:29,297 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 21:33:30,809 | server.py:125 | fit progress: (2, 2.29874324798584, {'accuracy': 0.2503, 'data_size': 10000}, 37.07974987000489)
INFO flwr 2024-04-16 21:33:30,809 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 21:33:30,810 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:33:41,235 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 21:33:42,523 | server.py:125 | fit progress: (3, 2.293058395385742, {'accuracy': 0.6849, 'data_size': 10000}, 48.79403290399932)
INFO flwr 2024-04-16 21:33:42,524 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 21:33:42,524 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:33:53,269 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 21:33:54,734 | server.py:125 | fit progress: (4, 2.2823054790496826, {'accuracy': 0.8268, 'data_size': 10000}, 61.00503561300138)
INFO flwr 2024-04-16 21:33:54,735 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 21:33:54,735 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:34:05,700 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 21:34:07,030 | server.py:125 | fit progress: (5, 2.2630529403686523, {'accuracy': 0.8425, 'data_size': 10000}, 73.30087259800348)
INFO flwr 2024-04-16 21:34:07,030 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 21:34:07,031 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:34:17,991 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 21:34:19,437 | server.py:125 | fit progress: (6, 2.2299013137817383, {'accuracy': 0.84, 'data_size': 10000}, 85.70762288600235)
INFO flwr 2024-04-16 21:34:19,437 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 21:34:19,437 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:34:29,810 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 21:34:31,067 | server.py:125 | fit progress: (7, 2.1758623123168945, {'accuracy': 0.8329, 'data_size': 10000}, 97.33815910800331)
INFO flwr 2024-04-16 21:34:31,068 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 21:34:31,068 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:34:42,093 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 21:34:43,576 | server.py:125 | fit progress: (8, 2.0965747833251953, {'accuracy': 0.824, 'data_size': 10000}, 109.84672728000442)
INFO flwr 2024-04-16 21:34:43,576 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 21:34:43,577 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:34:54,475 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 21:34:55,961 | server.py:125 | fit progress: (9, 1.9972196817398071, {'accuracy': 0.8162, 'data_size': 10000}, 122.23151846100518)
INFO flwr 2024-04-16 21:34:55,961 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 21:34:55,961 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:35:06,911 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 21:35:08,386 | server.py:125 | fit progress: (10, 1.8949662446975708, {'accuracy': 0.8102, 'data_size': 10000}, 134.65616072899866)
INFO flwr 2024-04-16 21:35:08,386 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 21:35:08,386 | server.py:153 | FL finished in 134.656775989999
INFO flwr 2024-04-16 21:35:08,386 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 21:35:08,386 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 21:35:08,387 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 21:35:08,387 | app.py:229 | app_fit: losses_centralized [(0, 2.3024065494537354), (1, 2.3013806343078613), (2, 2.29874324798584), (3, 2.293058395385742), (4, 2.2823054790496826), (5, 2.2630529403686523), (6, 2.2299013137817383), (7, 2.1758623123168945), (8, 2.0965747833251953), (9, 1.9972196817398071), (10, 1.8949662446975708)]
INFO flwr 2024-04-16 21:35:08,387 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.0986), (2, 0.2503), (3, 0.6849), (4, 0.8268), (5, 0.8425), (6, 0.84), (7, 0.8329), (8, 0.824), (9, 0.8162), (10, 0.8102)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8102
wandb:     loss 1.89497
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_213232-xykwu455
wandb: Find logs at: ./wandb/offline-run-20240416_213232-xykwu455/logs
INFO flwr 2024-04-16 21:35:11,966 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 21:42:30,617 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=157536)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=157536)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 21:42:54,398	ERROR services.py:1207 -- Failed to start the dashboard 
2024-04-16 21:42:54,400	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-04-16 21:42:54,400	ERROR services.py:1242 -- Couldn't read dashboard.log file. Error: [Errno 2] No such file or directory: '/local/ray/session_2024-04-16_21-42-32_776059_101421/logs/dashboard.log'. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.
2024-04-16 21:42:54,401	ERROR services.py:1276 -- Failed to read dashboard.err file: cannot mmap an empty file. It is unexpected. Please report an issue to Ray github. https://github.com/ray-project/ray/issues
2024-04-16 21:42:54,765	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 21:43:09,500	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 21:43:10,012	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 21:43:10,369	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b7b5f80806c0b82e.zip' (36.37MiB) to Ray cluster...
2024-04-16 21:43:10,486	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b7b5f80806c0b82e.zip'.
INFO flwr 2024-04-16 21:43:21,630 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 75356601139.0, 'memory': 165832069325.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-16 21:43:21,630 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 21:43:21,631 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 21:43:21,653 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 21:43:21,654 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 21:43:21,654 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 21:43:21,654 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 21:43:25,891 | server.py:94 | initial parameters (loss, other metrics): 2.302610397338867, {'accuracy': 0.0977, 'data_size': 10000}
INFO flwr 2024-04-16 21:43:25,891 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 21:43:25,892 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=162412)[0m 2024-04-16 21:43:37.688434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=162412)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=162412)[0m 2024-04-16 21:44:00.574760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=162420)[0m 2024-04-16 21:43:37.693686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=162420)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=162412)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=162412)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=162419)[0m 2024-04-16 21:44:00.574760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 21:45:35,776 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 21:45:37,319 | server.py:125 | fit progress: (1, 2.3025448322296143, {'accuracy': 0.099, 'data_size': 10000}, 131.4276150980004)
INFO flwr 2024-04-16 21:45:37,319 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 21:45:37,320 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:45:48,831 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 21:45:50,069 | server.py:125 | fit progress: (2, 2.302445650100708, {'accuracy': 0.1011, 'data_size': 10000}, 144.177652063001)
INFO flwr 2024-04-16 21:45:50,070 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 21:45:50,070 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:46:00,811 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 21:46:02,279 | server.py:125 | fit progress: (3, 2.302323341369629, {'accuracy': 0.1044, 'data_size': 10000}, 156.38714568100113)
INFO flwr 2024-04-16 21:46:02,279 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 21:46:02,279 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:46:12,622 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 21:46:14,092 | server.py:125 | fit progress: (4, 2.302171230316162, {'accuracy': 0.1104, 'data_size': 10000}, 168.2002489489969)
INFO flwr 2024-04-16 21:46:14,092 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 21:46:14,092 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:46:24,982 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 21:46:26,463 | server.py:125 | fit progress: (5, 2.3019909858703613, {'accuracy': 0.1235, 'data_size': 10000}, 180.57171242199547)
INFO flwr 2024-04-16 21:46:26,463 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 21:46:26,464 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:46:37,754 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 21:46:39,024 | server.py:125 | fit progress: (6, 2.3017776012420654, {'accuracy': 0.1479, 'data_size': 10000}, 193.13273217000096)
INFO flwr 2024-04-16 21:46:39,024 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 21:46:39,025 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:46:49,999 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 21:46:51,547 | server.py:125 | fit progress: (7, 2.3015265464782715, {'accuracy': 0.1811, 'data_size': 10000}, 205.65508633699937)
INFO flwr 2024-04-16 21:46:51,547 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 21:46:51,547 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:47:01,726 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 21:47:03,012 | server.py:125 | fit progress: (8, 2.301236152648926, {'accuracy': 0.2089, 'data_size': 10000}, 217.12054180999985)
INFO flwr 2024-04-16 21:47:03,012 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 21:47:03,012 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:47:13,773 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 21:47:15,264 | server.py:125 | fit progress: (9, 2.3009049892425537, {'accuracy': 0.2306, 'data_size': 10000}, 229.37301720799587)
INFO flwr 2024-04-16 21:47:15,265 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 21:47:15,265 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:47:26,289 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 21:47:27,442 | server.py:125 | fit progress: (10, 2.3005287647247314, {'accuracy': 0.2472, 'data_size': 10000}, 241.55095333699865)
INFO flwr 2024-04-16 21:47:27,443 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 21:47:27,443 | server.py:153 | FL finished in 241.5514132669996
INFO flwr 2024-04-16 21:47:27,443 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 21:47:27,443 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 21:47:27,443 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 21:47:27,443 | app.py:229 | app_fit: losses_centralized [(0, 2.302610397338867), (1, 2.3025448322296143), (2, 2.302445650100708), (3, 2.302323341369629), (4, 2.302171230316162), (5, 2.3019909858703613), (6, 2.3017776012420654), (7, 2.3015265464782715), (8, 2.301236152648926), (9, 2.3009049892425537), (10, 2.3005287647247314)]
INFO flwr 2024-04-16 21:47:27,444 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0977), (1, 0.099), (2, 0.1011), (3, 0.1044), (4, 0.1104), (5, 0.1235), (6, 0.1479), (7, 0.1811), (8, 0.2089), (9, 0.2306), (10, 0.2472)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2472
wandb:     loss 2.30053
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_214227-ulv3viv0
wandb: Find logs at: ./wandb/offline-run-20240416_214227-ulv3viv0/logs
INFO flwr 2024-04-16 21:47:30,967 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 21:54:40,110 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=162419)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=162419)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 21:54:45,955	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 21:54:47,205	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 21:54:47,687	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 21:54:48,041	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a385f76ed93fe45f.zip' (36.38MiB) to Ray cluster...
2024-04-16 21:54:48,146	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a385f76ed93fe45f.zip'.
INFO flwr 2024-04-16 21:54:59,379 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 70477846118.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 154448307610.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 21:54:59,380 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 21:54:59,380 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 21:54:59,398 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 21:54:59,399 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 21:54:59,399 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 21:54:59,399 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 21:55:01,986 | server.py:94 | initial parameters (loss, other metrics): 2.302450656890869, {'accuracy': 0.1029, 'data_size': 10000}
INFO flwr 2024-04-16 21:55:01,987 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 21:55:01,987 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=167787)[0m 2024-04-16 21:55:05.745711: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=167787)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=167787)[0m 2024-04-16 21:55:08.273450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=167787)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=167787)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=167785)[0m 2024-04-16 21:55:05.834203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=167785)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=167785)[0m 2024-04-16 21:55:08.273451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 21:55:28,370 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 21:55:29,790 | server.py:125 | fit progress: (1, 2.302443265914917, {'accuracy': 0.1029, 'data_size': 10000}, 27.802890020997438)
INFO flwr 2024-04-16 21:55:29,790 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 21:55:29,790 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:55:41,260 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 21:55:42,738 | server.py:125 | fit progress: (2, 2.302433967590332, {'accuracy': 0.1029, 'data_size': 10000}, 40.750716736998584)
INFO flwr 2024-04-16 21:55:42,738 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 21:55:42,738 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:55:53,560 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 21:55:54,759 | server.py:125 | fit progress: (3, 2.302422523498535, {'accuracy': 0.1031, 'data_size': 10000}, 52.77229344499938)
INFO flwr 2024-04-16 21:55:54,759 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 21:55:54,760 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:56:05,587 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 21:56:07,005 | server.py:125 | fit progress: (4, 2.3024110794067383, {'accuracy': 0.1031, 'data_size': 10000}, 65.01841493899701)
INFO flwr 2024-04-16 21:56:07,006 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 21:56:07,006 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:56:17,278 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 21:56:18,464 | server.py:125 | fit progress: (5, 2.302398920059204, {'accuracy': 0.1031, 'data_size': 10000}, 76.47738926600141)
INFO flwr 2024-04-16 21:56:18,465 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 21:56:18,465 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:56:29,390 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 21:56:30,808 | server.py:125 | fit progress: (6, 2.302384376525879, {'accuracy': 0.1034, 'data_size': 10000}, 88.82127619899984)
INFO flwr 2024-04-16 21:56:30,808 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 21:56:30,809 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:56:41,717 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 21:56:42,920 | server.py:125 | fit progress: (7, 2.30237078666687, {'accuracy': 0.1034, 'data_size': 10000}, 100.93329131299834)
INFO flwr 2024-04-16 21:56:42,920 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 21:56:42,921 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:56:53,264 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 21:56:54,660 | server.py:125 | fit progress: (8, 2.302356481552124, {'accuracy': 0.1036, 'data_size': 10000}, 112.67306107100012)
INFO flwr 2024-04-16 21:56:54,660 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 21:56:54,661 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:57:05,546 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 21:57:06,983 | server.py:125 | fit progress: (9, 2.3023416996002197, {'accuracy': 0.1037, 'data_size': 10000}, 124.99581041499914)
INFO flwr 2024-04-16 21:57:06,983 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 21:57:06,983 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 21:57:17,870 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 21:57:19,278 | server.py:125 | fit progress: (10, 2.3023269176483154, {'accuracy': 0.1037, 'data_size': 10000}, 137.29134237299877)
INFO flwr 2024-04-16 21:57:19,279 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 21:57:19,279 | server.py:153 | FL finished in 137.29177800499747
INFO flwr 2024-04-16 21:57:19,279 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 21:57:19,279 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 21:57:19,279 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 21:57:19,279 | app.py:229 | app_fit: losses_centralized [(0, 2.302450656890869), (1, 2.302443265914917), (2, 2.302433967590332), (3, 2.302422523498535), (4, 2.3024110794067383), (5, 2.302398920059204), (6, 2.302384376525879), (7, 2.30237078666687), (8, 2.302356481552124), (9, 2.3023416996002197), (10, 2.3023269176483154)]
INFO flwr 2024-04-16 21:57:19,279 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1029), (1, 0.1029), (2, 0.1029), (3, 0.1031), (4, 0.1031), (5, 0.1031), (6, 0.1034), (7, 0.1034), (8, 0.1036), (9, 0.1037), (10, 0.1037)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1037
wandb:     loss 2.30233
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_215439-ypf32t03
wandb: Find logs at: ./wandb/offline-run-20240416_215439-ypf32t03/logs
INFO flwr 2024-04-16 21:57:22,830 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 22:04:32,793 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=167785)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=167785)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 22:04:37,690	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 22:04:38,488	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 22:04:38,954	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 22:04:39,337	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_72f1047d87ccc43f.zip' (36.39MiB) to Ray cluster...
2024-04-16 22:04:39,441	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_72f1047d87ccc43f.zip'.
INFO flwr 2024-04-16 22:04:51,494 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 70161568972.0, 'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 153710327604.0}
INFO flwr 2024-04-16 22:04:51,494 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 22:04:51,494 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 22:04:51,514 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 22:04:51,516 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 22:04:51,516 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 22:04:51,516 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 22:04:54,198 | server.py:94 | initial parameters (loss, other metrics): 2.3023674488067627, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-16 22:04:54,199 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 22:04:54,199 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=172183)[0m 2024-04-16 22:04:58.018384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=172183)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=172183)[0m 2024-04-16 22:05:00.456312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=172182)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=172182)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=172182)[0m 2024-04-16 22:04:58.230881: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=172182)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=172182)[0m 2024-04-16 22:05:00.677573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 22:05:23,198 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 22:05:24,688 | server.py:125 | fit progress: (1, 2.136406898498535, {'accuracy': 0.3247, 'data_size': 10000}, 30.488669323000067)
INFO flwr 2024-04-16 22:05:24,688 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 22:05:24,688 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:05:39,915 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 22:05:41,223 | server.py:125 | fit progress: (2, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 47.023734529997455)
INFO flwr 2024-04-16 22:05:41,223 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 22:05:41,223 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:05:55,575 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 22:05:57,100 | server.py:125 | fit progress: (3, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 62.90118594299565)
INFO flwr 2024-04-16 22:05:57,101 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 22:05:57,101 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:06:12,483 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 22:06:14,010 | server.py:125 | fit progress: (4, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 79.81122866499936)
INFO flwr 2024-04-16 22:06:14,011 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 22:06:14,011 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:06:29,006 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 22:06:30,509 | server.py:125 | fit progress: (5, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 96.3100062079975)
INFO flwr 2024-04-16 22:06:30,509 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 22:06:30,510 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:06:44,793 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 22:06:46,093 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 111.89437045899831)
INFO flwr 2024-04-16 22:06:46,094 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 22:06:46,094 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:07:00,170 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 22:07:01,657 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 127.45805379299418)
INFO flwr 2024-04-16 22:07:01,657 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 22:07:01,658 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:07:16,365 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 22:07:17,652 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 143.4530734899963)
INFO flwr 2024-04-16 22:07:17,652 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 22:07:17,653 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:07:30,635 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 22:07:32,220 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 158.0210466130011)
INFO flwr 2024-04-16 22:07:32,220 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 22:07:32,221 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:07:45,884 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 22:07:47,189 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 172.99030392599525)
INFO flwr 2024-04-16 22:07:47,190 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 22:07:47,190 | server.py:153 | FL finished in 172.99082550699677
INFO flwr 2024-04-16 22:07:47,190 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 22:07:47,191 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 22:07:47,191 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 22:07:47,191 | app.py:229 | app_fit: losses_centralized [(0, 2.3023674488067627), (1, 2.136406898498535), (2, 2.358342170715332), (3, 2.358342170715332), (4, 2.358342170715332), (5, 2.358342170715332), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-16 22:07:47,191 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.3247), (2, 0.1028), (3, 0.1028), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_220432-bku7kg4c
wandb: Find logs at: ./wandb/offline-run-20240416_220432-bku7kg4c/logs
INFO flwr 2024-04-16 22:07:50,891 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 22:15:00,573 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=172169)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=172169)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 22:15:05,015	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 22:15:06,051	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 22:15:06,505	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 22:15:06,876	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_300be4e2d69e962f.zip' (36.40MiB) to Ray cluster...
2024-04-16 22:15:06,987	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_300be4e2d69e962f.zip'.
INFO flwr 2024-04-16 22:15:18,144 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 70240183910.0, 'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 153893762458.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 22:15:18,145 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 22:15:18,145 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 22:15:18,165 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 22:15:18,166 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 22:15:18,166 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 22:15:18,166 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 22:15:21,082 | server.py:94 | initial parameters (loss, other metrics): 2.3025574684143066, {'accuracy': 0.1282, 'data_size': 10000}
INFO flwr 2024-04-16 22:15:21,082 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 22:15:21,083 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=176906)[0m 2024-04-16 22:15:24.263939: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=176906)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=176906)[0m 2024-04-16 22:15:26.605531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=176910)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=176910)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=176914)[0m 2024-04-16 22:15:24.717261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=176914)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=176914)[0m 2024-04-16 22:15:26.875350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 22:15:48,613 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 22:15:50,073 | server.py:125 | fit progress: (1, 2.2079455852508545, {'accuracy': 0.5778, 'data_size': 10000}, 28.990539849000925)
INFO flwr 2024-04-16 22:15:50,074 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 22:15:50,074 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:16:04,364 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 22:16:05,583 | server.py:125 | fit progress: (2, 1.7995578050613403, {'accuracy': 0.69, 'data_size': 10000}, 44.50047965999693)
INFO flwr 2024-04-16 22:16:05,584 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 22:16:05,584 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:16:19,311 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 22:16:20,797 | server.py:125 | fit progress: (3, 1.6546605825424194, {'accuracy': 0.8075, 'data_size': 10000}, 59.71457186000043)
INFO flwr 2024-04-16 22:16:20,798 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 22:16:20,798 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:16:34,076 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 22:16:35,319 | server.py:125 | fit progress: (4, 1.5685423612594604, {'accuracy': 0.8917, 'data_size': 10000}, 74.23589447799895)
INFO flwr 2024-04-16 22:16:35,319 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 22:16:35,319 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:16:48,491 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 22:16:49,950 | server.py:125 | fit progress: (5, 1.5645365715026855, {'accuracy': 0.8961, 'data_size': 10000}, 88.8675195859978)
INFO flwr 2024-04-16 22:16:49,951 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 22:16:49,951 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:17:03,660 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 22:17:04,896 | server.py:125 | fit progress: (6, 1.551973581314087, {'accuracy': 0.9085, 'data_size': 10000}, 103.81297767700016)
INFO flwr 2024-04-16 22:17:04,896 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 22:17:04,897 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:17:18,646 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 22:17:20,128 | server.py:125 | fit progress: (7, 1.5469921827316284, {'accuracy': 0.9138, 'data_size': 10000}, 119.04544930699922)
INFO flwr 2024-04-16 22:17:20,129 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 22:17:20,129 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:17:32,954 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 22:17:34,182 | server.py:125 | fit progress: (8, 1.5358588695526123, {'accuracy': 0.9249, 'data_size': 10000}, 133.09919794799498)
INFO flwr 2024-04-16 22:17:34,182 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 22:17:34,183 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:17:48,301 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 22:17:49,737 | server.py:125 | fit progress: (9, 1.5250792503356934, {'accuracy': 0.9362, 'data_size': 10000}, 148.65450633799628)
INFO flwr 2024-04-16 22:17:49,738 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 22:17:49,738 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:18:04,089 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 22:18:05,352 | server.py:125 | fit progress: (10, 1.525842308998108, {'accuracy': 0.935, 'data_size': 10000}, 164.2688734559997)
INFO flwr 2024-04-16 22:18:05,352 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 22:18:05,352 | server.py:153 | FL finished in 164.26964293699712
INFO flwr 2024-04-16 22:18:05,353 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 22:18:05,353 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 22:18:05,353 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 22:18:05,353 | app.py:229 | app_fit: losses_centralized [(0, 2.3025574684143066), (1, 2.2079455852508545), (2, 1.7995578050613403), (3, 1.6546605825424194), (4, 1.5685423612594604), (5, 1.5645365715026855), (6, 1.551973581314087), (7, 1.5469921827316284), (8, 1.5358588695526123), (9, 1.5250792503356934), (10, 1.525842308998108)]
INFO flwr 2024-04-16 22:18:05,353 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1282), (1, 0.5778), (2, 0.69), (3, 0.8075), (4, 0.8917), (5, 0.8961), (6, 0.9085), (7, 0.9138), (8, 0.9249), (9, 0.9362), (10, 0.935)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.935
wandb:     loss 1.52584
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_221500-gldha60l
wandb: Find logs at: ./wandb/offline-run-20240416_221500-gldha60l/logs
INFO flwr 2024-04-16 22:18:08,923 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 22:25:18,736 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=176899)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=176899)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 22:25:23,338	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 22:25:24,252	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 22:25:24,703	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 22:25:25,063	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1d9caee16bc566bd.zip' (36.41MiB) to Ray cluster...
2024-04-16 22:25:25,187	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1d9caee16bc566bd.zip'.
INFO flwr 2024-04-16 22:25:37,123 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 70167375052.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'memory': 153723875124.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-16 22:25:37,124 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 22:25:37,124 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 22:25:37,146 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 22:25:37,147 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 22:25:37,148 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 22:25:37,148 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 22:25:40,361 | server.py:94 | initial parameters (loss, other metrics): 2.3025667667388916, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-16 22:25:40,361 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 22:25:40,362 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=181304)[0m 2024-04-16 22:25:43.418378: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=181304)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=181304)[0m 2024-04-16 22:25:45.790625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=181301)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=181301)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=181305)[0m 2024-04-16 22:25:43.699030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=181305)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=181305)[0m 2024-04-16 22:25:45.987588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 22:26:08,615 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 22:26:10,109 | server.py:125 | fit progress: (1, 2.301652431488037, {'accuracy': 0.1071, 'data_size': 10000}, 29.74740911599656)
INFO flwr 2024-04-16 22:26:10,110 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 22:26:10,110 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:26:26,394 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 22:26:27,627 | server.py:125 | fit progress: (2, 2.299135684967041, {'accuracy': 0.2381, 'data_size': 10000}, 47.264708581999)
INFO flwr 2024-04-16 22:26:27,627 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 22:26:27,627 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:26:41,631 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 22:26:43,116 | server.py:125 | fit progress: (3, 2.293328285217285, {'accuracy': 0.4377, 'data_size': 10000}, 62.75422325600084)
INFO flwr 2024-04-16 22:26:43,116 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 22:26:43,117 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:26:56,519 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 22:26:58,007 | server.py:125 | fit progress: (4, 2.2815911769866943, {'accuracy': 0.602, 'data_size': 10000}, 77.64475225999922)
INFO flwr 2024-04-16 22:26:58,007 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 22:26:58,007 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:27:11,581 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 22:27:13,088 | server.py:125 | fit progress: (5, 2.2595837116241455, {'accuracy': 0.6824, 'data_size': 10000}, 92.72629658300139)
INFO flwr 2024-04-16 22:27:13,089 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 22:27:13,089 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:27:26,184 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 22:27:27,419 | server.py:125 | fit progress: (6, 2.220710039138794, {'accuracy': 0.7173, 'data_size': 10000}, 107.05683630899875)
INFO flwr 2024-04-16 22:27:27,419 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 22:27:27,419 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:27:40,997 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 22:27:42,473 | server.py:125 | fit progress: (7, 2.157557725906372, {'accuracy': 0.7425, 'data_size': 10000}, 122.11085707399616)
INFO flwr 2024-04-16 22:27:42,473 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 22:27:42,473 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:27:56,515 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 22:27:57,750 | server.py:125 | fit progress: (8, 2.068589925765991, {'accuracy': 0.7638, 'data_size': 10000}, 137.388295206998)
INFO flwr 2024-04-16 22:27:57,750 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 22:27:57,751 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:28:11,159 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 22:28:12,634 | server.py:125 | fit progress: (9, 1.9654020071029663, {'accuracy': 0.784, 'data_size': 10000}, 152.2722174390001)
INFO flwr 2024-04-16 22:28:12,634 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 22:28:12,635 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:28:26,355 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 22:28:27,828 | server.py:125 | fit progress: (10, 1.8650718927383423, {'accuracy': 0.8019, 'data_size': 10000}, 167.4660064130003)
INFO flwr 2024-04-16 22:28:27,828 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 22:28:27,828 | server.py:153 | FL finished in 167.4664846739979
INFO flwr 2024-04-16 22:28:27,828 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 22:28:27,829 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 22:28:27,829 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 22:28:27,829 | app.py:229 | app_fit: losses_centralized [(0, 2.3025667667388916), (1, 2.301652431488037), (2, 2.299135684967041), (3, 2.293328285217285), (4, 2.2815911769866943), (5, 2.2595837116241455), (6, 2.220710039138794), (7, 2.157557725906372), (8, 2.068589925765991), (9, 1.9654020071029663), (10, 1.8650718927383423)]
INFO flwr 2024-04-16 22:28:27,829 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.1071), (2, 0.2381), (3, 0.4377), (4, 0.602), (5, 0.6824), (6, 0.7173), (7, 0.7425), (8, 0.7638), (9, 0.784), (10, 0.8019)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8019
wandb:     loss 1.86507
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_222518-kjvhbyjp
wandb: Find logs at: ./wandb/offline-run-20240416_222518-kjvhbyjp/logs
INFO flwr 2024-04-16 22:28:31,390 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 22:35:40,973 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=181296)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=181296)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 22:35:45,369	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 22:35:46,265	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 22:35:46,771	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 22:35:47,140	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_95af2b527765a3db.zip' (36.42MiB) to Ray cluster...
2024-04-16 22:35:47,252	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_95af2b527765a3db.zip'.
INFO flwr 2024-04-16 22:35:58,413 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 153758573978.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 70182245990.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-16 22:35:58,413 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 22:35:58,413 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 22:35:58,431 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 22:35:58,432 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 22:35:58,433 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 22:35:58,433 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 22:36:01,138 | server.py:94 | initial parameters (loss, other metrics): 2.3027219772338867, {'accuracy': 0.0744, 'data_size': 10000}
INFO flwr 2024-04-16 22:36:01,138 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 22:36:01,139 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=185662)[0m 2024-04-16 22:36:04.551440: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=185662)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=185662)[0m 2024-04-16 22:36:06.865539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=185666)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=185666)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=185659)[0m 2024-04-16 22:36:04.876982: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=185659)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=185659)[0m 2024-04-16 22:36:07.266075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 22:36:28,417 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 22:36:29,921 | server.py:125 | fit progress: (1, 2.302639961242676, {'accuracy': 0.0822, 'data_size': 10000}, 28.78198311199958)
INFO flwr 2024-04-16 22:36:29,921 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 22:36:29,921 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:36:44,471 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 22:36:45,779 | server.py:125 | fit progress: (2, 2.3025200366973877, {'accuracy': 0.0908, 'data_size': 10000}, 44.64077507000184)
INFO flwr 2024-04-16 22:36:45,780 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 22:36:45,780 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:37:00,514 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 22:37:02,036 | server.py:125 | fit progress: (3, 2.3023695945739746, {'accuracy': 0.1009, 'data_size': 10000}, 60.89771702100552)
INFO flwr 2024-04-16 22:37:02,037 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 22:37:02,037 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:37:15,995 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 22:37:17,522 | server.py:125 | fit progress: (4, 2.302184581756592, {'accuracy': 0.1134, 'data_size': 10000}, 76.38359908200073)
INFO flwr 2024-04-16 22:37:17,523 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 22:37:17,523 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:37:31,191 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 22:37:32,652 | server.py:125 | fit progress: (5, 2.30196475982666, {'accuracy': 0.1252, 'data_size': 10000}, 91.51371734800341)
INFO flwr 2024-04-16 22:37:32,653 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 22:37:32,653 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:37:46,966 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 22:37:48,262 | server.py:125 | fit progress: (6, 2.3017053604125977, {'accuracy': 0.1391, 'data_size': 10000}, 107.12364733300637)
INFO flwr 2024-04-16 22:37:48,263 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 22:37:48,263 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:38:01,935 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 22:38:03,444 | server.py:125 | fit progress: (7, 2.3014025688171387, {'accuracy': 0.1528, 'data_size': 10000}, 122.30504258000292)
INFO flwr 2024-04-16 22:38:03,444 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 22:38:03,444 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:38:19,830 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 22:38:21,131 | server.py:125 | fit progress: (8, 2.301058292388916, {'accuracy': 0.165, 'data_size': 10000}, 139.99212310500297)
INFO flwr 2024-04-16 22:38:21,131 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 22:38:21,131 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:38:38,275 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 22:38:39,836 | server.py:125 | fit progress: (9, 2.300664186477661, {'accuracy': 0.1782, 'data_size': 10000}, 158.6971948920036)
INFO flwr 2024-04-16 22:38:39,836 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 22:38:39,836 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:38:53,895 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 22:38:55,160 | server.py:125 | fit progress: (10, 2.3002114295959473, {'accuracy': 0.1903, 'data_size': 10000}, 174.02174090000335)
INFO flwr 2024-04-16 22:38:55,161 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 22:38:55,161 | server.py:153 | FL finished in 174.0222191110006
INFO flwr 2024-04-16 22:38:55,161 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 22:38:55,161 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 22:38:55,161 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 22:38:55,161 | app.py:229 | app_fit: losses_centralized [(0, 2.3027219772338867), (1, 2.302639961242676), (2, 2.3025200366973877), (3, 2.3023695945739746), (4, 2.302184581756592), (5, 2.30196475982666), (6, 2.3017053604125977), (7, 2.3014025688171387), (8, 2.301058292388916), (9, 2.300664186477661), (10, 2.3002114295959473)]
INFO flwr 2024-04-16 22:38:55,162 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0744), (1, 0.0822), (2, 0.0908), (3, 0.1009), (4, 0.1134), (5, 0.1252), (6, 0.1391), (7, 0.1528), (8, 0.165), (9, 0.1782), (10, 0.1903)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1903
wandb:     loss 2.30021
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_223540-x1ncwsei
wandb: Find logs at: ./wandb/offline-run-20240416_223540-x1ncwsei/logs
INFO flwr 2024-04-16 22:38:58,827 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 22:46:08,639 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=185657)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=185657)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 22:46:13,014	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 22:46:13,829	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 22:46:14,375	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 22:46:14,748	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_be57136414af79c3.zip' (36.43MiB) to Ray cluster...
2024-04-16 22:46:14,867	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_be57136414af79c3.zip'.
INFO flwr 2024-04-16 22:46:26,018 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 70181514854.0, 'CPU': 64.0, 'memory': 153756867994.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-16 22:46:26,018 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 22:46:26,018 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 22:46:26,034 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 22:46:26,035 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 22:46:26,036 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 22:46:26,036 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 22:46:28,645 | server.py:94 | initial parameters (loss, other metrics): 2.302467107772827, {'accuracy': 0.1042, 'data_size': 10000}
INFO flwr 2024-04-16 22:46:28,645 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 22:46:28,646 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=190401)[0m 2024-04-16 22:46:32.199349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=190401)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=190401)[0m 2024-04-16 22:46:34.465771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=190397)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=190397)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=190396)[0m 2024-04-16 22:46:32.327051: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=190396)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=190396)[0m 2024-04-16 22:46:34.643067: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 22:46:56,857 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 22:46:58,330 | server.py:125 | fit progress: (1, 2.3024606704711914, {'accuracy': 0.1041, 'data_size': 10000}, 29.684079001999635)
INFO flwr 2024-04-16 22:46:58,330 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 22:46:58,330 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:47:12,599 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 22:47:13,844 | server.py:125 | fit progress: (2, 2.3024516105651855, {'accuracy': 0.1042, 'data_size': 10000}, 45.198636912995426)
INFO flwr 2024-04-16 22:47:13,844 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 22:47:13,845 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:47:27,815 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 22:47:29,320 | server.py:125 | fit progress: (3, 2.302441120147705, {'accuracy': 0.104, 'data_size': 10000}, 60.67417567299708)
INFO flwr 2024-04-16 22:47:29,320 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 22:47:29,320 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:47:43,259 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 22:47:44,771 | server.py:125 | fit progress: (4, 2.3024303913116455, {'accuracy': 0.104, 'data_size': 10000}, 76.12583215699851)
INFO flwr 2024-04-16 22:47:44,772 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 22:47:44,772 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:47:57,231 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 22:47:58,689 | server.py:125 | fit progress: (5, 2.3024179935455322, {'accuracy': 0.1042, 'data_size': 10000}, 90.04342393200204)
INFO flwr 2024-04-16 22:47:58,689 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 22:47:58,689 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:48:11,924 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 22:48:13,196 | server.py:125 | fit progress: (6, 2.3024046421051025, {'accuracy': 0.1043, 'data_size': 10000}, 104.55074719199911)
INFO flwr 2024-04-16 22:48:13,196 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 22:48:13,197 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:48:27,194 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 22:48:28,675 | server.py:125 | fit progress: (7, 2.3023903369903564, {'accuracy': 0.1044, 'data_size': 10000}, 120.02974650599936)
INFO flwr 2024-04-16 22:48:28,675 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 22:48:28,676 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:48:42,442 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 22:48:43,705 | server.py:125 | fit progress: (8, 2.3023760318756104, {'accuracy': 0.1048, 'data_size': 10000}, 135.0592581929959)
INFO flwr 2024-04-16 22:48:43,705 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 22:48:43,705 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:48:59,371 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 22:49:00,870 | server.py:125 | fit progress: (9, 2.302361488342285, {'accuracy': 0.1051, 'data_size': 10000}, 152.2240903580023)
INFO flwr 2024-04-16 22:49:00,870 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 22:49:00,870 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:49:15,407 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 22:49:16,664 | server.py:125 | fit progress: (10, 2.3023459911346436, {'accuracy': 0.1054, 'data_size': 10000}, 168.01817511399713)
INFO flwr 2024-04-16 22:49:16,664 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 22:49:16,664 | server.py:153 | FL finished in 168.01867300800222
INFO flwr 2024-04-16 22:49:16,664 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 22:49:16,664 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 22:49:16,665 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 22:49:16,665 | app.py:229 | app_fit: losses_centralized [(0, 2.302467107772827), (1, 2.3024606704711914), (2, 2.3024516105651855), (3, 2.302441120147705), (4, 2.3024303913116455), (5, 2.3024179935455322), (6, 2.3024046421051025), (7, 2.3023903369903564), (8, 2.3023760318756104), (9, 2.302361488342285), (10, 2.3023459911346436)]
INFO flwr 2024-04-16 22:49:16,665 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1042), (1, 0.1041), (2, 0.1042), (3, 0.104), (4, 0.104), (5, 0.1042), (6, 0.1043), (7, 0.1044), (8, 0.1048), (9, 0.1051), (10, 0.1054)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1054
wandb:     loss 2.30235
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_224608-jchn0sn7
wandb: Find logs at: ./wandb/offline-run-20240416_224608-jchn0sn7/logs
INFO flwr 2024-04-16 22:49:20,217 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 22:56:29,901 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=190395)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=190395)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 22:56:35,775	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 22:56:36,560	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 22:56:37,026	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 22:56:37,394	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c5eb7e7d6433d260.zip' (36.45MiB) to Ray cluster...
2024-04-16 22:56:37,511	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c5eb7e7d6433d260.zip'.
INFO flwr 2024-04-16 22:56:48,726 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 70074569932.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 153507329844.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-16 22:56:48,726 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 22:56:48,726 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 22:56:48,744 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 22:56:48,745 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 22:56:48,746 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 22:56:48,746 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 22:56:51,588 | server.py:94 | initial parameters (loss, other metrics): 2.3024063110351562, {'accuracy': 0.1184, 'data_size': 10000}
INFO flwr 2024-04-16 22:56:51,589 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 22:56:51,590 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=194769)[0m 2024-04-16 22:56:54.817295: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=194769)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=194769)[0m 2024-04-16 22:56:57.158174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=194773)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=194773)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=194761)[0m 2024-04-16 22:56:55.030963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=194761)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=194761)[0m 2024-04-16 22:56:57.300669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 22:57:20,842 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 22:57:22,313 | server.py:125 | fit progress: (1, 1.9611769914627075, {'accuracy': 0.4997, 'data_size': 10000}, 30.72308061199874)
INFO flwr 2024-04-16 22:57:22,313 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 22:57:22,313 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:57:37,078 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 22:57:38,336 | server.py:125 | fit progress: (2, 2.3644421100616455, {'accuracy': 0.0967, 'data_size': 10000}, 46.746582968997245)
INFO flwr 2024-04-16 22:57:38,336 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 22:57:38,337 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:57:52,433 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 22:57:53,958 | server.py:125 | fit progress: (3, 2.279043197631836, {'accuracy': 0.1821, 'data_size': 10000}, 62.36872227099957)
INFO flwr 2024-04-16 22:57:53,958 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 22:57:53,959 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:58:07,320 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 22:58:08,790 | server.py:125 | fit progress: (4, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 77.20073795699864)
INFO flwr 2024-04-16 22:58:08,790 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 22:58:08,791 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:58:22,265 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 22:58:23,797 | server.py:125 | fit progress: (5, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 92.20719698000175)
INFO flwr 2024-04-16 22:58:23,797 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 22:58:23,797 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:58:38,166 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 22:58:39,415 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 107.8252120899997)
INFO flwr 2024-04-16 22:58:39,415 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 22:58:39,415 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:58:53,607 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 22:58:55,112 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 123.52229532099591)
INFO flwr 2024-04-16 22:58:55,112 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 22:58:55,112 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:59:08,600 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 22:59:09,854 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 138.26441073499882)
INFO flwr 2024-04-16 22:59:09,854 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 22:59:09,854 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:59:23,882 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 22:59:25,408 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 153.81899229899864)
INFO flwr 2024-04-16 22:59:25,409 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 22:59:25,409 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 22:59:39,075 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 22:59:40,398 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 168.8086134719997)
INFO flwr 2024-04-16 22:59:40,398 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 22:59:40,398 | server.py:153 | FL finished in 168.80902944599802
INFO flwr 2024-04-16 22:59:40,399 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 22:59:40,399 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 22:59:40,399 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 22:59:40,399 | app.py:229 | app_fit: losses_centralized [(0, 2.3024063110351562), (1, 1.9611769914627075), (2, 2.3644421100616455), (3, 2.279043197631836), (4, 2.358342170715332), (5, 2.358342170715332), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-16 22:59:40,399 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1184), (1, 0.4997), (2, 0.0967), (3, 0.1821), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_225629-fyvc9x0w
wandb: Find logs at: ./wandb/offline-run-20240416_225629-fyvc9x0w/logs
INFO flwr 2024-04-16 22:59:44,031 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 23:06:53,588 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=194761)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=194761)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 23:06:58,329	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 23:06:59,166	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 23:06:59,622	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 23:06:59,987	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_99db56760a76c678.zip' (36.46MiB) to Ray cluster...
2024-04-16 23:07:00,101	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_99db56760a76c678.zip'.
INFO flwr 2024-04-16 23:07:17,864 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 153462716212.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'object_store_memory': 70055449804.0, 'GPU': 1.0}
INFO flwr 2024-04-16 23:07:17,864 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 23:07:17,865 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 23:07:17,884 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 23:07:17,885 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 23:07:17,885 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 23:07:17,885 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 23:07:22,211 | server.py:94 | initial parameters (loss, other metrics): 2.3024022579193115, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-16 23:07:22,212 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 23:07:22,212 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=199173)[0m 2024-04-16 23:07:24.167704: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=199173)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=199167)[0m 2024-04-16 23:07:26.643222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=199174)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=199174)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=199172)[0m 2024-04-16 23:07:24.521193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=199172)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=199174)[0m 2024-04-16 23:07:26.806850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 23:07:49,543 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 23:07:51,063 | server.py:125 | fit progress: (1, 2.1979005336761475, {'accuracy': 0.588, 'data_size': 10000}, 28.851359658998263)
INFO flwr 2024-04-16 23:07:51,064 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 23:07:51,064 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:08:05,773 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 23:08:07,053 | server.py:125 | fit progress: (2, 1.7873204946517944, {'accuracy': 0.6949, 'data_size': 10000}, 44.84136929899978)
INFO flwr 2024-04-16 23:08:07,054 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 23:08:07,054 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:08:22,203 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 23:08:23,731 | server.py:125 | fit progress: (3, 1.6537017822265625, {'accuracy': 0.8113, 'data_size': 10000}, 61.51950125300209)
INFO flwr 2024-04-16 23:08:23,732 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 23:08:23,732 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:08:38,089 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 23:08:39,592 | server.py:125 | fit progress: (4, 1.5796152353286743, {'accuracy': 0.8805, 'data_size': 10000}, 77.37994871500268)
INFO flwr 2024-04-16 23:08:39,592 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 23:08:39,592 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:08:53,146 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 23:08:54,661 | server.py:125 | fit progress: (5, 1.5599212646484375, {'accuracy': 0.9008, 'data_size': 10000}, 92.44930880000175)
INFO flwr 2024-04-16 23:08:54,661 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 23:08:54,662 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:09:11,223 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 23:09:12,462 | server.py:125 | fit progress: (6, 1.5488014221191406, {'accuracy': 0.9123, 'data_size': 10000}, 110.25028771699726)
INFO flwr 2024-04-16 23:09:12,463 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 23:09:12,463 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:09:26,685 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 23:09:28,349 | server.py:125 | fit progress: (7, 1.541246771812439, {'accuracy': 0.92, 'data_size': 10000}, 126.13674758299749)
INFO flwr 2024-04-16 23:09:28,349 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 23:09:28,349 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:09:42,436 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 23:09:43,789 | server.py:125 | fit progress: (8, 1.5441018342971802, {'accuracy': 0.9165, 'data_size': 10000}, 141.57708644000377)
INFO flwr 2024-04-16 23:09:43,789 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 23:09:43,789 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:09:58,882 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 23:10:00,494 | server.py:125 | fit progress: (9, 1.5432711839675903, {'accuracy': 0.9178, 'data_size': 10000}, 158.28254367499903)
INFO flwr 2024-04-16 23:10:00,495 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 23:10:00,495 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:10:14,602 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 23:10:15,942 | server.py:125 | fit progress: (10, 1.5513432025909424, {'accuracy': 0.9096, 'data_size': 10000}, 173.72993078199943)
INFO flwr 2024-04-16 23:10:15,942 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 23:10:15,942 | server.py:153 | FL finished in 173.73037783100153
INFO flwr 2024-04-16 23:10:15,942 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 23:10:15,943 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 23:10:15,943 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 23:10:15,943 | app.py:229 | app_fit: losses_centralized [(0, 2.3024022579193115), (1, 2.1979005336761475), (2, 1.7873204946517944), (3, 1.6537017822265625), (4, 1.5796152353286743), (5, 1.5599212646484375), (6, 1.5488014221191406), (7, 1.541246771812439), (8, 1.5441018342971802), (9, 1.5432711839675903), (10, 1.5513432025909424)]
INFO flwr 2024-04-16 23:10:15,943 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.588), (2, 0.6949), (3, 0.8113), (4, 0.8805), (5, 0.9008), (6, 0.9123), (7, 0.92), (8, 0.9165), (9, 0.9178), (10, 0.9096)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9096
wandb:     loss 1.55134
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_230653-yme7qlxf
wandb: Find logs at: ./wandb/offline-run-20240416_230653-yme7qlxf/logs
INFO flwr 2024-04-16 23:10:19,624 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 23:17:28,035 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=199162)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=199162)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 23:17:32,373	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 23:17:33,205	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 23:17:33,641	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 23:17:33,990	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0e917e441364d610.zip' (36.47MiB) to Ray cluster...
2024-04-16 23:17:34,108	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0e917e441364d610.zip'.
INFO flwr 2024-04-16 23:17:45,147 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72189200793.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 158441468519.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 23:17:45,148 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 23:17:45,148 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 23:17:45,166 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 23:17:45,167 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 23:17:45,168 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 23:17:45,168 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 23:17:48,577 | server.py:94 | initial parameters (loss, other metrics): 2.3025107383728027, {'accuracy': 0.0756, 'data_size': 10000}
INFO flwr 2024-04-16 23:17:48,577 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 23:17:48,577 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=211088)[0m 2024-04-16 23:17:51.193048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=211088)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=211083)[0m 2024-04-16 23:17:53.528511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=211088)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=211088)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=211089)[0m 2024-04-16 23:17:51.376111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=211089)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=211089)[0m 2024-04-16 23:17:53.749769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 23:18:15,225 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 23:18:16,705 | server.py:125 | fit progress: (1, 2.3015944957733154, {'accuracy': 0.1378, 'data_size': 10000}, 28.128113185004622)
INFO flwr 2024-04-16 23:18:16,706 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 23:18:16,706 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:18:31,527 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 23:18:33,002 | server.py:125 | fit progress: (2, 2.2991528511047363, {'accuracy': 0.2343, 'data_size': 10000}, 44.42480969100143)
INFO flwr 2024-04-16 23:18:33,002 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 23:18:33,003 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:18:46,123 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 23:18:47,346 | server.py:125 | fit progress: (3, 2.293726921081543, {'accuracy': 0.4891, 'data_size': 10000}, 58.76895462399989)
INFO flwr 2024-04-16 23:18:47,347 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 23:18:47,347 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:19:00,496 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 23:19:01,988 | server.py:125 | fit progress: (4, 2.28287672996521, {'accuracy': 0.6438, 'data_size': 10000}, 73.41057505600475)
INFO flwr 2024-04-16 23:19:01,988 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 23:19:01,988 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:19:16,518 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 23:19:17,734 | server.py:125 | fit progress: (5, 2.262690782546997, {'accuracy': 0.6925, 'data_size': 10000}, 89.15711133500008)
INFO flwr 2024-04-16 23:19:17,735 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 23:19:17,735 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:19:30,856 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 23:19:32,337 | server.py:125 | fit progress: (6, 2.226719856262207, {'accuracy': 0.714, 'data_size': 10000}, 103.75984419700399)
INFO flwr 2024-04-16 23:19:32,337 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 23:19:32,338 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:19:45,397 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 23:19:46,624 | server.py:125 | fit progress: (7, 2.1660637855529785, {'accuracy': 0.7288, 'data_size': 10000}, 118.04678739800147)
INFO flwr 2024-04-16 23:19:46,624 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 23:19:46,625 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:20:00,743 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 23:20:02,227 | server.py:125 | fit progress: (8, 2.077160358428955, {'accuracy': 0.7466, 'data_size': 10000}, 133.64984822700353)
INFO flwr 2024-04-16 23:20:02,227 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 23:20:02,228 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:20:14,551 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 23:20:15,991 | server.py:125 | fit progress: (9, 1.9702264070510864, {'accuracy': 0.7612, 'data_size': 10000}, 147.4136024610052)
INFO flwr 2024-04-16 23:20:15,991 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 23:20:15,991 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:20:30,512 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 23:20:31,989 | server.py:125 | fit progress: (10, 1.8697290420532227, {'accuracy': 0.7768, 'data_size': 10000}, 163.41188183600025)
INFO flwr 2024-04-16 23:20:31,989 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 23:20:31,990 | server.py:153 | FL finished in 163.41232213900366
INFO flwr 2024-04-16 23:20:31,990 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 23:20:31,990 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 23:20:31,990 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 23:20:31,990 | app.py:229 | app_fit: losses_centralized [(0, 2.3025107383728027), (1, 2.3015944957733154), (2, 2.2991528511047363), (3, 2.293726921081543), (4, 2.28287672996521), (5, 2.262690782546997), (6, 2.226719856262207), (7, 2.1660637855529785), (8, 2.077160358428955), (9, 1.9702264070510864), (10, 1.8697290420532227)]
INFO flwr 2024-04-16 23:20:31,990 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0756), (1, 0.1378), (2, 0.2343), (3, 0.4891), (4, 0.6438), (5, 0.6925), (6, 0.714), (7, 0.7288), (8, 0.7466), (9, 0.7612), (10, 0.7768)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7768
wandb:     loss 1.86973
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_231727-b3bl23aq
wandb: Find logs at: ./wandb/offline-run-20240416_231727-b3bl23aq/logs
INFO flwr 2024-04-16 23:20:35,520 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 23:27:45,180 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=211081)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=211081)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 23:27:49,803	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 23:27:50,655	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 23:27:51,111	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 23:27:51,479	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_efef4f2e0176fab4.zip' (36.48MiB) to Ray cluster...
2024-04-16 23:27:51,590	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_efef4f2e0176fab4.zip'.
INFO flwr 2024-04-16 23:28:03,332 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 158560758375.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 72240325017.0, 'CPU': 64.0}
INFO flwr 2024-04-16 23:28:03,332 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 23:28:03,333 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 23:28:03,349 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 23:28:03,350 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 23:28:03,351 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 23:28:03,351 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 23:28:06,820 | server.py:94 | initial parameters (loss, other metrics): 2.3026223182678223, {'accuracy': 0.1402, 'data_size': 10000}
INFO flwr 2024-04-16 23:28:06,820 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 23:28:06,821 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=215472)[0m 2024-04-16 23:28:09.465580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=215472)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=215471)[0m 2024-04-16 23:28:11.869209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=215476)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=215476)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=215477)[0m 2024-04-16 23:28:09.923779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=215477)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=215477)[0m 2024-04-16 23:28:12.368029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 23:28:34,327 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 23:28:35,846 | server.py:125 | fit progress: (1, 2.302539110183716, {'accuracy': 0.1502, 'data_size': 10000}, 29.025641462998465)
INFO flwr 2024-04-16 23:28:35,847 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 23:28:35,847 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:28:50,733 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 23:28:51,987 | server.py:125 | fit progress: (2, 2.302422046661377, {'accuracy': 0.1587, 'data_size': 10000}, 45.166411571000936)
INFO flwr 2024-04-16 23:28:51,987 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 23:28:51,987 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:29:06,514 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 23:29:07,981 | server.py:125 | fit progress: (3, 2.3022758960723877, {'accuracy': 0.1705, 'data_size': 10000}, 61.16069413499645)
INFO flwr 2024-04-16 23:29:07,982 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 23:29:07,982 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:29:21,138 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 23:29:22,591 | server.py:125 | fit progress: (4, 2.3020987510681152, {'accuracy': 0.1792, 'data_size': 10000}, 75.77023175099748)
INFO flwr 2024-04-16 23:29:22,591 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 23:29:22,591 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:29:35,007 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 23:29:36,496 | server.py:125 | fit progress: (5, 2.301889181137085, {'accuracy': 0.188, 'data_size': 10000}, 89.6751628130005)
INFO flwr 2024-04-16 23:29:36,496 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 23:29:36,496 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:29:49,787 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 23:29:51,023 | server.py:125 | fit progress: (6, 2.301647186279297, {'accuracy': 0.1935, 'data_size': 10000}, 104.20261265299632)
INFO flwr 2024-04-16 23:29:51,023 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 23:29:51,024 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:30:04,598 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 23:30:06,083 | server.py:125 | fit progress: (7, 2.3013687133789062, {'accuracy': 0.1998, 'data_size': 10000}, 119.2628306849947)
INFO flwr 2024-04-16 23:30:06,084 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 23:30:06,084 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:30:19,984 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 23:30:21,239 | server.py:125 | fit progress: (8, 2.3010551929473877, {'accuracy': 0.2098, 'data_size': 10000}, 134.41847876500105)
INFO flwr 2024-04-16 23:30:21,239 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 23:30:21,240 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:30:35,495 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 23:30:36,955 | server.py:125 | fit progress: (9, 2.3007020950317383, {'accuracy': 0.2225, 'data_size': 10000}, 150.13421892000042)
INFO flwr 2024-04-16 23:30:36,955 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 23:30:36,955 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:30:49,997 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 23:30:51,480 | server.py:125 | fit progress: (10, 2.3003056049346924, {'accuracy': 0.242, 'data_size': 10000}, 164.659227266995)
INFO flwr 2024-04-16 23:30:51,480 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 23:30:51,480 | server.py:153 | FL finished in 164.65966133299662
INFO flwr 2024-04-16 23:30:51,480 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 23:30:51,481 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 23:30:51,481 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 23:30:51,481 | app.py:229 | app_fit: losses_centralized [(0, 2.3026223182678223), (1, 2.302539110183716), (2, 2.302422046661377), (3, 2.3022758960723877), (4, 2.3020987510681152), (5, 2.301889181137085), (6, 2.301647186279297), (7, 2.3013687133789062), (8, 2.3010551929473877), (9, 2.3007020950317383), (10, 2.3003056049346924)]
INFO flwr 2024-04-16 23:30:51,481 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1402), (1, 0.1502), (2, 0.1587), (3, 0.1705), (4, 0.1792), (5, 0.188), (6, 0.1935), (7, 0.1998), (8, 0.2098), (9, 0.2225), (10, 0.242)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.242
wandb:     loss 2.30031
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_232744-kklvg2rl
wandb: Find logs at: ./wandb/offline-run-20240416_232744-kklvg2rl/logs
INFO flwr 2024-04-16 23:30:55,039 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 23:38:04,545 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=215466)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=215466)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 23:38:09,042	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 23:38:09,894	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 23:38:10,349	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 23:38:10,710	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_edc4366dff0f75c2.zip' (36.49MiB) to Ray cluster...
2024-04-16 23:38:10,819	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_edc4366dff0f75c2.zip'.
INFO flwr 2024-04-16 23:38:21,947 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 158583323239.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 72249995673.0}
INFO flwr 2024-04-16 23:38:21,947 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 23:38:21,947 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 23:38:21,965 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 23:38:21,966 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 23:38:21,966 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 23:38:21,967 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 23:38:24,853 | server.py:94 | initial parameters (loss, other metrics): 2.3026084899902344, {'accuracy': 0.1026, 'data_size': 10000}
INFO flwr 2024-04-16 23:38:24,853 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 23:38:24,853 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=219840)[0m 2024-04-16 23:38:28.337291: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=219840)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=219837)[0m 2024-04-16 23:38:30.718103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=219837)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=219837)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=219835)[0m 2024-04-16 23:38:28.780758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=219835)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=219835)[0m 2024-04-16 23:38:31.313277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 23:38:52,438 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 23:38:53,969 | server.py:125 | fit progress: (1, 2.302602767944336, {'accuracy': 0.1026, 'data_size': 10000}, 29.11560645099962)
INFO flwr 2024-04-16 23:38:53,969 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 23:38:53,969 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:39:08,500 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 23:39:09,739 | server.py:125 | fit progress: (2, 2.302595376968384, {'accuracy': 0.1026, 'data_size': 10000}, 44.885496014001546)
INFO flwr 2024-04-16 23:39:09,739 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 23:39:09,739 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:39:23,846 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 23:39:25,336 | server.py:125 | fit progress: (3, 2.302586078643799, {'accuracy': 0.1026, 'data_size': 10000}, 60.483082405997266)
INFO flwr 2024-04-16 23:39:25,337 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 23:39:25,337 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:39:37,985 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 23:39:39,514 | server.py:125 | fit progress: (4, 2.3025763034820557, {'accuracy': 0.1026, 'data_size': 10000}, 74.66104098399956)
INFO flwr 2024-04-16 23:39:39,515 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 23:39:39,515 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:39:53,953 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 23:39:55,396 | server.py:125 | fit progress: (5, 2.302565813064575, {'accuracy': 0.1026, 'data_size': 10000}, 90.54250240200054)
INFO flwr 2024-04-16 23:39:55,396 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 23:39:55,396 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:40:09,297 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 23:40:10,574 | server.py:125 | fit progress: (6, 2.3025548458099365, {'accuracy': 0.1026, 'data_size': 10000}, 105.72036522300186)
INFO flwr 2024-04-16 23:40:10,574 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 23:40:10,574 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:40:26,133 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 23:40:27,657 | server.py:125 | fit progress: (7, 2.3025431632995605, {'accuracy': 0.1026, 'data_size': 10000}, 122.80350865899527)
INFO flwr 2024-04-16 23:40:27,657 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 23:40:27,657 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:40:41,632 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 23:40:42,898 | server.py:125 | fit progress: (8, 2.3025312423706055, {'accuracy': 0.1026, 'data_size': 10000}, 138.0446457500002)
INFO flwr 2024-04-16 23:40:42,898 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 23:40:42,898 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:40:56,566 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 23:40:58,065 | server.py:125 | fit progress: (9, 2.302518844604492, {'accuracy': 0.1026, 'data_size': 10000}, 153.21166085200093)
INFO flwr 2024-04-16 23:40:58,065 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 23:40:58,066 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:41:11,857 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 23:41:13,154 | server.py:125 | fit progress: (10, 2.3025059700012207, {'accuracy': 0.1026, 'data_size': 10000}, 168.30089048000082)
INFO flwr 2024-04-16 23:41:13,154 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 23:41:13,155 | server.py:153 | FL finished in 168.30134114799876
INFO flwr 2024-04-16 23:41:13,155 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 23:41:13,155 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 23:41:13,155 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 23:41:13,155 | app.py:229 | app_fit: losses_centralized [(0, 2.3026084899902344), (1, 2.302602767944336), (2, 2.302595376968384), (3, 2.302586078643799), (4, 2.3025763034820557), (5, 2.302565813064575), (6, 2.3025548458099365), (7, 2.3025431632995605), (8, 2.3025312423706055), (9, 2.302518844604492), (10, 2.3025059700012207)]
INFO flwr 2024-04-16 23:41:13,155 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1026), (1, 0.1026), (2, 0.1026), (3, 0.1026), (4, 0.1026), (5, 0.1026), (6, 0.1026), (7, 0.1026), (8, 0.1026), (9, 0.1026), (10, 0.1026)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1026
wandb:     loss 2.30251
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_233804-o8fqfqni
wandb: Find logs at: ./wandb/offline-run-20240416_233804-o8fqfqni/logs
INFO flwr 2024-04-16 23:41:16,767 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 23:48:26,560 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=219832)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=219832)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 23:48:31,095	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 23:48:32,011	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 23:48:32,545	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 23:48:32,915	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ede74fadf251fdf9.zip' (36.50MiB) to Ray cluster...
2024-04-16 23:48:33,025	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ede74fadf251fdf9.zip'.
INFO flwr 2024-04-16 23:48:44,153 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'object_store_memory': 72203264409.0, 'accelerator_type:TITAN': 1.0, 'memory': 158474283623.0}
INFO flwr 2024-04-16 23:48:44,154 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 23:48:44,154 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 23:48:44,176 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 23:48:44,177 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 23:48:44,177 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 23:48:44,177 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 23:48:47,337 | server.py:94 | initial parameters (loss, other metrics): 2.3024754524230957, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-16 23:48:47,337 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 23:48:47,338 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=224568)[0m 2024-04-16 23:48:50.098262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=224568)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=224571)[0m 2024-04-16 23:48:52.507057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=224571)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=224571)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=224566)[0m 2024-04-16 23:48:50.377284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=224566)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=224566)[0m 2024-04-16 23:48:52.679958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 23:49:14,179 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 23:49:15,673 | server.py:125 | fit progress: (1, 1.913642406463623, {'accuracy': 0.5476, 'data_size': 10000}, 28.33510386999842)
INFO flwr 2024-04-16 23:49:15,673 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 23:49:15,673 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:49:30,480 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 23:49:31,754 | server.py:125 | fit progress: (2, 2.1052310466766357, {'accuracy': 0.3558, 'data_size': 10000}, 44.41662899799849)
INFO flwr 2024-04-16 23:49:31,754 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 23:49:31,755 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:49:45,149 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-16 23:49:46,648 | server.py:125 | fit progress: (3, 2.162304639816284, {'accuracy': 0.2988, 'data_size': 10000}, 59.310147012001835)
INFO flwr 2024-04-16 23:49:46,648 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-16 23:49:46,648 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:50:00,407 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-16 23:50:01,885 | server.py:125 | fit progress: (4, 2.162998914718628, {'accuracy': 0.2981, 'data_size': 10000}, 74.54738530300529)
INFO flwr 2024-04-16 23:50:01,885 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-16 23:50:01,886 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:50:15,510 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-16 23:50:17,016 | server.py:125 | fit progress: (5, 2.1945438385009766, {'accuracy': 0.2666, 'data_size': 10000}, 89.67796766100219)
INFO flwr 2024-04-16 23:50:17,016 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-16 23:50:17,016 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:50:31,676 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-16 23:50:32,938 | server.py:125 | fit progress: (6, 2.237839460372925, {'accuracy': 0.2233, 'data_size': 10000}, 105.6007187130017)
INFO flwr 2024-04-16 23:50:32,939 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-16 23:50:32,939 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:50:45,911 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-16 23:50:47,448 | server.py:125 | fit progress: (7, 2.2089438438415527, {'accuracy': 0.2522, 'data_size': 10000}, 120.11064490400167)
INFO flwr 2024-04-16 23:50:47,449 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-16 23:50:47,449 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:51:01,487 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-16 23:51:02,742 | server.py:125 | fit progress: (8, 2.2167434692382812, {'accuracy': 0.2444, 'data_size': 10000}, 135.4043594110044)
INFO flwr 2024-04-16 23:51:02,742 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-16 23:51:02,742 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:51:16,946 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-16 23:51:18,453 | server.py:125 | fit progress: (9, 2.228243589401245, {'accuracy': 0.2329, 'data_size': 10000}, 151.11592919200484)
INFO flwr 2024-04-16 23:51:18,454 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-16 23:51:18,454 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:51:32,222 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-16 23:51:33,464 | server.py:125 | fit progress: (10, 2.2366435527801514, {'accuracy': 0.2245, 'data_size': 10000}, 166.12692023900308)
INFO flwr 2024-04-16 23:51:33,465 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-16 23:51:33,465 | server.py:153 | FL finished in 166.1274950720035
INFO flwr 2024-04-16 23:51:33,465 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-16 23:51:33,465 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-16 23:51:33,466 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-16 23:51:33,466 | app.py:229 | app_fit: losses_centralized [(0, 2.3024754524230957), (1, 1.913642406463623), (2, 2.1052310466766357), (3, 2.162304639816284), (4, 2.162998914718628), (5, 2.1945438385009766), (6, 2.237839460372925), (7, 2.2089438438415527), (8, 2.2167434692382812), (9, 2.228243589401245), (10, 2.2366435527801514)]
INFO flwr 2024-04-16 23:51:33,466 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.5476), (2, 0.3558), (3, 0.2988), (4, 0.2981), (5, 0.2666), (6, 0.2233), (7, 0.2522), (8, 0.2444), (9, 0.2329), (10, 0.2245)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2245
wandb:     loss 2.23664
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_234826-o1g8nosg
wandb: Find logs at: ./wandb/offline-run-20240416_234826-o1g8nosg/logs
INFO flwr 2024-04-16 23:51:36,984 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-16 23:58:45,115 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=224566)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=224566)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-16 23:58:49,501	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-16 23:58:50,310	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-16 23:58:50,771	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-16 23:58:51,130	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dac0c4720c8e2d4b.zip' (36.52MiB) to Ray cluster...
2024-04-16 23:58:51,239	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dac0c4720c8e2d4b.zip'.
INFO flwr 2024-04-16 23:59:02,352 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77194680729.0, 'GPU': 1.0, 'memory': 170120921703.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-16 23:59:02,352 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-16 23:59:02,353 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-16 23:59:02,374 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-16 23:59:02,374 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-16 23:59:02,375 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-16 23:59:02,375 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-16 23:59:05,012 | server.py:94 | initial parameters (loss, other metrics): 2.302752733230591, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-16 23:59:05,012 | server.py:104 | FL starting
DEBUG flwr 2024-04-16 23:59:05,013 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=228949)[0m 2024-04-16 23:59:08.494606: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=228949)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=228949)[0m 2024-04-16 23:59:10.819325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=228949)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=228949)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=228945)[0m 2024-04-16 23:59:08.857330: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=228945)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=228945)[0m 2024-04-16 23:59:11.090840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-16 23:59:33,089 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-16 23:59:34,499 | server.py:125 | fit progress: (1, 2.226749897003174, {'accuracy': 0.601, 'data_size': 10000}, 29.48705436599994)
INFO flwr 2024-04-16 23:59:34,500 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-16 23:59:34,500 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-16 23:59:49,000 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-16 23:59:50,180 | server.py:125 | fit progress: (2, 1.787855863571167, {'accuracy': 0.7271, 'data_size': 10000}, 45.16767282799992)
INFO flwr 2024-04-16 23:59:50,180 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-16 23:59:50,181 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:00:02,975 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 00:00:04,389 | server.py:125 | fit progress: (3, 1.640038251876831, {'accuracy': 0.8253, 'data_size': 10000}, 59.376206488996104)
INFO flwr 2024-04-17 00:00:04,389 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 00:00:04,389 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:00:16,199 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 00:00:17,616 | server.py:125 | fit progress: (4, 1.5845513343811035, {'accuracy': 0.8773, 'data_size': 10000}, 72.60375789499813)
INFO flwr 2024-04-17 00:00:17,616 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 00:00:17,617 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:00:30,164 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 00:00:31,541 | server.py:125 | fit progress: (5, 1.5636087656021118, {'accuracy': 0.8971, 'data_size': 10000}, 86.52809582899499)
INFO flwr 2024-04-17 00:00:31,541 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 00:00:31,541 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:00:44,083 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 00:00:45,258 | server.py:125 | fit progress: (6, 1.5596543550491333, {'accuracy': 0.9013, 'data_size': 10000}, 100.24539373899461)
INFO flwr 2024-04-17 00:00:45,258 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 00:00:45,258 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:00:59,516 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 00:01:00,910 | server.py:125 | fit progress: (7, 1.5632869005203247, {'accuracy': 0.8975, 'data_size': 10000}, 115.89734157699422)
INFO flwr 2024-04-17 00:01:00,910 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 00:01:00,910 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:01:14,272 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 00:01:15,447 | server.py:125 | fit progress: (8, 1.5625214576721191, {'accuracy': 0.8983, 'data_size': 10000}, 130.43413213199528)
INFO flwr 2024-04-17 00:01:15,447 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 00:01:15,447 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:01:28,414 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 00:01:29,814 | server.py:125 | fit progress: (9, 1.5561130046844482, {'accuracy': 0.9048, 'data_size': 10000}, 144.80150197899638)
INFO flwr 2024-04-17 00:01:29,814 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 00:01:29,814 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:01:42,822 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 00:01:43,990 | server.py:125 | fit progress: (10, 1.553402066230774, {'accuracy': 0.9076, 'data_size': 10000}, 158.97742307099543)
INFO flwr 2024-04-17 00:01:43,990 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 00:01:43,990 | server.py:153 | FL finished in 158.97790706999513
INFO flwr 2024-04-17 00:01:43,991 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 00:01:43,991 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 00:01:43,991 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 00:01:43,991 | app.py:229 | app_fit: losses_centralized [(0, 2.302752733230591), (1, 2.226749897003174), (2, 1.787855863571167), (3, 1.640038251876831), (4, 1.5845513343811035), (5, 1.5636087656021118), (6, 1.5596543550491333), (7, 1.5632869005203247), (8, 1.5625214576721191), (9, 1.5561130046844482), (10, 1.553402066230774)]
INFO flwr 2024-04-17 00:01:43,991 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.601), (2, 0.7271), (3, 0.8253), (4, 0.8773), (5, 0.8971), (6, 0.9013), (7, 0.8975), (8, 0.8983), (9, 0.9048), (10, 0.9076)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9076
wandb:     loss 1.5534
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240416_235844-debkf2gm
wandb: Find logs at: ./wandb/offline-run-20240416_235844-debkf2gm/logs
INFO flwr 2024-04-17 00:01:47,522 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 00:08:55,828 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=228943)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=228943)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 00:09:00,194	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 00:09:01,123	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 00:09:01,580	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 00:09:01,925	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ceb31f7b90ccf5b8.zip' (36.53MiB) to Ray cluster...
2024-04-17 00:09:02,012	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ceb31f7b90ccf5b8.zip'.
INFO flwr 2024-04-17 00:09:13,113 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77151546163.0, 'accelerator_type:TITAN': 1.0, 'memory': 170020274381.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 00:09:13,114 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 00:09:13,114 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 00:09:13,132 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 00:09:13,134 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 00:09:13,134 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 00:09:13,134 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 00:09:15,728 | server.py:94 | initial parameters (loss, other metrics): 2.3026187419891357, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-17 00:09:15,729 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 00:09:15,729 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=233430)[0m 2024-04-17 00:09:19.261762: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=233430)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=233429)[0m 2024-04-17 00:09:21.582970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=233431)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=233431)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=233436)[0m 2024-04-17 00:09:19.570751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=233436)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=233436)[0m 2024-04-17 00:09:22.033707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 00:09:42,247 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 00:09:43,659 | server.py:125 | fit progress: (1, 2.301743507385254, {'accuracy': 0.0974, 'data_size': 10000}, 27.929748745002144)
INFO flwr 2024-04-17 00:09:43,659 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 00:09:43,659 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:09:58,263 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 00:09:59,440 | server.py:125 | fit progress: (2, 2.2995193004608154, {'accuracy': 0.11, 'data_size': 10000}, 43.71074641200539)
INFO flwr 2024-04-17 00:09:59,440 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 00:09:59,440 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:10:13,257 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 00:10:14,677 | server.py:125 | fit progress: (3, 2.294785499572754, {'accuracy': 0.3449, 'data_size': 10000}, 58.94845414100564)
INFO flwr 2024-04-17 00:10:14,678 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 00:10:14,678 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:10:27,818 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 00:10:29,245 | server.py:125 | fit progress: (4, 2.285799503326416, {'accuracy': 0.6576, 'data_size': 10000}, 73.51567582999996)
INFO flwr 2024-04-17 00:10:29,245 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 00:10:29,245 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:10:42,799 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 00:10:44,203 | server.py:125 | fit progress: (5, 2.269890069961548, {'accuracy': 0.714, 'data_size': 10000}, 88.47386766300042)
INFO flwr 2024-04-17 00:10:44,203 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 00:10:44,203 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:10:57,441 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 00:10:58,641 | server.py:125 | fit progress: (6, 2.2427139282226562, {'accuracy': 0.7158, 'data_size': 10000}, 102.9123070170026)
INFO flwr 2024-04-17 00:10:58,642 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 00:10:58,642 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:11:11,799 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 00:11:13,212 | server.py:125 | fit progress: (7, 2.1981000900268555, {'accuracy': 0.73, 'data_size': 10000}, 117.48283805600659)
INFO flwr 2024-04-17 00:11:13,212 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 00:11:13,212 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:11:25,045 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 00:11:26,221 | server.py:125 | fit progress: (8, 2.129603862762451, {'accuracy': 0.7459, 'data_size': 10000}, 130.49246946000494)
INFO flwr 2024-04-17 00:11:26,222 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 00:11:26,222 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:11:38,958 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 00:11:40,378 | server.py:125 | fit progress: (9, 2.0392680168151855, {'accuracy': 0.7595, 'data_size': 10000}, 144.64922256700083)
INFO flwr 2024-04-17 00:11:40,379 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 00:11:40,379 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:11:53,443 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 00:11:54,854 | server.py:125 | fit progress: (10, 1.9406445026397705, {'accuracy': 0.7786, 'data_size': 10000}, 159.125420837001)
INFO flwr 2024-04-17 00:11:54,855 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 00:11:54,855 | server.py:153 | FL finished in 159.1259040740042
INFO flwr 2024-04-17 00:11:54,855 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 00:11:54,855 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 00:11:54,855 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 00:11:54,855 | app.py:229 | app_fit: losses_centralized [(0, 2.3026187419891357), (1, 2.301743507385254), (2, 2.2995193004608154), (3, 2.294785499572754), (4, 2.285799503326416), (5, 2.269890069961548), (6, 2.2427139282226562), (7, 2.1981000900268555), (8, 2.129603862762451), (9, 2.0392680168151855), (10, 1.9406445026397705)]
INFO flwr 2024-04-17 00:11:54,856 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.0974), (2, 0.11), (3, 0.3449), (4, 0.6576), (5, 0.714), (6, 0.7158), (7, 0.73), (8, 0.7459), (9, 0.7595), (10, 0.7786)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7786
wandb:     loss 1.94064
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_000855-tnifji49
wandb: Find logs at: ./wandb/offline-run-20240417_000855-tnifji49/logs
INFO flwr 2024-04-17 00:11:58,364 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 00:19:07,321 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=233428)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=233428)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 00:19:11,812	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 00:19:12,719	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 00:19:13,168	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 00:19:13,536	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_87b2a25ba5c115e8.zip' (36.54MiB) to Ray cluster...
2024-04-17 00:19:13,644	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_87b2a25ba5c115e8.zip'.
INFO flwr 2024-04-17 00:19:24,653 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77180098560.0, 'GPU': 1.0, 'memory': 170086896640.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 00:19:24,653 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 00:19:24,653 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 00:19:24,673 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 00:19:24,675 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 00:19:24,675 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 00:19:24,675 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 00:19:27,783 | server.py:94 | initial parameters (loss, other metrics): 2.302555799484253, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-17 00:19:27,783 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 00:19:27,784 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=238159)[0m 2024-04-17 00:19:30.739822: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=238159)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=238159)[0m 2024-04-17 00:19:33.015994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=238165)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=238165)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=238166)[0m 2024-04-17 00:19:31.233247: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=238166)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=238166)[0m 2024-04-17 00:19:33.672728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 00:19:55,848 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 00:19:57,267 | server.py:125 | fit progress: (1, 2.3024799823760986, {'accuracy': 0.1009, 'data_size': 10000}, 29.48353840100026)
INFO flwr 2024-04-17 00:19:57,267 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 00:19:57,268 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:20:11,193 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 00:20:12,362 | server.py:125 | fit progress: (2, 2.3023698329925537, {'accuracy': 0.1009, 'data_size': 10000}, 44.578748471001745)
INFO flwr 2024-04-17 00:20:12,363 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 00:20:12,363 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:20:25,100 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 00:20:26,462 | server.py:125 | fit progress: (3, 2.302232027053833, {'accuracy': 0.1013, 'data_size': 10000}, 58.67856267699972)
INFO flwr 2024-04-17 00:20:26,462 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 00:20:26,463 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:20:39,519 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 00:20:40,958 | server.py:125 | fit progress: (4, 2.3020641803741455, {'accuracy': 0.1018, 'data_size': 10000}, 73.17452117000357)
INFO flwr 2024-04-17 00:20:40,958 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 00:20:40,959 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:20:54,155 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 00:20:55,550 | server.py:125 | fit progress: (5, 2.301865577697754, {'accuracy': 0.104, 'data_size': 10000}, 87.76611494400277)
INFO flwr 2024-04-17 00:20:55,550 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 00:20:55,550 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:21:11,017 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 00:21:12,189 | server.py:125 | fit progress: (6, 2.301636219024658, {'accuracy': 0.1081, 'data_size': 10000}, 104.40568168400205)
INFO flwr 2024-04-17 00:21:12,190 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 00:21:12,190 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:21:25,503 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 00:21:26,889 | server.py:125 | fit progress: (7, 2.3013691902160645, {'accuracy': 0.1165, 'data_size': 10000}, 119.10589907100075)
INFO flwr 2024-04-17 00:21:26,890 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 00:21:26,890 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:21:39,792 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 00:21:40,963 | server.py:125 | fit progress: (8, 2.3010668754577637, {'accuracy': 0.1279, 'data_size': 10000}, 133.17961637199915)
INFO flwr 2024-04-17 00:21:40,963 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 00:21:40,964 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:21:55,275 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 00:21:56,678 | server.py:125 | fit progress: (9, 2.3007302284240723, {'accuracy': 0.1418, 'data_size': 10000}, 148.89426892899792)
INFO flwr 2024-04-17 00:21:56,678 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 00:21:56,678 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:22:09,733 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 00:22:11,149 | server.py:125 | fit progress: (10, 2.30035400390625, {'accuracy': 0.1573, 'data_size': 10000}, 163.36497516999952)
INFO flwr 2024-04-17 00:22:11,149 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 00:22:11,149 | server.py:153 | FL finished in 163.36551326300105
INFO flwr 2024-04-17 00:22:11,149 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 00:22:11,149 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 00:22:11,150 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 00:22:11,150 | app.py:229 | app_fit: losses_centralized [(0, 2.302555799484253), (1, 2.3024799823760986), (2, 2.3023698329925537), (3, 2.302232027053833), (4, 2.3020641803741455), (5, 2.301865577697754), (6, 2.301636219024658), (7, 2.3013691902160645), (8, 2.3010668754577637), (9, 2.3007302284240723), (10, 2.30035400390625)]
INFO flwr 2024-04-17 00:22:11,150 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.1009), (2, 0.1009), (3, 0.1013), (4, 0.1018), (5, 0.104), (6, 0.1081), (7, 0.1165), (8, 0.1279), (9, 0.1418), (10, 0.1573)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1573
wandb:     loss 2.30035
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_001906-jgqt4782
wandb: Find logs at: ./wandb/offline-run-20240417_001906-jgqt4782/logs
INFO flwr 2024-04-17 00:22:14,686 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 00:29:23,388 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=238159)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=238159)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 00:29:28,910	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 00:29:29,744	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 00:29:30,202	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 00:29:30,557	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_eaa8524ff568f07b.zip' (36.55MiB) to Ray cluster...
2024-04-17 00:29:30,671	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_eaa8524ff568f07b.zip'.
INFO flwr 2024-04-17 00:29:41,704 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77157596774.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 170034392474.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 00:29:41,704 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 00:29:41,705 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 00:29:41,727 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 00:29:41,727 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 00:29:41,728 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 00:29:41,728 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 00:29:44,261 | server.py:94 | initial parameters (loss, other metrics): 2.302591562271118, {'accuracy': 0.1049, 'data_size': 10000}
INFO flwr 2024-04-17 00:29:44,267 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 00:29:44,267 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=242540)[0m 2024-04-17 00:29:47.855956: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=242540)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=242540)[0m 2024-04-17 00:29:50.139563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=242540)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=242540)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=242544)[0m 2024-04-17 00:29:48.210627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=242544)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=242544)[0m 2024-04-17 00:29:50.505782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 00:30:11,575 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 00:30:13,021 | server.py:125 | fit progress: (1, 2.3025851249694824, {'accuracy': 0.1057, 'data_size': 10000}, 28.753529252004228)
INFO flwr 2024-04-17 00:30:13,021 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 00:30:13,021 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:30:27,141 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 00:30:28,321 | server.py:125 | fit progress: (2, 2.3025760650634766, {'accuracy': 0.1061, 'data_size': 10000}, 44.053621459999704)
INFO flwr 2024-04-17 00:30:28,321 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 00:30:28,321 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:30:42,156 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 00:30:43,591 | server.py:125 | fit progress: (3, 2.3025662899017334, {'accuracy': 0.1068, 'data_size': 10000}, 59.32420475800609)
INFO flwr 2024-04-17 00:30:43,592 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 00:30:43,592 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:30:56,192 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 00:30:57,608 | server.py:125 | fit progress: (4, 2.3025546073913574, {'accuracy': 0.1077, 'data_size': 10000}, 73.34027956900536)
INFO flwr 2024-04-17 00:30:57,608 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 00:30:57,608 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:31:10,134 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 00:31:11,527 | server.py:125 | fit progress: (5, 2.302542209625244, {'accuracy': 0.1079, 'data_size': 10000}, 87.25959517600131)
INFO flwr 2024-04-17 00:31:11,527 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 00:31:11,527 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:31:23,849 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 00:31:25,027 | server.py:125 | fit progress: (6, 2.3025293350219727, {'accuracy': 0.1082, 'data_size': 10000}, 100.75954609400651)
INFO flwr 2024-04-17 00:31:25,027 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 00:31:25,027 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:31:38,654 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 00:31:40,065 | server.py:125 | fit progress: (7, 2.3025147914886475, {'accuracy': 0.1089, 'data_size': 10000}, 115.79787014400063)
INFO flwr 2024-04-17 00:31:40,065 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 00:31:40,066 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:31:53,135 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 00:31:54,316 | server.py:125 | fit progress: (8, 2.3025004863739014, {'accuracy': 0.11, 'data_size': 10000}, 130.04825570600224)
INFO flwr 2024-04-17 00:31:54,316 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 00:31:54,316 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:32:07,894 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 00:32:09,308 | server.py:125 | fit progress: (9, 2.302485227584839, {'accuracy': 0.1105, 'data_size': 10000}, 145.04081395200046)
INFO flwr 2024-04-17 00:32:09,308 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 00:32:09,309 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:32:22,601 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 00:32:24,030 | server.py:125 | fit progress: (10, 2.302469253540039, {'accuracy': 0.1115, 'data_size': 10000}, 159.76282901000377)
INFO flwr 2024-04-17 00:32:24,030 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 00:32:24,031 | server.py:153 | FL finished in 159.76329105900368
INFO flwr 2024-04-17 00:32:24,031 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 00:32:24,031 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 00:32:24,031 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 00:32:24,031 | app.py:229 | app_fit: losses_centralized [(0, 2.302591562271118), (1, 2.3025851249694824), (2, 2.3025760650634766), (3, 2.3025662899017334), (4, 2.3025546073913574), (5, 2.302542209625244), (6, 2.3025293350219727), (7, 2.3025147914886475), (8, 2.3025004863739014), (9, 2.302485227584839), (10, 2.302469253540039)]
INFO flwr 2024-04-17 00:32:24,031 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1049), (1, 0.1057), (2, 0.1061), (3, 0.1068), (4, 0.1077), (5, 0.1079), (6, 0.1082), (7, 0.1089), (8, 0.11), (9, 0.1105), (10, 0.1115)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1115
wandb:     loss 2.30247
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_002922-sx8bk5zu
wandb: Find logs at: ./wandb/offline-run-20240417_002922-sx8bk5zu/logs
INFO flwr 2024-04-17 00:32:27,578 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 00:39:36,067 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=242528)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=242528)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 00:39:40,471	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 00:39:41,253	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 00:39:41,714	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 00:39:42,070	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_315ddc6946cc0540.zip' (36.56MiB) to Ray cluster...
2024-04-17 00:39:42,181	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_315ddc6946cc0540.zip'.
INFO flwr 2024-04-17 00:39:53,187 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77173984051.0, 'GPU': 1.0, 'memory': 170072629453.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 00:39:53,187 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 00:39:53,187 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 00:39:53,210 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 00:39:53,211 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 00:39:53,212 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 00:39:53,212 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 00:39:57,007 | server.py:94 | initial parameters (loss, other metrics): 2.302717924118042, {'accuracy': 0.0981, 'data_size': 10000}
INFO flwr 2024-04-17 00:39:57,007 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 00:39:57,008 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=246912)[0m 2024-04-17 00:39:59.277372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=246912)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=246912)[0m 2024-04-17 00:40:01.597288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=246912)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=246912)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=246909)[0m 2024-04-17 00:39:59.532664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=246909)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=246909)[0m 2024-04-17 00:40:01.795691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 00:40:34,747 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 00:40:36,148 | server.py:125 | fit progress: (1, 2.005145788192749, {'accuracy': 0.4559, 'data_size': 10000}, 39.140456198998436)
INFO flwr 2024-04-17 00:40:36,148 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 00:40:36,148 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:40:59,135 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 00:41:00,312 | server.py:125 | fit progress: (2, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 63.30444200099737)
INFO flwr 2024-04-17 00:41:00,312 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 00:41:00,312 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:41:25,346 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 00:41:26,759 | server.py:125 | fit progress: (3, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 89.7512869219936)
INFO flwr 2024-04-17 00:41:26,759 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 00:41:26,759 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:41:50,331 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 00:41:51,701 | server.py:125 | fit progress: (4, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 114.69349164599407)
INFO flwr 2024-04-17 00:41:51,701 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 00:41:51,702 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:42:16,105 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 00:42:17,532 | server.py:125 | fit progress: (5, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 140.52493001999392)
INFO flwr 2024-04-17 00:42:17,533 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 00:42:17,533 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:42:42,104 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 00:42:43,296 | server.py:125 | fit progress: (6, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 166.28864954699384)
INFO flwr 2024-04-17 00:42:43,296 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 00:42:43,297 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:43:05,914 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 00:43:07,303 | server.py:125 | fit progress: (7, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 190.295694471999)
INFO flwr 2024-04-17 00:43:07,303 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 00:43:07,304 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:43:32,221 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 00:43:33,376 | server.py:125 | fit progress: (8, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 216.36841794599604)
INFO flwr 2024-04-17 00:43:33,376 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 00:43:33,376 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:43:57,397 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 00:43:58,800 | server.py:125 | fit progress: (9, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 241.7927439979976)
INFO flwr 2024-04-17 00:43:58,801 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 00:43:58,801 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:44:24,830 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 00:44:26,263 | server.py:125 | fit progress: (10, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 269.25536640299833)
INFO flwr 2024-04-17 00:44:26,263 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 00:44:26,263 | server.py:153 | FL finished in 269.25582235199545
INFO flwr 2024-04-17 00:44:26,263 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 00:44:26,264 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 00:44:26,264 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 00:44:26,264 | app.py:229 | app_fit: losses_centralized [(0, 2.302717924118042), (1, 2.005145788192749), (2, 2.360142230987549), (3, 2.360142230987549), (4, 2.360142230987549), (5, 2.360142230987549), (6, 2.360142230987549), (7, 2.360142230987549), (8, 2.360142230987549), (9, 2.360142230987549), (10, 2.360142230987549)]
INFO flwr 2024-04-17 00:44:26,264 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0981), (1, 0.4559), (2, 0.101), (3, 0.101), (4, 0.101), (5, 0.101), (6, 0.101), (7, 0.101), (8, 0.101), (9, 0.101), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.36014
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_003935-13pbwevv
wandb: Find logs at: ./wandb/offline-run-20240417_003935-13pbwevv/logs
INFO flwr 2024-04-17 00:44:29,767 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 00:51:37,322 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=246897)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=246897)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 00:51:42,847	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 00:51:43,640	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 00:51:44,182	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 00:51:44,533	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fc330660706fba4c.zip' (36.57MiB) to Ray cluster...
2024-04-17 00:51:44,653	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fc330660706fba4c.zip'.
INFO flwr 2024-04-17 00:51:55,679 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'object_store_memory': 77153803468.0, 'memory': 170025541428.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 00:51:55,679 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 00:51:55,679 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 00:51:55,697 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 00:51:55,698 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 00:51:55,698 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 00:51:55,698 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 00:51:58,873 | server.py:94 | initial parameters (loss, other metrics): 2.3025662899017334, {'accuracy': 0.1137, 'data_size': 10000}
INFO flwr 2024-04-17 00:51:58,874 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 00:51:58,875 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=252272)[0m 2024-04-17 00:52:01.723884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=252272)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=252272)[0m 2024-04-17 00:52:04.052291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=252276)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=252276)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=252271)[0m 2024-04-17 00:52:02.202503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=252271)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=252271)[0m 2024-04-17 00:52:04.563624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 00:52:36,720 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 00:52:38,130 | server.py:125 | fit progress: (1, 2.213609457015991, {'accuracy': 0.5974, 'data_size': 10000}, 39.25539574599679)
INFO flwr 2024-04-17 00:52:38,130 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 00:52:38,130 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:53:01,314 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 00:53:02,738 | server.py:125 | fit progress: (2, 1.7663244009017944, {'accuracy': 0.75, 'data_size': 10000}, 63.863550764996035)
INFO flwr 2024-04-17 00:53:02,738 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 00:53:02,738 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:53:28,473 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 00:53:29,674 | server.py:125 | fit progress: (3, 1.6236213445663452, {'accuracy': 0.8383, 'data_size': 10000}, 90.79950491099589)
INFO flwr 2024-04-17 00:53:29,674 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 00:53:29,674 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:53:51,236 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 00:53:52,641 | server.py:125 | fit progress: (4, 1.6006673574447632, {'accuracy': 0.8606, 'data_size': 10000}, 113.76630197099439)
INFO flwr 2024-04-17 00:53:52,641 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 00:53:52,641 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:54:20,146 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 00:54:21,341 | server.py:125 | fit progress: (5, 1.5899964570999146, {'accuracy': 0.8699, 'data_size': 10000}, 142.46717970899772)
INFO flwr 2024-04-17 00:54:21,342 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 00:54:21,342 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:54:52,960 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 00:54:54,331 | server.py:125 | fit progress: (6, 1.5534125566482544, {'accuracy': 0.9078, 'data_size': 10000}, 175.45694083099806)
INFO flwr 2024-04-17 00:54:54,332 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 00:54:54,332 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:55:20,723 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 00:55:21,917 | server.py:125 | fit progress: (7, 1.5317484140396118, {'accuracy': 0.9292, 'data_size': 10000}, 203.0431821749953)
INFO flwr 2024-04-17 00:55:21,918 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 00:55:21,918 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:55:47,255 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 00:55:48,684 | server.py:125 | fit progress: (8, 1.5308189392089844, {'accuracy': 0.9295, 'data_size': 10000}, 229.80966940699727)
INFO flwr 2024-04-17 00:55:48,684 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 00:55:48,685 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:56:14,717 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 00:56:16,115 | server.py:125 | fit progress: (9, 1.5308737754821777, {'accuracy': 0.93, 'data_size': 10000}, 257.240876455995)
INFO flwr 2024-04-17 00:56:16,115 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 00:56:16,116 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 00:56:40,543 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 00:56:41,984 | server.py:125 | fit progress: (10, 1.529691457748413, {'accuracy': 0.9313, 'data_size': 10000}, 283.1099875399959)
INFO flwr 2024-04-17 00:56:41,985 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 00:56:41,985 | server.py:153 | FL finished in 283.1104268499985
INFO flwr 2024-04-17 00:56:41,985 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 00:56:41,985 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 00:56:41,985 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 00:56:41,985 | app.py:229 | app_fit: losses_centralized [(0, 2.3025662899017334), (1, 2.213609457015991), (2, 1.7663244009017944), (3, 1.6236213445663452), (4, 1.6006673574447632), (5, 1.5899964570999146), (6, 1.5534125566482544), (7, 1.5317484140396118), (8, 1.5308189392089844), (9, 1.5308737754821777), (10, 1.529691457748413)]
INFO flwr 2024-04-17 00:56:41,985 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1137), (1, 0.5974), (2, 0.75), (3, 0.8383), (4, 0.8606), (5, 0.8699), (6, 0.9078), (7, 0.9292), (8, 0.9295), (9, 0.93), (10, 0.9313)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9313
wandb:     loss 1.52969
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_005136-utbnke1b
wandb: Find logs at: ./wandb/offline-run-20240417_005136-utbnke1b/logs
INFO flwr 2024-04-17 00:56:45,494 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 01:03:52,760 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=252268)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=252268)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 01:03:57,091	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 01:03:57,927	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 01:03:58,421	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 01:03:58,784	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6585e2068b7e5bd9.zip' (36.59MiB) to Ray cluster...
2024-04-17 01:03:58,893	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6585e2068b7e5bd9.zip'.
INFO flwr 2024-04-17 01:04:09,946 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 77233794662.0, 'CPU': 64.0, 'memory': 170212187546.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 01:04:09,947 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 01:04:09,947 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 01:04:09,962 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 01:04:09,964 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 01:04:09,964 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 01:04:09,964 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 01:04:12,638 | server.py:94 | initial parameters (loss, other metrics): 2.3026654720306396, {'accuracy': 0.0782, 'data_size': 10000}
INFO flwr 2024-04-17 01:04:12,638 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 01:04:12,639 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=256672)[0m 2024-04-17 01:04:16.055712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=256672)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=256672)[0m 2024-04-17 01:04:18.380453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=256677)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=256677)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=256677)[0m 2024-04-17 01:04:16.285394: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=256677)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=256677)[0m 2024-04-17 01:04:18.599986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 01:04:53,539 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 01:04:54,962 | server.py:125 | fit progress: (1, 2.301504135131836, {'accuracy': 0.1277, 'data_size': 10000}, 42.32310822299769)
INFO flwr 2024-04-17 01:04:54,962 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 01:04:54,962 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:05:19,910 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 01:05:21,348 | server.py:125 | fit progress: (2, 2.298544406890869, {'accuracy': 0.3007, 'data_size': 10000}, 68.70908327799407)
INFO flwr 2024-04-17 01:05:21,348 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 01:05:21,348 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:05:47,910 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 01:05:49,093 | server.py:125 | fit progress: (3, 2.292055130004883, {'accuracy': 0.4398, 'data_size': 10000}, 96.45450813699426)
INFO flwr 2024-04-17 01:05:49,093 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 01:05:49,094 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:06:14,175 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 01:06:15,551 | server.py:125 | fit progress: (4, 2.2792468070983887, {'accuracy': 0.6041, 'data_size': 10000}, 122.91286721999495)
INFO flwr 2024-04-17 01:06:15,552 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 01:06:15,552 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:06:40,002 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 01:06:41,200 | server.py:125 | fit progress: (5, 2.2556216716766357, {'accuracy': 0.701, 'data_size': 10000}, 148.56130382399715)
INFO flwr 2024-04-17 01:06:41,200 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 01:06:41,200 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:07:05,232 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 01:07:06,661 | server.py:125 | fit progress: (6, 2.2134947776794434, {'accuracy': 0.7403, 'data_size': 10000}, 174.02273120899918)
INFO flwr 2024-04-17 01:07:06,661 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 01:07:06,662 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:07:32,024 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 01:07:33,217 | server.py:125 | fit progress: (7, 2.144505023956299, {'accuracy': 0.7539, 'data_size': 10000}, 200.5789205589972)
INFO flwr 2024-04-17 01:07:33,218 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 01:07:33,218 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:08:03,825 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 01:08:05,258 | server.py:125 | fit progress: (8, 2.047070264816284, {'accuracy': 0.7694, 'data_size': 10000}, 232.6195204639953)
INFO flwr 2024-04-17 01:08:05,258 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 01:08:05,258 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:08:28,138 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 01:08:29,508 | server.py:125 | fit progress: (9, 1.9356805086135864, {'accuracy': 0.7911, 'data_size': 10000}, 256.8699702199956)
INFO flwr 2024-04-17 01:08:29,509 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 01:08:29,509 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:08:55,110 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 01:08:56,514 | server.py:125 | fit progress: (10, 1.8318400382995605, {'accuracy': 0.8146, 'data_size': 10000}, 283.8750822669972)
INFO flwr 2024-04-17 01:08:56,514 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 01:08:56,514 | server.py:153 | FL finished in 283.87553448999824
INFO flwr 2024-04-17 01:08:56,514 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 01:08:56,514 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 01:08:56,514 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 01:08:56,515 | app.py:229 | app_fit: losses_centralized [(0, 2.3026654720306396), (1, 2.301504135131836), (2, 2.298544406890869), (3, 2.292055130004883), (4, 2.2792468070983887), (5, 2.2556216716766357), (6, 2.2134947776794434), (7, 2.144505023956299), (8, 2.047070264816284), (9, 1.9356805086135864), (10, 1.8318400382995605)]
INFO flwr 2024-04-17 01:08:56,515 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0782), (1, 0.1277), (2, 0.3007), (3, 0.4398), (4, 0.6041), (5, 0.701), (6, 0.7403), (7, 0.7539), (8, 0.7694), (9, 0.7911), (10, 0.8146)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8146
wandb:     loss 1.83184
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_010352-1quu40uu
wandb: Find logs at: ./wandb/offline-run-20240417_010352-1quu40uu/logs
INFO flwr 2024-04-17 01:09:00,054 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 01:16:07,305 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=256671)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=256671)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 01:16:11,659	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 01:16:12,544	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 01:16:13,006	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 01:16:13,367	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_547ff7d6e97fe880.zip' (36.60MiB) to Ray cluster...
2024-04-17 01:16:13,473	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_547ff7d6e97fe880.zip'.
INFO flwr 2024-04-17 01:16:24,590 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 170102121472.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 77186623488.0}
INFO flwr 2024-04-17 01:16:24,590 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 01:16:24,590 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 01:16:24,607 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 01:16:24,609 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 01:16:24,610 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 01:16:24,610 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 01:16:28,422 | server.py:94 | initial parameters (loss, other metrics): 2.3026978969573975, {'accuracy': 0.0947, 'data_size': 10000}
INFO flwr 2024-04-17 01:16:28,423 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 01:16:28,423 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=262033)[0m 2024-04-17 01:16:30.659265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=262033)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=262033)[0m 2024-04-17 01:16:32.954886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=262036)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=262036)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=262030)[0m 2024-04-17 01:16:30.971340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=262030)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=262030)[0m 2024-04-17 01:16:33.259268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 01:17:05,245 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 01:17:06,642 | server.py:125 | fit progress: (1, 2.302629232406616, {'accuracy': 0.1006, 'data_size': 10000}, 38.21916041100485)
INFO flwr 2024-04-17 01:17:06,642 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 01:17:06,643 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:17:32,624 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 01:17:34,052 | server.py:125 | fit progress: (2, 2.302532911300659, {'accuracy': 0.1061, 'data_size': 10000}, 65.62910089400248)
INFO flwr 2024-04-17 01:17:34,052 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 01:17:34,053 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:17:58,683 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 01:17:59,894 | server.py:125 | fit progress: (3, 2.3024086952209473, {'accuracy': 0.1106, 'data_size': 10000}, 91.47130745600589)
INFO flwr 2024-04-17 01:17:59,895 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 01:17:59,895 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:18:24,501 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 01:18:25,895 | server.py:125 | fit progress: (4, 2.3022513389587402, {'accuracy': 0.1173, 'data_size': 10000}, 117.47204767900257)
INFO flwr 2024-04-17 01:18:25,895 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 01:18:25,896 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:18:45,709 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 01:18:46,913 | server.py:125 | fit progress: (5, 2.302060604095459, {'accuracy': 0.123, 'data_size': 10000}, 138.49008123000385)
INFO flwr 2024-04-17 01:18:46,913 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 01:18:46,914 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:19:14,719 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 01:19:16,147 | server.py:125 | fit progress: (6, 2.301832675933838, {'accuracy': 0.1269, 'data_size': 10000}, 167.72386790400196)
INFO flwr 2024-04-17 01:19:16,147 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 01:19:16,147 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:19:43,333 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 01:19:44,549 | server.py:125 | fit progress: (7, 2.301567316055298, {'accuracy': 0.1323, 'data_size': 10000}, 196.12558982500195)
INFO flwr 2024-04-17 01:19:44,549 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 01:19:44,549 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:20:10,935 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 01:20:12,388 | server.py:125 | fit progress: (8, 2.3012592792510986, {'accuracy': 0.1391, 'data_size': 10000}, 223.96508504000667)
INFO flwr 2024-04-17 01:20:12,388 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 01:20:12,389 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:20:38,814 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 01:20:40,210 | server.py:125 | fit progress: (9, 2.300906181335449, {'accuracy': 0.1478, 'data_size': 10000}, 251.78715415800252)
INFO flwr 2024-04-17 01:20:40,210 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 01:20:40,211 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:21:06,000 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 01:21:07,384 | server.py:125 | fit progress: (10, 2.3005001544952393, {'accuracy': 0.1559, 'data_size': 10000}, 278.9613793130047)
INFO flwr 2024-04-17 01:21:07,385 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 01:21:07,385 | server.py:153 | FL finished in 278.9618214200018
INFO flwr 2024-04-17 01:21:07,385 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 01:21:07,385 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 01:21:07,385 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 01:21:07,385 | app.py:229 | app_fit: losses_centralized [(0, 2.3026978969573975), (1, 2.302629232406616), (2, 2.302532911300659), (3, 2.3024086952209473), (4, 2.3022513389587402), (5, 2.302060604095459), (6, 2.301832675933838), (7, 2.301567316055298), (8, 2.3012592792510986), (9, 2.300906181335449), (10, 2.3005001544952393)]
INFO flwr 2024-04-17 01:21:07,385 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0947), (1, 0.1006), (2, 0.1061), (3, 0.1106), (4, 0.1173), (5, 0.123), (6, 0.1269), (7, 0.1323), (8, 0.1391), (9, 0.1478), (10, 0.1559)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1559
wandb:     loss 2.3005
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_011606-oeb6i8sj
wandb: Find logs at: ./wandb/offline-run-20240417_011606-oeb6i8sj/logs
INFO flwr 2024-04-17 01:21:10,926 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 01:28:18,512 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=262029)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=262029)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 01:28:23,349	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 01:28:24,228	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 01:28:24,684	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 01:28:25,052	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_778b179d7a0f0a49.zip' (36.61MiB) to Ray cluster...
2024-04-17 01:28:25,161	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_778b179d7a0f0a49.zip'.
INFO flwr 2024-04-17 01:28:36,262 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77174500147.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 170073833677.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 01:28:36,262 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 01:28:36,262 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 01:28:36,282 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 01:28:36,287 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 01:28:36,288 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 01:28:36,288 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 01:28:38,917 | server.py:94 | initial parameters (loss, other metrics): 2.3024990558624268, {'accuracy': 0.0922, 'data_size': 10000}
INFO flwr 2024-04-17 01:28:38,917 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 01:28:38,917 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=266435)[0m 2024-04-17 01:28:42.435623: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=266435)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=266435)[0m 2024-04-17 01:28:44.789081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=266442)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=266442)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=266436)[0m 2024-04-17 01:28:42.672931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=266436)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=266436)[0m 2024-04-17 01:28:44.964190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 01:29:15,858 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 01:29:17,269 | server.py:125 | fit progress: (1, 2.3024919033050537, {'accuracy': 0.0924, 'data_size': 10000}, 38.351843999000266)
INFO flwr 2024-04-17 01:29:17,269 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 01:29:17,270 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:29:40,908 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 01:29:42,322 | server.py:125 | fit progress: (2, 2.302483081817627, {'accuracy': 0.0929, 'data_size': 10000}, 63.4050438589984)
INFO flwr 2024-04-17 01:29:42,323 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 01:29:42,323 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:30:07,613 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 01:30:08,783 | server.py:125 | fit progress: (3, 2.3024728298187256, {'accuracy': 0.0945, 'data_size': 10000}, 89.86567145300069)
INFO flwr 2024-04-17 01:30:08,783 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 01:30:08,783 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:30:33,120 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 01:30:34,539 | server.py:125 | fit progress: (4, 2.3024613857269287, {'accuracy': 0.0954, 'data_size': 10000}, 115.621777983004)
INFO flwr 2024-04-17 01:30:34,539 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 01:30:34,540 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:30:58,339 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 01:30:59,528 | server.py:125 | fit progress: (5, 2.3024494647979736, {'accuracy': 0.0962, 'data_size': 10000}, 140.61066344500432)
INFO flwr 2024-04-17 01:30:59,528 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 01:30:59,529 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:31:25,640 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 01:31:27,034 | server.py:125 | fit progress: (6, 2.3024368286132812, {'accuracy': 0.0974, 'data_size': 10000}, 168.11684262700146)
INFO flwr 2024-04-17 01:31:27,034 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 01:31:27,035 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:31:55,271 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 01:31:56,459 | server.py:125 | fit progress: (7, 2.3024232387542725, {'accuracy': 0.0982, 'data_size': 10000}, 197.54179321200354)
INFO flwr 2024-04-17 01:31:56,459 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 01:31:56,460 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:32:23,635 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 01:32:24,997 | server.py:125 | fit progress: (8, 2.3024096488952637, {'accuracy': 0.0999, 'data_size': 10000}, 226.07934003099945)
INFO flwr 2024-04-17 01:32:24,997 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 01:32:24,997 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:32:49,876 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 01:32:51,290 | server.py:125 | fit progress: (9, 2.3023951053619385, {'accuracy': 0.1011, 'data_size': 10000}, 252.37232043300173)
INFO flwr 2024-04-17 01:32:51,290 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 01:32:51,290 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:33:17,307 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 01:33:18,720 | server.py:125 | fit progress: (10, 2.302380323410034, {'accuracy': 0.1021, 'data_size': 10000}, 279.80253745100345)
INFO flwr 2024-04-17 01:33:18,720 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 01:33:18,720 | server.py:153 | FL finished in 279.80300663199887
INFO flwr 2024-04-17 01:33:18,721 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 01:33:18,721 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 01:33:18,721 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 01:33:18,721 | app.py:229 | app_fit: losses_centralized [(0, 2.3024990558624268), (1, 2.3024919033050537), (2, 2.302483081817627), (3, 2.3024728298187256), (4, 2.3024613857269287), (5, 2.3024494647979736), (6, 2.3024368286132812), (7, 2.3024232387542725), (8, 2.3024096488952637), (9, 2.3023951053619385), (10, 2.302380323410034)]
INFO flwr 2024-04-17 01:33:18,721 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0922), (1, 0.0924), (2, 0.0929), (3, 0.0945), (4, 0.0954), (5, 0.0962), (6, 0.0974), (7, 0.0982), (8, 0.0999), (9, 0.1011), (10, 0.1021)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1021
wandb:     loss 2.30238
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_012818-m09d6tr0
wandb: Find logs at: ./wandb/offline-run-20240417_012818-m09d6tr0/logs
INFO flwr 2024-04-17 01:33:22,218 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 01:40:29,618 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=266434)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=266434)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 01:40:34,216	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 01:40:35,048	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 01:40:35,493	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 01:40:35,828	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6adb9da3c41a031d.zip' (36.62MiB) to Ray cluster...
2024-04-17 01:40:35,942	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6adb9da3c41a031d.zip'.
INFO flwr 2024-04-17 01:40:47,415 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77162688921.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170046274151.0, 'GPU': 1.0}
INFO flwr 2024-04-17 01:40:47,415 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 01:40:47,415 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 01:40:47,435 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 01:40:47,436 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 01:40:47,436 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 01:40:47,436 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 01:40:50,038 | server.py:94 | initial parameters (loss, other metrics): 2.302473545074463, {'accuracy': 0.1771, 'data_size': 10000}
INFO flwr 2024-04-17 01:40:50,038 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 01:40:50,039 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=271282)[0m 2024-04-17 01:40:53.604012: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=271282)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=271343)[0m 2024-04-17 01:40:55.939962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=271343)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=271343)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=271351)[0m 2024-04-17 01:40:53.872960: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=271351)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=271352)[0m 2024-04-17 01:40:56.111755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 01:41:31,918 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 01:41:33,280 | server.py:125 | fit progress: (1, 1.9528261423110962, {'accuracy': 0.5082, 'data_size': 10000}, 43.24181870100438)
INFO flwr 2024-04-17 01:41:33,281 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 01:41:33,281 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:41:57,550 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 01:41:58,928 | server.py:125 | fit progress: (2, 2.258491039276123, {'accuracy': 0.2026, 'data_size': 10000}, 68.88973732600425)
INFO flwr 2024-04-17 01:41:58,929 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 01:41:58,929 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:42:21,421 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 01:42:22,612 | server.py:125 | fit progress: (3, 2.254742383956909, {'accuracy': 0.2064, 'data_size': 10000}, 92.57363374400302)
INFO flwr 2024-04-17 01:42:22,613 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 01:42:22,613 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:42:48,047 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 01:42:49,454 | server.py:125 | fit progress: (4, 2.254343271255493, {'accuracy': 0.2068, 'data_size': 10000}, 119.41540360000363)
INFO flwr 2024-04-17 01:42:49,454 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 01:42:49,455 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:43:16,348 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 01:43:17,547 | server.py:125 | fit progress: (5, 2.255643129348755, {'accuracy': 0.2055, 'data_size': 10000}, 147.5087620330014)
INFO flwr 2024-04-17 01:43:17,548 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 01:43:17,548 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:43:43,029 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 01:43:44,427 | server.py:125 | fit progress: (6, 2.2557432651519775, {'accuracy': 0.2054, 'data_size': 10000}, 174.38840293799876)
INFO flwr 2024-04-17 01:43:44,427 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 01:43:44,428 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:44:09,327 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 01:44:10,497 | server.py:125 | fit progress: (7, 2.2559432983398438, {'accuracy': 0.2052, 'data_size': 10000}, 200.45797964600206)
INFO flwr 2024-04-17 01:44:10,497 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 01:44:10,497 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:44:34,502 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 01:44:35,911 | server.py:125 | fit progress: (8, 2.2559432983398438, {'accuracy': 0.2052, 'data_size': 10000}, 225.87249403700116)
INFO flwr 2024-04-17 01:44:35,911 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 01:44:35,912 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:45:00,014 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 01:45:01,432 | server.py:125 | fit progress: (9, 2.2559432983398438, {'accuracy': 0.2052, 'data_size': 10000}, 251.39371352400485)
INFO flwr 2024-04-17 01:45:01,433 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 01:45:01,433 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:45:23,034 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 01:45:24,438 | server.py:125 | fit progress: (10, 2.25614333152771, {'accuracy': 0.205, 'data_size': 10000}, 274.3990069310021)
INFO flwr 2024-04-17 01:45:24,438 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 01:45:24,438 | server.py:153 | FL finished in 274.3994801700028
INFO flwr 2024-04-17 01:45:24,438 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 01:45:24,438 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 01:45:24,439 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 01:45:24,439 | app.py:229 | app_fit: losses_centralized [(0, 2.302473545074463), (1, 1.9528261423110962), (2, 2.258491039276123), (3, 2.254742383956909), (4, 2.254343271255493), (5, 2.255643129348755), (6, 2.2557432651519775), (7, 2.2559432983398438), (8, 2.2559432983398438), (9, 2.2559432983398438), (10, 2.25614333152771)]
INFO flwr 2024-04-17 01:45:24,439 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1771), (1, 0.5082), (2, 0.2026), (3, 0.2064), (4, 0.2068), (5, 0.2055), (6, 0.2054), (7, 0.2052), (8, 0.2052), (9, 0.2052), (10, 0.205)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.205
wandb:     loss 2.25614
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_014029-8x2zubw1
wandb: Find logs at: ./wandb/offline-run-20240417_014029-8x2zubw1/logs
INFO flwr 2024-04-17 01:45:27,945 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 01:52:35,315 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=271282)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=271282)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 01:52:39,758	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 01:52:40,645	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 01:52:41,112	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 01:52:41,468	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_37f1bcd3c76fc2de.zip' (36.63MiB) to Ray cluster...
2024-04-17 01:52:41,571	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_37f1bcd3c76fc2de.zip'.
INFO flwr 2024-04-17 01:52:52,570 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77172436992.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170069019648.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 01:52:52,570 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 01:52:52,570 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 01:52:52,591 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 01:52:52,592 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 01:52:52,592 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 01:52:52,593 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 01:52:55,415 | server.py:94 | initial parameters (loss, other metrics): 2.302797317504883, {'accuracy': 0.0684, 'data_size': 10000}
INFO flwr 2024-04-17 01:52:55,416 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 01:52:55,416 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=276190)[0m 2024-04-17 01:52:58.691345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=276190)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=276190)[0m 2024-04-17 01:53:00.971566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=276183)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=276183)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=276186)[0m 2024-04-17 01:52:58.817212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=276186)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=276185)[0m 2024-04-17 01:53:01.487413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 01:53:37,047 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 01:53:38,457 | server.py:125 | fit progress: (1, 2.21893572807312, {'accuracy': 0.5292, 'data_size': 10000}, 43.04079321699828)
INFO flwr 2024-04-17 01:53:38,457 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 01:53:38,458 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:54:02,070 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 01:54:03,506 | server.py:125 | fit progress: (2, 1.8187898397445679, {'accuracy': 0.6568, 'data_size': 10000}, 68.08924051900249)
INFO flwr 2024-04-17 01:54:03,506 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 01:54:03,506 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:54:27,267 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 01:54:28,439 | server.py:125 | fit progress: (3, 1.7209724187850952, {'accuracy': 0.7345, 'data_size': 10000}, 93.02262827300001)
INFO flwr 2024-04-17 01:54:28,439 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 01:54:28,439 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:54:52,535 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 01:54:53,940 | server.py:125 | fit progress: (4, 1.625400424003601, {'accuracy': 0.835, 'data_size': 10000}, 118.52322143499623)
INFO flwr 2024-04-17 01:54:53,940 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 01:54:53,940 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:55:19,230 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 01:55:20,417 | server.py:125 | fit progress: (5, 1.558062195777893, {'accuracy': 0.9033, 'data_size': 10000}, 145.00028088600084)
INFO flwr 2024-04-17 01:55:20,417 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 01:55:20,417 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:55:50,417 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 01:55:51,783 | server.py:125 | fit progress: (6, 1.5438050031661987, {'accuracy': 0.9178, 'data_size': 10000}, 176.36670464299823)
INFO flwr 2024-04-17 01:55:51,783 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 01:55:51,784 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:56:16,639 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 01:56:17,844 | server.py:125 | fit progress: (7, 1.5463101863861084, {'accuracy': 0.9147, 'data_size': 10000}, 202.42775401000108)
INFO flwr 2024-04-17 01:56:17,844 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 01:56:17,845 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:56:42,608 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 01:56:44,025 | server.py:125 | fit progress: (8, 1.5440857410430908, {'accuracy': 0.9166, 'data_size': 10000}, 228.60839881200081)
INFO flwr 2024-04-17 01:56:44,025 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 01:56:44,025 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:57:09,741 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 01:57:11,145 | server.py:125 | fit progress: (9, 1.5445386171340942, {'accuracy': 0.9164, 'data_size': 10000}, 255.7290473499961)
INFO flwr 2024-04-17 01:57:11,146 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 01:57:11,146 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 01:57:38,859 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 01:57:40,229 | server.py:125 | fit progress: (10, 1.5486596822738647, {'accuracy': 0.9122, 'data_size': 10000}, 284.8127170279986)
INFO flwr 2024-04-17 01:57:40,229 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 01:57:40,230 | server.py:153 | FL finished in 284.8132528139977
INFO flwr 2024-04-17 01:57:40,230 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 01:57:40,230 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 01:57:40,230 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 01:57:40,230 | app.py:229 | app_fit: losses_centralized [(0, 2.302797317504883), (1, 2.21893572807312), (2, 1.8187898397445679), (3, 1.7209724187850952), (4, 1.625400424003601), (5, 1.558062195777893), (6, 1.5438050031661987), (7, 1.5463101863861084), (8, 1.5440857410430908), (9, 1.5445386171340942), (10, 1.5486596822738647)]
INFO flwr 2024-04-17 01:57:40,230 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0684), (1, 0.5292), (2, 0.6568), (3, 0.7345), (4, 0.835), (5, 0.9033), (6, 0.9178), (7, 0.9147), (8, 0.9166), (9, 0.9164), (10, 0.9122)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9122
wandb:     loss 1.54866
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_015234-3vdhaq0n
wandb: Find logs at: ./wandb/offline-run-20240417_015234-3vdhaq0n/logs
INFO flwr 2024-04-17 01:57:43,771 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 02:04:51,147 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=276178)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=276178)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 02:04:55,627	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 02:04:56,471	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 02:04:56,930	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 02:04:57,290	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3c0417db49ec291f.zip' (36.64MiB) to Ray cluster...
2024-04-17 02:04:57,402	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3c0417db49ec291f.zip'.
INFO flwr 2024-04-17 02:05:08,386 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:TITAN': 1.0, 'memory': 170022255616.0, 'CPU': 64.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 77152395264.0}
INFO flwr 2024-04-17 02:05:08,386 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 02:05:08,386 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 02:05:08,404 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 02:05:08,405 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 02:05:08,405 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 02:05:08,405 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 02:05:10,993 | server.py:94 | initial parameters (loss, other metrics): 2.302461862564087, {'accuracy': 0.1244, 'data_size': 10000}
INFO flwr 2024-04-17 02:05:10,993 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 02:05:10,994 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=280595)[0m 2024-04-17 02:05:14.531909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=280595)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=280595)[0m 2024-04-17 02:05:16.878951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=280593)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=280593)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=280589)[0m 2024-04-17 02:05:14.699930: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=280589)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=280589)[0m 2024-04-17 02:05:16.989700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 02:05:49,528 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 02:05:50,961 | server.py:125 | fit progress: (1, 2.3015642166137695, {'accuracy': 0.1397, 'data_size': 10000}, 39.967388256998674)
INFO flwr 2024-04-17 02:05:50,961 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 02:05:50,961 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:06:16,879 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 02:06:18,286 | server.py:125 | fit progress: (2, 2.2993197441101074, {'accuracy': 0.3923, 'data_size': 10000}, 67.29240206900431)
INFO flwr 2024-04-17 02:06:18,286 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 02:06:18,286 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:06:44,616 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 02:06:45,809 | server.py:125 | fit progress: (3, 2.2945353984832764, {'accuracy': 0.6446, 'data_size': 10000}, 94.81583602500177)
INFO flwr 2024-04-17 02:06:45,810 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 02:06:45,810 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:07:11,976 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 02:07:13,411 | server.py:125 | fit progress: (4, 2.285313606262207, {'accuracy': 0.7677, 'data_size': 10000}, 122.41798201799975)
INFO flwr 2024-04-17 02:07:13,412 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 02:07:13,412 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:07:39,387 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 02:07:40,577 | server.py:125 | fit progress: (5, 2.2684903144836426, {'accuracy': 0.8119, 'data_size': 10000}, 149.5838436539998)
INFO flwr 2024-04-17 02:07:40,578 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 02:07:40,578 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:08:05,495 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 02:08:06,872 | server.py:125 | fit progress: (6, 2.2381653785705566, {'accuracy': 0.8195, 'data_size': 10000}, 175.87901021700236)
INFO flwr 2024-04-17 02:08:06,873 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 02:08:06,873 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:08:34,785 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 02:08:35,969 | server.py:125 | fit progress: (7, 2.1863656044006348, {'accuracy': 0.823, 'data_size': 10000}, 204.97543785800372)
INFO flwr 2024-04-17 02:08:35,969 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 02:08:35,969 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:09:01,610 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 02:09:03,001 | server.py:125 | fit progress: (8, 2.106139898300171, {'accuracy': 0.827, 'data_size': 10000}, 232.00710936199903)
INFO flwr 2024-04-17 02:09:03,001 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 02:09:03,001 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:09:30,453 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 02:09:31,860 | server.py:125 | fit progress: (9, 2.0025877952575684, {'accuracy': 0.831, 'data_size': 10000}, 260.866690775998)
INFO flwr 2024-04-17 02:09:31,860 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 02:09:31,861 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:09:57,424 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 02:09:58,822 | server.py:125 | fit progress: (10, 1.895107388496399, {'accuracy': 0.8372, 'data_size': 10000}, 287.828171576999)
INFO flwr 2024-04-17 02:09:58,822 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 02:09:58,822 | server.py:153 | FL finished in 287.8286166149992
INFO flwr 2024-04-17 02:09:58,822 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 02:09:58,822 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 02:09:58,823 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 02:09:58,823 | app.py:229 | app_fit: losses_centralized [(0, 2.302461862564087), (1, 2.3015642166137695), (2, 2.2993197441101074), (3, 2.2945353984832764), (4, 2.285313606262207), (5, 2.2684903144836426), (6, 2.2381653785705566), (7, 2.1863656044006348), (8, 2.106139898300171), (9, 2.0025877952575684), (10, 1.895107388496399)]
INFO flwr 2024-04-17 02:09:58,823 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1244), (1, 0.1397), (2, 0.3923), (3, 0.6446), (4, 0.7677), (5, 0.8119), (6, 0.8195), (7, 0.823), (8, 0.827), (9, 0.831), (10, 0.8372)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8372
wandb:     loss 1.89511
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_020450-69abden3
wandb: Find logs at: ./wandb/offline-run-20240417_020450-69abden3/logs
INFO flwr 2024-04-17 02:10:02,321 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 02:17:09,590 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=280584)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=280584)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 02:17:14,018	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 02:17:14,813	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 02:17:15,258	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 02:17:15,609	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1a6dafbd3b1d8abf.zip' (36.66MiB) to Ray cluster...
2024-04-17 02:17:15,721	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1a6dafbd3b1d8abf.zip'.
INFO flwr 2024-04-17 02:17:26,746 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'object_store_memory': 77171922124.0, 'CPU': 64.0, 'memory': 170067818292.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 02:17:26,747 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 02:17:26,747 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 02:17:26,765 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 02:17:26,784 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 02:17:26,784 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 02:17:26,785 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 02:17:29,462 | server.py:94 | initial parameters (loss, other metrics): 2.3026673793792725, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-17 02:17:29,462 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 02:17:29,463 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=286007)[0m 2024-04-17 02:17:32.988830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=286007)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=286011)[0m 2024-04-17 02:17:35.265111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=286011)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=286011)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=286009)[0m 2024-04-17 02:17:33.185539: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=286009)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=286009)[0m 2024-04-17 02:17:35.679184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 02:18:04,492 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 02:18:05,891 | server.py:125 | fit progress: (1, 2.302597761154175, {'accuracy': 0.1035, 'data_size': 10000}, 36.42798523199599)
INFO flwr 2024-04-17 02:18:05,891 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 02:18:05,891 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:18:30,768 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 02:18:32,155 | server.py:125 | fit progress: (2, 2.302496910095215, {'accuracy': 0.1055, 'data_size': 10000}, 62.69279390000156)
INFO flwr 2024-04-17 02:18:32,156 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 02:18:32,156 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:18:57,227 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 02:18:58,376 | server.py:125 | fit progress: (3, 2.302372694015503, {'accuracy': 0.1085, 'data_size': 10000}, 88.91382566699758)
INFO flwr 2024-04-17 02:18:58,377 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 02:18:58,377 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:19:22,713 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 02:19:24,110 | server.py:125 | fit progress: (4, 2.3022267818450928, {'accuracy': 0.1138, 'data_size': 10000}, 114.64730654400046)
INFO flwr 2024-04-17 02:19:24,110 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 02:19:24,110 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:19:50,382 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 02:19:51,533 | server.py:125 | fit progress: (5, 2.302056074142456, {'accuracy': 0.1201, 'data_size': 10000}, 142.0702152840022)
INFO flwr 2024-04-17 02:19:51,533 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 02:19:51,533 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:20:13,520 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 02:20:14,909 | server.py:125 | fit progress: (6, 2.301858901977539, {'accuracy': 0.1283, 'data_size': 10000}, 165.44620840199786)
INFO flwr 2024-04-17 02:20:14,909 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 02:20:14,909 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:20:38,958 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 02:20:40,103 | server.py:125 | fit progress: (7, 2.3016319274902344, {'accuracy': 0.1405, 'data_size': 10000}, 190.64061280700116)
INFO flwr 2024-04-17 02:20:40,104 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 02:20:40,104 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:21:05,836 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 02:21:07,257 | server.py:125 | fit progress: (8, 2.3013758659362793, {'accuracy': 0.1504, 'data_size': 10000}, 217.79412011100067)
INFO flwr 2024-04-17 02:21:07,257 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 02:21:07,257 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:21:32,172 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 02:21:33,567 | server.py:125 | fit progress: (9, 2.301086902618408, {'accuracy': 0.1622, 'data_size': 10000}, 244.10413659099868)
INFO flwr 2024-04-17 02:21:33,567 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 02:21:33,567 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:21:59,991 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 02:22:01,386 | server.py:125 | fit progress: (10, 2.300762891769409, {'accuracy': 0.1707, 'data_size': 10000}, 271.9234712339967)
INFO flwr 2024-04-17 02:22:01,386 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 02:22:01,387 | server.py:153 | FL finished in 271.9239093610013
INFO flwr 2024-04-17 02:22:01,387 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 02:22:01,387 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 02:22:01,387 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 02:22:01,387 | app.py:229 | app_fit: losses_centralized [(0, 2.3026673793792725), (1, 2.302597761154175), (2, 2.302496910095215), (3, 2.302372694015503), (4, 2.3022267818450928), (5, 2.302056074142456), (6, 2.301858901977539), (7, 2.3016319274902344), (8, 2.3013758659362793), (9, 2.301086902618408), (10, 2.300762891769409)]
INFO flwr 2024-04-17 02:22:01,387 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.1035), (2, 0.1055), (3, 0.1085), (4, 0.1138), (5, 0.1201), (6, 0.1283), (7, 0.1405), (8, 0.1504), (9, 0.1622), (10, 0.1707)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1707
wandb:     loss 2.30076
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_021709-jnzkna37
wandb: Find logs at: ./wandb/offline-run-20240417_021709-jnzkna37/logs
INFO flwr 2024-04-17 02:22:04,942 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 02:29:12,179 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=286006)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=286006)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 02:29:17,870	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 02:29:18,607	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 02:29:19,057	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 02:29:19,419	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3e1e04078888689c.zip' (36.67MiB) to Ray cluster...
2024-04-17 02:29:19,533	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3e1e04078888689c.zip'.
INFO flwr 2024-04-17 02:29:30,518 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 170082489754.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77178209894.0}
INFO flwr 2024-04-17 02:29:30,519 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 02:29:30,519 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 02:29:30,538 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 02:29:30,540 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 02:29:30,540 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 02:29:30,541 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 02:29:33,302 | server.py:94 | initial parameters (loss, other metrics): 2.302553415298462, {'accuracy': 0.109, 'data_size': 10000}
INFO flwr 2024-04-17 02:29:33,302 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 02:29:33,303 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=290417)[0m 2024-04-17 02:29:40.844546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=290417)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=290417)[0m 2024-04-17 02:29:43.124858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=290418)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=290418)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=290415)[0m 2024-04-17 02:29:41.104980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=290415)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=290415)[0m 2024-04-17 02:29:43.236510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 02:30:16,864 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 02:30:18,244 | server.py:125 | fit progress: (1, 2.3025479316711426, {'accuracy': 0.1092, 'data_size': 10000}, 44.941824045999965)
INFO flwr 2024-04-17 02:30:18,245 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 02:30:18,245 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:30:46,371 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 02:30:47,777 | server.py:125 | fit progress: (2, 2.3025403022766113, {'accuracy': 0.1094, 'data_size': 10000}, 74.47481898200203)
INFO flwr 2024-04-17 02:30:47,778 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 02:30:47,778 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:31:13,556 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 02:31:14,732 | server.py:125 | fit progress: (3, 2.3025310039520264, {'accuracy': 0.1103, 'data_size': 10000}, 101.42976812000416)
INFO flwr 2024-04-17 02:31:14,733 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 02:31:14,733 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:31:39,859 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 02:31:41,263 | server.py:125 | fit progress: (4, 2.302520275115967, {'accuracy': 0.1109, 'data_size': 10000}, 127.95999785700405)
INFO flwr 2024-04-17 02:31:41,263 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 02:31:41,263 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:32:05,572 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 02:32:06,735 | server.py:125 | fit progress: (5, 2.3025100231170654, {'accuracy': 0.1115, 'data_size': 10000}, 153.43218009400152)
INFO flwr 2024-04-17 02:32:06,735 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 02:32:06,735 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:32:31,604 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 02:32:32,997 | server.py:125 | fit progress: (6, 2.302497625350952, {'accuracy': 0.1126, 'data_size': 10000}, 179.69473303700215)
INFO flwr 2024-04-17 02:32:32,998 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 02:32:32,998 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:32:56,688 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 02:32:57,867 | server.py:125 | fit progress: (7, 2.3024847507476807, {'accuracy': 0.1142, 'data_size': 10000}, 204.56426524300332)
INFO flwr 2024-04-17 02:32:57,867 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 02:32:57,867 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:33:21,978 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 02:33:23,364 | server.py:125 | fit progress: (8, 2.302471876144409, {'accuracy': 0.1149, 'data_size': 10000}, 230.06110173599882)
INFO flwr 2024-04-17 02:33:23,364 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 02:33:23,364 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:33:47,903 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 02:33:49,292 | server.py:125 | fit progress: (9, 2.3024585247039795, {'accuracy': 0.1161, 'data_size': 10000}, 255.9892092220034)
INFO flwr 2024-04-17 02:33:49,292 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 02:33:49,292 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:34:15,783 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 02:34:17,181 | server.py:125 | fit progress: (10, 2.302445411682129, {'accuracy': 0.1168, 'data_size': 10000}, 283.8783439980034)
INFO flwr 2024-04-17 02:34:17,181 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 02:34:17,181 | server.py:153 | FL finished in 283.8787952310013
INFO flwr 2024-04-17 02:34:17,181 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 02:34:17,182 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 02:34:17,182 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 02:34:17,182 | app.py:229 | app_fit: losses_centralized [(0, 2.302553415298462), (1, 2.3025479316711426), (2, 2.3025403022766113), (3, 2.3025310039520264), (4, 2.302520275115967), (5, 2.3025100231170654), (6, 2.302497625350952), (7, 2.3024847507476807), (8, 2.302471876144409), (9, 2.3024585247039795), (10, 2.302445411682129)]
INFO flwr 2024-04-17 02:34:17,182 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.109), (1, 0.1092), (2, 0.1094), (3, 0.1103), (4, 0.1109), (5, 0.1115), (6, 0.1126), (7, 0.1142), (8, 0.1149), (9, 0.1161), (10, 0.1168)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1168
wandb:     loss 2.30245
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_022911-mrq79y3s
wandb: Find logs at: ./wandb/offline-run-20240417_022911-mrq79y3s/logs
INFO flwr 2024-04-17 02:34:20,726 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 02:41:28,360 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=290415)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=290415)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 02:41:33,100	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 02:41:33,885	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 02:41:34,369	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 02:41:34,726	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8906a06c68b5b012.zip' (36.68MiB) to Ray cluster...
2024-04-17 02:41:34,844	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8906a06c68b5b012.zip'.
INFO flwr 2024-04-17 02:41:45,966 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77124877516.0, 'CPU': 64.0, 'memory': 169958047540.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 02:41:45,967 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 02:41:45,967 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 02:41:45,986 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 02:41:45,986 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 02:41:45,987 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 02:41:45,987 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 02:41:49,566 | server.py:94 | initial parameters (loss, other metrics): 2.3025448322296143, {'accuracy': 0.1005, 'data_size': 10000}
INFO flwr 2024-04-17 02:41:49,566 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 02:41:49,566 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=295655)[0m 2024-04-17 02:41:51.998545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=295655)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=295655)[0m 2024-04-17 02:41:54.329953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=295663)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=295663)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=295654)[0m 2024-04-17 02:41:52.254883: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=295654)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=295662)[0m 2024-04-17 02:41:55.022997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 02:42:30,764 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 02:42:32,171 | server.py:125 | fit progress: (1, 1.738383173942566, {'accuracy': 0.7222, 'data_size': 10000}, 42.60459099799482)
INFO flwr 2024-04-17 02:42:32,171 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 02:42:32,171 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:42:56,025 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 02:42:57,385 | server.py:125 | fit progress: (2, 2.1555356979370117, {'accuracy': 0.3056, 'data_size': 10000}, 67.81875467299687)
INFO flwr 2024-04-17 02:42:57,385 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 02:42:57,385 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:43:23,358 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 02:43:24,542 | server.py:125 | fit progress: (3, 2.22994327545166, {'accuracy': 0.2312, 'data_size': 10000}, 94.97614922999492)
INFO flwr 2024-04-17 02:43:24,543 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 02:43:24,543 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:43:49,474 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 02:43:50,835 | server.py:125 | fit progress: (4, 2.1693429946899414, {'accuracy': 0.2918, 'data_size': 10000}, 121.268937558998)
INFO flwr 2024-04-17 02:43:50,835 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 02:43:50,836 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:44:15,767 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 02:44:16,933 | server.py:125 | fit progress: (5, 2.2657418251037598, {'accuracy': 0.1954, 'data_size': 10000}, 147.36673491799593)
INFO flwr 2024-04-17 02:44:16,933 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 02:44:16,933 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:44:42,447 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 02:44:43,825 | server.py:125 | fit progress: (6, 2.26684308052063, {'accuracy': 0.1943, 'data_size': 10000}, 174.2591399449957)
INFO flwr 2024-04-17 02:44:43,826 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 02:44:43,826 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:45:08,002 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 02:45:09,159 | server.py:125 | fit progress: (7, 2.2773430347442627, {'accuracy': 0.1838, 'data_size': 10000}, 199.5924306300003)
INFO flwr 2024-04-17 02:45:09,159 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 02:45:09,160 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:45:29,729 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 02:45:31,113 | server.py:125 | fit progress: (8, 2.317842483520508, {'accuracy': 0.1433, 'data_size': 10000}, 221.54671977999533)
INFO flwr 2024-04-17 02:45:31,113 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 02:45:31,113 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:45:55,820 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 02:45:57,185 | server.py:125 | fit progress: (9, 2.356842279434204, {'accuracy': 0.1043, 'data_size': 10000}, 247.6186632809986)
INFO flwr 2024-04-17 02:45:57,185 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 02:45:57,185 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:46:24,637 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 02:46:26,033 | server.py:125 | fit progress: (10, 2.360042095184326, {'accuracy': 0.1011, 'data_size': 10000}, 276.4665755199967)
INFO flwr 2024-04-17 02:46:26,033 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 02:46:26,033 | server.py:153 | FL finished in 276.46703421299753
INFO flwr 2024-04-17 02:46:26,033 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 02:46:26,034 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 02:46:26,034 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 02:46:26,034 | app.py:229 | app_fit: losses_centralized [(0, 2.3025448322296143), (1, 1.738383173942566), (2, 2.1555356979370117), (3, 2.22994327545166), (4, 2.1693429946899414), (5, 2.2657418251037598), (6, 2.26684308052063), (7, 2.2773430347442627), (8, 2.317842483520508), (9, 2.356842279434204), (10, 2.360042095184326)]
INFO flwr 2024-04-17 02:46:26,034 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1005), (1, 0.7222), (2, 0.3056), (3, 0.2312), (4, 0.2918), (5, 0.1954), (6, 0.1943), (7, 0.1838), (8, 0.1433), (9, 0.1043), (10, 0.1011)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1011
wandb:     loss 2.36004
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_024128-n8t6crvq
wandb: Find logs at: ./wandb/offline-run-20240417_024128-n8t6crvq/logs
INFO flwr 2024-04-17 02:46:29,586 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 02:53:36,865 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=295653)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=295653)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 02:53:41,380	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 02:53:42,205	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 02:53:42,696	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 02:53:43,048	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_894d0254c3b08f50.zip' (36.69MiB) to Ray cluster...
2024-04-17 02:53:43,163	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_894d0254c3b08f50.zip'.
INFO flwr 2024-04-17 02:53:54,238 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77141295513.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169996356199.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 02:53:54,239 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 02:53:54,239 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 02:53:54,256 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 02:53:54,257 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 02:53:54,257 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 02:53:54,257 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 02:53:57,348 | server.py:94 | initial parameters (loss, other metrics): 2.3026821613311768, {'accuracy': 0.089, 'data_size': 10000}
INFO flwr 2024-04-17 02:53:57,349 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 02:53:57,349 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=300146)[0m 2024-04-17 02:54:00.351381: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=300146)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=300146)[0m 2024-04-17 02:54:02.744199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=300235)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=300235)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=300151)[0m 2024-04-17 02:54:00.617073: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=300151)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=300151)[0m 2024-04-17 02:54:02.881813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 02:54:33,990 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 02:54:35,394 | server.py:125 | fit progress: (1, 2.241593837738037, {'accuracy': 0.7793, 'data_size': 10000}, 38.04507630500302)
INFO flwr 2024-04-17 02:54:35,394 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 02:54:35,395 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:55:03,287 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 02:55:04,699 | server.py:125 | fit progress: (2, 1.7582687139511108, {'accuracy': 0.7892, 'data_size': 10000}, 67.34978467100154)
INFO flwr 2024-04-17 02:55:04,699 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 02:55:04,699 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:55:28,710 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 02:55:29,862 | server.py:125 | fit progress: (3, 1.621626615524292, {'accuracy': 0.8408, 'data_size': 10000}, 92.51243614900159)
INFO flwr 2024-04-17 02:55:29,862 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 02:55:29,862 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:55:57,450 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 02:55:58,871 | server.py:125 | fit progress: (4, 1.584660530090332, {'accuracy': 0.876, 'data_size': 10000}, 121.52193606000219)
INFO flwr 2024-04-17 02:55:58,871 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 02:55:58,872 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:56:25,616 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 02:56:26,767 | server.py:125 | fit progress: (5, 1.5654964447021484, {'accuracy': 0.8951, 'data_size': 10000}, 149.41805091100105)
INFO flwr 2024-04-17 02:56:26,767 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 02:56:26,768 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:56:50,455 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 02:56:51,840 | server.py:125 | fit progress: (6, 1.5581222772598267, {'accuracy': 0.9028, 'data_size': 10000}, 174.4910244929997)
INFO flwr 2024-04-17 02:56:51,840 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 02:56:51,841 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:57:19,594 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 02:57:20,754 | server.py:125 | fit progress: (7, 1.5660078525543213, {'accuracy': 0.8948, 'data_size': 10000}, 203.40486135899846)
INFO flwr 2024-04-17 02:57:20,754 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 02:57:20,754 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:57:45,387 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 02:57:46,805 | server.py:125 | fit progress: (8, 1.5763185024261475, {'accuracy': 0.8841, 'data_size': 10000}, 229.45640686099796)
INFO flwr 2024-04-17 02:57:46,806 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 02:57:46,806 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:58:10,571 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 02:58:11,931 | server.py:125 | fit progress: (9, 1.583389163017273, {'accuracy': 0.8775, 'data_size': 10000}, 254.5822995080016)
INFO flwr 2024-04-17 02:58:11,932 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 02:58:11,932 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 02:58:37,218 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 02:58:38,597 | server.py:125 | fit progress: (10, 1.5868233442306519, {'accuracy': 0.8741, 'data_size': 10000}, 281.2481289580028)
INFO flwr 2024-04-17 02:58:38,598 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 02:58:38,598 | server.py:153 | FL finished in 281.2485744989972
INFO flwr 2024-04-17 02:58:38,598 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 02:58:38,598 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 02:58:38,598 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 02:58:38,598 | app.py:229 | app_fit: losses_centralized [(0, 2.3026821613311768), (1, 2.241593837738037), (2, 1.7582687139511108), (3, 1.621626615524292), (4, 1.584660530090332), (5, 1.5654964447021484), (6, 1.5581222772598267), (7, 1.5660078525543213), (8, 1.5763185024261475), (9, 1.583389163017273), (10, 1.5868233442306519)]
INFO flwr 2024-04-17 02:58:38,598 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.089), (1, 0.7793), (2, 0.7892), (3, 0.8408), (4, 0.876), (5, 0.8951), (6, 0.9028), (7, 0.8948), (8, 0.8841), (9, 0.8775), (10, 0.8741)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8741
wandb:     loss 1.58682
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_025336-pi1ufrz3
wandb: Find logs at: ./wandb/offline-run-20240417_025336-pi1ufrz3/logs
INFO flwr 2024-04-17 02:58:42,150 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 03:05:49,640 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=300146)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=300146)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 03:05:54,359	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 03:05:55,241	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 03:05:55,723	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 03:05:56,075	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b8026b57f6e714da.zip' (36.70MiB) to Ray cluster...
2024-04-17 03:05:56,197	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b8026b57f6e714da.zip'.
INFO flwr 2024-04-17 03:06:07,336 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77141069414.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 169995828634.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 03:06:07,337 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 03:06:07,337 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 03:06:07,355 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 03:06:07,356 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 03:06:07,357 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 03:06:07,357 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 03:06:10,156 | server.py:94 | initial parameters (loss, other metrics): 2.302520275115967, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-17 03:06:10,156 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 03:06:10,157 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=304564)[0m 2024-04-17 03:06:13.533663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=304564)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=304561)[0m 2024-04-17 03:06:15.853395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=304564)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=304564)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=304565)[0m 2024-04-17 03:06:13.812365: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=304565)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=304565)[0m 2024-04-17 03:06:16.087577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 03:06:51,693 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 03:06:53,091 | server.py:125 | fit progress: (1, 2.301656723022461, {'accuracy': 0.0958, 'data_size': 10000}, 42.933921894997184)
INFO flwr 2024-04-17 03:06:53,091 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 03:06:53,091 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:07:16,830 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 03:07:18,209 | server.py:125 | fit progress: (2, 2.2997350692749023, {'accuracy': 0.1214, 'data_size': 10000}, 68.05231024299428)
INFO flwr 2024-04-17 03:07:18,209 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 03:07:18,210 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:07:42,221 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 03:07:43,389 | server.py:125 | fit progress: (3, 2.2958827018737793, {'accuracy': 0.2373, 'data_size': 10000}, 93.23205646000133)
INFO flwr 2024-04-17 03:07:43,389 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 03:07:43,390 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:08:07,243 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 03:08:08,619 | server.py:125 | fit progress: (4, 2.2887964248657227, {'accuracy': 0.4816, 'data_size': 10000}, 118.46197014599602)
INFO flwr 2024-04-17 03:08:08,619 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 03:08:08,619 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:08:32,925 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 03:08:34,102 | server.py:125 | fit progress: (5, 2.27628493309021, {'accuracy': 0.7201, 'data_size': 10000}, 143.94516890399973)
INFO flwr 2024-04-17 03:08:34,102 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 03:08:34,103 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:08:58,967 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 03:09:00,341 | server.py:125 | fit progress: (6, 2.2549750804901123, {'accuracy': 0.829, 'data_size': 10000}, 170.18439901500096)
INFO flwr 2024-04-17 03:09:00,342 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 03:09:00,342 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:09:27,181 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 03:09:28,347 | server.py:125 | fit progress: (7, 2.219440221786499, {'accuracy': 0.8648, 'data_size': 10000}, 198.189684484998)
INFO flwr 2024-04-17 03:09:28,347 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 03:09:28,347 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:09:54,453 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 03:09:55,824 | server.py:125 | fit progress: (8, 2.1616904735565186, {'accuracy': 0.8799, 'data_size': 10000}, 225.66730609299702)
INFO flwr 2024-04-17 03:09:55,825 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 03:09:55,825 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:10:21,555 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 03:10:22,940 | server.py:125 | fit progress: (9, 2.075911283493042, {'accuracy': 0.885, 'data_size': 10000}, 252.78357787099958)
INFO flwr 2024-04-17 03:10:22,941 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 03:10:22,941 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:10:50,744 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 03:10:52,145 | server.py:125 | fit progress: (10, 1.9690779447555542, {'accuracy': 0.8857, 'data_size': 10000}, 281.987991875998)
INFO flwr 2024-04-17 03:10:52,145 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 03:10:52,145 | server.py:153 | FL finished in 281.98851128800015
INFO flwr 2024-04-17 03:10:52,146 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 03:10:52,146 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 03:10:52,146 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 03:10:52,146 | app.py:229 | app_fit: losses_centralized [(0, 2.302520275115967), (1, 2.301656723022461), (2, 2.2997350692749023), (3, 2.2958827018737793), (4, 2.2887964248657227), (5, 2.27628493309021), (6, 2.2549750804901123), (7, 2.219440221786499), (8, 2.1616904735565186), (9, 2.075911283493042), (10, 1.9690779447555542)]
INFO flwr 2024-04-17 03:10:52,146 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.0958), (2, 0.1214), (3, 0.2373), (4, 0.4816), (5, 0.7201), (6, 0.829), (7, 0.8648), (8, 0.8799), (9, 0.885), (10, 0.8857)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8857
wandb:     loss 1.96908
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_030549-11pwarwb
wandb: Find logs at: ./wandb/offline-run-20240417_030549-11pwarwb/logs
INFO flwr 2024-04-17 03:10:55,694 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 03:18:03,374 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=304555)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=304555)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 03:18:08,117	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 03:18:08,902	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 03:18:09,402	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 03:18:09,755	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7197a76e32ffd924.zip' (36.71MiB) to Ray cluster...
2024-04-17 03:18:09,875	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7197a76e32ffd924.zip'.
INFO flwr 2024-04-17 03:18:20,983 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77140329676.0, 'accelerator_type:TITAN': 1.0, 'memory': 169994102580.0, 'CPU': 64.0}
INFO flwr 2024-04-17 03:18:20,984 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 03:18:20,984 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 03:18:21,000 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 03:18:21,001 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 03:18:21,001 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 03:18:21,002 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 03:18:23,669 | server.py:94 | initial parameters (loss, other metrics): 2.302640914916992, {'accuracy': 0.1017, 'data_size': 10000}
INFO flwr 2024-04-17 03:18:23,669 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 03:18:23,670 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=309907)[0m 2024-04-17 03:18:27.215761: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=309907)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=309907)[0m 2024-04-17 03:18:29.584848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=309902)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=309902)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=309894)[0m 2024-04-17 03:18:27.357263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=309894)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=309902)[0m 2024-04-17 03:18:29.649066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 03:19:03,498 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 03:19:04,919 | server.py:125 | fit progress: (1, 2.302582025527954, {'accuracy': 0.1036, 'data_size': 10000}, 41.2496397549985)
INFO flwr 2024-04-17 03:19:04,920 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 03:19:04,920 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:19:29,521 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 03:19:30,903 | server.py:125 | fit progress: (2, 2.302501916885376, {'accuracy': 0.1069, 'data_size': 10000}, 67.2330519629977)
INFO flwr 2024-04-17 03:19:30,903 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 03:19:30,903 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:19:54,323 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 03:19:55,462 | server.py:125 | fit progress: (3, 2.302401065826416, {'accuracy': 0.1111, 'data_size': 10000}, 91.7929683350012)
INFO flwr 2024-04-17 03:19:55,463 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 03:19:55,463 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:20:23,647 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 03:20:25,039 | server.py:125 | fit progress: (4, 2.3022801876068115, {'accuracy': 0.1172, 'data_size': 10000}, 121.36919352399855)
INFO flwr 2024-04-17 03:20:25,039 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 03:20:25,039 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:20:53,543 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 03:20:54,713 | server.py:125 | fit progress: (5, 2.302137613296509, {'accuracy': 0.1234, 'data_size': 10000}, 151.04361843600054)
INFO flwr 2024-04-17 03:20:54,713 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 03:20:54,714 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:21:17,660 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 03:21:19,007 | server.py:125 | fit progress: (6, 2.301976442337036, {'accuracy': 0.1309, 'data_size': 10000}, 175.33727568200266)
INFO flwr 2024-04-17 03:21:19,007 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 03:21:19,007 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:21:44,363 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 03:21:45,604 | server.py:125 | fit progress: (7, 2.301793336868286, {'accuracy': 0.1381, 'data_size': 10000}, 201.9341431080029)
INFO flwr 2024-04-17 03:21:45,604 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 03:21:45,604 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:22:12,094 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 03:22:13,460 | server.py:125 | fit progress: (8, 2.3015873432159424, {'accuracy': 0.1449, 'data_size': 10000}, 229.79090981100308)
INFO flwr 2024-04-17 03:22:13,461 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 03:22:13,461 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:22:39,044 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 03:22:40,450 | server.py:125 | fit progress: (9, 2.301356315612793, {'accuracy': 0.1508, 'data_size': 10000}, 256.78017683899816)
INFO flwr 2024-04-17 03:22:40,450 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 03:22:40,450 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:23:09,240 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 03:23:10,639 | server.py:125 | fit progress: (10, 2.3010973930358887, {'accuracy': 0.1587, 'data_size': 10000}, 286.9694884440032)
INFO flwr 2024-04-17 03:23:10,639 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 03:23:10,639 | server.py:153 | FL finished in 286.96995124300156
INFO flwr 2024-04-17 03:23:10,640 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 03:23:10,640 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 03:23:10,640 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 03:23:10,640 | app.py:229 | app_fit: losses_centralized [(0, 2.302640914916992), (1, 2.302582025527954), (2, 2.302501916885376), (3, 2.302401065826416), (4, 2.3022801876068115), (5, 2.302137613296509), (6, 2.301976442337036), (7, 2.301793336868286), (8, 2.3015873432159424), (9, 2.301356315612793), (10, 2.3010973930358887)]
INFO flwr 2024-04-17 03:23:10,640 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1017), (1, 0.1036), (2, 0.1069), (3, 0.1111), (4, 0.1172), (5, 0.1234), (6, 0.1309), (7, 0.1381), (8, 0.1449), (9, 0.1508), (10, 0.1587)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1587
wandb:     loss 2.3011
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_031802-gr2z46w3
wandb: Find logs at: ./wandb/offline-run-20240417_031802-gr2z46w3/logs
INFO flwr 2024-04-17 03:23:14,186 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 03:30:22,093 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=309894)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=309894)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 03:30:26,589	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 03:30:27,417	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 03:30:27,876	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 03:30:28,225	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b18fbe09880240c1.zip' (36.73MiB) to Ray cluster...
2024-04-17 03:30:28,341	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b18fbe09880240c1.zip'.
INFO flwr 2024-04-17 03:30:39,374 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169992416666.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'object_store_memory': 77139607142.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 03:30:39,375 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 03:30:39,375 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 03:30:39,392 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 03:30:39,394 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 03:30:39,394 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 03:30:39,394 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 03:30:42,267 | server.py:94 | initial parameters (loss, other metrics): 2.302516222000122, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-17 03:30:42,267 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 03:30:42,267 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=314309)[0m 2024-04-17 03:30:45.524030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=314309)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=314309)[0m 2024-04-17 03:30:47.817911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=314400)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=314400)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=314306)[0m 2024-04-17 03:30:45.766820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=314306)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=314305)[0m 2024-04-17 03:30:47.927183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 03:31:23,049 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 03:31:24,463 | server.py:125 | fit progress: (1, 2.3025100231170654, {'accuracy': 0.0958, 'data_size': 10000}, 42.19584365699848)
INFO flwr 2024-04-17 03:31:24,463 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 03:31:24,464 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:31:51,705 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 03:31:53,161 | server.py:125 | fit progress: (2, 2.302499532699585, {'accuracy': 0.0958, 'data_size': 10000}, 70.89325757899496)
INFO flwr 2024-04-17 03:31:53,161 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 03:31:53,161 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:32:19,423 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 03:32:20,590 | server.py:125 | fit progress: (3, 2.3024890422821045, {'accuracy': 0.0958, 'data_size': 10000}, 98.32247131099575)
INFO flwr 2024-04-17 03:32:20,590 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 03:32:20,590 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:32:46,106 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 03:32:47,508 | server.py:125 | fit progress: (4, 2.3024771213531494, {'accuracy': 0.0958, 'data_size': 10000}, 125.24103029199614)
INFO flwr 2024-04-17 03:32:47,509 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 03:32:47,509 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:33:11,358 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 03:33:12,518 | server.py:125 | fit progress: (5, 2.302464723587036, {'accuracy': 0.0958, 'data_size': 10000}, 150.25058694199834)
INFO flwr 2024-04-17 03:33:12,518 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 03:33:12,519 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:33:39,837 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 03:33:41,219 | server.py:125 | fit progress: (6, 2.302450656890869, {'accuracy': 0.0958, 'data_size': 10000}, 178.95206683099968)
INFO flwr 2024-04-17 03:33:41,220 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 03:33:41,236 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:34:07,226 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 03:34:08,419 | server.py:125 | fit progress: (7, 2.302436590194702, {'accuracy': 0.0958, 'data_size': 10000}, 206.15195079399564)
INFO flwr 2024-04-17 03:34:08,420 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 03:34:08,420 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:34:39,935 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 03:34:41,339 | server.py:125 | fit progress: (8, 2.302422523498535, {'accuracy': 0.0958, 'data_size': 10000}, 239.07189502199617)
INFO flwr 2024-04-17 03:34:41,340 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 03:34:41,340 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:35:05,694 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 03:35:07,068 | server.py:125 | fit progress: (9, 2.3024075031280518, {'accuracy': 0.0958, 'data_size': 10000}, 264.80065829099476)
INFO flwr 2024-04-17 03:35:07,068 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 03:35:07,068 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:35:32,282 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 03:35:33,665 | server.py:125 | fit progress: (10, 2.3023924827575684, {'accuracy': 0.0958, 'data_size': 10000}, 291.3980210689988)
INFO flwr 2024-04-17 03:35:33,666 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 03:35:33,666 | server.py:153 | FL finished in 291.3985486899983
INFO flwr 2024-04-17 03:35:33,666 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 03:35:33,666 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 03:35:33,666 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 03:35:33,666 | app.py:229 | app_fit: losses_centralized [(0, 2.302516222000122), (1, 2.3025100231170654), (2, 2.302499532699585), (3, 2.3024890422821045), (4, 2.3024771213531494), (5, 2.302464723587036), (6, 2.302450656890869), (7, 2.302436590194702), (8, 2.302422523498535), (9, 2.3024075031280518), (10, 2.3023924827575684)]
INFO flwr 2024-04-17 03:35:33,667 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.0958), (2, 0.0958), (3, 0.0958), (4, 0.0958), (5, 0.0958), (6, 0.0958), (7, 0.0958), (8, 0.0958), (9, 0.0958), (10, 0.0958)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0958
wandb:     loss 2.30239
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_033021-rlkaptav
wandb: Find logs at: ./wandb/offline-run-20240417_033021-rlkaptav/logs
INFO flwr 2024-04-17 03:35:37,203 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 03:42:44,244 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=314304)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=314304)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 03:42:48,959	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 03:42:49,777	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 03:42:50,245	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 03:42:50,606	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_97f299b219d8c328.zip' (36.74MiB) to Ray cluster...
2024-04-17 03:42:50,724	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_97f299b219d8c328.zip'.
INFO flwr 2024-04-17 03:43:01,775 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77156302848.0, 'memory': 170031373312.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 03:43:01,776 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 03:43:01,776 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 03:43:01,795 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 03:43:01,796 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 03:43:01,796 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 03:43:01,796 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 03:43:04,926 | server.py:94 | initial parameters (loss, other metrics): 2.302703619003296, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-17 03:43:04,932 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 03:43:04,933 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=319669)[0m 2024-04-17 03:43:07.906337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=319669)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=319669)[0m 2024-04-17 03:43:10.269212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=319679)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=319679)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=319667)[0m 2024-04-17 03:43:08.365151: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=319667)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=319667)[0m 2024-04-17 03:43:10.734224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 03:43:26,399 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 03:43:27,795 | server.py:125 | fit progress: (1, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 22.86190791800618)
INFO flwr 2024-04-17 03:43:27,795 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 03:43:27,795 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:43:37,260 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 03:43:38,624 | server.py:125 | fit progress: (2, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 33.690899714005354)
INFO flwr 2024-04-17 03:43:38,624 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 03:43:38,624 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:43:47,349 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 03:43:48,504 | server.py:125 | fit progress: (3, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 43.57099076300074)
INFO flwr 2024-04-17 03:43:48,504 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 03:43:48,504 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:43:57,192 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 03:43:58,561 | server.py:125 | fit progress: (4, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 53.62862961200153)
INFO flwr 2024-04-17 03:43:58,562 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 03:43:58,562 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:44:07,410 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 03:44:08,574 | server.py:125 | fit progress: (5, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 63.641744879001635)
INFO flwr 2024-04-17 03:44:08,575 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 03:44:08,575 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:44:17,761 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 03:44:18,934 | server.py:125 | fit progress: (6, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 74.00162811000337)
INFO flwr 2024-04-17 03:44:18,935 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 03:44:18,935 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:44:27,599 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 03:44:28,961 | server.py:125 | fit progress: (7, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 84.02830533900124)
INFO flwr 2024-04-17 03:44:28,961 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 03:44:28,962 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:44:37,639 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 03:44:38,781 | server.py:125 | fit progress: (8, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 93.84861457300576)
INFO flwr 2024-04-17 03:44:38,782 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 03:44:38,782 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:44:47,836 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 03:44:49,232 | server.py:125 | fit progress: (9, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 104.29914931500389)
INFO flwr 2024-04-17 03:44:49,232 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 03:44:49,233 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:44:58,268 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 03:44:59,420 | server.py:125 | fit progress: (10, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 114.48721639300493)
INFO flwr 2024-04-17 03:44:59,420 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 03:44:59,420 | server.py:153 | FL finished in 114.48768419700355
INFO flwr 2024-04-17 03:44:59,421 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 03:44:59,421 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 03:44:59,421 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 03:44:59,421 | app.py:229 | app_fit: losses_centralized [(0, 2.302703619003296), (1, 2.3631420135498047), (2, 2.3631420135498047), (3, 2.3631420135498047), (4, 2.3631420135498047), (5, 2.3631420135498047), (6, 2.3631420135498047), (7, 2.3631420135498047), (8, 2.3631420135498047), (9, 2.3631420135498047), (10, 2.3631420135498047)]
INFO flwr 2024-04-17 03:44:59,421 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.098), (2, 0.098), (3, 0.098), (4, 0.098), (5, 0.098), (6, 0.098), (7, 0.098), (8, 0.098), (9, 0.098), (10, 0.098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.098
wandb:     loss 2.36314
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_034243-i6ev4h2t
wandb: Find logs at: ./wandb/offline-run-20240417_034243-i6ev4h2t/logs
INFO flwr 2024-04-17 03:45:02,974 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 03:52:10,963 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=319667)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=319667)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 03:52:16,086	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 03:52:16,966	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 03:52:17,379	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 03:52:17,712	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8222279df586eac2.zip' (36.75MiB) to Ray cluster...
2024-04-17 03:52:17,832	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8222279df586eac2.zip'.
INFO flwr 2024-04-17 03:52:28,865 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 77082479001.0, 'memory': 169859117671.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 03:52:28,866 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 03:52:28,866 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 03:52:28,884 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 03:52:28,885 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 03:52:28,885 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 03:52:28,885 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 03:52:32,140 | server.py:94 | initial parameters (loss, other metrics): 2.3026130199432373, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-17 03:52:32,140 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 03:52:32,140 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=324032)[0m 2024-04-17 03:52:35.115727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=324032)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=324031)[0m 2024-04-17 03:52:37.402424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=324031)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=324031)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=324025)[0m 2024-04-17 03:52:35.560475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=324025)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=324025)[0m 2024-04-17 03:52:37.886016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 03:52:53,496 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 03:52:54,921 | server.py:125 | fit progress: (1, 2.204740285873413, {'accuracy': 0.4125, 'data_size': 10000}, 22.780382996003027)
INFO flwr 2024-04-17 03:52:54,921 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 03:52:54,921 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:53:04,129 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 03:53:05,506 | server.py:125 | fit progress: (2, 1.8836569786071777, {'accuracy': 0.5959, 'data_size': 10000}, 33.36533344000054)
INFO flwr 2024-04-17 03:53:05,506 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 03:53:05,506 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:53:14,545 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 03:53:15,928 | server.py:125 | fit progress: (3, 1.7188987731933594, {'accuracy': 0.7394, 'data_size': 10000}, 43.78797896899778)
INFO flwr 2024-04-17 03:53:15,928 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 03:53:15,929 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:53:24,523 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 03:53:25,900 | server.py:125 | fit progress: (4, 1.6122325658798218, {'accuracy': 0.8474, 'data_size': 10000}, 53.75973458900262)
INFO flwr 2024-04-17 03:53:25,900 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 03:53:25,900 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:53:34,716 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 03:53:36,122 | server.py:125 | fit progress: (5, 1.5736230611801147, {'accuracy': 0.8866, 'data_size': 10000}, 63.98179059100221)
INFO flwr 2024-04-17 03:53:36,122 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 03:53:36,123 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:53:44,872 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 03:53:46,043 | server.py:125 | fit progress: (6, 1.5559836626052856, {'accuracy': 0.9048, 'data_size': 10000}, 73.90232626700163)
INFO flwr 2024-04-17 03:53:46,043 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 03:53:46,043 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:53:54,913 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 03:53:56,109 | server.py:125 | fit progress: (7, 1.5478309392929077, {'accuracy': 0.9131, 'data_size': 10000}, 83.96905522000452)
INFO flwr 2024-04-17 03:53:56,110 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 03:53:56,110 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:54:05,126 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 03:54:06,495 | server.py:125 | fit progress: (8, 1.538936972618103, {'accuracy': 0.9219, 'data_size': 10000}, 94.35482226600288)
INFO flwr 2024-04-17 03:54:06,495 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 03:54:06,496 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:54:15,270 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 03:54:16,653 | server.py:125 | fit progress: (9, 1.538108468055725, {'accuracy': 0.923, 'data_size': 10000}, 104.51314211299905)
INFO flwr 2024-04-17 03:54:16,654 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 03:54:16,654 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 03:54:25,496 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 03:54:26,865 | server.py:125 | fit progress: (10, 1.534212350845337, {'accuracy': 0.9269, 'data_size': 10000}, 114.72478709599818)
INFO flwr 2024-04-17 03:54:26,865 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 03:54:26,865 | server.py:153 | FL finished in 114.72528641000099
INFO flwr 2024-04-17 03:54:26,866 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 03:54:26,866 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 03:54:26,866 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 03:54:26,866 | app.py:229 | app_fit: losses_centralized [(0, 2.3026130199432373), (1, 2.204740285873413), (2, 1.8836569786071777), (3, 1.7188987731933594), (4, 1.6122325658798218), (5, 1.5736230611801147), (6, 1.5559836626052856), (7, 1.5478309392929077), (8, 1.538936972618103), (9, 1.538108468055725), (10, 1.534212350845337)]
INFO flwr 2024-04-17 03:54:26,866 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.4125), (2, 0.5959), (3, 0.7394), (4, 0.8474), (5, 0.8866), (6, 0.9048), (7, 0.9131), (8, 0.9219), (9, 0.923), (10, 0.9269)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9269
wandb:     loss 1.53421
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_035210-z4z7qfnk
wandb: Find logs at: ./wandb/offline-run-20240417_035210-z4z7qfnk/logs
INFO flwr 2024-04-17 03:54:30,414 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 04:01:38,371 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=324022)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=324022)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 04:01:43,374	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 04:01:44,195	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 04:01:44,691	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 04:01:45,042	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0a201af31b8bece0.zip' (36.76MiB) to Ray cluster...
2024-04-17 04:01:45,155	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0a201af31b8bece0.zip'.
INFO flwr 2024-04-17 04:01:56,145 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77080859443.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169855338701.0}
INFO flwr 2024-04-17 04:01:56,145 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 04:01:56,145 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 04:01:56,171 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 04:01:56,173 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 04:01:56,174 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 04:01:56,174 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 04:02:00,300 | server.py:94 | initial parameters (loss, other metrics): 2.3025851249694824, {'accuracy': 0.1031, 'data_size': 10000}
INFO flwr 2024-04-17 04:02:00,301 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 04:02:00,301 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=327806)[0m 2024-04-17 04:02:02.224251: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=327806)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=327806)[0m 2024-04-17 04:02:04.562858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=327806)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=327806)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=327793)[0m 2024-04-17 04:02:02.584698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=327793)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=327793)[0m 2024-04-17 04:02:04.806327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 04:02:20,740 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 04:02:22,112 | server.py:125 | fit progress: (1, 2.301671028137207, {'accuracy': 0.1264, 'data_size': 10000}, 21.811258977999387)
INFO flwr 2024-04-17 04:02:22,113 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 04:02:22,113 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:02:31,597 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 04:02:32,766 | server.py:125 | fit progress: (2, 2.299084424972534, {'accuracy': 0.232, 'data_size': 10000}, 32.46505973400053)
INFO flwr 2024-04-17 04:02:32,767 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 04:02:32,767 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:02:41,917 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 04:02:43,290 | server.py:125 | fit progress: (3, 2.2929670810699463, {'accuracy': 0.4377, 'data_size': 10000}, 42.988569301996904)
INFO flwr 2024-04-17 04:02:43,290 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 04:02:43,290 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:02:51,841 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 04:02:53,206 | server.py:125 | fit progress: (4, 2.280191421508789, {'accuracy': 0.4986, 'data_size': 10000}, 52.90431327999977)
INFO flwr 2024-04-17 04:02:53,206 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 04:02:53,206 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:03:01,858 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 04:03:03,225 | server.py:125 | fit progress: (5, 2.2551448345184326, {'accuracy': 0.5226, 'data_size': 10000}, 62.923575567001535)
INFO flwr 2024-04-17 04:03:03,225 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 04:03:03,225 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:03:12,198 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 04:03:13,347 | server.py:125 | fit progress: (6, 2.210452079772949, {'accuracy': 0.5384, 'data_size': 10000}, 73.0455520109972)
INFO flwr 2024-04-17 04:03:13,347 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 04:03:13,347 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:03:22,211 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 04:03:23,589 | server.py:125 | fit progress: (7, 2.1386585235595703, {'accuracy': 0.5531, 'data_size': 10000}, 83.28812461000052)
INFO flwr 2024-04-17 04:03:23,590 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 04:03:23,590 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:03:32,224 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 04:03:33,405 | server.py:125 | fit progress: (8, 2.0458240509033203, {'accuracy': 0.5745, 'data_size': 10000}, 93.10372719600127)
INFO flwr 2024-04-17 04:03:33,405 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 04:03:33,405 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:03:42,449 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 04:03:43,810 | server.py:125 | fit progress: (9, 1.955316185951233, {'accuracy': 0.6068, 'data_size': 10000}, 103.50836110000091)
INFO flwr 2024-04-17 04:03:43,810 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 04:03:43,810 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:03:52,638 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 04:03:53,787 | server.py:125 | fit progress: (10, 1.882434606552124, {'accuracy': 0.6455, 'data_size': 10000}, 113.48562007700093)
INFO flwr 2024-04-17 04:03:53,787 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 04:03:53,787 | server.py:153 | FL finished in 113.4860567709984
INFO flwr 2024-04-17 04:03:53,787 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 04:03:53,788 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 04:03:53,788 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 04:03:53,788 | app.py:229 | app_fit: losses_centralized [(0, 2.3025851249694824), (1, 2.301671028137207), (2, 2.299084424972534), (3, 2.2929670810699463), (4, 2.280191421508789), (5, 2.2551448345184326), (6, 2.210452079772949), (7, 2.1386585235595703), (8, 2.0458240509033203), (9, 1.955316185951233), (10, 1.882434606552124)]
INFO flwr 2024-04-17 04:03:53,788 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1031), (1, 0.1264), (2, 0.232), (3, 0.4377), (4, 0.4986), (5, 0.5226), (6, 0.5384), (7, 0.5531), (8, 0.5745), (9, 0.6068), (10, 0.6455)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6455
wandb:     loss 1.88243
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_040137-c6obh4ll
wandb: Find logs at: ./wandb/offline-run-20240417_040137-c6obh4ll/logs
INFO flwr 2024-04-17 04:03:57,330 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 04:11:05,365 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=327793)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=327793)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 04:11:09,938	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 04:11:10,864	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 04:11:11,311	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 04:11:11,662	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cf3b365b07741e17.zip' (36.77MiB) to Ray cluster...
2024-04-17 04:11:11,774	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cf3b365b07741e17.zip'.
INFO flwr 2024-04-17 04:11:22,805 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169852138906.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 77079488102.0, 'GPU': 1.0}
INFO flwr 2024-04-17 04:11:22,805 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 04:11:22,806 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 04:11:22,828 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 04:11:22,829 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 04:11:22,829 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 04:11:22,829 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 04:11:27,626 | server.py:94 | initial parameters (loss, other metrics): 2.3024520874023438, {'accuracy': 0.1035, 'data_size': 10000}
INFO flwr 2024-04-17 04:11:27,627 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 04:11:27,627 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=332157)[0m 2024-04-17 04:11:28.734571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=332157)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=332157)[0m 2024-04-17 04:11:31.142694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=332161)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=332161)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=332161)[0m 2024-04-17 04:11:29.004533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=332161)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=332161)[0m 2024-04-17 04:11:31.267008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 04:11:46,558 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 04:11:47,979 | server.py:125 | fit progress: (1, 2.3023648262023926, {'accuracy': 0.1038, 'data_size': 10000}, 20.3521684619991)
INFO flwr 2024-04-17 04:11:47,979 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 04:11:47,980 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:11:57,206 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 04:11:58,368 | server.py:125 | fit progress: (2, 2.3022422790527344, {'accuracy': 0.1068, 'data_size': 10000}, 30.740843519997725)
INFO flwr 2024-04-17 04:11:58,368 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 04:11:58,368 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:12:07,277 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 04:12:08,646 | server.py:125 | fit progress: (3, 2.3020851612091064, {'accuracy': 0.114, 'data_size': 10000}, 41.01896830699843)
INFO flwr 2024-04-17 04:12:08,646 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 04:12:08,647 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:12:16,976 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 04:12:18,147 | server.py:125 | fit progress: (4, 2.3018858432769775, {'accuracy': 0.1269, 'data_size': 10000}, 50.52025650399446)
INFO flwr 2024-04-17 04:12:18,148 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 04:12:18,148 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:12:27,146 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 04:12:28,556 | server.py:125 | fit progress: (5, 2.3016366958618164, {'accuracy': 0.1523, 'data_size': 10000}, 60.92857348299731)
INFO flwr 2024-04-17 04:12:28,556 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 04:12:28,556 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:12:36,904 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 04:12:38,093 | server.py:125 | fit progress: (6, 2.301332950592041, {'accuracy': 0.1776, 'data_size': 10000}, 70.46604153199587)
INFO flwr 2024-04-17 04:12:38,093 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 04:12:38,094 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:12:46,731 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 04:12:48,157 | server.py:125 | fit progress: (7, 2.300966262817383, {'accuracy': 0.2077, 'data_size': 10000}, 80.53035346399702)
INFO flwr 2024-04-17 04:12:48,158 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 04:12:48,158 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:12:56,471 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 04:12:57,849 | server.py:125 | fit progress: (8, 2.300532341003418, {'accuracy': 0.2324, 'data_size': 10000}, 90.22171615499974)
INFO flwr 2024-04-17 04:12:57,849 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 04:12:57,849 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:13:06,501 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 04:13:07,865 | server.py:125 | fit progress: (9, 2.3000247478485107, {'accuracy': 0.2566, 'data_size': 10000}, 100.23823929199716)
INFO flwr 2024-04-17 04:13:07,866 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 04:13:07,866 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:13:16,840 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 04:13:18,213 | server.py:125 | fit progress: (10, 2.299442768096924, {'accuracy': 0.278, 'data_size': 10000}, 110.58610475000023)
INFO flwr 2024-04-17 04:13:18,213 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 04:13:18,214 | server.py:153 | FL finished in 110.58654191999813
INFO flwr 2024-04-17 04:13:18,214 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 04:13:18,214 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 04:13:18,214 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 04:13:18,214 | app.py:229 | app_fit: losses_centralized [(0, 2.3024520874023438), (1, 2.3023648262023926), (2, 2.3022422790527344), (3, 2.3020851612091064), (4, 2.3018858432769775), (5, 2.3016366958618164), (6, 2.301332950592041), (7, 2.300966262817383), (8, 2.300532341003418), (9, 2.3000247478485107), (10, 2.299442768096924)]
INFO flwr 2024-04-17 04:13:18,214 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1035), (1, 0.1038), (2, 0.1068), (3, 0.114), (4, 0.1269), (5, 0.1523), (6, 0.1776), (7, 0.2077), (8, 0.2324), (9, 0.2566), (10, 0.278)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.278
wandb:     loss 2.29944
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_041104-8nlks9nt
wandb: Find logs at: ./wandb/offline-run-20240417_041104-8nlks9nt/logs
INFO flwr 2024-04-17 04:13:21,754 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 04:20:29,413 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=332154)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=332154)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 04:20:34,099	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 04:20:34,948	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 04:20:35,434	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 04:20:35,785	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6ebab70b4036d502.zip' (36.78MiB) to Ray cluster...
2024-04-17 04:20:35,896	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6ebab70b4036d502.zip'.
INFO flwr 2024-04-17 04:20:46,964 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169850943284.0, 'CPU': 64.0, 'object_store_memory': 77078975692.0}
INFO flwr 2024-04-17 04:20:46,965 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 04:20:46,965 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 04:20:46,983 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 04:20:46,984 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 04:20:46,984 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 04:20:46,984 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 04:20:50,608 | server.py:94 | initial parameters (loss, other metrics): 2.302743673324585, {'accuracy': 0.0496, 'data_size': 10000}
INFO flwr 2024-04-17 04:20:50,608 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 04:20:50,608 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=336896)[0m 2024-04-17 04:20:53.104541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=336896)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=336896)[0m 2024-04-17 04:20:55.458746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=336896)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=336896)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=336894)[0m 2024-04-17 04:20:53.412858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=336894)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=336894)[0m 2024-04-17 04:20:55.618917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 04:21:10,899 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 04:21:12,309 | server.py:125 | fit progress: (1, 2.30273699760437, {'accuracy': 0.0502, 'data_size': 10000}, 21.70055861799483)
INFO flwr 2024-04-17 04:21:12,309 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 04:21:12,309 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:21:21,605 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 04:21:22,786 | server.py:125 | fit progress: (2, 2.3027281761169434, {'accuracy': 0.0507, 'data_size': 10000}, 32.17761366799823)
INFO flwr 2024-04-17 04:21:22,786 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 04:21:22,786 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:21:31,803 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 04:21:33,175 | server.py:125 | fit progress: (3, 2.3027167320251465, {'accuracy': 0.0513, 'data_size': 10000}, 42.566527267998026)
INFO flwr 2024-04-17 04:21:33,175 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 04:21:33,175 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:21:41,775 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 04:21:42,934 | server.py:125 | fit progress: (4, 2.3027050495147705, {'accuracy': 0.0516, 'data_size': 10000}, 52.32592385899625)
INFO flwr 2024-04-17 04:21:42,935 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 04:21:42,935 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:21:51,627 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 04:21:52,786 | server.py:125 | fit progress: (5, 2.3026926517486572, {'accuracy': 0.0521, 'data_size': 10000}, 62.177753473995836)
INFO flwr 2024-04-17 04:21:52,786 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 04:21:52,787 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:22:01,420 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 04:22:02,823 | server.py:125 | fit progress: (6, 2.3026793003082275, {'accuracy': 0.0528, 'data_size': 10000}, 72.21505782099848)
INFO flwr 2024-04-17 04:22:02,824 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 04:22:02,824 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:22:11,707 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 04:22:13,099 | server.py:125 | fit progress: (7, 2.3026652336120605, {'accuracy': 0.0539, 'data_size': 10000}, 82.49045844200009)
INFO flwr 2024-04-17 04:22:13,099 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 04:22:13,099 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:22:21,881 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 04:22:23,284 | server.py:125 | fit progress: (8, 2.3026506900787354, {'accuracy': 0.0552, 'data_size': 10000}, 92.67583747000026)
INFO flwr 2024-04-17 04:22:23,284 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 04:22:23,285 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:22:31,744 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 04:22:33,161 | server.py:125 | fit progress: (9, 2.302635431289673, {'accuracy': 0.0552, 'data_size': 10000}, 102.55236354799854)
INFO flwr 2024-04-17 04:22:33,161 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 04:22:33,161 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:22:41,956 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 04:22:43,118 | server.py:125 | fit progress: (10, 2.3026199340820312, {'accuracy': 0.0565, 'data_size': 10000}, 112.5094307899999)
INFO flwr 2024-04-17 04:22:43,118 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 04:22:43,118 | server.py:153 | FL finished in 112.50986367199948
INFO flwr 2024-04-17 04:22:43,118 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 04:22:43,119 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 04:22:43,119 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 04:22:43,119 | app.py:229 | app_fit: losses_centralized [(0, 2.302743673324585), (1, 2.30273699760437), (2, 2.3027281761169434), (3, 2.3027167320251465), (4, 2.3027050495147705), (5, 2.3026926517486572), (6, 2.3026793003082275), (7, 2.3026652336120605), (8, 2.3026506900787354), (9, 2.302635431289673), (10, 2.3026199340820312)]
INFO flwr 2024-04-17 04:22:43,119 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0496), (1, 0.0502), (2, 0.0507), (3, 0.0513), (4, 0.0516), (5, 0.0521), (6, 0.0528), (7, 0.0539), (8, 0.0552), (9, 0.0552), (10, 0.0565)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0565
wandb:     loss 2.30262
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_042029-lrff3809
wandb: Find logs at: ./wandb/offline-run-20240417_042029-lrff3809/logs
INFO flwr 2024-04-17 04:22:46,635 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 04:29:54,546 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=336890)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=336890)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 04:29:59,255	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 04:30:00,116	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 04:30:00,556	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 04:30:00,918	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cf1630101f5b2196.zip' (36.79MiB) to Ray cluster...
2024-04-17 04:30:01,028	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cf1630101f5b2196.zip'.
INFO flwr 2024-04-17 04:30:12,150 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77078663577.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 169850215015.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 04:30:12,151 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 04:30:12,151 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 04:30:12,167 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 04:30:12,168 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 04:30:12,168 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 04:30:12,169 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 04:30:15,451 | server.py:94 | initial parameters (loss, other metrics): 2.3024911880493164, {'accuracy': 0.1065, 'data_size': 10000}
INFO flwr 2024-04-17 04:30:15,451 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 04:30:15,451 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=341247)[0m 2024-04-17 04:30:18.282646: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=341247)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=341254)[0m 2024-04-17 04:30:20.661141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=341256)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=341256)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=341262)[0m 2024-04-17 04:30:18.642527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=341262)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=341262)[0m 2024-04-17 04:30:20.993079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 04:30:36,624 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 04:30:37,825 | server.py:125 | fit progress: (1, 2.2515223026275635, {'accuracy': 0.2096, 'data_size': 10000}, 22.374179316000664)
INFO flwr 2024-04-17 04:30:37,826 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 04:30:37,826 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:30:47,343 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 04:30:48,760 | server.py:125 | fit progress: (2, 2.3184421062469482, {'accuracy': 0.1427, 'data_size': 10000}, 33.30858551200072)
INFO flwr 2024-04-17 04:30:48,760 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 04:30:48,760 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:30:57,468 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 04:30:58,864 | server.py:125 | fit progress: (3, 2.249940872192383, {'accuracy': 0.2112, 'data_size': 10000}, 43.41304456400394)
INFO flwr 2024-04-17 04:30:58,865 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 04:30:58,865 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:31:07,877 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 04:31:09,257 | server.py:125 | fit progress: (4, 2.253643274307251, {'accuracy': 0.2075, 'data_size': 10000}, 53.80583483200462)
INFO flwr 2024-04-17 04:31:09,257 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 04:31:09,258 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:31:17,971 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 04:31:19,131 | server.py:125 | fit progress: (5, 2.292642831802368, {'accuracy': 0.1685, 'data_size': 10000}, 63.6799596780038)
INFO flwr 2024-04-17 04:31:19,132 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 04:31:19,132 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:31:28,027 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 04:31:29,412 | server.py:125 | fit progress: (6, 2.3321423530578613, {'accuracy': 0.129, 'data_size': 10000}, 73.96060322099947)
INFO flwr 2024-04-17 04:31:29,412 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 04:31:29,412 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:31:38,001 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 04:31:39,164 | server.py:125 | fit progress: (7, 2.3423421382904053, {'accuracy': 0.1188, 'data_size': 10000}, 83.71315056199819)
INFO flwr 2024-04-17 04:31:39,165 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 04:31:39,165 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:31:48,161 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 04:31:49,584 | server.py:125 | fit progress: (8, 2.344942092895508, {'accuracy': 0.1162, 'data_size': 10000}, 94.13230800600286)
INFO flwr 2024-04-17 04:31:49,584 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 04:31:49,584 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:31:58,311 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 04:31:59,489 | server.py:125 | fit progress: (9, 2.3460421562194824, {'accuracy': 0.1151, 'data_size': 10000}, 104.03807087799942)
INFO flwr 2024-04-17 04:31:59,490 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 04:31:59,490 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:32:07,965 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 04:32:09,365 | server.py:125 | fit progress: (10, 2.3468422889709473, {'accuracy': 0.1143, 'data_size': 10000}, 113.91365102599957)
INFO flwr 2024-04-17 04:32:09,365 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 04:32:09,365 | server.py:153 | FL finished in 113.914118091001
INFO flwr 2024-04-17 04:32:09,366 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 04:32:09,366 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 04:32:09,366 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 04:32:09,366 | app.py:229 | app_fit: losses_centralized [(0, 2.3024911880493164), (1, 2.2515223026275635), (2, 2.3184421062469482), (3, 2.249940872192383), (4, 2.253643274307251), (5, 2.292642831802368), (6, 2.3321423530578613), (7, 2.3423421382904053), (8, 2.344942092895508), (9, 2.3460421562194824), (10, 2.3468422889709473)]
INFO flwr 2024-04-17 04:32:09,366 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1065), (1, 0.2096), (2, 0.1427), (3, 0.2112), (4, 0.2075), (5, 0.1685), (6, 0.129), (7, 0.1188), (8, 0.1162), (9, 0.1151), (10, 0.1143)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1143
wandb:     loss 2.34684
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_042954-ttnf45ob
wandb: Find logs at: ./wandb/offline-run-20240417_042954-ttnf45ob/logs
INFO flwr 2024-04-17 04:32:12,906 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 04:39:20,615 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=341247)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=341247)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 04:39:25,423	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 04:39:26,272	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 04:39:26,743	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 04:39:27,095	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f491c1eeefa2adfc.zip' (36.81MiB) to Ray cluster...
2024-04-17 04:39:27,211	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f491c1eeefa2adfc.zip'.
INFO flwr 2024-04-17 04:39:38,314 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77168420044.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170059646772.0}
INFO flwr 2024-04-17 04:39:38,314 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 04:39:38,314 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 04:39:38,332 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 04:39:38,333 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 04:39:38,334 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 04:39:38,334 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 04:39:42,105 | server.py:94 | initial parameters (loss, other metrics): 2.302623748779297, {'accuracy': 0.082, 'data_size': 10000}
INFO flwr 2024-04-17 04:39:42,106 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 04:39:42,107 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=345608)[0m 2024-04-17 04:39:44.357042: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=345608)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=345609)[0m 2024-04-17 04:39:46.741614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=345613)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=345613)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=345615)[0m 2024-04-17 04:39:44.655200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=345615)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=345606)[0m 2024-04-17 04:39:46.871563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 04:40:02,577 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 04:40:03,981 | server.py:125 | fit progress: (1, 2.1908376216888428, {'accuracy': 0.5898, 'data_size': 10000}, 21.8742137320005)
INFO flwr 2024-04-17 04:40:03,981 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 04:40:03,981 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:40:13,828 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 04:40:15,001 | server.py:125 | fit progress: (2, 1.8306554555892944, {'accuracy': 0.6478, 'data_size': 10000}, 32.89428272600344)
INFO flwr 2024-04-17 04:40:15,001 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 04:40:15,001 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:40:24,059 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 04:40:25,212 | server.py:125 | fit progress: (3, 1.681778907775879, {'accuracy': 0.7799, 'data_size': 10000}, 43.10545429200283)
INFO flwr 2024-04-17 04:40:25,212 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 04:40:25,213 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:40:33,937 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 04:40:35,320 | server.py:125 | fit progress: (4, 1.6430718898773193, {'accuracy': 0.8173, 'data_size': 10000}, 53.212991635002254)
INFO flwr 2024-04-17 04:40:35,320 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 04:40:35,320 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:40:43,885 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 04:40:45,078 | server.py:125 | fit progress: (5, 1.633949637413025, {'accuracy': 0.8264, 'data_size': 10000}, 62.971279211000365)
INFO flwr 2024-04-17 04:40:45,078 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 04:40:45,078 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:40:53,955 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 04:40:55,328 | server.py:125 | fit progress: (6, 1.6235591173171997, {'accuracy': 0.837, 'data_size': 10000}, 73.22193425099977)
INFO flwr 2024-04-17 04:40:55,329 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 04:40:55,329 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:41:04,207 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 04:41:05,360 | server.py:125 | fit progress: (7, 1.6231110095977783, {'accuracy': 0.8374, 'data_size': 10000}, 83.25357264400373)
INFO flwr 2024-04-17 04:41:05,360 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 04:41:05,361 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:41:13,484 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 04:41:14,845 | server.py:125 | fit progress: (8, 1.6182841062545776, {'accuracy': 0.8419, 'data_size': 10000}, 92.73859299600008)
INFO flwr 2024-04-17 04:41:14,845 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 04:41:14,846 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:41:23,271 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 04:41:24,468 | server.py:125 | fit progress: (9, 1.5683177709579468, {'accuracy': 0.8918, 'data_size': 10000}, 102.3616666490052)
INFO flwr 2024-04-17 04:41:24,469 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 04:41:24,469 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:41:33,345 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 04:41:34,725 | server.py:125 | fit progress: (10, 1.5439507961273193, {'accuracy': 0.917, 'data_size': 10000}, 112.61811450200184)
INFO flwr 2024-04-17 04:41:34,725 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 04:41:34,725 | server.py:153 | FL finished in 112.61863045200153
INFO flwr 2024-04-17 04:41:34,725 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 04:41:34,725 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 04:41:34,726 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 04:41:34,726 | app.py:229 | app_fit: losses_centralized [(0, 2.302623748779297), (1, 2.1908376216888428), (2, 1.8306554555892944), (3, 1.681778907775879), (4, 1.6430718898773193), (5, 1.633949637413025), (6, 1.6235591173171997), (7, 1.6231110095977783), (8, 1.6182841062545776), (9, 1.5683177709579468), (10, 1.5439507961273193)]
INFO flwr 2024-04-17 04:41:34,726 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.082), (1, 0.5898), (2, 0.6478), (3, 0.7799), (4, 0.8173), (5, 0.8264), (6, 0.837), (7, 0.8374), (8, 0.8419), (9, 0.8918), (10, 0.917)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.917
wandb:     loss 1.54395
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_043920-7bsk23lz
wandb: Find logs at: ./wandb/offline-run-20240417_043920-7bsk23lz/logs
INFO flwr 2024-04-17 04:41:38,274 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 04:48:46,306 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=345606)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=345606)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 04:48:51,007	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 04:48:51,792	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 04:48:52,233	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 04:48:52,593	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_00aabffd51ef7643.zip' (36.82MiB) to Ray cluster...
2024-04-17 04:48:52,708	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_00aabffd51ef7643.zip'.
INFO flwr 2024-04-17 04:49:03,800 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 77094021120.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 169886049280.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 04:49:03,801 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 04:49:03,801 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 04:49:03,819 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 04:49:03,820 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 04:49:03,820 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 04:49:03,820 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 04:49:07,728 | server.py:94 | initial parameters (loss, other metrics): 2.302476167678833, {'accuracy': 0.1219, 'data_size': 10000}
INFO flwr 2024-04-17 04:49:07,728 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 04:49:07,729 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=350351)[0m 2024-04-17 04:49:09.836113: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=350351)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=350351)[0m 2024-04-17 04:49:12.169082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=350347)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=350347)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=350346)[0m 2024-04-17 04:49:10.109829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=350346)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=350340)[0m 2024-04-17 04:49:12.375984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 04:49:27,816 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 04:49:29,017 | server.py:125 | fit progress: (1, 2.3014211654663086, {'accuracy': 0.1598, 'data_size': 10000}, 21.288930729999265)
INFO flwr 2024-04-17 04:49:29,018 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 04:49:29,018 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:49:38,602 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 04:49:40,001 | server.py:125 | fit progress: (2, 2.2983641624450684, {'accuracy': 0.321, 'data_size': 10000}, 32.272417655003665)
INFO flwr 2024-04-17 04:49:40,001 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 04:49:40,001 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:49:48,928 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 04:49:50,086 | server.py:125 | fit progress: (3, 2.2911579608917236, {'accuracy': 0.4058, 'data_size': 10000}, 42.357795652002096)
INFO flwr 2024-04-17 04:49:50,086 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 04:49:50,087 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:49:59,056 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 04:50:00,437 | server.py:125 | fit progress: (4, 2.2763924598693848, {'accuracy': 0.4467, 'data_size': 10000}, 52.70830742399994)
INFO flwr 2024-04-17 04:50:00,437 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 04:50:00,437 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:50:08,972 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 04:50:10,130 | server.py:125 | fit progress: (5, 2.2487075328826904, {'accuracy': 0.4861, 'data_size': 10000}, 62.40120429100352)
INFO flwr 2024-04-17 04:50:10,130 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 04:50:10,130 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:50:19,069 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 04:50:20,471 | server.py:125 | fit progress: (6, 2.2021291255950928, {'accuracy': 0.5303, 'data_size': 10000}, 72.74222035799903)
INFO flwr 2024-04-17 04:50:20,471 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 04:50:20,471 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:50:29,338 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 04:50:30,535 | server.py:125 | fit progress: (7, 2.1323540210723877, {'accuracy': 0.5763, 'data_size': 10000}, 82.80638426400401)
INFO flwr 2024-04-17 04:50:30,535 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 04:50:30,535 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:50:39,083 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 04:50:40,452 | server.py:125 | fit progress: (8, 2.045616865158081, {'accuracy': 0.6112, 'data_size': 10000}, 92.72371361200203)
INFO flwr 2024-04-17 04:50:40,452 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 04:50:40,453 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:50:49,085 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 04:50:50,257 | server.py:125 | fit progress: (9, 1.9584290981292725, {'accuracy': 0.6387, 'data_size': 10000}, 102.52880816700053)
INFO flwr 2024-04-17 04:50:50,258 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 04:50:50,258 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:50:59,124 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 04:51:00,290 | server.py:125 | fit progress: (10, 1.8823860883712769, {'accuracy': 0.661, 'data_size': 10000}, 112.56158209800196)
INFO flwr 2024-04-17 04:51:00,290 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 04:51:00,290 | server.py:153 | FL finished in 112.56209307599784
INFO flwr 2024-04-17 04:51:00,291 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 04:51:00,291 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 04:51:00,291 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 04:51:00,291 | app.py:229 | app_fit: losses_centralized [(0, 2.302476167678833), (1, 2.3014211654663086), (2, 2.2983641624450684), (3, 2.2911579608917236), (4, 2.2763924598693848), (5, 2.2487075328826904), (6, 2.2021291255950928), (7, 2.1323540210723877), (8, 2.045616865158081), (9, 1.9584290981292725), (10, 1.8823860883712769)]
INFO flwr 2024-04-17 04:51:00,291 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1219), (1, 0.1598), (2, 0.321), (3, 0.4058), (4, 0.4467), (5, 0.4861), (6, 0.5303), (7, 0.5763), (8, 0.6112), (9, 0.6387), (10, 0.661)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.661
wandb:     loss 1.88239
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_044845-w4deipem
wandb: Find logs at: ./wandb/offline-run-20240417_044845-w4deipem/logs
INFO flwr 2024-04-17 04:51:03,796 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 04:58:11,792 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=350340)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=350340)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 04:58:16,664	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 04:58:17,540	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 04:58:18,032	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 04:58:18,385	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8f6bb566063dc8c2.zip' (36.83MiB) to Ray cluster...
2024-04-17 04:58:18,498	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8f6bb566063dc8c2.zip'.
INFO flwr 2024-04-17 04:58:29,560 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77140110950.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'memory': 169993592218.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 04:58:29,561 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 04:58:29,561 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 04:58:29,576 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 04:58:29,578 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 04:58:29,578 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 04:58:29,579 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 04:58:32,375 | server.py:94 | initial parameters (loss, other metrics): 2.302431106567383, {'accuracy': 0.1156, 'data_size': 10000}
INFO flwr 2024-04-17 04:58:32,375 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 04:58:32,376 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=354705)[0m 2024-04-17 04:58:35.647973: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=354705)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=354705)[0m 2024-04-17 04:58:38.012553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=354705)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=354705)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=354703)[0m 2024-04-17 04:58:35.780224: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=354703)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=354703)[0m 2024-04-17 04:58:38.022425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 04:58:53,861 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 04:58:55,283 | server.py:125 | fit progress: (1, 2.302351236343384, {'accuracy': 0.1214, 'data_size': 10000}, 22.907724138996855)
INFO flwr 2024-04-17 04:58:55,284 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 04:58:55,284 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:59:04,521 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 04:59:05,675 | server.py:125 | fit progress: (2, 2.302238941192627, {'accuracy': 0.132, 'data_size': 10000}, 33.29983581000124)
INFO flwr 2024-04-17 04:59:05,676 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 04:59:05,676 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:59:14,843 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 04:59:16,218 | server.py:125 | fit progress: (3, 2.302091598510742, {'accuracy': 0.142, 'data_size': 10000}, 43.84260436399927)
INFO flwr 2024-04-17 04:59:16,219 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 04:59:16,219 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:59:24,950 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 04:59:26,138 | server.py:125 | fit progress: (4, 2.3019063472747803, {'accuracy': 0.1519, 'data_size': 10000}, 53.762426918998244)
INFO flwr 2024-04-17 04:59:26,139 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 04:59:26,139 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:59:35,016 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 04:59:36,393 | server.py:125 | fit progress: (5, 2.301680564880371, {'accuracy': 0.1653, 'data_size': 10000}, 64.01709813699563)
INFO flwr 2024-04-17 04:59:36,393 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 04:59:36,393 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:59:45,225 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 04:59:46,384 | server.py:125 | fit progress: (6, 2.3014121055603027, {'accuracy': 0.1755, 'data_size': 10000}, 74.00795703299809)
INFO flwr 2024-04-17 04:59:46,384 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 04:59:46,384 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 04:59:54,900 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 04:59:56,069 | server.py:125 | fit progress: (7, 2.3010916709899902, {'accuracy': 0.1829, 'data_size': 10000}, 83.69376974699844)
INFO flwr 2024-04-17 04:59:56,070 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 04:59:56,070 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:00:05,050 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 05:00:06,404 | server.py:125 | fit progress: (8, 2.3007125854492188, {'accuracy': 0.1945, 'data_size': 10000}, 94.02863898399664)
INFO flwr 2024-04-17 05:00:06,405 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 05:00:06,405 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:00:14,915 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 05:00:16,088 | server.py:125 | fit progress: (9, 2.300269603729248, {'accuracy': 0.2218, 'data_size': 10000}, 103.7123506880016)
INFO flwr 2024-04-17 05:00:16,088 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 05:00:16,089 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:00:24,970 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 05:00:26,371 | server.py:125 | fit progress: (10, 2.2997565269470215, {'accuracy': 0.2779, 'data_size': 10000}, 113.994908339002)
INFO flwr 2024-04-17 05:00:26,371 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 05:00:26,371 | server.py:153 | FL finished in 113.99542470299639
INFO flwr 2024-04-17 05:00:26,371 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 05:00:26,372 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 05:00:26,372 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 05:00:26,372 | app.py:229 | app_fit: losses_centralized [(0, 2.302431106567383), (1, 2.302351236343384), (2, 2.302238941192627), (3, 2.302091598510742), (4, 2.3019063472747803), (5, 2.301680564880371), (6, 2.3014121055603027), (7, 2.3010916709899902), (8, 2.3007125854492188), (9, 2.300269603729248), (10, 2.2997565269470215)]
INFO flwr 2024-04-17 05:00:26,372 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1156), (1, 0.1214), (2, 0.132), (3, 0.142), (4, 0.1519), (5, 0.1653), (6, 0.1755), (7, 0.1829), (8, 0.1945), (9, 0.2218), (10, 0.2779)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2779
wandb:     loss 2.29976
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_045811-5hgypsya
wandb: Find logs at: ./wandb/offline-run-20240417_045811-5hgypsya/logs
INFO flwr 2024-04-17 05:00:29,890 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 05:07:37,629 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=354702)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=354702)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 05:07:42,326	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 05:07:43,110	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 05:07:43,569	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 05:07:43,948	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b7f2df54aa1206d1.zip' (36.84MiB) to Ray cluster...
2024-04-17 05:07:44,070	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b7f2df54aa1206d1.zip'.
INFO flwr 2024-04-17 05:07:55,125 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169847878247.0, 'GPU': 1.0, 'object_store_memory': 77077662105.0}
INFO flwr 2024-04-17 05:07:55,126 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 05:07:55,126 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 05:07:55,146 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 05:07:55,148 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 05:07:55,148 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 05:07:55,148 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 05:07:58,454 | server.py:94 | initial parameters (loss, other metrics): 2.302560806274414, {'accuracy': 0.0956, 'data_size': 10000}
INFO flwr 2024-04-17 05:07:58,455 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 05:07:58,455 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=359095)[0m 2024-04-17 05:08:01.297789: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=359095)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=359095)[0m 2024-04-17 05:08:03.623995: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=359095)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=359095)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=359088)[0m 2024-04-17 05:08:01.452153: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=359088)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=359088)[0m 2024-04-17 05:08:03.724162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 05:08:18,558 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 05:08:19,957 | server.py:125 | fit progress: (1, 2.30255389213562, {'accuracy': 0.0956, 'data_size': 10000}, 21.502079461002722)
INFO flwr 2024-04-17 05:08:19,957 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 05:08:19,958 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:08:29,193 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 05:08:30,580 | server.py:125 | fit progress: (2, 2.302544593811035, {'accuracy': 0.0956, 'data_size': 10000}, 32.12447570700169)
INFO flwr 2024-04-17 05:08:30,580 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 05:08:30,580 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:08:39,609 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 05:08:41,009 | server.py:125 | fit progress: (3, 2.3025341033935547, {'accuracy': 0.0956, 'data_size': 10000}, 42.55424174100335)
INFO flwr 2024-04-17 05:08:41,010 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 05:08:41,010 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:08:49,519 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 05:08:50,687 | server.py:125 | fit progress: (4, 2.3025224208831787, {'accuracy': 0.0956, 'data_size': 10000}, 52.232269769003324)
INFO flwr 2024-04-17 05:08:50,688 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 05:08:50,688 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:08:59,089 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 05:09:00,507 | server.py:125 | fit progress: (5, 2.3025095462799072, {'accuracy': 0.0956, 'data_size': 10000}, 62.051601277999)
INFO flwr 2024-04-17 05:09:00,507 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 05:09:00,507 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:09:09,050 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 05:09:10,233 | server.py:125 | fit progress: (6, 2.3024966716766357, {'accuracy': 0.0956, 'data_size': 10000}, 71.77803511200182)
INFO flwr 2024-04-17 05:09:10,233 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 05:09:10,234 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:09:19,055 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 05:09:20,440 | server.py:125 | fit progress: (7, 2.302483320236206, {'accuracy': 0.0956, 'data_size': 10000}, 81.9852206420037)
INFO flwr 2024-04-17 05:09:20,441 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 05:09:20,441 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:09:29,398 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 05:09:30,568 | server.py:125 | fit progress: (8, 2.3024680614471436, {'accuracy': 0.0956, 'data_size': 10000}, 92.11330933000136)
INFO flwr 2024-04-17 05:09:30,569 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 05:09:30,569 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:09:39,022 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 05:09:40,404 | server.py:125 | fit progress: (9, 2.3024532794952393, {'accuracy': 0.0956, 'data_size': 10000}, 101.94924904800428)
INFO flwr 2024-04-17 05:09:40,405 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 05:09:40,405 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:09:49,662 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 05:09:50,849 | server.py:125 | fit progress: (10, 2.302438259124756, {'accuracy': 0.0956, 'data_size': 10000}, 112.39392793200386)
INFO flwr 2024-04-17 05:09:50,849 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 05:09:50,849 | server.py:153 | FL finished in 112.39436063600442
INFO flwr 2024-04-17 05:09:50,850 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 05:09:50,850 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 05:09:50,850 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 05:09:50,850 | app.py:229 | app_fit: losses_centralized [(0, 2.302560806274414), (1, 2.30255389213562), (2, 2.302544593811035), (3, 2.3025341033935547), (4, 2.3025224208831787), (5, 2.3025095462799072), (6, 2.3024966716766357), (7, 2.302483320236206), (8, 2.3024680614471436), (9, 2.3024532794952393), (10, 2.302438259124756)]
INFO flwr 2024-04-17 05:09:50,850 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0956), (1, 0.0956), (2, 0.0956), (3, 0.0956), (4, 0.0956), (5, 0.0956), (6, 0.0956), (7, 0.0956), (8, 0.0956), (9, 0.0956), (10, 0.0956)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0956
wandb:     loss 2.30244
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_050737-10ctd071
wandb: Find logs at: ./wandb/offline-run-20240417_050737-10ctd071/logs
INFO flwr 2024-04-17 05:09:54,381 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 05:17:02,714 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=359077)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=359077)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 05:17:07,801	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 05:17:08,601	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 05:17:09,048	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 05:17:09,413	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_acad54e09c651ca7.zip' (36.85MiB) to Ray cluster...
2024-04-17 05:17:09,534	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_acad54e09c651ca7.zip'.
INFO flwr 2024-04-17 05:17:20,535 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77075495731.0, 'CPU': 64.0, 'memory': 169842823373.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 05:17:20,535 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 05:17:20,536 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 05:17:20,554 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 05:17:20,555 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 05:17:20,555 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 05:17:20,556 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 05:17:24,138 | server.py:94 | initial parameters (loss, other metrics): 2.302462339401245, {'accuracy': 0.0879, 'data_size': 10000}
INFO flwr 2024-04-17 05:17:24,139 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 05:17:24,139 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=363199)[0m 2024-04-17 05:17:26.649447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=363199)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=363204)[0m 2024-04-17 05:17:28.981570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=363209)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=363209)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=363210)[0m 2024-04-17 05:17:26.989655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=363210)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=363210)[0m 2024-04-17 05:17:29.498697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 05:17:44,960 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 05:17:46,171 | server.py:125 | fit progress: (1, 2.12919545173645, {'accuracy': 0.3318, 'data_size': 10000}, 22.032496004998393)
INFO flwr 2024-04-17 05:17:46,172 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 05:17:46,172 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:17:55,891 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 05:17:57,276 | server.py:125 | fit progress: (2, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 33.136915652001335)
INFO flwr 2024-04-17 05:17:57,276 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 05:17:57,276 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:18:06,059 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 05:18:07,426 | server.py:125 | fit progress: (3, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 43.287398216998554)
INFO flwr 2024-04-17 05:18:07,427 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 05:18:07,427 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:18:16,440 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 05:18:17,812 | server.py:125 | fit progress: (4, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 53.673678767998354)
INFO flwr 2024-04-17 05:18:17,813 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 05:18:17,813 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:18:26,264 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 05:18:27,666 | server.py:125 | fit progress: (5, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 63.52678683299746)
INFO flwr 2024-04-17 05:18:27,666 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 05:18:27,666 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:18:36,697 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 05:18:38,118 | server.py:125 | fit progress: (6, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 73.97956158200395)
INFO flwr 2024-04-17 05:18:38,119 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 05:18:38,119 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:18:47,049 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 05:18:48,229 | server.py:125 | fit progress: (7, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 84.09023567399709)
INFO flwr 2024-04-17 05:18:48,229 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 05:18:48,230 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:18:56,860 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 05:18:58,020 | server.py:125 | fit progress: (8, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 93.88124012500339)
INFO flwr 2024-04-17 05:18:58,020 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 05:18:58,021 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:19:06,881 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 05:19:08,260 | server.py:125 | fit progress: (9, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 104.12113446200237)
INFO flwr 2024-04-17 05:19:08,260 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 05:19:08,260 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:19:17,410 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 05:19:18,796 | server.py:125 | fit progress: (10, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 114.65703663600289)
INFO flwr 2024-04-17 05:19:18,796 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 05:19:18,796 | server.py:153 | FL finished in 114.65750529999787
INFO flwr 2024-04-17 05:19:18,796 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 05:19:18,797 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 05:19:18,797 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 05:19:18,797 | app.py:229 | app_fit: losses_centralized [(0, 2.302462339401245), (1, 2.12919545173645), (2, 2.347642183303833), (3, 2.347642183303833), (4, 2.347642183303833), (5, 2.347642183303833), (6, 2.347642183303833), (7, 2.347642183303833), (8, 2.347642183303833), (9, 2.347642183303833), (10, 2.347642183303833)]
INFO flwr 2024-04-17 05:19:18,797 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0879), (1, 0.3318), (2, 0.1135), (3, 0.1135), (4, 0.1135), (5, 0.1135), (6, 0.1135), (7, 0.1135), (8, 0.1135), (9, 0.1135), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.34764
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_051702-lou1qmlc
wandb: Find logs at: ./wandb/offline-run-20240417_051702-lou1qmlc/logs
INFO flwr 2024-04-17 05:19:22,302 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 05:26:30,283 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=363199)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=363199)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 05:26:34,970	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 05:26:35,790	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 05:26:36,243	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 05:26:36,593	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_bfc0607052413a92.zip' (36.86MiB) to Ray cluster...
2024-04-17 05:26:36,711	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_bfc0607052413a92.zip'.
INFO flwr 2024-04-17 05:26:47,738 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 77075435520.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 169842682880.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 05:26:47,739 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 05:26:47,739 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 05:26:47,754 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 05:26:47,755 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 05:26:47,755 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 05:26:47,755 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 05:26:50,358 | server.py:94 | initial parameters (loss, other metrics): 2.3027961254119873, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-17 05:26:50,359 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 05:26:50,359 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=367572)[0m 2024-04-17 05:26:53.920613: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=367572)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=367571)[0m 2024-04-17 05:26:56.253112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=367576)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=367576)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=367573)[0m 2024-04-17 05:26:54.100788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=367573)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=367573)[0m 2024-04-17 05:26:56.410959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 05:27:11,570 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 05:27:12,976 | server.py:125 | fit progress: (1, 2.191013813018799, {'accuracy': 0.4967, 'data_size': 10000}, 22.616855167012545)
INFO flwr 2024-04-17 05:27:12,976 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 05:27:12,977 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:27:22,818 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 05:27:24,031 | server.py:125 | fit progress: (2, 1.860045075416565, {'accuracy': 0.6073, 'data_size': 10000}, 33.67203934601275)
INFO flwr 2024-04-17 05:27:24,032 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 05:27:24,032 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:27:32,977 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 05:27:34,361 | server.py:125 | fit progress: (3, 1.6292678117752075, {'accuracy': 0.8363, 'data_size': 10000}, 44.00215660700633)
INFO flwr 2024-04-17 05:27:34,362 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 05:27:34,362 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:27:43,309 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 05:27:44,474 | server.py:125 | fit progress: (4, 1.577414631843567, {'accuracy': 0.8849, 'data_size': 10000}, 54.11441651401401)
INFO flwr 2024-04-17 05:27:44,474 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 05:27:44,474 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:27:53,224 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 05:27:54,595 | server.py:125 | fit progress: (5, 1.5759847164154053, {'accuracy': 0.8846, 'data_size': 10000}, 64.23522731300909)
INFO flwr 2024-04-17 05:27:54,595 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 05:27:54,595 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:28:03,609 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 05:28:04,977 | server.py:125 | fit progress: (6, 1.5620193481445312, {'accuracy': 0.8996, 'data_size': 10000}, 74.617603332008)
INFO flwr 2024-04-17 05:28:04,977 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 05:28:04,977 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:28:13,444 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 05:28:14,620 | server.py:125 | fit progress: (7, 1.538376808166504, {'accuracy': 0.9217, 'data_size': 10000}, 84.26015839900356)
INFO flwr 2024-04-17 05:28:14,620 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 05:28:14,620 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:28:23,444 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 05:28:24,847 | server.py:125 | fit progress: (8, 1.5422875881195068, {'accuracy': 0.9188, 'data_size': 10000}, 94.48807049200695)
INFO flwr 2024-04-17 05:28:24,848 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 05:28:24,848 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:28:33,838 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 05:28:35,028 | server.py:125 | fit progress: (9, 1.5408774614334106, {'accuracy': 0.9201, 'data_size': 10000}, 104.66905305100954)
INFO flwr 2024-04-17 05:28:35,029 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 05:28:35,029 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:28:43,679 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 05:28:45,082 | server.py:125 | fit progress: (10, 1.5398443937301636, {'accuracy': 0.921, 'data_size': 10000}, 114.72249598501367)
INFO flwr 2024-04-17 05:28:45,082 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 05:28:45,082 | server.py:153 | FL finished in 114.72295050200773
INFO flwr 2024-04-17 05:28:45,082 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 05:28:45,083 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 05:28:45,083 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 05:28:45,083 | app.py:229 | app_fit: losses_centralized [(0, 2.3027961254119873), (1, 2.191013813018799), (2, 1.860045075416565), (3, 1.6292678117752075), (4, 1.577414631843567), (5, 1.5759847164154053), (6, 1.5620193481445312), (7, 1.538376808166504), (8, 1.5422875881195068), (9, 1.5408774614334106), (10, 1.5398443937301636)]
INFO flwr 2024-04-17 05:28:45,083 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.4967), (2, 0.6073), (3, 0.8363), (4, 0.8849), (5, 0.8846), (6, 0.8996), (7, 0.9217), (8, 0.9188), (9, 0.9201), (10, 0.921)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.921
wandb:     loss 1.53984
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_052629-m1kryvii
wandb: Find logs at: ./wandb/offline-run-20240417_052629-m1kryvii/logs
INFO flwr 2024-04-17 05:28:48,594 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 05:35:56,699 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=367571)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=367571)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 05:36:01,789	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 05:36:02,664	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 05:36:03,121	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 05:36:03,472	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_870e51df5b518793.zip' (36.88MiB) to Ray cluster...
2024-04-17 05:36:03,588	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_870e51df5b518793.zip'.
INFO flwr 2024-04-17 05:36:14,620 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77074896076.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169841424180.0}
INFO flwr 2024-04-17 05:36:14,621 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 05:36:14,621 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 05:36:14,642 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 05:36:14,643 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 05:36:14,644 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 05:36:14,644 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 05:36:17,976 | server.py:94 | initial parameters (loss, other metrics): 2.3023369312286377, {'accuracy': 0.1023, 'data_size': 10000}
INFO flwr 2024-04-17 05:36:17,981 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 05:36:17,982 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=371941)[0m 2024-04-17 05:36:20.769048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=371941)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=371941)[0m 2024-04-17 05:36:23.155074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=371940)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=371940)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=371942)[0m 2024-04-17 05:36:20.939055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=371942)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=371942)[0m 2024-04-17 05:36:23.288534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 05:36:40,018 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 05:36:41,234 | server.py:125 | fit progress: (1, 2.3012657165527344, {'accuracy': 0.1047, 'data_size': 10000}, 23.252724064004724)
INFO flwr 2024-04-17 05:36:41,235 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 05:36:41,235 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:36:51,006 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 05:36:52,171 | server.py:125 | fit progress: (2, 2.2982027530670166, {'accuracy': 0.2266, 'data_size': 10000}, 34.18910417500592)
INFO flwr 2024-04-17 05:36:52,171 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 05:36:52,171 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:37:00,999 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 05:37:02,378 | server.py:125 | fit progress: (3, 2.2910008430480957, {'accuracy': 0.4123, 'data_size': 10000}, 44.39641060899885)
INFO flwr 2024-04-17 05:37:02,378 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 05:37:02,379 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:37:10,672 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 05:37:12,075 | server.py:125 | fit progress: (4, 2.276289939880371, {'accuracy': 0.5517, 'data_size': 10000}, 54.09379097899364)
INFO flwr 2024-04-17 05:37:12,076 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 05:37:12,076 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:37:20,928 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 05:37:22,087 | server.py:125 | fit progress: (5, 2.248868227005005, {'accuracy': 0.6384, 'data_size': 10000}, 64.1052491349983)
INFO flwr 2024-04-17 05:37:22,087 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 05:37:22,087 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:37:31,385 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 05:37:32,544 | server.py:125 | fit progress: (6, 2.200671434402466, {'accuracy': 0.6701, 'data_size': 10000}, 74.56266504799714)
INFO flwr 2024-04-17 05:37:32,545 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 05:37:32,545 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:37:41,548 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 05:37:42,959 | server.py:125 | fit progress: (7, 2.1260526180267334, {'accuracy': 0.6844, 'data_size': 10000}, 84.97693357299431)
INFO flwr 2024-04-17 05:37:42,959 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 05:37:42,959 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:37:51,714 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 05:37:52,877 | server.py:125 | fit progress: (8, 2.0304815769195557, {'accuracy': 0.7054, 'data_size': 10000}, 94.89544619999651)
INFO flwr 2024-04-17 05:37:52,877 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 05:37:52,878 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:38:02,054 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 05:38:03,469 | server.py:125 | fit progress: (9, 1.931262731552124, {'accuracy': 0.7328, 'data_size': 10000}, 105.48714780100272)
INFO flwr 2024-04-17 05:38:03,469 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 05:38:03,469 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:38:12,444 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 05:38:13,810 | server.py:125 | fit progress: (10, 1.841933012008667, {'accuracy': 0.7637, 'data_size': 10000}, 115.82825692900224)
INFO flwr 2024-04-17 05:38:13,810 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 05:38:13,810 | server.py:153 | FL finished in 115.82869309399393
INFO flwr 2024-04-17 05:38:13,811 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 05:38:13,811 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 05:38:13,811 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 05:38:13,811 | app.py:229 | app_fit: losses_centralized [(0, 2.3023369312286377), (1, 2.3012657165527344), (2, 2.2982027530670166), (3, 2.2910008430480957), (4, 2.276289939880371), (5, 2.248868227005005), (6, 2.200671434402466), (7, 2.1260526180267334), (8, 2.0304815769195557), (9, 1.931262731552124), (10, 1.841933012008667)]
INFO flwr 2024-04-17 05:38:13,811 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1023), (1, 0.1047), (2, 0.2266), (3, 0.4123), (4, 0.5517), (5, 0.6384), (6, 0.6701), (7, 0.6844), (8, 0.7054), (9, 0.7328), (10, 0.7637)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7637
wandb:     loss 1.84193
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_053556-fv2kz3sh
wandb: Find logs at: ./wandb/offline-run-20240417_053556-fv2kz3sh/logs
INFO flwr 2024-04-17 05:38:17,331 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 05:45:25,207 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=371932)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=371932)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 05:45:29,965	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 05:45:30,746	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 05:45:31,199	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 05:45:31,563	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_708add1c04ecf29f.zip' (36.89MiB) to Ray cluster...
2024-04-17 05:45:31,684	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_708add1c04ecf29f.zip'.
INFO flwr 2024-04-17 05:45:42,720 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77087585894.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169871033754.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 05:45:42,720 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 05:45:42,721 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 05:45:42,739 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 05:45:42,741 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 05:45:42,741 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 05:45:42,741 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 05:45:46,117 | server.py:94 | initial parameters (loss, other metrics): 2.302738904953003, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-17 05:45:46,117 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 05:45:46,118 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=376677)[0m 2024-04-17 05:45:48.834577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=376677)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=376673)[0m 2024-04-17 05:45:51.277476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=376678)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=376678)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=376670)[0m 2024-04-17 05:45:49.104812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=376670)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=376670)[0m 2024-04-17 05:45:51.386053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 05:46:06,821 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 05:46:08,022 | server.py:125 | fit progress: (1, 2.30267333984375, {'accuracy': 0.0958, 'data_size': 10000}, 21.904058896994684)
INFO flwr 2024-04-17 05:46:08,022 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 05:46:08,022 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:46:17,739 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 05:46:19,141 | server.py:125 | fit progress: (2, 2.3025739192962646, {'accuracy': 0.0958, 'data_size': 10000}, 33.022884188991156)
INFO flwr 2024-04-17 05:46:19,141 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 05:46:19,141 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:46:28,187 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 05:46:29,347 | server.py:125 | fit progress: (3, 2.302447557449341, {'accuracy': 0.0958, 'data_size': 10000}, 43.22893853500136)
INFO flwr 2024-04-17 05:46:29,347 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 05:46:29,347 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:46:37,786 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 05:46:39,172 | server.py:125 | fit progress: (4, 2.3022873401641846, {'accuracy': 0.0958, 'data_size': 10000}, 53.0543702540017)
INFO flwr 2024-04-17 05:46:39,172 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 05:46:39,173 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:46:47,708 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 05:46:48,875 | server.py:125 | fit progress: (5, 2.3020899295806885, {'accuracy': 0.0958, 'data_size': 10000}, 62.75694158099941)
INFO flwr 2024-04-17 05:46:48,875 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 05:46:48,875 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:46:57,606 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 05:46:59,024 | server.py:125 | fit progress: (6, 2.3018527030944824, {'accuracy': 0.0959, 'data_size': 10000}, 72.90626777999569)
INFO flwr 2024-04-17 05:46:59,024 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 05:46:59,024 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:47:07,457 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 05:47:08,621 | server.py:125 | fit progress: (7, 2.3015663623809814, {'accuracy': 0.0961, 'data_size': 10000}, 82.50383357399551)
INFO flwr 2024-04-17 05:47:08,622 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 05:47:08,622 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:47:17,312 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 05:47:18,474 | server.py:125 | fit progress: (8, 2.301225185394287, {'accuracy': 0.0968, 'data_size': 10000}, 92.3566276759957)
INFO flwr 2024-04-17 05:47:18,475 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 05:47:18,475 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:47:27,501 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 05:47:28,934 | server.py:125 | fit progress: (9, 2.3008265495300293, {'accuracy': 0.0986, 'data_size': 10000}, 102.81641077100358)
INFO flwr 2024-04-17 05:47:28,934 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 05:47:28,935 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:47:37,805 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 05:47:39,181 | server.py:125 | fit progress: (10, 2.3003647327423096, {'accuracy': 0.1024, 'data_size': 10000}, 113.06376932800049)
INFO flwr 2024-04-17 05:47:39,182 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 05:47:39,182 | server.py:153 | FL finished in 113.06429725600174
INFO flwr 2024-04-17 05:47:39,182 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 05:47:39,182 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 05:47:39,183 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 05:47:39,183 | app.py:229 | app_fit: losses_centralized [(0, 2.302738904953003), (1, 2.30267333984375), (2, 2.3025739192962646), (3, 2.302447557449341), (4, 2.3022873401641846), (5, 2.3020899295806885), (6, 2.3018527030944824), (7, 2.3015663623809814), (8, 2.301225185394287), (9, 2.3008265495300293), (10, 2.3003647327423096)]
INFO flwr 2024-04-17 05:47:39,183 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.0958), (2, 0.0958), (3, 0.0958), (4, 0.0958), (5, 0.0958), (6, 0.0959), (7, 0.0961), (8, 0.0968), (9, 0.0986), (10, 0.1024)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1024
wandb:     loss 2.30036
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_054524-x9wpxhpe
wandb: Find logs at: ./wandb/offline-run-20240417_054524-x9wpxhpe/logs
INFO flwr 2024-04-17 05:47:42,729 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 05:54:50,742 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=376668)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=376668)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 05:54:55,667	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 05:54:56,658	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 05:54:57,107	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 05:54:57,464	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_49da719d6749df47.zip' (36.90MiB) to Ray cluster...
2024-04-17 05:54:57,577	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_49da719d6749df47.zip'.
INFO flwr 2024-04-17 05:55:08,818 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169856413901.0, 'object_store_memory': 77081320243.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 05:55:08,819 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 05:55:08,819 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 05:55:08,839 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 05:55:08,840 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 05:55:08,841 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 05:55:08,841 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 05:55:11,557 | server.py:94 | initial parameters (loss, other metrics): 2.302691698074341, {'accuracy': 0.0762, 'data_size': 10000}
INFO flwr 2024-04-17 05:55:11,557 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 05:55:11,558 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=381044)[0m 2024-04-17 05:55:15.035528: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=381044)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=381044)[0m 2024-04-17 05:55:17.375377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=381040)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=381040)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=381042)[0m 2024-04-17 05:55:15.275055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=381042)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=381042)[0m 2024-04-17 05:55:17.579531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 05:55:33,550 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 05:55:34,919 | server.py:125 | fit progress: (1, 2.302684783935547, {'accuracy': 0.0766, 'data_size': 10000}, 23.36163490099716)
INFO flwr 2024-04-17 05:55:34,919 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 05:55:34,920 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:55:44,269 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 05:55:45,458 | server.py:125 | fit progress: (2, 2.3026745319366455, {'accuracy': 0.0773, 'data_size': 10000}, 33.90004639700055)
INFO flwr 2024-04-17 05:55:45,458 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 05:55:45,458 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:55:54,576 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 05:55:55,947 | server.py:125 | fit progress: (3, 2.3026626110076904, {'accuracy': 0.0781, 'data_size': 10000}, 44.38931993600272)
INFO flwr 2024-04-17 05:55:55,947 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 05:55:55,947 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:56:04,289 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 05:56:05,457 | server.py:125 | fit progress: (4, 2.302650213241577, {'accuracy': 0.0797, 'data_size': 10000}, 53.899825642991345)
INFO flwr 2024-04-17 05:56:05,458 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 05:56:05,458 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:56:14,389 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 05:56:15,774 | server.py:125 | fit progress: (5, 2.3026368618011475, {'accuracy': 0.0805, 'data_size': 10000}, 64.21609595300106)
INFO flwr 2024-04-17 05:56:15,774 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 05:56:15,774 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:56:25,011 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 05:56:26,178 | server.py:125 | fit progress: (6, 2.3026225566864014, {'accuracy': 0.0818, 'data_size': 10000}, 74.62017588799063)
INFO flwr 2024-04-17 05:56:26,178 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 05:56:26,178 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:56:34,982 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 05:56:36,388 | server.py:125 | fit progress: (7, 2.302608013153076, {'accuracy': 0.083, 'data_size': 10000}, 84.8308939129929)
INFO flwr 2024-04-17 05:56:36,389 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 05:56:36,389 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:56:45,133 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 05:56:46,318 | server.py:125 | fit progress: (8, 2.3025925159454346, {'accuracy': 0.085, 'data_size': 10000}, 94.76083759400353)
INFO flwr 2024-04-17 05:56:46,319 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 05:56:46,319 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:56:55,198 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 05:56:56,595 | server.py:125 | fit progress: (9, 2.302577018737793, {'accuracy': 0.0868, 'data_size': 10000}, 105.0369801659981)
INFO flwr 2024-04-17 05:56:56,595 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 05:56:56,595 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 05:57:05,676 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 05:57:06,825 | server.py:125 | fit progress: (10, 2.302561044692993, {'accuracy': 0.0881, 'data_size': 10000}, 115.26781846399535)
INFO flwr 2024-04-17 05:57:06,826 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 05:57:06,826 | server.py:153 | FL finished in 115.26831835399207
INFO flwr 2024-04-17 05:57:06,826 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 05:57:06,826 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 05:57:06,826 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 05:57:06,827 | app.py:229 | app_fit: losses_centralized [(0, 2.302691698074341), (1, 2.302684783935547), (2, 2.3026745319366455), (3, 2.3026626110076904), (4, 2.302650213241577), (5, 2.3026368618011475), (6, 2.3026225566864014), (7, 2.302608013153076), (8, 2.3025925159454346), (9, 2.302577018737793), (10, 2.302561044692993)]
INFO flwr 2024-04-17 05:57:06,827 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0762), (1, 0.0766), (2, 0.0773), (3, 0.0781), (4, 0.0797), (5, 0.0805), (6, 0.0818), (7, 0.083), (8, 0.085), (9, 0.0868), (10, 0.0881)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0881
wandb:     loss 2.30256
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_055450-7m1nqvh4
wandb: Find logs at: ./wandb/offline-run-20240417_055450-7m1nqvh4/logs
INFO flwr 2024-04-17 05:57:10,335 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 06:04:18,421 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=381037)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=381037)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 06:04:23,180	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 06:04:24,024	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 06:04:24,474	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 06:04:24,835	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_40547b7bceab935f.zip' (36.91MiB) to Ray cluster...
2024-04-17 06:04:24,946	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_40547b7bceab935f.zip'.
INFO flwr 2024-04-17 06:04:37,345 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169983353447.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77135722905.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0}
INFO flwr 2024-04-17 06:04:37,345 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 06:04:37,345 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 06:04:37,362 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 06:04:37,364 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 06:04:37,364 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 06:04:37,364 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 06:04:40,376 | server.py:94 | initial parameters (loss, other metrics): 2.302668809890747, {'accuracy': 0.0989, 'data_size': 10000}
INFO flwr 2024-04-17 06:04:40,376 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 06:04:40,377 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=385413)[0m 2024-04-17 06:04:43.411663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=385413)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=385413)[0m 2024-04-17 06:04:45.727667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=385413)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=385413)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=385411)[0m 2024-04-17 06:04:43.668172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=385411)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=385411)[0m 2024-04-17 06:04:45.881833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 06:05:02,813 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 06:05:04,231 | server.py:125 | fit progress: (1, 2.170804262161255, {'accuracy': 0.29, 'data_size': 10000}, 23.85440647699579)
INFO flwr 2024-04-17 06:05:04,231 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 06:05:04,232 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:05:15,175 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 06:05:16,359 | server.py:125 | fit progress: (2, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 35.98209629100165)
INFO flwr 2024-04-17 06:05:16,359 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 06:05:16,359 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:05:27,031 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 06:05:28,411 | server.py:125 | fit progress: (3, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 48.03388717499911)
INFO flwr 2024-04-17 06:05:28,411 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 06:05:28,411 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:05:38,460 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 06:05:39,610 | server.py:125 | fit progress: (4, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 59.233725231999415)
INFO flwr 2024-04-17 06:05:39,611 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 06:05:39,611 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:05:49,832 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 06:05:51,219 | server.py:125 | fit progress: (5, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 70.84240041299199)
INFO flwr 2024-04-17 06:05:51,219 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 06:05:51,220 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:06:01,728 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 06:06:03,102 | server.py:125 | fit progress: (6, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 82.72501563800324)
INFO flwr 2024-04-17 06:06:03,102 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 06:06:03,102 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:06:13,514 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 06:06:14,679 | server.py:125 | fit progress: (7, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 94.30235560599249)
INFO flwr 2024-04-17 06:06:14,679 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 06:06:14,680 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:06:25,904 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 06:06:27,280 | server.py:125 | fit progress: (8, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 106.90363713499391)
INFO flwr 2024-04-17 06:06:27,281 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 06:06:27,281 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:06:37,893 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 06:06:39,061 | server.py:125 | fit progress: (9, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 118.68431074699038)
INFO flwr 2024-04-17 06:06:39,061 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 06:06:39,062 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:06:49,925 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 06:06:51,307 | server.py:125 | fit progress: (10, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 130.93041610599903)
INFO flwr 2024-04-17 06:06:51,307 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 06:06:51,308 | server.py:153 | FL finished in 130.93086306199257
INFO flwr 2024-04-17 06:06:51,308 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 06:06:51,308 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 06:06:51,308 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 06:06:51,308 | app.py:229 | app_fit: losses_centralized [(0, 2.302668809890747), (1, 2.170804262161255), (2, 2.3631420135498047), (3, 2.3631420135498047), (4, 2.3631420135498047), (5, 2.3631420135498047), (6, 2.3631420135498047), (7, 2.3631420135498047), (8, 2.3631420135498047), (9, 2.3631420135498047), (10, 2.3631420135498047)]
INFO flwr 2024-04-17 06:06:51,308 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0989), (1, 0.29), (2, 0.098), (3, 0.098), (4, 0.098), (5, 0.098), (6, 0.098), (7, 0.098), (8, 0.098), (9, 0.098), (10, 0.098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.098
wandb:     loss 2.36314
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_060418-47dsboky
wandb: Find logs at: ./wandb/offline-run-20240417_060418-47dsboky/logs
INFO flwr 2024-04-17 06:06:54,857 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 06:14:02,806 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=385401)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=385401)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 06:14:07,573	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 06:14:08,439	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 06:14:08,873	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 06:14:09,225	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_937671ffcc0580a7.zip' (36.92MiB) to Ray cluster...
2024-04-17 06:14:09,340	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_937671ffcc0580a7.zip'.
INFO flwr 2024-04-17 06:14:20,365 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77074640486.0, 'CPU': 64.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169840827802.0}
INFO flwr 2024-04-17 06:14:20,365 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 06:14:20,365 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 06:14:20,386 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 06:14:20,388 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 06:14:20,388 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 06:14:20,388 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 06:14:24,048 | server.py:94 | initial parameters (loss, other metrics): 2.3025295734405518, {'accuracy': 0.0959, 'data_size': 10000}
INFO flwr 2024-04-17 06:14:24,048 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 06:14:24,048 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=390200)[0m 2024-04-17 06:14:26.519261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=390200)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=390200)[0m 2024-04-17 06:14:28.824693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=390200)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=390200)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=390195)[0m 2024-04-17 06:14:26.988521: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=390195)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=390195)[0m 2024-04-17 06:14:29.314020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 06:14:46,929 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 06:14:48,128 | server.py:125 | fit progress: (1, 2.207249402999878, {'accuracy': 0.3066, 'data_size': 10000}, 24.07964246199117)
INFO flwr 2024-04-17 06:14:48,128 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 06:14:48,129 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:14:59,465 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 06:15:00,648 | server.py:125 | fit progress: (2, 1.9049793481826782, {'accuracy': 0.5577, 'data_size': 10000}, 36.59996416600188)
INFO flwr 2024-04-17 06:15:00,649 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 06:15:00,649 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:15:12,019 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 06:15:13,417 | server.py:125 | fit progress: (3, 1.648082971572876, {'accuracy': 0.807, 'data_size': 10000}, 49.36883279599715)
INFO flwr 2024-04-17 06:15:13,418 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 06:15:13,418 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:15:23,753 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 06:15:24,933 | server.py:125 | fit progress: (4, 1.6039645671844482, {'accuracy': 0.8557, 'data_size': 10000}, 60.88482537199161)
INFO flwr 2024-04-17 06:15:24,934 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 06:15:24,934 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:15:35,489 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 06:15:36,877 | server.py:125 | fit progress: (5, 1.5889235734939575, {'accuracy': 0.8719, 'data_size': 10000}, 72.8288679010002)
INFO flwr 2024-04-17 06:15:36,878 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 06:15:36,878 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:15:48,063 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 06:15:49,219 | server.py:125 | fit progress: (6, 1.553195834159851, {'accuracy': 0.907, 'data_size': 10000}, 85.17111073499836)
INFO flwr 2024-04-17 06:15:49,220 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 06:15:49,220 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:15:59,501 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 06:16:00,876 | server.py:125 | fit progress: (7, 1.538272738456726, {'accuracy': 0.9219, 'data_size': 10000}, 96.82762773400464)
INFO flwr 2024-04-17 06:16:00,876 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 06:16:00,877 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:16:11,224 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 06:16:12,381 | server.py:125 | fit progress: (8, 1.5322117805480957, {'accuracy': 0.9285, 'data_size': 10000}, 108.33235571499972)
INFO flwr 2024-04-17 06:16:12,381 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 06:16:12,381 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:16:23,191 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 06:16:24,565 | server.py:125 | fit progress: (9, 1.5257025957107544, {'accuracy': 0.9355, 'data_size': 10000}, 120.51656249399821)
INFO flwr 2024-04-17 06:16:24,565 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 06:16:24,565 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:16:34,859 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 06:16:36,271 | server.py:125 | fit progress: (10, 1.5269649028778076, {'accuracy': 0.934, 'data_size': 10000}, 132.2227536940045)
INFO flwr 2024-04-17 06:16:36,271 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 06:16:36,272 | server.py:153 | FL finished in 132.22319197999605
INFO flwr 2024-04-17 06:16:36,272 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 06:16:36,272 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 06:16:36,272 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 06:16:36,272 | app.py:229 | app_fit: losses_centralized [(0, 2.3025295734405518), (1, 2.207249402999878), (2, 1.9049793481826782), (3, 1.648082971572876), (4, 1.6039645671844482), (5, 1.5889235734939575), (6, 1.553195834159851), (7, 1.538272738456726), (8, 1.5322117805480957), (9, 1.5257025957107544), (10, 1.5269649028778076)]
INFO flwr 2024-04-17 06:16:36,272 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0959), (1, 0.3066), (2, 0.5577), (3, 0.807), (4, 0.8557), (5, 0.8719), (6, 0.907), (7, 0.9219), (8, 0.9285), (9, 0.9355), (10, 0.934)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.934
wandb:     loss 1.52696
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_061402-282hreuz
wandb: Find logs at: ./wandb/offline-run-20240417_061402-282hreuz/logs
INFO flwr 2024-04-17 06:16:39,787 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 06:23:48,051 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=390195)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=390195)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 06:23:53,482	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 06:23:54,367	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 06:23:54,826	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 06:23:55,182	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_470f778671cc7748.zip' (36.93MiB) to Ray cluster...
2024-04-17 06:23:55,293	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_470f778671cc7748.zip'.
INFO flwr 2024-04-17 06:24:06,328 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'memory': 169832994612.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77071283404.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 06:24:06,329 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 06:24:06,329 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 06:24:06,350 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 06:24:06,351 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 06:24:06,352 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 06:24:06,352 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 06:24:09,881 | server.py:94 | initial parameters (loss, other metrics): 2.302673816680908, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-17 06:24:09,881 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 06:24:09,881 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=394595)[0m 2024-04-17 06:24:12.392863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=394595)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=394595)[0m 2024-04-17 06:24:14.748285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=394595)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=394595)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=394596)[0m 2024-04-17 06:24:12.740573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=394596)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=394596)[0m 2024-04-17 06:24:15.010087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 06:24:33,127 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 06:24:34,323 | server.py:125 | fit progress: (1, 2.301664352416992, {'accuracy': 0.1152, 'data_size': 10000}, 24.44169675199373)
INFO flwr 2024-04-17 06:24:34,323 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 06:24:34,324 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:24:45,395 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 06:24:46,772 | server.py:125 | fit progress: (2, 2.2985284328460693, {'accuracy': 0.1795, 'data_size': 10000}, 36.89040761099022)
INFO flwr 2024-04-17 06:24:46,772 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 06:24:46,772 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:24:57,574 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 06:24:58,962 | server.py:125 | fit progress: (3, 2.2913718223571777, {'accuracy': 0.208, 'data_size': 10000}, 49.08022000399069)
INFO flwr 2024-04-17 06:24:58,962 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 06:24:58,962 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:25:09,101 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 06:25:10,490 | server.py:125 | fit progress: (4, 2.277301073074341, {'accuracy': 0.3485, 'data_size': 10000}, 60.60902640900167)
INFO flwr 2024-04-17 06:25:10,491 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 06:25:10,491 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:25:22,123 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 06:25:23,298 | server.py:125 | fit progress: (5, 2.251858711242676, {'accuracy': 0.502, 'data_size': 10000}, 73.41683296399424)
INFO flwr 2024-04-17 06:25:23,298 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 06:25:23,299 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:25:33,127 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 06:25:34,509 | server.py:125 | fit progress: (6, 2.2091102600097656, {'accuracy': 0.5997, 'data_size': 10000}, 84.62765498200315)
INFO flwr 2024-04-17 06:25:34,509 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 06:25:34,509 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:25:45,506 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 06:25:46,676 | server.py:125 | fit progress: (7, 2.1430275440216064, {'accuracy': 0.6585, 'data_size': 10000}, 96.79433348499879)
INFO flwr 2024-04-17 06:25:46,676 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 06:25:46,676 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:25:57,487 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 06:25:58,866 | server.py:125 | fit progress: (8, 2.0527162551879883, {'accuracy': 0.7041, 'data_size': 10000}, 108.98459883699252)
INFO flwr 2024-04-17 06:25:58,866 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 06:25:58,866 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:26:09,451 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 06:26:10,615 | server.py:125 | fit progress: (9, 1.9483931064605713, {'accuracy': 0.7435, 'data_size': 10000}, 120.73331278799742)
INFO flwr 2024-04-17 06:26:10,615 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 06:26:10,615 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:26:20,785 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 06:26:22,203 | server.py:125 | fit progress: (10, 1.8476321697235107, {'accuracy': 0.7807, 'data_size': 10000}, 132.3213856240036)
INFO flwr 2024-04-17 06:26:22,203 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 06:26:22,203 | server.py:153 | FL finished in 132.3218333699915
INFO flwr 2024-04-17 06:26:22,203 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 06:26:22,203 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 06:26:22,204 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 06:26:22,204 | app.py:229 | app_fit: losses_centralized [(0, 2.302673816680908), (1, 2.301664352416992), (2, 2.2985284328460693), (3, 2.2913718223571777), (4, 2.277301073074341), (5, 2.251858711242676), (6, 2.2091102600097656), (7, 2.1430275440216064), (8, 2.0527162551879883), (9, 1.9483931064605713), (10, 1.8476321697235107)]
INFO flwr 2024-04-17 06:26:22,204 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.1152), (2, 0.1795), (3, 0.208), (4, 0.3485), (5, 0.502), (6, 0.5997), (7, 0.6585), (8, 0.7041), (9, 0.7435), (10, 0.7807)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7807
wandb:     loss 1.84763
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_062347-70ox0ggs
wandb: Find logs at: ./wandb/offline-run-20240417_062347-70ox0ggs/logs
INFO flwr 2024-04-17 06:26:25,639 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 06:33:33,871 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=394585)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=394585)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 06:33:38,788	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 06:33:39,564	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 06:33:40,004	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 06:33:40,368	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5264437362bff6a2.zip' (36.95MiB) to Ray cluster...
2024-04-17 06:33:40,487	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5264437362bff6a2.zip'.
INFO flwr 2024-04-17 06:33:51,506 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 77034604953.0, 'memory': 169747411559.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 06:33:51,506 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 06:33:51,507 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 06:33:51,526 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 06:33:51,528 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 06:33:51,528 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 06:33:51,528 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 06:33:55,291 | server.py:94 | initial parameters (loss, other metrics): 2.302583932876587, {'accuracy': 0.085, 'data_size': 10000}
INFO flwr 2024-04-17 06:33:55,292 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 06:33:55,292 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=399057)[0m 2024-04-17 06:33:57.586993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=399057)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=399057)[0m 2024-04-17 06:33:59.921452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=398985)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=398985)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=398982)[0m 2024-04-17 06:33:57.811882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=398982)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=398982)[0m 2024-04-17 06:34:00.097759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 06:34:17,611 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 06:34:18,804 | server.py:125 | fit progress: (1, 2.302509069442749, {'accuracy': 0.0921, 'data_size': 10000}, 23.512294995001866)
INFO flwr 2024-04-17 06:34:18,805 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 06:34:18,805 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:34:31,158 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 06:34:32,560 | server.py:125 | fit progress: (2, 2.302403211593628, {'accuracy': 0.1006, 'data_size': 10000}, 37.26779583499592)
INFO flwr 2024-04-17 06:34:32,560 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 06:34:32,561 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:34:42,712 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 06:34:43,904 | server.py:125 | fit progress: (3, 2.302264928817749, {'accuracy': 0.1105, 'data_size': 10000}, 48.612326987000415)
INFO flwr 2024-04-17 06:34:43,905 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 06:34:43,905 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:34:53,834 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 06:34:55,231 | server.py:125 | fit progress: (4, 2.3020899295806885, {'accuracy': 0.1201, 'data_size': 10000}, 59.93844549699861)
INFO flwr 2024-04-17 06:34:55,231 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 06:34:55,231 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:35:05,854 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 06:35:07,097 | server.py:125 | fit progress: (5, 2.3018734455108643, {'accuracy': 0.1292, 'data_size': 10000}, 71.80475115899753)
INFO flwr 2024-04-17 06:35:07,097 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 06:35:07,097 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:35:17,825 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 06:35:19,211 | server.py:125 | fit progress: (6, 2.3016157150268555, {'accuracy': 0.1444, 'data_size': 10000}, 83.91928370000096)
INFO flwr 2024-04-17 06:35:19,212 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 06:35:19,212 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:35:29,066 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 06:35:30,249 | server.py:125 | fit progress: (7, 2.301302671432495, {'accuracy': 0.1604, 'data_size': 10000}, 94.95671426999616)
INFO flwr 2024-04-17 06:35:30,249 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 06:35:30,250 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:35:40,430 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 06:35:41,818 | server.py:125 | fit progress: (8, 2.3009374141693115, {'accuracy': 0.1761, 'data_size': 10000}, 106.52629249500751)
INFO flwr 2024-04-17 06:35:41,819 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 06:35:41,819 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:35:52,565 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 06:35:53,761 | server.py:125 | fit progress: (9, 2.300516128540039, {'accuracy': 0.1976, 'data_size': 10000}, 118.46854605499539)
INFO flwr 2024-04-17 06:35:53,761 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 06:35:53,761 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:36:04,160 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 06:36:05,340 | server.py:125 | fit progress: (10, 2.3000376224517822, {'accuracy': 0.2219, 'data_size': 10000}, 130.04778241099848)
INFO flwr 2024-04-17 06:36:05,340 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 06:36:05,340 | server.py:153 | FL finished in 130.04829416899884
INFO flwr 2024-04-17 06:36:05,341 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 06:36:05,341 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 06:36:05,356 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 06:36:05,357 | app.py:229 | app_fit: losses_centralized [(0, 2.302583932876587), (1, 2.302509069442749), (2, 2.302403211593628), (3, 2.302264928817749), (4, 2.3020899295806885), (5, 2.3018734455108643), (6, 2.3016157150268555), (7, 2.301302671432495), (8, 2.3009374141693115), (9, 2.300516128540039), (10, 2.3000376224517822)]
INFO flwr 2024-04-17 06:36:05,357 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.085), (1, 0.0921), (2, 0.1006), (3, 0.1105), (4, 0.1201), (5, 0.1292), (6, 0.1444), (7, 0.1604), (8, 0.1761), (9, 0.1976), (10, 0.2219)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2219
wandb:     loss 2.30004
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_063333-la4mtbht
wandb: Find logs at: ./wandb/offline-run-20240417_063333-la4mtbht/logs
INFO flwr 2024-04-17 06:36:08,871 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 06:43:16,923 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=398982)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=398982)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 06:43:21,417	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 06:43:22,288	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 06:43:22,746	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 06:43:23,099	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_85c82d2ec68c78eb.zip' (36.96MiB) to Ray cluster...
2024-04-17 06:43:23,208	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_85c82d2ec68c78eb.zip'.
INFO flwr 2024-04-17 06:43:34,228 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77169312153.0, 'accelerator_type:TITAN': 1.0, 'memory': 170061728359.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 06:43:34,229 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 06:43:34,229 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 06:43:34,248 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 06:43:34,249 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 06:43:34,249 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 06:43:34,249 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 06:43:38,385 | server.py:94 | initial parameters (loss, other metrics): 2.3025217056274414, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-17 06:43:38,387 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 06:43:38,390 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=403144)[0m 2024-04-17 06:43:40.094143: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=403144)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=403144)[0m 2024-04-17 06:43:42.512863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=403144)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=403144)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=403142)[0m 2024-04-17 06:43:40.548378: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=403142)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=403142)[0m 2024-04-17 06:43:42.779889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 06:43:59,374 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 06:44:00,792 | server.py:125 | fit progress: (1, 2.3025143146514893, {'accuracy': 0.1135, 'data_size': 10000}, 22.402476458999445)
INFO flwr 2024-04-17 06:44:00,792 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 06:44:00,793 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:44:11,248 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 06:44:12,419 | server.py:125 | fit progress: (2, 2.302504539489746, {'accuracy': 0.1135, 'data_size': 10000}, 34.02944120198663)
INFO flwr 2024-04-17 06:44:12,419 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 06:44:12,419 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:44:22,975 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 06:44:24,168 | server.py:125 | fit progress: (3, 2.3024935722351074, {'accuracy': 0.1135, 'data_size': 10000}, 45.778679366994766)
INFO flwr 2024-04-17 06:44:24,168 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 06:44:24,169 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:44:34,894 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 06:44:36,269 | server.py:125 | fit progress: (4, 2.3024802207946777, {'accuracy': 0.1135, 'data_size': 10000}, 57.879337708000094)
INFO flwr 2024-04-17 06:44:36,269 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 06:44:36,269 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:44:46,836 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 06:44:48,222 | server.py:125 | fit progress: (5, 2.30246639251709, {'accuracy': 0.1135, 'data_size': 10000}, 69.83240716799628)
INFO flwr 2024-04-17 06:44:48,222 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 06:44:48,222 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:44:58,894 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 06:45:00,067 | server.py:125 | fit progress: (6, 2.302450656890869, {'accuracy': 0.1135, 'data_size': 10000}, 81.67792538399226)
INFO flwr 2024-04-17 06:45:00,068 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 06:45:00,068 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:45:10,131 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 06:45:11,277 | server.py:125 | fit progress: (7, 2.3024353981018066, {'accuracy': 0.1135, 'data_size': 10000}, 92.88787332999345)
INFO flwr 2024-04-17 06:45:11,278 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 06:45:11,278 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:45:22,157 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 06:45:23,527 | server.py:125 | fit progress: (8, 2.3024182319641113, {'accuracy': 0.1135, 'data_size': 10000}, 105.13720813299005)
INFO flwr 2024-04-17 06:45:23,527 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 06:45:23,527 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:45:33,256 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 06:45:34,426 | server.py:125 | fit progress: (9, 2.302400827407837, {'accuracy': 0.1135, 'data_size': 10000}, 116.03608660699683)
INFO flwr 2024-04-17 06:45:34,426 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 06:45:34,426 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:45:44,892 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 06:45:46,267 | server.py:125 | fit progress: (10, 2.3023834228515625, {'accuracy': 0.1135, 'data_size': 10000}, 127.87788541599002)
INFO flwr 2024-04-17 06:45:46,268 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 06:45:46,268 | server.py:153 | FL finished in 127.87835114799964
INFO flwr 2024-04-17 06:45:46,268 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 06:45:46,268 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 06:45:46,268 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 06:45:46,268 | app.py:229 | app_fit: losses_centralized [(0, 2.3025217056274414), (1, 2.3025143146514893), (2, 2.302504539489746), (3, 2.3024935722351074), (4, 2.3024802207946777), (5, 2.30246639251709), (6, 2.302450656890869), (7, 2.3024353981018066), (8, 2.3024182319641113), (9, 2.302400827407837), (10, 2.3023834228515625)]
INFO flwr 2024-04-17 06:45:46,269 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1135), (2, 0.1135), (3, 0.1135), (4, 0.1135), (5, 0.1135), (6, 0.1135), (7, 0.1135), (8, 0.1135), (9, 0.1135), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.30238
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_064316-zz27vkll
wandb: Find logs at: ./wandb/offline-run-20240417_064316-zz27vkll/logs
INFO flwr 2024-04-17 06:45:49,818 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 06:52:57,661 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=403133)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=403133)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 06:53:02,326	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 06:53:03,321	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 06:53:03,786	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 06:53:04,144	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7b3674612550dd5f.zip' (36.97MiB) to Ray cluster...
2024-04-17 06:53:04,229	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7b3674612550dd5f.zip'.
INFO flwr 2024-04-17 06:53:15,290 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77040777216.0, 'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169761813504.0}
INFO flwr 2024-04-17 06:53:15,291 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 06:53:15,291 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 06:53:15,313 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 06:53:15,315 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 06:53:15,315 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 06:53:15,316 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 06:53:18,642 | server.py:94 | initial parameters (loss, other metrics): 2.3025450706481934, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-17 06:53:18,642 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 06:53:18,643 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=407487)[0m 2024-04-17 06:53:21.433300: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=407487)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=407487)[0m 2024-04-17 06:53:23.752335: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=407496)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=407496)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=407496)[0m 2024-04-17 06:53:21.757249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=407496)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=407496)[0m 2024-04-17 06:53:23.985191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 06:53:42,788 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 06:53:43,984 | server.py:125 | fit progress: (1, 2.1463680267333984, {'accuracy': 0.3147, 'data_size': 10000}, 25.340889084007358)
INFO flwr 2024-04-17 06:53:43,984 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 06:53:43,984 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:53:55,085 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 06:53:56,467 | server.py:125 | fit progress: (2, 2.2837300300598145, {'accuracy': 0.1774, 'data_size': 10000}, 37.82391788701352)
INFO flwr 2024-04-17 06:53:56,467 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 06:53:56,467 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:54:06,777 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 06:54:07,927 | server.py:125 | fit progress: (3, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 49.284346102009295)
INFO flwr 2024-04-17 06:54:07,927 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 06:54:07,928 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:54:18,100 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 06:54:19,506 | server.py:125 | fit progress: (4, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 60.862709821012686)
INFO flwr 2024-04-17 06:54:19,506 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 06:54:19,506 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:54:29,679 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 06:54:30,842 | server.py:125 | fit progress: (5, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 72.1992928450054)
INFO flwr 2024-04-17 06:54:30,842 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 06:54:30,843 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:54:41,143 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 06:54:42,349 | server.py:125 | fit progress: (6, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 83.70645844200044)
INFO flwr 2024-04-17 06:54:42,350 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 06:54:42,350 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:54:52,517 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 06:54:53,880 | server.py:125 | fit progress: (7, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 95.23673351200705)
INFO flwr 2024-04-17 06:54:53,880 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 06:54:53,880 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:55:04,876 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 06:55:06,249 | server.py:125 | fit progress: (8, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 107.60610029401141)
INFO flwr 2024-04-17 06:55:06,249 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 06:55:06,250 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:55:16,361 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 06:55:17,512 | server.py:125 | fit progress: (9, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 118.86886270801187)
INFO flwr 2024-04-17 06:55:17,512 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 06:55:17,512 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 06:55:28,790 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 06:55:30,154 | server.py:125 | fit progress: (10, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 131.5112682700128)
INFO flwr 2024-04-17 06:55:30,155 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 06:55:30,155 | server.py:153 | FL finished in 131.51201925500936
INFO flwr 2024-04-17 06:55:30,155 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 06:55:30,155 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 06:55:30,155 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 06:55:30,156 | app.py:229 | app_fit: losses_centralized [(0, 2.3025450706481934), (1, 2.1463680267333984), (2, 2.2837300300598145), (3, 2.3629422187805176), (4, 2.3629422187805176), (5, 2.3629422187805176), (6, 2.3629422187805176), (7, 2.3629422187805176), (8, 2.3629422187805176), (9, 2.3629422187805176), (10, 2.3629422187805176)]
INFO flwr 2024-04-17 06:55:30,156 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.3147), (2, 0.1774), (3, 0.0982), (4, 0.0982), (5, 0.0982), (6, 0.0982), (7, 0.0982), (8, 0.0982), (9, 0.0982), (10, 0.0982)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0982
wandb:     loss 2.36294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_065257-8m9losif
wandb: Find logs at: ./wandb/offline-run-20240417_065257-8m9losif/logs
INFO flwr 2024-04-17 06:55:33,714 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 07:02:41,746 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=407485)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=407485)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 07:02:46,911	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 07:02:47,884	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 07:02:48,336	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 07:02:48,691	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e2d0f788e80dc291.zip' (36.98MiB) to Ray cluster...
2024-04-17 07:02:48,777	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e2d0f788e80dc291.zip'.
INFO flwr 2024-04-17 07:02:59,855 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77031993753.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169741318759.0, 'CPU': 64.0}
INFO flwr 2024-04-17 07:02:59,856 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 07:02:59,856 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 07:02:59,875 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 07:02:59,876 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 07:02:59,877 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 07:02:59,877 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 07:03:03,921 | server.py:94 | initial parameters (loss, other metrics): 2.302626132965088, {'accuracy': 0.1091, 'data_size': 10000}
INFO flwr 2024-04-17 07:03:03,921 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 07:03:03,921 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=411879)[0m 2024-04-17 07:03:05.934284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=411879)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=411879)[0m 2024-04-17 07:03:08.253655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=411880)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=411880)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=411883)[0m 2024-04-17 07:03:06.095688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=411883)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=411868)[0m 2024-04-17 07:03:08.563370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 07:03:25,990 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 07:03:27,166 | server.py:125 | fit progress: (1, 2.17391300201416, {'accuracy': 0.5786, 'data_size': 10000}, 23.24431044999801)
INFO flwr 2024-04-17 07:03:27,166 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 07:03:27,166 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:03:38,337 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 07:03:39,490 | server.py:125 | fit progress: (2, 1.7978695631027222, {'accuracy': 0.6865, 'data_size': 10000}, 35.56873786498909)
INFO flwr 2024-04-17 07:03:39,490 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 07:03:39,491 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:03:50,205 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 07:03:51,612 | server.py:125 | fit progress: (3, 1.6408597230911255, {'accuracy': 0.8215, 'data_size': 10000}, 47.69106156799535)
INFO flwr 2024-04-17 07:03:51,613 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 07:03:51,613 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:04:02,352 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 07:04:03,719 | server.py:125 | fit progress: (4, 1.5723994970321655, {'accuracy': 0.8884, 'data_size': 10000}, 59.79777094999736)
INFO flwr 2024-04-17 07:04:03,719 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 07:04:03,720 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:04:14,418 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 07:04:15,597 | server.py:125 | fit progress: (5, 1.5519050359725952, {'accuracy': 0.9089, 'data_size': 10000}, 71.6761057269905)
INFO flwr 2024-04-17 07:04:15,598 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 07:04:15,598 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:04:25,547 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 07:04:26,699 | server.py:125 | fit progress: (6, 1.5491915941238403, {'accuracy': 0.9114, 'data_size': 10000}, 82.77726705498935)
INFO flwr 2024-04-17 07:04:26,699 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 07:04:26,699 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:04:36,808 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 07:04:38,175 | server.py:125 | fit progress: (7, 1.5404720306396484, {'accuracy': 0.9206, 'data_size': 10000}, 94.25330419099191)
INFO flwr 2024-04-17 07:04:38,175 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 07:04:38,175 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:04:48,771 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 07:04:49,935 | server.py:125 | fit progress: (8, 1.5358905792236328, {'accuracy': 0.9251, 'data_size': 10000}, 106.01368179298879)
INFO flwr 2024-04-17 07:04:49,935 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 07:04:49,936 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:05:00,673 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 07:05:02,051 | server.py:125 | fit progress: (9, 1.5303291082382202, {'accuracy': 0.9309, 'data_size': 10000}, 118.1296412590018)
INFO flwr 2024-04-17 07:05:02,051 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 07:05:02,051 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:05:12,464 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 07:05:13,882 | server.py:125 | fit progress: (10, 1.5263298749923706, {'accuracy': 0.9348, 'data_size': 10000}, 129.96044585898926)
INFO flwr 2024-04-17 07:05:13,882 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 07:05:13,882 | server.py:153 | FL finished in 129.96089405300154
INFO flwr 2024-04-17 07:05:13,882 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 07:05:13,882 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 07:05:13,883 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 07:05:13,883 | app.py:229 | app_fit: losses_centralized [(0, 2.302626132965088), (1, 2.17391300201416), (2, 1.7978695631027222), (3, 1.6408597230911255), (4, 1.5723994970321655), (5, 1.5519050359725952), (6, 1.5491915941238403), (7, 1.5404720306396484), (8, 1.5358905792236328), (9, 1.5303291082382202), (10, 1.5263298749923706)]
INFO flwr 2024-04-17 07:05:13,883 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1091), (1, 0.5786), (2, 0.6865), (3, 0.8215), (4, 0.8884), (5, 0.9089), (6, 0.9114), (7, 0.9206), (8, 0.9251), (9, 0.9309), (10, 0.9348)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9348
wandb:     loss 1.52633
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_070241-2is1qgnv
wandb: Find logs at: ./wandb/offline-run-20240417_070241-2is1qgnv/logs
INFO flwr 2024-04-17 07:05:17,391 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 07:12:25,349 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=411867)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=411867)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 07:12:30,157	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 07:12:30,992	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 07:12:31,477	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 07:12:31,826	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7b14854b85dc42eb.zip' (36.99MiB) to Ray cluster...
2024-04-17 07:12:31,907	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7b14854b85dc42eb.zip'.
INFO flwr 2024-04-17 07:12:42,952 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'memory': 169738807092.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 77030917324.0}
INFO flwr 2024-04-17 07:12:42,952 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 07:12:42,953 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 07:12:42,971 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 07:12:42,974 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 07:12:42,974 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 07:12:42,974 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 07:12:46,122 | server.py:94 | initial parameters (loss, other metrics): 2.302656888961792, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-17 07:12:46,123 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 07:12:46,123 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=416592)[0m 2024-04-17 07:12:48.631511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=416592)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=416592)[0m 2024-04-17 07:12:51.671695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=416597)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=416597)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=416595)[0m 2024-04-17 07:12:49.899263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=416595)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=416591)[0m 2024-04-17 07:12:52.062074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 07:13:10,597 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 07:13:11,766 | server.py:125 | fit progress: (1, 2.3016929626464844, {'accuracy': 0.1134, 'data_size': 10000}, 25.64323884800251)
INFO flwr 2024-04-17 07:13:11,766 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 07:13:11,767 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:13:22,400 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 07:13:23,771 | server.py:125 | fit progress: (2, 2.29887056350708, {'accuracy': 0.2834, 'data_size': 10000}, 37.647732907993486)
INFO flwr 2024-04-17 07:13:23,771 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 07:13:23,771 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:13:34,854 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 07:13:36,002 | server.py:125 | fit progress: (3, 2.2921273708343506, {'accuracy': 0.5967, 'data_size': 10000}, 49.87922213699494)
INFO flwr 2024-04-17 07:13:36,002 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 07:13:36,003 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:13:46,951 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 07:13:48,349 | server.py:125 | fit progress: (4, 2.2784481048583984, {'accuracy': 0.7014, 'data_size': 10000}, 62.22641303599812)
INFO flwr 2024-04-17 07:13:48,349 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 07:13:48,350 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:13:58,619 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 07:13:59,786 | server.py:125 | fit progress: (5, 2.2521464824676514, {'accuracy': 0.7235, 'data_size': 10000}, 73.66321168100694)
INFO flwr 2024-04-17 07:13:59,786 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 07:13:59,786 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:14:10,063 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 07:14:11,443 | server.py:125 | fit progress: (6, 2.204172372817993, {'accuracy': 0.7407, 'data_size': 10000}, 85.31984533900686)
INFO flwr 2024-04-17 07:14:11,443 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 07:14:11,443 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:14:21,751 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 07:14:23,119 | server.py:125 | fit progress: (7, 2.128058910369873, {'accuracy': 0.7626, 'data_size': 10000}, 96.9961983539979)
INFO flwr 2024-04-17 07:14:23,119 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 07:14:23,119 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:14:33,748 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 07:14:35,131 | server.py:125 | fit progress: (8, 2.0305089950561523, {'accuracy': 0.7795, 'data_size': 10000}, 109.00837762400624)
INFO flwr 2024-04-17 07:14:35,131 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 07:14:35,132 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:14:45,525 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 07:14:46,670 | server.py:125 | fit progress: (9, 1.927462100982666, {'accuracy': 0.7895, 'data_size': 10000}, 120.54685787200287)
INFO flwr 2024-04-17 07:14:46,670 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 07:14:46,670 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:14:58,080 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 07:14:59,504 | server.py:125 | fit progress: (10, 1.8361122608184814, {'accuracy': 0.7958, 'data_size': 10000}, 133.3816458479996)
INFO flwr 2024-04-17 07:14:59,505 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 07:14:59,505 | server.py:153 | FL finished in 133.38209511499736
INFO flwr 2024-04-17 07:14:59,505 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 07:14:59,505 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 07:14:59,505 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 07:14:59,505 | app.py:229 | app_fit: losses_centralized [(0, 2.302656888961792), (1, 2.3016929626464844), (2, 2.29887056350708), (3, 2.2921273708343506), (4, 2.2784481048583984), (5, 2.2521464824676514), (6, 2.204172372817993), (7, 2.128058910369873), (8, 2.0305089950561523), (9, 1.927462100982666), (10, 1.8361122608184814)]
INFO flwr 2024-04-17 07:14:59,506 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.1134), (2, 0.2834), (3, 0.5967), (4, 0.7014), (5, 0.7235), (6, 0.7407), (7, 0.7626), (8, 0.7795), (9, 0.7895), (10, 0.7958)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7958
wandb:     loss 1.83611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_071224-heremfkq
wandb: Find logs at: ./wandb/offline-run-20240417_071224-heremfkq/logs
INFO flwr 2024-04-17 07:15:02,785 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 07:22:10,708 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=416591)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=416591)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 07:22:16,435	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 07:22:17,321	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 07:22:17,858	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 07:22:18,213	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_326f18ccb9a80605.zip' (37.00MiB) to Ray cluster...
2024-04-17 07:22:18,299	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_326f18ccb9a80605.zip'.
INFO flwr 2024-04-17 07:22:29,326 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77030266060.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169737287476.0}
INFO flwr 2024-04-17 07:22:29,326 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 07:22:29,327 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 07:22:29,345 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 07:22:29,346 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 07:22:29,347 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 07:22:29,347 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 07:22:32,898 | server.py:94 | initial parameters (loss, other metrics): 2.3026978969573975, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-17 07:22:32,899 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 07:22:32,899 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=420988)[0m 2024-04-17 07:22:35.445057: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=420988)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=420988)[0m 2024-04-17 07:22:37.804064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=420981)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=420981)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=420989)[0m 2024-04-17 07:22:35.680038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=420989)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=420989)[0m 2024-04-17 07:22:37.955089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 07:22:55,926 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 07:22:57,112 | server.py:125 | fit progress: (1, 2.3026363849639893, {'accuracy': 0.1135, 'data_size': 10000}, 24.213284747005673)
INFO flwr 2024-04-17 07:22:57,112 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 07:22:57,113 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:23:09,992 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 07:23:11,158 | server.py:125 | fit progress: (2, 2.3025476932525635, {'accuracy': 0.1135, 'data_size': 10000}, 38.25950189300056)
INFO flwr 2024-04-17 07:23:11,159 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 07:23:11,159 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:23:22,115 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 07:23:23,523 | server.py:125 | fit progress: (3, 2.3024325370788574, {'accuracy': 0.1135, 'data_size': 10000}, 50.62441744199896)
INFO flwr 2024-04-17 07:23:23,523 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 07:23:23,524 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:23:33,644 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 07:23:34,827 | server.py:125 | fit progress: (4, 2.30228853225708, {'accuracy': 0.1135, 'data_size': 10000}, 61.92803133399866)
INFO flwr 2024-04-17 07:23:34,827 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 07:23:34,827 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:23:44,817 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 07:23:46,186 | server.py:125 | fit progress: (5, 2.302110433578491, {'accuracy': 0.1135, 'data_size': 10000}, 73.28680984200037)
INFO flwr 2024-04-17 07:23:46,186 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 07:23:46,186 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:23:56,306 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 07:23:57,681 | server.py:125 | fit progress: (6, 2.301896572113037, {'accuracy': 0.1136, 'data_size': 10000}, 84.78272840099817)
INFO flwr 2024-04-17 07:23:57,682 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 07:23:57,682 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:24:07,799 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 07:24:09,190 | server.py:125 | fit progress: (7, 2.301640510559082, {'accuracy': 0.115, 'data_size': 10000}, 96.29117265000241)
INFO flwr 2024-04-17 07:24:09,190 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 07:24:09,190 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:24:19,128 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 07:24:20,483 | server.py:125 | fit progress: (8, 2.301339864730835, {'accuracy': 0.1178, 'data_size': 10000}, 107.58441124399542)
INFO flwr 2024-04-17 07:24:20,483 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 07:24:20,484 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:24:30,831 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 07:24:32,197 | server.py:125 | fit progress: (9, 2.3009893894195557, {'accuracy': 0.1289, 'data_size': 10000}, 119.29823470200063)
INFO flwr 2024-04-17 07:24:32,197 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 07:24:32,197 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:24:42,487 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 07:24:43,878 | server.py:125 | fit progress: (10, 2.3005831241607666, {'accuracy': 0.1544, 'data_size': 10000}, 130.97951416300202)
INFO flwr 2024-04-17 07:24:43,879 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 07:24:43,879 | server.py:153 | FL finished in 130.97995181199803
INFO flwr 2024-04-17 07:24:43,879 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 07:24:43,879 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 07:24:43,879 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 07:24:43,879 | app.py:229 | app_fit: losses_centralized [(0, 2.3026978969573975), (1, 2.3026363849639893), (2, 2.3025476932525635), (3, 2.3024325370788574), (4, 2.30228853225708), (5, 2.302110433578491), (6, 2.301896572113037), (7, 2.301640510559082), (8, 2.301339864730835), (9, 2.3009893894195557), (10, 2.3005831241607666)]
INFO flwr 2024-04-17 07:24:43,879 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1135), (2, 0.1135), (3, 0.1135), (4, 0.1135), (5, 0.1135), (6, 0.1136), (7, 0.115), (8, 0.1178), (9, 0.1289), (10, 0.1544)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1544
wandb:     loss 2.30058
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_072210-osra9trg
wandb: Find logs at: ./wandb/offline-run-20240417_072210-osra9trg/logs
INFO flwr 2024-04-17 07:24:47,438 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 07:31:55,735 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=420976)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=420976)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 07:32:01,629	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 07:32:02,457	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 07:32:02,923	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 07:32:03,261	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6ff3d77b68c7bf6e.zip' (37.02MiB) to Ray cluster...
2024-04-17 07:32:03,348	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6ff3d77b68c7bf6e.zip'.
INFO flwr 2024-04-17 07:32:14,442 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169736923341.0, 'object_store_memory': 77030110003.0}
INFO flwr 2024-04-17 07:32:14,442 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 07:32:14,442 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 07:32:14,461 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 07:32:14,462 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 07:32:14,462 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 07:32:14,462 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 07:32:18,065 | server.py:94 | initial parameters (loss, other metrics): 2.302424430847168, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-17 07:32:18,066 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 07:32:18,066 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=425339)[0m 2024-04-17 07:32:20.520475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=425339)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=425339)[0m 2024-04-17 07:32:22.868126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=425342)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=425342)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=425347)[0m 2024-04-17 07:32:20.767655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=425347)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=425345)[0m 2024-04-17 07:32:23.206309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 07:32:40,317 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 07:32:41,495 | server.py:125 | fit progress: (1, 2.302415609359741, {'accuracy': 0.1037, 'data_size': 10000}, 23.429261012992356)
INFO flwr 2024-04-17 07:32:41,495 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 07:32:41,496 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:32:52,889 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 07:32:54,259 | server.py:125 | fit progress: (2, 2.3024044036865234, {'accuracy': 0.1051, 'data_size': 10000}, 36.19347315598861)
INFO flwr 2024-04-17 07:32:54,260 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 07:32:54,260 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:33:03,965 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 07:33:05,116 | server.py:125 | fit progress: (3, 2.3023910522460938, {'accuracy': 0.1058, 'data_size': 10000}, 47.05027294099273)
INFO flwr 2024-04-17 07:33:05,116 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 07:33:05,117 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:33:14,922 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 07:33:16,090 | server.py:125 | fit progress: (4, 2.3023762702941895, {'accuracy': 0.1071, 'data_size': 10000}, 58.02415322700108)
INFO flwr 2024-04-17 07:33:16,090 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 07:33:16,091 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:33:26,080 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 07:33:27,466 | server.py:125 | fit progress: (5, 2.3023605346679688, {'accuracy': 0.1086, 'data_size': 10000}, 69.4002276659885)
INFO flwr 2024-04-17 07:33:27,466 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 07:33:27,467 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:33:38,240 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 07:33:39,651 | server.py:125 | fit progress: (6, 2.3023433685302734, {'accuracy': 0.11, 'data_size': 10000}, 81.58469211999909)
INFO flwr 2024-04-17 07:33:39,651 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 07:33:39,651 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:33:49,724 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 07:33:50,901 | server.py:125 | fit progress: (7, 2.3023266792297363, {'accuracy': 0.112, 'data_size': 10000}, 92.83556655999564)
INFO flwr 2024-04-17 07:33:50,902 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 07:33:50,902 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:34:01,497 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 07:34:02,644 | server.py:125 | fit progress: (8, 2.3023083209991455, {'accuracy': 0.1141, 'data_size': 10000}, 104.57831548398826)
INFO flwr 2024-04-17 07:34:02,644 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 07:34:02,645 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:34:13,331 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 07:34:14,697 | server.py:125 | fit progress: (9, 2.3022894859313965, {'accuracy': 0.1161, 'data_size': 10000}, 116.63123553599871)
INFO flwr 2024-04-17 07:34:14,697 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 07:34:14,698 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:34:25,276 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 07:34:26,427 | server.py:125 | fit progress: (10, 2.30226993560791, {'accuracy': 0.1172, 'data_size': 10000}, 128.36096486099996)
INFO flwr 2024-04-17 07:34:26,427 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 07:34:26,427 | server.py:153 | FL finished in 128.3614085799927
INFO flwr 2024-04-17 07:34:26,427 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 07:34:26,428 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 07:34:26,428 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 07:34:26,428 | app.py:229 | app_fit: losses_centralized [(0, 2.302424430847168), (1, 2.302415609359741), (2, 2.3024044036865234), (3, 2.3023910522460938), (4, 2.3023762702941895), (5, 2.3023605346679688), (6, 2.3023433685302734), (7, 2.3023266792297363), (8, 2.3023083209991455), (9, 2.3022894859313965), (10, 2.30226993560791)]
INFO flwr 2024-04-17 07:34:26,428 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.1037), (2, 0.1051), (3, 0.1058), (4, 0.1071), (5, 0.1086), (6, 0.11), (7, 0.112), (8, 0.1141), (9, 0.1161), (10, 0.1172)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1172
wandb:     loss 2.30227
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_073155-racuuxmm
wandb: Find logs at: ./wandb/offline-run-20240417_073155-racuuxmm/logs
INFO flwr 2024-04-17 07:34:29,998 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 07:41:38,071 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=425338)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=425338)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 07:41:42,823	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 07:41:43,702	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 07:41:44,160	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 07:41:44,537	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c3b4761a305e785e.zip' (37.03MiB) to Ray cluster...
2024-04-17 07:41:44,655	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c3b4761a305e785e.zip'.
INFO flwr 2024-04-17 07:41:55,735 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 77016757862.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169705768346.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 07:41:55,736 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 07:41:55,736 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 07:41:55,755 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 07:41:55,755 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 07:41:55,756 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 07:41:55,756 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 07:41:59,641 | server.py:94 | initial parameters (loss, other metrics): 2.3027634620666504, {'accuracy': 0.0579, 'data_size': 10000}
INFO flwr 2024-04-17 07:41:59,656 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 07:41:59,656 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=430091)[0m 2024-04-17 07:42:01.876352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=430091)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=430091)[0m 2024-04-17 07:42:04.188157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=430091)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=430091)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=430085)[0m 2024-04-17 07:42:02.106593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=430085)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=430088)[0m 2024-04-17 07:42:04.410486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 07:42:21,352 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 07:42:22,788 | server.py:125 | fit progress: (1, 2.0542283058166504, {'accuracy': 0.4068, 'data_size': 10000}, 23.131891473007272)
INFO flwr 2024-04-17 07:42:22,788 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 07:42:22,789 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:42:33,429 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 07:42:34,611 | server.py:125 | fit progress: (2, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 34.95500426599756)
INFO flwr 2024-04-17 07:42:34,612 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 07:42:34,612 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:42:45,189 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 07:42:46,620 | server.py:125 | fit progress: (3, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 46.96367465500953)
INFO flwr 2024-04-17 07:42:46,620 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 07:42:46,620 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:42:57,096 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 07:42:58,473 | server.py:125 | fit progress: (4, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 58.816542311004014)
INFO flwr 2024-04-17 07:42:58,473 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 07:42:58,473 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:43:08,688 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 07:43:10,089 | server.py:125 | fit progress: (5, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 70.43286617301055)
INFO flwr 2024-04-17 07:43:10,089 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 07:43:10,090 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:43:20,619 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 07:43:21,796 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 82.13963806300308)
INFO flwr 2024-04-17 07:43:21,796 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 07:43:21,796 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:43:32,076 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 07:43:33,251 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 93.59436310600722)
INFO flwr 2024-04-17 07:43:33,251 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 07:43:33,251 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:43:43,664 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 07:43:45,042 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 105.38557420500729)
INFO flwr 2024-04-17 07:43:45,042 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 07:43:45,042 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:43:54,942 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 07:43:56,339 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 116.68310691700026)
INFO flwr 2024-04-17 07:43:56,340 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 07:43:56,340 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:44:06,208 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 07:44:07,399 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 127.74291600400466)
INFO flwr 2024-04-17 07:44:07,399 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 07:44:07,400 | server.py:153 | FL finished in 127.74340517399833
INFO flwr 2024-04-17 07:44:07,400 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 07:44:07,400 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 07:44:07,400 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 07:44:07,400 | app.py:229 | app_fit: losses_centralized [(0, 2.3027634620666504), (1, 2.0542283058166504), (2, 2.358342170715332), (3, 2.358342170715332), (4, 2.358342170715332), (5, 2.358342170715332), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-17 07:44:07,400 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0579), (1, 0.4068), (2, 0.1028), (3, 0.1028), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_074137-3km5zhp8
wandb: Find logs at: ./wandb/offline-run-20240417_074137-3km5zhp8/logs
INFO flwr 2024-04-17 07:44:10,913 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 07:51:19,120 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=430082)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=430082)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 07:51:23,793	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 07:51:24,691	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 07:51:25,148	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 07:51:25,505	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7174e5c96f65be55.zip' (37.04MiB) to Ray cluster...
2024-04-17 07:51:25,619	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7174e5c96f65be55.zip'.
INFO flwr 2024-04-17 07:51:36,679 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77049803980.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169782875956.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 07:51:36,679 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 07:51:36,679 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 07:51:36,695 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 07:51:36,699 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 07:51:36,699 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 07:51:36,700 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 07:51:40,961 | server.py:94 | initial parameters (loss, other metrics): 2.302609443664551, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-17 07:51:40,962 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 07:51:40,962 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=434451)[0m 2024-04-17 07:51:42.692867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=434451)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=434453)[0m 2024-04-17 07:51:45.008102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=434453)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=434453)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=434446)[0m 2024-04-17 07:51:42.883856: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=434446)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=434446)[0m 2024-04-17 07:51:45.375713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 07:52:02,942 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 07:52:04,140 | server.py:125 | fit progress: (1, 2.1976335048675537, {'accuracy': 0.6472, 'data_size': 10000}, 23.17789394200372)
INFO flwr 2024-04-17 07:52:04,140 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 07:52:04,140 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:52:15,097 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 07:52:16,466 | server.py:125 | fit progress: (2, 1.7093359231948853, {'accuracy': 0.7831, 'data_size': 10000}, 35.50381689099595)
INFO flwr 2024-04-17 07:52:16,466 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 07:52:16,466 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:52:27,020 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 07:52:28,180 | server.py:125 | fit progress: (3, 1.5839343070983887, {'accuracy': 0.8794, 'data_size': 10000}, 47.217892211003345)
INFO flwr 2024-04-17 07:52:28,180 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 07:52:28,180 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:52:38,961 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 07:52:40,348 | server.py:125 | fit progress: (4, 1.5602846145629883, {'accuracy': 0.8995, 'data_size': 10000}, 59.38617811500444)
INFO flwr 2024-04-17 07:52:40,348 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 07:52:40,349 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:52:50,776 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 07:52:51,958 | server.py:125 | fit progress: (5, 1.5488975048065186, {'accuracy': 0.9128, 'data_size': 10000}, 70.99631455000781)
INFO flwr 2024-04-17 07:52:51,959 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 07:52:51,959 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:53:02,522 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 07:53:03,941 | server.py:125 | fit progress: (6, 1.5413540601730347, {'accuracy': 0.9198, 'data_size': 10000}, 82.97928842699912)
INFO flwr 2024-04-17 07:53:03,942 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 07:53:03,942 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:53:14,027 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 07:53:15,406 | server.py:125 | fit progress: (7, 1.534820795059204, {'accuracy': 0.9254, 'data_size': 10000}, 94.44450999899709)
INFO flwr 2024-04-17 07:53:15,407 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 07:53:15,407 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:53:25,840 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 07:53:27,231 | server.py:125 | fit progress: (8, 1.5318894386291504, {'accuracy': 0.9289, 'data_size': 10000}, 106.26878693500475)
INFO flwr 2024-04-17 07:53:27,231 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 07:53:27,231 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:53:37,743 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 07:53:38,916 | server.py:125 | fit progress: (9, 1.5250885486602783, {'accuracy': 0.936, 'data_size': 10000}, 117.95422330600559)
INFO flwr 2024-04-17 07:53:38,916 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 07:53:38,917 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 07:53:49,120 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 07:53:50,490 | server.py:125 | fit progress: (10, 1.5202529430389404, {'accuracy': 0.9409, 'data_size': 10000}, 129.52851114599616)
INFO flwr 2024-04-17 07:53:50,491 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 07:53:50,491 | server.py:153 | FL finished in 129.52894892400946
INFO flwr 2024-04-17 07:53:50,491 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 07:53:50,491 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 07:53:50,491 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 07:53:50,491 | app.py:229 | app_fit: losses_centralized [(0, 2.302609443664551), (1, 2.1976335048675537), (2, 1.7093359231948853), (3, 1.5839343070983887), (4, 1.5602846145629883), (5, 1.5488975048065186), (6, 1.5413540601730347), (7, 1.534820795059204), (8, 1.5318894386291504), (9, 1.5250885486602783), (10, 1.5202529430389404)]
INFO flwr 2024-04-17 07:53:50,492 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.6472), (2, 0.7831), (3, 0.8794), (4, 0.8995), (5, 0.9128), (6, 0.9198), (7, 0.9254), (8, 0.9289), (9, 0.936), (10, 0.9409)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9409
wandb:     loss 1.52025
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_075118-kj0560uo
wandb: Find logs at: ./wandb/offline-run-20240417_075118-kj0560uo/logs
INFO flwr 2024-04-17 07:53:54,050 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 08:01:01,881 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=434444)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=434444)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 08:01:06,684	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 08:01:07,487	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 08:01:07,925	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 08:01:08,280	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_20377c9283ed4ca8.zip' (37.05MiB) to Ray cluster...
2024-04-17 08:01:08,394	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_20377c9283ed4ca8.zip'.
INFO flwr 2024-04-17 08:01:19,467 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 169771028685.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77044726579.0, 'CPU': 64.0}
INFO flwr 2024-04-17 08:01:19,467 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 08:01:19,467 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 08:01:19,486 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 08:01:19,487 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 08:01:19,487 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 08:01:19,488 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 08:01:23,639 | server.py:94 | initial parameters (loss, other metrics): 2.302713632583618, {'accuracy': 0.0954, 'data_size': 10000}
INFO flwr 2024-04-17 08:01:23,645 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 08:01:23,647 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=438834)[0m 2024-04-17 08:01:25.557239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=438834)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=438834)[0m 2024-04-17 08:01:27.845411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=438833)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=438833)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=438831)[0m 2024-04-17 08:01:25.781434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=438831)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=438831)[0m 2024-04-17 08:01:28.029581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 08:01:46,005 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 08:01:47,241 | server.py:125 | fit progress: (1, 2.301549196243286, {'accuracy': 0.165, 'data_size': 10000}, 23.594237659999635)
INFO flwr 2024-04-17 08:01:47,241 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 08:01:47,241 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:01:58,084 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 08:01:59,252 | server.py:125 | fit progress: (2, 2.2983715534210205, {'accuracy': 0.3101, 'data_size': 10000}, 35.60515013600525)
INFO flwr 2024-04-17 08:01:59,252 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 08:01:59,252 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:02:09,803 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 08:02:11,181 | server.py:125 | fit progress: (3, 2.2911007404327393, {'accuracy': 0.4939, 'data_size': 10000}, 47.533894318999955)
INFO flwr 2024-04-17 08:02:11,181 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 08:02:11,181 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:02:21,957 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 08:02:23,345 | server.py:125 | fit progress: (4, 2.2762956619262695, {'accuracy': 0.6156, 'data_size': 10000}, 59.69858871100587)
INFO flwr 2024-04-17 08:02:23,345 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 08:02:23,346 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:02:33,769 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 08:02:34,955 | server.py:125 | fit progress: (5, 2.248220443725586, {'accuracy': 0.6542, 'data_size': 10000}, 71.30869632100803)
INFO flwr 2024-04-17 08:02:34,956 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 08:02:34,956 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:02:45,436 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 08:02:46,620 | server.py:125 | fit progress: (6, 2.1983280181884766, {'accuracy': 0.6731, 'data_size': 10000}, 82.97312736700405)
INFO flwr 2024-04-17 08:02:46,620 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 08:02:46,620 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:02:56,321 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 08:02:57,723 | server.py:125 | fit progress: (7, 2.120990753173828, {'accuracy': 0.6852, 'data_size': 10000}, 94.07686898400425)
INFO flwr 2024-04-17 08:02:57,724 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 08:02:57,724 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:03:08,632 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 08:03:09,808 | server.py:125 | fit progress: (8, 2.024444818496704, {'accuracy': 0.7034, 'data_size': 10000}, 106.16157185600605)
INFO flwr 2024-04-17 08:03:09,808 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 08:03:09,809 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:03:20,608 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 08:03:21,999 | server.py:125 | fit progress: (9, 1.9280062913894653, {'accuracy': 0.7216, 'data_size': 10000}, 118.35235121200094)
INFO flwr 2024-04-17 08:03:21,999 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 08:03:21,999 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:03:31,939 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 08:03:33,358 | server.py:125 | fit progress: (10, 1.8470474481582642, {'accuracy': 0.7391, 'data_size': 10000}, 129.71121434100496)
INFO flwr 2024-04-17 08:03:33,358 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 08:03:33,358 | server.py:153 | FL finished in 129.71168227000453
INFO flwr 2024-04-17 08:03:33,358 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 08:03:33,359 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 08:03:33,359 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 08:03:33,359 | app.py:229 | app_fit: losses_centralized [(0, 2.302713632583618), (1, 2.301549196243286), (2, 2.2983715534210205), (3, 2.2911007404327393), (4, 2.2762956619262695), (5, 2.248220443725586), (6, 2.1983280181884766), (7, 2.120990753173828), (8, 2.024444818496704), (9, 1.9280062913894653), (10, 1.8470474481582642)]
INFO flwr 2024-04-17 08:03:33,359 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0954), (1, 0.165), (2, 0.3101), (3, 0.4939), (4, 0.6156), (5, 0.6542), (6, 0.6731), (7, 0.6852), (8, 0.7034), (9, 0.7216), (10, 0.7391)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7391
wandb:     loss 1.84705
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_080101-rxvksked
wandb: Find logs at: ./wandb/offline-run-20240417_080101-rxvksked/logs
INFO flwr 2024-04-17 08:03:36,878 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 08:10:44,518 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=438823)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=438823)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 08:10:49,291	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 08:10:50,056	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 08:10:50,513	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 08:10:50,865	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1204ebb12d2709b5.zip' (37.06MiB) to Ray cluster...
2024-04-17 08:10:50,981	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1204ebb12d2709b5.zip'.
INFO flwr 2024-04-17 08:11:02,013 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77044624588.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169770790708.0}
INFO flwr 2024-04-17 08:11:02,013 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 08:11:02,014 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 08:11:02,030 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 08:11:02,033 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 08:11:02,033 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 08:11:02,033 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 08:11:05,594 | server.py:94 | initial parameters (loss, other metrics): 2.3025598526000977, {'accuracy': 0.1317, 'data_size': 10000}
INFO flwr 2024-04-17 08:11:05,595 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 08:11:05,600 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=443189)[0m 2024-04-17 08:11:08.108325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=443189)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=443189)[0m 2024-04-17 08:11:10.417779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=443185)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=443185)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=443181)[0m 2024-04-17 08:11:08.434145: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=443181)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=443181)[0m 2024-04-17 08:11:10.633017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 08:11:27,706 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 08:11:28,915 | server.py:125 | fit progress: (1, 2.3024778366088867, {'accuracy': 0.1409, 'data_size': 10000}, 23.31503109500045)
INFO flwr 2024-04-17 08:11:28,915 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 08:11:28,915 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:11:39,434 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 08:11:40,827 | server.py:125 | fit progress: (2, 2.3023598194122314, {'accuracy': 0.1548, 'data_size': 10000}, 35.22689300500497)
INFO flwr 2024-04-17 08:11:40,827 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 08:11:40,827 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:11:51,663 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 08:11:52,851 | server.py:125 | fit progress: (3, 2.3022098541259766, {'accuracy': 0.1728, 'data_size': 10000}, 47.25080837300629)
INFO flwr 2024-04-17 08:11:52,851 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 08:11:52,851 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:12:03,269 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 08:12:04,700 | server.py:125 | fit progress: (4, 2.302022933959961, {'accuracy': 0.1865, 'data_size': 10000}, 59.10040853499959)
INFO flwr 2024-04-17 08:12:04,701 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 08:12:04,701 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:12:15,237 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 08:12:16,421 | server.py:125 | fit progress: (5, 2.3017971515655518, {'accuracy': 0.2012, 'data_size': 10000}, 70.82124223200663)
INFO flwr 2024-04-17 08:12:16,421 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 08:12:16,422 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:12:27,337 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 08:12:28,739 | server.py:125 | fit progress: (6, 2.3015284538269043, {'accuracy': 0.2142, 'data_size': 10000}, 83.13941402299679)
INFO flwr 2024-04-17 08:12:28,740 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 08:12:28,740 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:12:39,048 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 08:12:40,426 | server.py:125 | fit progress: (7, 2.3012123107910156, {'accuracy': 0.2262, 'data_size': 10000}, 94.82570727300481)
INFO flwr 2024-04-17 08:12:40,426 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 08:12:40,426 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:12:50,450 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 08:12:51,821 | server.py:125 | fit progress: (8, 2.3008432388305664, {'accuracy': 0.2347, 'data_size': 10000}, 106.22102481201)
INFO flwr 2024-04-17 08:12:51,821 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 08:12:51,821 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:13:01,852 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 08:13:03,017 | server.py:125 | fit progress: (9, 2.3004159927368164, {'accuracy': 0.2449, 'data_size': 10000}, 117.41695065199747)
INFO flwr 2024-04-17 08:13:03,017 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 08:13:03,017 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:13:13,222 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 08:13:14,416 | server.py:125 | fit progress: (10, 2.2999267578125, {'accuracy': 0.2506, 'data_size': 10000}, 128.8156922019989)
INFO flwr 2024-04-17 08:13:14,416 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 08:13:14,416 | server.py:153 | FL finished in 128.81614403100684
INFO flwr 2024-04-17 08:13:14,416 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 08:13:14,416 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 08:13:14,416 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 08:13:14,417 | app.py:229 | app_fit: losses_centralized [(0, 2.3025598526000977), (1, 2.3024778366088867), (2, 2.3023598194122314), (3, 2.3022098541259766), (4, 2.302022933959961), (5, 2.3017971515655518), (6, 2.3015284538269043), (7, 2.3012123107910156), (8, 2.3008432388305664), (9, 2.3004159927368164), (10, 2.2999267578125)]
INFO flwr 2024-04-17 08:13:14,417 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1317), (1, 0.1409), (2, 0.1548), (3, 0.1728), (4, 0.1865), (5, 0.2012), (6, 0.2142), (7, 0.2262), (8, 0.2347), (9, 0.2449), (10, 0.2506)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2506
wandb:     loss 2.29993
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_081044-8a07y931
wandb: Find logs at: ./wandb/offline-run-20240417_081044-8a07y931/logs
INFO flwr 2024-04-17 08:13:17,919 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 08:20:25,876 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=443179)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=443179)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 08:20:30,266	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 08:20:31,072	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 08:20:31,523	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 08:20:31,889	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dbd5c010e907cdff.zip' (37.07MiB) to Ray cluster...
2024-04-17 08:20:32,006	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dbd5c010e907cdff.zip'.
INFO flwr 2024-04-17 08:20:43,027 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169889725031.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'object_store_memory': 77095596441.0}
INFO flwr 2024-04-17 08:20:43,027 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 08:20:43,027 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 08:20:43,042 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 08:20:43,043 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 08:20:43,043 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 08:20:43,043 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 08:20:46,761 | server.py:94 | initial parameters (loss, other metrics): 2.3027164936065674, {'accuracy': 0.1057, 'data_size': 10000}
INFO flwr 2024-04-17 08:20:46,762 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 08:20:46,763 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=447933)[0m 2024-04-17 08:20:49.064657: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=447933)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=447933)[0m 2024-04-17 08:20:51.361141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=447944)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=447944)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=447932)[0m 2024-04-17 08:20:49.272007: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=447932)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=447932)[0m 2024-04-17 08:20:51.789800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 08:21:09,124 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 08:21:10,547 | server.py:125 | fit progress: (1, 2.3027095794677734, {'accuracy': 0.1055, 'data_size': 10000}, 23.784339197998634)
INFO flwr 2024-04-17 08:21:10,547 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 08:21:10,547 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:21:21,256 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 08:21:22,437 | server.py:125 | fit progress: (2, 2.302699327468872, {'accuracy': 0.1056, 'data_size': 10000}, 35.674182799994014)
INFO flwr 2024-04-17 08:21:22,437 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 08:21:22,437 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:21:32,530 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 08:21:33,700 | server.py:125 | fit progress: (3, 2.3026881217956543, {'accuracy': 0.1055, 'data_size': 10000}, 46.93782446499972)
INFO flwr 2024-04-17 08:21:33,701 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 08:21:33,701 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:21:44,012 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 08:21:45,386 | server.py:125 | fit progress: (4, 2.302675485610962, {'accuracy': 0.1057, 'data_size': 10000}, 58.62336410599528)
INFO flwr 2024-04-17 08:21:45,386 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 08:21:45,386 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:21:56,144 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 08:21:57,529 | server.py:125 | fit progress: (5, 2.3026623725891113, {'accuracy': 0.106, 'data_size': 10000}, 70.76675327299745)
INFO flwr 2024-04-17 08:21:57,530 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 08:21:57,530 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:22:07,749 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 08:22:08,929 | server.py:125 | fit progress: (6, 2.3026480674743652, {'accuracy': 0.106, 'data_size': 10000}, 82.16655900600017)
INFO flwr 2024-04-17 08:22:08,929 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 08:22:08,930 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:22:19,171 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 08:22:20,346 | server.py:125 | fit progress: (7, 2.3026325702667236, {'accuracy': 0.1065, 'data_size': 10000}, 93.58389078099572)
INFO flwr 2024-04-17 08:22:20,347 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 08:22:20,347 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:22:30,757 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 08:22:32,142 | server.py:125 | fit progress: (8, 2.302617073059082, {'accuracy': 0.1066, 'data_size': 10000}, 105.37926244799746)
INFO flwr 2024-04-17 08:22:32,142 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 08:22:32,142 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:22:41,588 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 08:22:42,742 | server.py:125 | fit progress: (9, 2.3026015758514404, {'accuracy': 0.1067, 'data_size': 10000}, 115.97912009200081)
INFO flwr 2024-04-17 08:22:42,742 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 08:22:42,742 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:22:52,683 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 08:22:54,081 | server.py:125 | fit progress: (10, 2.302584648132324, {'accuracy': 0.1069, 'data_size': 10000}, 127.31805993699527)
INFO flwr 2024-04-17 08:22:54,081 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 08:22:54,081 | server.py:153 | FL finished in 127.31853777299693
INFO flwr 2024-04-17 08:22:54,081 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 08:22:54,081 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 08:22:54,081 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 08:22:54,082 | app.py:229 | app_fit: losses_centralized [(0, 2.3027164936065674), (1, 2.3027095794677734), (2, 2.302699327468872), (3, 2.3026881217956543), (4, 2.302675485610962), (5, 2.3026623725891113), (6, 2.3026480674743652), (7, 2.3026325702667236), (8, 2.302617073059082), (9, 2.3026015758514404), (10, 2.302584648132324)]
INFO flwr 2024-04-17 08:22:54,082 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1057), (1, 0.1055), (2, 0.1056), (3, 0.1055), (4, 0.1057), (5, 0.106), (6, 0.106), (7, 0.1065), (8, 0.1066), (9, 0.1067), (10, 0.1069)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1069
wandb:     loss 2.30258
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_082025-le45nyrs
wandb: Find logs at: ./wandb/offline-run-20240417_082025-le45nyrs/logs
INFO flwr 2024-04-17 08:22:57,586 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 08:30:05,895 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=447928)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=447928)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 08:30:11,702	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 08:30:12,536	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 08:30:12,980	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 08:30:13,341	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_871970b50a661dd9.zip' (37.09MiB) to Ray cluster...
2024-04-17 08:30:13,455	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_871970b50a661dd9.zip'.
INFO flwr 2024-04-17 08:30:24,491 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 77038345420.0, 'accelerator_type:TITAN': 1.0, 'memory': 169756139316.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 08:30:24,491 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 08:30:24,491 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 08:30:24,509 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 08:30:24,511 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 08:30:24,511 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 08:30:24,511 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 08:30:27,949 | server.py:94 | initial parameters (loss, other metrics): 2.302563190460205, {'accuracy': 0.0933, 'data_size': 10000}
INFO flwr 2024-04-17 08:30:27,950 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 08:30:27,950 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=452288)[0m 2024-04-17 08:30:30.747364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=452288)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=452293)[0m 2024-04-17 08:30:33.152529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=452293)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=452293)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=452293)[0m 2024-04-17 08:30:30.890940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=452293)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=452294)[0m 2024-04-17 08:30:33.220991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 08:30:56,370 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 08:30:57,778 | server.py:125 | fit progress: (1, 2.357769250869751, {'accuracy': 0.1034, 'data_size': 10000}, 29.827638709990424)
INFO flwr 2024-04-17 08:30:57,778 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 08:30:57,778 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:31:13,944 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 08:31:15,139 | server.py:125 | fit progress: (2, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 47.18913223299023)
INFO flwr 2024-04-17 08:31:15,140 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 08:31:15,140 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:31:30,541 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 08:31:31,932 | server.py:125 | fit progress: (3, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 63.98246972099878)
INFO flwr 2024-04-17 08:31:31,933 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 08:31:31,933 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:31:47,871 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 08:31:49,051 | server.py:125 | fit progress: (4, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 81.10068751599465)
INFO flwr 2024-04-17 08:31:49,051 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 08:31:49,051 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:32:04,339 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 08:32:05,777 | server.py:125 | fit progress: (5, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 97.82716425599938)
INFO flwr 2024-04-17 08:32:05,778 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 08:32:05,778 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:32:20,550 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 08:32:21,954 | server.py:125 | fit progress: (6, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 114.0041036879993)
INFO flwr 2024-04-17 08:32:21,954 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 08:32:21,955 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:32:36,649 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 08:32:38,066 | server.py:125 | fit progress: (7, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 130.11585901099897)
INFO flwr 2024-04-17 08:32:38,066 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 08:32:38,066 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:32:52,870 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 08:32:54,282 | server.py:125 | fit progress: (8, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 146.33162044000346)
INFO flwr 2024-04-17 08:32:54,282 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 08:32:54,282 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:33:09,878 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 08:33:11,267 | server.py:125 | fit progress: (9, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 163.31691894899996)
INFO flwr 2024-04-17 08:33:11,267 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 08:33:11,267 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:33:27,271 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 08:33:28,657 | server.py:125 | fit progress: (10, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 180.70686664999812)
INFO flwr 2024-04-17 08:33:28,657 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 08:33:28,657 | server.py:153 | FL finished in 180.70732006100297
INFO flwr 2024-04-17 08:33:28,658 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 08:33:28,658 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 08:33:28,658 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 08:33:28,658 | app.py:229 | app_fit: losses_centralized [(0, 2.302563190460205), (1, 2.357769250869751), (2, 2.3631420135498047), (3, 2.3631420135498047), (4, 2.3631420135498047), (5, 2.3631420135498047), (6, 2.3631420135498047), (7, 2.3631420135498047), (8, 2.3631420135498047), (9, 2.3631420135498047), (10, 2.3631420135498047)]
INFO flwr 2024-04-17 08:33:28,658 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0933), (1, 0.1034), (2, 0.098), (3, 0.098), (4, 0.098), (5, 0.098), (6, 0.098), (7, 0.098), (8, 0.098), (9, 0.098), (10, 0.098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.098
wandb:     loss 2.36314
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_083005-kzen26fd
wandb: Find logs at: ./wandb/offline-run-20240417_083005-kzen26fd/logs
INFO flwr 2024-04-17 08:33:32,163 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 08:40:39,480 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=452287)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=452287)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 08:40:45,363	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 08:40:46,137	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 08:40:46,603	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 08:40:46,961	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a25297a20add993b.zip' (37.10MiB) to Ray cluster...
2024-04-17 08:40:47,078	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a25297a20add993b.zip'.
INFO flwr 2024-04-17 08:40:58,100 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 169886883636.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77094378700.0}
INFO flwr 2024-04-17 08:40:58,101 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 08:40:58,101 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 08:40:58,120 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 08:40:58,121 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 08:40:58,121 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 08:40:58,121 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 08:41:01,188 | server.py:94 | initial parameters (loss, other metrics): 2.3027360439300537, {'accuracy': 0.117, 'data_size': 10000}
INFO flwr 2024-04-17 08:41:01,189 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 08:41:01,190 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=456681)[0m 2024-04-17 08:41:04.233752: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=456681)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=456681)[0m 2024-04-17 08:41:06.666156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=456685)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=456685)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=456693)[0m 2024-04-17 08:41:04.550793: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=456693)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=456693)[0m 2024-04-17 08:41:06.729215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 08:41:29,886 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 08:41:31,288 | server.py:125 | fit progress: (1, 2.213085651397705, {'accuracy': 0.4543, 'data_size': 10000}, 30.09826135500043)
INFO flwr 2024-04-17 08:41:31,288 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 08:41:31,288 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:41:47,365 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 08:41:48,555 | server.py:125 | fit progress: (2, 1.8753150701522827, {'accuracy': 0.5781, 'data_size': 10000}, 47.36559555999702)
INFO flwr 2024-04-17 08:41:48,555 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 08:41:48,556 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:42:03,798 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 08:42:05,211 | server.py:125 | fit progress: (3, 1.7125451564788818, {'accuracy': 0.7484, 'data_size': 10000}, 64.02178323500266)
INFO flwr 2024-04-17 08:42:05,212 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 08:42:05,212 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:42:20,275 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 08:42:21,473 | server.py:125 | fit progress: (4, 1.6054750680923462, {'accuracy': 0.8548, 'data_size': 10000}, 80.28302771500603)
INFO flwr 2024-04-17 08:42:21,473 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 08:42:21,473 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:42:37,838 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 08:42:39,240 | server.py:125 | fit progress: (5, 1.5623587369918823, {'accuracy': 0.8986, 'data_size': 10000}, 98.05038831400452)
INFO flwr 2024-04-17 08:42:39,240 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 08:42:39,241 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:42:52,928 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 08:42:54,125 | server.py:125 | fit progress: (6, 1.539434552192688, {'accuracy': 0.9214, 'data_size': 10000}, 112.93529564399796)
INFO flwr 2024-04-17 08:42:54,125 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 08:42:54,125 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:43:08,948 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 08:43:10,353 | server.py:125 | fit progress: (7, 1.5356425046920776, {'accuracy': 0.9252, 'data_size': 10000}, 129.16360011600773)
INFO flwr 2024-04-17 08:43:10,353 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 08:43:10,354 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:43:25,805 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 08:43:26,959 | server.py:125 | fit progress: (8, 1.5289902687072754, {'accuracy': 0.9316, 'data_size': 10000}, 145.76979997400485)
INFO flwr 2024-04-17 08:43:26,960 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 08:43:26,960 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:43:41,648 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 08:43:43,041 | server.py:125 | fit progress: (9, 1.5276057720184326, {'accuracy': 0.9335, 'data_size': 10000}, 161.85106696900039)
INFO flwr 2024-04-17 08:43:43,041 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 08:43:43,041 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:43:57,586 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 08:43:58,737 | server.py:125 | fit progress: (10, 1.5276685953140259, {'accuracy': 0.9331, 'data_size': 10000}, 177.54744851400028)
INFO flwr 2024-04-17 08:43:58,737 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 08:43:58,737 | server.py:153 | FL finished in 177.5479212640057
INFO flwr 2024-04-17 08:43:58,738 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 08:43:58,738 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 08:43:58,738 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 08:43:58,738 | app.py:229 | app_fit: losses_centralized [(0, 2.3027360439300537), (1, 2.213085651397705), (2, 1.8753150701522827), (3, 1.7125451564788818), (4, 1.6054750680923462), (5, 1.5623587369918823), (6, 1.539434552192688), (7, 1.5356425046920776), (8, 1.5289902687072754), (9, 1.5276057720184326), (10, 1.5276685953140259)]
INFO flwr 2024-04-17 08:43:58,738 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.117), (1, 0.4543), (2, 0.5781), (3, 0.7484), (4, 0.8548), (5, 0.8986), (6, 0.9214), (7, 0.9252), (8, 0.9316), (9, 0.9335), (10, 0.9331)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9331
wandb:     loss 1.52767
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_084039-j5y6asrt
wandb: Find logs at: ./wandb/offline-run-20240417_084039-j5y6asrt/logs
INFO flwr 2024-04-17 08:44:02,302 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 08:51:10,721 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=456680)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=456680)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 08:51:16,403	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 08:51:17,212	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 08:51:17,658	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 08:51:18,013	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_07d34bb1d549854b.zip' (37.11MiB) to Ray cluster...
2024-04-17 08:51:18,129	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_07d34bb1d549854b.zip'.
INFO flwr 2024-04-17 08:51:29,198 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77108933836.0, 'accelerator_type:TITAN': 1.0, 'memory': 169920845620.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 08:51:29,199 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 08:51:29,199 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 08:51:29,215 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 08:51:29,216 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 08:51:29,217 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 08:51:29,217 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 08:51:31,817 | server.py:94 | initial parameters (loss, other metrics): 2.3024535179138184, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-17 08:51:31,817 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 08:51:31,817 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=461419)[0m 2024-04-17 08:51:35.367022: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=461419)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=461414)[0m 2024-04-17 08:51:37.707275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=461419)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=461419)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=461415)[0m 2024-04-17 08:51:35.547216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=461415)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=461415)[0m 2024-04-17 08:51:37.825221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 08:52:03,098 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 08:52:04,518 | server.py:125 | fit progress: (1, 2.3014116287231445, {'accuracy': 0.1009, 'data_size': 10000}, 32.70113207200484)
INFO flwr 2024-04-17 08:52:04,519 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 08:52:04,519 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:52:19,871 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 08:52:21,070 | server.py:125 | fit progress: (2, 2.2985291481018066, {'accuracy': 0.1911, 'data_size': 10000}, 49.25240183300048)
INFO flwr 2024-04-17 08:52:21,070 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 08:52:21,070 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:52:36,077 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 08:52:37,286 | server.py:125 | fit progress: (3, 2.2918522357940674, {'accuracy': 0.4696, 'data_size': 10000}, 65.46880725900701)
INFO flwr 2024-04-17 08:52:37,286 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 08:52:37,287 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:52:53,257 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 08:52:54,667 | server.py:125 | fit progress: (4, 2.2784385681152344, {'accuracy': 0.683, 'data_size': 10000}, 82.8500259959983)
INFO flwr 2024-04-17 08:52:54,668 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 08:52:54,668 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:53:10,389 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 08:53:11,844 | server.py:125 | fit progress: (5, 2.2533061504364014, {'accuracy': 0.7721, 'data_size': 10000}, 100.02647060000163)
INFO flwr 2024-04-17 08:53:11,844 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 08:53:11,844 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:53:27,567 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 08:53:28,773 | server.py:125 | fit progress: (6, 2.2078869342803955, {'accuracy': 0.8037, 'data_size': 10000}, 116.9557287189964)
INFO flwr 2024-04-17 08:53:28,773 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 08:53:28,773 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:53:43,601 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 08:53:44,775 | server.py:125 | fit progress: (7, 2.1329293251037598, {'accuracy': 0.8101, 'data_size': 10000}, 132.9576867900032)
INFO flwr 2024-04-17 08:53:44,775 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 08:53:44,775 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:53:58,846 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 08:54:00,270 | server.py:125 | fit progress: (8, 2.028339147567749, {'accuracy': 0.8103, 'data_size': 10000}, 148.4523955820041)
INFO flwr 2024-04-17 08:54:00,270 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 08:54:00,270 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:54:15,735 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 08:54:16,928 | server.py:125 | fit progress: (9, 1.9135589599609375, {'accuracy': 0.8103, 'data_size': 10000}, 165.11062167299679)
INFO flwr 2024-04-17 08:54:16,928 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 08:54:16,928 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 08:54:31,811 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 08:54:33,220 | server.py:125 | fit progress: (10, 1.81461501121521, {'accuracy': 0.8078, 'data_size': 10000}, 181.40228963999834)
INFO flwr 2024-04-17 08:54:33,220 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 08:54:33,220 | server.py:153 | FL finished in 181.4027726399945
INFO flwr 2024-04-17 08:54:33,220 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 08:54:33,220 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 08:54:33,220 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 08:54:33,221 | app.py:229 | app_fit: losses_centralized [(0, 2.3024535179138184), (1, 2.3014116287231445), (2, 2.2985291481018066), (3, 2.2918522357940674), (4, 2.2784385681152344), (5, 2.2533061504364014), (6, 2.2078869342803955), (7, 2.1329293251037598), (8, 2.028339147567749), (9, 1.9135589599609375), (10, 1.81461501121521)]
INFO flwr 2024-04-17 08:54:33,221 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.1009), (2, 0.1911), (3, 0.4696), (4, 0.683), (5, 0.7721), (6, 0.8037), (7, 0.8101), (8, 0.8103), (9, 0.8103), (10, 0.8078)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8078
wandb:     loss 1.81462
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_085110-iv2722nh
wandb: Find logs at: ./wandb/offline-run-20240417_085110-iv2722nh/logs
INFO flwr 2024-04-17 08:54:36,743 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 09:01:44,430 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=461408)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=461408)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 09:01:49,683	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 09:01:50,468	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 09:01:50,925	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 09:01:51,283	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6fbed92ef6ef95b7.zip' (37.12MiB) to Ray cluster...
2024-04-17 09:01:51,396	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6fbed92ef6ef95b7.zip'.
INFO flwr 2024-04-17 09:02:02,473 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77096744140.0, 'accelerator_type:TITAN': 1.0, 'memory': 169892402996.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 09:02:02,473 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 09:02:02,473 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 09:02:02,491 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 09:02:02,492 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 09:02:02,492 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 09:02:02,493 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 09:02:05,068 | server.py:94 | initial parameters (loss, other metrics): 2.302640199661255, {'accuracy': 0.0552, 'data_size': 10000}
INFO flwr 2024-04-17 09:02:05,069 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 09:02:05,069 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=465806)[0m 2024-04-17 09:02:08.620423: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=465806)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=465807)[0m 2024-04-17 09:02:11.071127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=465810)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=465810)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=465804)[0m 2024-04-17 09:02:08.750820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=465804)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=465808)[0m 2024-04-17 09:02:11.101101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 09:02:36,526 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 09:02:37,950 | server.py:125 | fit progress: (1, 2.3025684356689453, {'accuracy': 0.0616, 'data_size': 10000}, 32.8810077119997)
INFO flwr 2024-04-17 09:02:37,950 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 09:02:37,951 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:02:54,298 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 09:02:55,496 | server.py:125 | fit progress: (2, 2.302462577819824, {'accuracy': 0.07, 'data_size': 10000}, 50.426522385998396)
INFO flwr 2024-04-17 09:02:55,496 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 09:02:55,496 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:03:10,412 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 09:03:11,786 | server.py:125 | fit progress: (3, 2.3023245334625244, {'accuracy': 0.0838, 'data_size': 10000}, 66.71717769899988)
INFO flwr 2024-04-17 09:03:11,786 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 09:03:11,787 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:03:27,022 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 09:03:28,178 | server.py:125 | fit progress: (4, 2.3021461963653564, {'accuracy': 0.0987, 'data_size': 10000}, 83.10857028899773)
INFO flwr 2024-04-17 09:03:28,178 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 09:03:28,178 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:03:43,234 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 09:03:44,648 | server.py:125 | fit progress: (5, 2.301931619644165, {'accuracy': 0.1144, 'data_size': 10000}, 99.57857952300401)
INFO flwr 2024-04-17 09:03:44,648 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 09:03:44,648 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:04:00,116 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 09:04:01,277 | server.py:125 | fit progress: (6, 2.3016693592071533, {'accuracy': 0.1346, 'data_size': 10000}, 116.20807135700306)
INFO flwr 2024-04-17 09:04:01,277 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 09:04:01,278 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:04:16,050 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 09:04:17,471 | server.py:125 | fit progress: (7, 2.301358699798584, {'accuracy': 0.1525, 'data_size': 10000}, 132.4022426980082)
INFO flwr 2024-04-17 09:04:17,472 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 09:04:17,472 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:04:32,678 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 09:04:33,841 | server.py:125 | fit progress: (8, 2.300995111465454, {'accuracy': 0.1645, 'data_size': 10000}, 148.77171239499876)
INFO flwr 2024-04-17 09:04:33,841 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 09:04:33,841 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:04:50,057 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 09:04:51,464 | server.py:125 | fit progress: (9, 2.300574541091919, {'accuracy': 0.1719, 'data_size': 10000}, 166.39472546499746)
INFO flwr 2024-04-17 09:04:51,464 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 09:04:51,465 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:05:06,151 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 09:05:07,331 | server.py:125 | fit progress: (10, 2.30008602142334, {'accuracy': 0.181, 'data_size': 10000}, 182.26197378500365)
INFO flwr 2024-04-17 09:05:07,331 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 09:05:07,331 | server.py:153 | FL finished in 182.26241736899829
INFO flwr 2024-04-17 09:05:07,332 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 09:05:07,332 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 09:05:07,332 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 09:05:07,332 | app.py:229 | app_fit: losses_centralized [(0, 2.302640199661255), (1, 2.3025684356689453), (2, 2.302462577819824), (3, 2.3023245334625244), (4, 2.3021461963653564), (5, 2.301931619644165), (6, 2.3016693592071533), (7, 2.301358699798584), (8, 2.300995111465454), (9, 2.300574541091919), (10, 2.30008602142334)]
INFO flwr 2024-04-17 09:05:07,332 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0552), (1, 0.0616), (2, 0.07), (3, 0.0838), (4, 0.0987), (5, 0.1144), (6, 0.1346), (7, 0.1525), (8, 0.1645), (9, 0.1719), (10, 0.181)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.181
wandb:     loss 2.30009
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_090144-qplslmyz
wandb: Find logs at: ./wandb/offline-run-20240417_090144-qplslmyz/logs
INFO flwr 2024-04-17 09:05:10,923 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 09:12:18,463 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=465804)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=465804)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 09:12:23,029	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 09:12:24,057	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 09:12:24,519	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 09:12:24,884	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b45fd76c2226524e.zip' (37.13MiB) to Ray cluster...
2024-04-17 09:12:25,002	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b45fd76c2226524e.zip'.
INFO flwr 2024-04-17 09:12:36,130 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77095874150.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 169890373018.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 09:12:36,130 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 09:12:36,130 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 09:12:36,145 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 09:12:36,147 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 09:12:36,147 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 09:12:36,147 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 09:12:38,905 | server.py:94 | initial parameters (loss, other metrics): 2.3026628494262695, {'accuracy': 0.088, 'data_size': 10000}
INFO flwr 2024-04-17 09:12:38,906 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 09:12:38,906 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=470532)[0m 2024-04-17 09:12:42.155814: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=470532)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=470532)[0m 2024-04-17 09:12:44.629728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=470533)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=470533)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=470533)[0m 2024-04-17 09:12:43.361971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=470533)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=470531)[0m 2024-04-17 09:12:45.558207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 09:13:09,154 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 09:13:10,535 | server.py:125 | fit progress: (1, 2.302657127380371, {'accuracy': 0.0882, 'data_size': 10000}, 31.629349535010988)
INFO flwr 2024-04-17 09:13:10,536 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 09:13:10,536 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:13:26,033 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 09:13:27,208 | server.py:125 | fit progress: (2, 2.3026487827301025, {'accuracy': 0.0892, 'data_size': 10000}, 48.302301769013866)
INFO flwr 2024-04-17 09:13:27,209 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 09:13:27,209 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:13:43,320 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 09:13:44,747 | server.py:125 | fit progress: (3, 2.3026394844055176, {'accuracy': 0.0896, 'data_size': 10000}, 65.84117502000299)
INFO flwr 2024-04-17 09:13:44,748 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 09:13:44,748 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:13:59,935 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 09:14:01,101 | server.py:125 | fit progress: (4, 2.302628993988037, {'accuracy': 0.0901, 'data_size': 10000}, 82.19492295800592)
INFO flwr 2024-04-17 09:14:01,101 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 09:14:01,102 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:14:16,530 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 09:14:17,910 | server.py:125 | fit progress: (5, 2.3026177883148193, {'accuracy': 0.0914, 'data_size': 10000}, 99.0041006829997)
INFO flwr 2024-04-17 09:14:17,910 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 09:14:17,911 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:14:33,653 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 09:14:34,827 | server.py:125 | fit progress: (6, 2.302605390548706, {'accuracy': 0.0921, 'data_size': 10000}, 115.9205913520127)
INFO flwr 2024-04-17 09:14:34,827 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 09:14:34,827 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:14:49,067 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 09:14:50,433 | server.py:125 | fit progress: (7, 2.3025922775268555, {'accuracy': 0.0925, 'data_size': 10000}, 131.52678316200036)
INFO flwr 2024-04-17 09:14:50,433 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 09:14:50,433 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:15:04,608 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 09:15:06,043 | server.py:125 | fit progress: (8, 2.3025801181793213, {'accuracy': 0.0931, 'data_size': 10000}, 147.13690413501172)
INFO flwr 2024-04-17 09:15:06,043 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 09:15:06,043 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:15:20,572 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 09:15:21,750 | server.py:125 | fit progress: (9, 2.3025662899017334, {'accuracy': 0.0935, 'data_size': 10000}, 162.84390389900364)
INFO flwr 2024-04-17 09:15:21,750 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 09:15:21,750 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:15:37,070 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 09:15:38,238 | server.py:125 | fit progress: (10, 2.3025529384613037, {'accuracy': 0.0947, 'data_size': 10000}, 179.33187942900986)
INFO flwr 2024-04-17 09:15:38,238 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 09:15:38,239 | server.py:153 | FL finished in 179.33251056700828
INFO flwr 2024-04-17 09:15:38,239 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 09:15:38,239 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 09:15:38,239 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 09:15:38,239 | app.py:229 | app_fit: losses_centralized [(0, 2.3026628494262695), (1, 2.302657127380371), (2, 2.3026487827301025), (3, 2.3026394844055176), (4, 2.302628993988037), (5, 2.3026177883148193), (6, 2.302605390548706), (7, 2.3025922775268555), (8, 2.3025801181793213), (9, 2.3025662899017334), (10, 2.3025529384613037)]
INFO flwr 2024-04-17 09:15:38,239 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.088), (1, 0.0882), (2, 0.0892), (3, 0.0896), (4, 0.0901), (5, 0.0914), (6, 0.0921), (7, 0.0925), (8, 0.0931), (9, 0.0935), (10, 0.0947)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0947
wandb:     loss 2.30255
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_091218-agxgaim6
wandb: Find logs at: ./wandb/offline-run-20240417_091218-agxgaim6/logs
INFO flwr 2024-04-17 09:15:41,795 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 09:22:49,801 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=470525)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=470525)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 09:22:54,498	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 09:22:55,346	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 09:22:55,799	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 09:22:56,152	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_260574003a5e9906.zip' (37.14MiB) to Ray cluster...
2024-04-17 09:22:56,253	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_260574003a5e9906.zip'.
INFO flwr 2024-04-17 09:23:07,323 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 169866451968.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'object_store_memory': 77085622272.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 09:23:07,324 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 09:23:07,324 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 09:23:07,344 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 09:23:07,345 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 09:23:07,346 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 09:23:07,346 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 09:23:10,265 | server.py:94 | initial parameters (loss, other metrics): 2.302501678466797, {'accuracy': 0.0961, 'data_size': 10000}
INFO flwr 2024-04-17 09:23:10,266 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 09:23:10,266 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=474926)[0m 2024-04-17 09:23:13.455995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=474926)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=474921)[0m 2024-04-17 09:23:15.851382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=474926)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=474926)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=474923)[0m 2024-04-17 09:23:13.623577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=474923)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=474928)[0m 2024-04-17 09:23:15.873230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 09:23:41,498 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 09:23:42,947 | server.py:125 | fit progress: (1, 2.109992027282715, {'accuracy': 0.3509, 'data_size': 10000}, 32.68103516299743)
INFO flwr 2024-04-17 09:23:42,947 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 09:23:42,947 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:23:58,124 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 09:23:59,312 | server.py:125 | fit progress: (2, 2.2407424449920654, {'accuracy': 0.2204, 'data_size': 10000}, 49.04577809800685)
INFO flwr 2024-04-17 09:23:59,312 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 09:23:59,312 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:24:15,787 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 09:24:17,189 | server.py:125 | fit progress: (3, 2.2211434841156006, {'accuracy': 0.24, 'data_size': 10000}, 66.92363785300404)
INFO flwr 2024-04-17 09:24:17,190 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 09:24:17,190 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:24:31,071 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 09:24:32,228 | server.py:125 | fit progress: (4, 2.2992422580718994, {'accuracy': 0.1619, 'data_size': 10000}, 81.96233976400981)
INFO flwr 2024-04-17 09:24:32,228 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 09:24:32,229 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:24:47,271 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 09:24:48,661 | server.py:125 | fit progress: (5, 2.3144402503967285, {'accuracy': 0.1467, 'data_size': 10000}, 98.3947815499996)
INFO flwr 2024-04-17 09:24:48,661 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 09:24:48,661 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:25:03,018 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 09:25:04,184 | server.py:125 | fit progress: (6, 2.3193423748016357, {'accuracy': 0.1418, 'data_size': 10000}, 113.91779258899624)
INFO flwr 2024-04-17 09:25:04,184 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 09:25:04,184 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:25:19,007 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 09:25:20,381 | server.py:125 | fit progress: (7, 2.3223423957824707, {'accuracy': 0.1388, 'data_size': 10000}, 130.1149884970073)
INFO flwr 2024-04-17 09:25:20,381 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 09:25:20,381 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:25:37,085 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 09:25:38,280 | server.py:125 | fit progress: (8, 2.324742317199707, {'accuracy': 0.1364, 'data_size': 10000}, 148.01402194899856)
INFO flwr 2024-04-17 09:25:38,280 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 09:25:38,280 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:25:53,959 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 09:25:55,125 | server.py:125 | fit progress: (9, 2.3269424438476562, {'accuracy': 0.1342, 'data_size': 10000}, 164.85932737099938)
INFO flwr 2024-04-17 09:25:55,125 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 09:25:55,126 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:26:10,367 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 09:26:11,780 | server.py:125 | fit progress: (10, 2.328742265701294, {'accuracy': 0.1324, 'data_size': 10000}, 181.51388106700324)
INFO flwr 2024-04-17 09:26:11,780 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 09:26:11,780 | server.py:153 | FL finished in 181.51438326200878
INFO flwr 2024-04-17 09:26:11,780 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 09:26:11,780 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 09:26:11,781 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 09:26:11,781 | app.py:229 | app_fit: losses_centralized [(0, 2.302501678466797), (1, 2.109992027282715), (2, 2.2407424449920654), (3, 2.2211434841156006), (4, 2.2992422580718994), (5, 2.3144402503967285), (6, 2.3193423748016357), (7, 2.3223423957824707), (8, 2.324742317199707), (9, 2.3269424438476562), (10, 2.328742265701294)]
INFO flwr 2024-04-17 09:26:11,781 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0961), (1, 0.3509), (2, 0.2204), (3, 0.24), (4, 0.1619), (5, 0.1467), (6, 0.1418), (7, 0.1388), (8, 0.1364), (9, 0.1342), (10, 0.1324)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1324
wandb:     loss 2.32874
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_092249-o283u92s
wandb: Find logs at: ./wandb/offline-run-20240417_092249-o283u92s/logs
INFO flwr 2024-04-17 09:26:15,342 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 09:33:22,860 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=474920)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=474920)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 09:33:27,670	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 09:33:28,527	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 09:33:29,008	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 09:33:29,364	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ac64431804a9eef4.zip' (37.15MiB) to Ray cluster...
2024-04-17 09:33:29,483	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ac64431804a9eef4.zip'.
INFO flwr 2024-04-17 09:33:40,531 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77107187712.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169916771328.0}
INFO flwr 2024-04-17 09:33:40,532 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 09:33:40,532 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 09:33:40,547 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 09:33:40,548 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 09:33:40,549 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 09:33:40,549 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 09:33:43,391 | server.py:94 | initial parameters (loss, other metrics): 2.3026211261749268, {'accuracy': 0.0815, 'data_size': 10000}
INFO flwr 2024-04-17 09:33:43,392 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 09:33:43,392 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=479858)[0m 2024-04-17 09:33:46.561511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=479858)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=479858)[0m 2024-04-17 09:33:48.935399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=479853)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=479853)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=479849)[0m 2024-04-17 09:33:46.844784: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=479849)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=479849)[0m 2024-04-17 09:33:49.080893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 09:34:12,573 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 09:34:13,968 | server.py:125 | fit progress: (1, 2.1901350021362305, {'accuracy': 0.6602, 'data_size': 10000}, 30.576234524996835)
INFO flwr 2024-04-17 09:34:13,969 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 09:34:13,969 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:34:30,522 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 09:34:31,682 | server.py:125 | fit progress: (2, 1.7901116609573364, {'accuracy': 0.6936, 'data_size': 10000}, 48.289666484997724)
INFO flwr 2024-04-17 09:34:31,682 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 09:34:31,682 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:34:46,592 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 09:34:48,006 | server.py:125 | fit progress: (3, 1.707335352897644, {'accuracy': 0.7506, 'data_size': 10000}, 64.61391986999661)
INFO flwr 2024-04-17 09:34:48,006 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 09:34:48,006 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:35:01,351 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 09:35:02,529 | server.py:125 | fit progress: (4, 1.604242205619812, {'accuracy': 0.8569, 'data_size': 10000}, 79.13677440700121)
INFO flwr 2024-04-17 09:35:02,529 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 09:35:02,529 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:35:18,154 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 09:35:19,518 | server.py:125 | fit progress: (5, 1.556472659111023, {'accuracy': 0.9047, 'data_size': 10000}, 96.12560123499134)
INFO flwr 2024-04-17 09:35:19,518 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 09:35:19,518 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:35:34,180 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 09:35:35,379 | server.py:125 | fit progress: (6, 1.5385199785232544, {'accuracy': 0.9229, 'data_size': 10000}, 111.98722378199454)
INFO flwr 2024-04-17 09:35:35,379 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 09:35:35,380 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:35:50,605 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 09:35:51,987 | server.py:125 | fit progress: (7, 1.5318495035171509, {'accuracy': 0.9296, 'data_size': 10000}, 128.59519555899897)
INFO flwr 2024-04-17 09:35:51,987 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 09:35:51,988 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:36:06,761 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 09:36:08,141 | server.py:125 | fit progress: (8, 1.5294489860534668, {'accuracy': 0.9316, 'data_size': 10000}, 144.74921637999068)
INFO flwr 2024-04-17 09:36:08,142 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 09:36:08,142 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:36:22,127 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 09:36:23,298 | server.py:125 | fit progress: (9, 1.5258439779281616, {'accuracy': 0.9352, 'data_size': 10000}, 159.90649849300098)
INFO flwr 2024-04-17 09:36:23,299 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 09:36:23,299 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:36:38,765 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 09:36:39,920 | server.py:125 | fit progress: (10, 1.5245147943496704, {'accuracy': 0.9365, 'data_size': 10000}, 176.52778609500092)
INFO flwr 2024-04-17 09:36:39,920 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 09:36:39,921 | server.py:153 | FL finished in 176.52855730098963
INFO flwr 2024-04-17 09:36:39,921 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 09:36:39,921 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 09:36:39,921 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 09:36:39,921 | app.py:229 | app_fit: losses_centralized [(0, 2.3026211261749268), (1, 2.1901350021362305), (2, 1.7901116609573364), (3, 1.707335352897644), (4, 1.604242205619812), (5, 1.556472659111023), (6, 1.5385199785232544), (7, 1.5318495035171509), (8, 1.5294489860534668), (9, 1.5258439779281616), (10, 1.5245147943496704)]
INFO flwr 2024-04-17 09:36:39,921 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0815), (1, 0.6602), (2, 0.6936), (3, 0.7506), (4, 0.8569), (5, 0.9047), (6, 0.9229), (7, 0.9296), (8, 0.9316), (9, 0.9352), (10, 0.9365)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9365
wandb:     loss 1.52451
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_093322-2tftr0vv
wandb: Find logs at: ./wandb/offline-run-20240417_093322-2tftr0vv/logs
INFO flwr 2024-04-17 09:36:43,434 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 09:43:51,348 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=479849)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=479849)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 09:43:55,996	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 09:43:56,774	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 09:43:57,226	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 09:43:57,573	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_300aa805d8e15e13.zip' (37.17MiB) to Ray cluster...
2024-04-17 09:43:57,690	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_300aa805d8e15e13.zip'.
INFO flwr 2024-04-17 09:44:08,765 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77088631603.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169873473741.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 09:44:08,765 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 09:44:08,766 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 09:44:08,784 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 09:44:08,785 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 09:44:08,785 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 09:44:08,785 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 09:44:11,364 | server.py:94 | initial parameters (loss, other metrics): 2.302757740020752, {'accuracy': 0.0888, 'data_size': 10000}
INFO flwr 2024-04-17 09:44:11,364 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 09:44:11,365 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=484596)[0m 2024-04-17 09:44:14.815845: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=484596)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=484596)[0m 2024-04-17 09:44:17.928855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=484589)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=484589)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=484598)[0m 2024-04-17 09:44:16.170038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=484598)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=484598)[0m 2024-04-17 09:44:18.583661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 09:44:43,906 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 09:44:45,331 | server.py:125 | fit progress: (1, 2.301802396774292, {'accuracy': 0.0905, 'data_size': 10000}, 33.96603907599638)
INFO flwr 2024-04-17 09:44:45,331 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 09:44:45,331 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:45:01,532 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 09:45:02,922 | server.py:125 | fit progress: (2, 2.299100637435913, {'accuracy': 0.141, 'data_size': 10000}, 51.55682663200423)
INFO flwr 2024-04-17 09:45:02,922 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 09:45:02,922 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:45:17,383 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 09:45:18,811 | server.py:125 | fit progress: (3, 2.292733907699585, {'accuracy': 0.2763, 'data_size': 10000}, 67.44635278300848)
INFO flwr 2024-04-17 09:45:18,811 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 09:45:18,812 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:45:33,342 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 09:45:34,745 | server.py:125 | fit progress: (4, 2.2798101902008057, {'accuracy': 0.5372, 'data_size': 10000}, 83.38032792600279)
INFO flwr 2024-04-17 09:45:34,745 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 09:45:34,746 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:45:49,408 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 09:45:50,821 | server.py:125 | fit progress: (5, 2.255626916885376, {'accuracy': 0.6243, 'data_size': 10000}, 99.4558143059985)
INFO flwr 2024-04-17 09:45:50,821 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 09:45:50,821 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:46:05,013 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 09:46:06,205 | server.py:125 | fit progress: (6, 2.212839126586914, {'accuracy': 0.6615, 'data_size': 10000}, 114.84033830600674)
INFO flwr 2024-04-17 09:46:06,205 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 09:46:06,206 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:46:21,562 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 09:46:22,988 | server.py:125 | fit progress: (7, 2.142650842666626, {'accuracy': 0.6698, 'data_size': 10000}, 131.62308230200142)
INFO flwr 2024-04-17 09:46:22,988 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 09:46:22,988 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:46:38,185 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 09:46:39,358 | server.py:125 | fit progress: (8, 2.0450854301452637, {'accuracy': 0.6707, 'data_size': 10000}, 147.99368542799493)
INFO flwr 2024-04-17 09:46:39,359 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 09:46:39,359 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:46:53,709 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 09:46:54,883 | server.py:125 | fit progress: (9, 1.9419137239456177, {'accuracy': 0.6751, 'data_size': 10000}, 163.51830804800557)
INFO flwr 2024-04-17 09:46:54,883 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 09:46:54,884 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:47:10,505 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 09:47:11,877 | server.py:125 | fit progress: (10, 1.8588277101516724, {'accuracy': 0.685, 'data_size': 10000}, 180.51181579200784)
INFO flwr 2024-04-17 09:47:11,877 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 09:47:11,877 | server.py:153 | FL finished in 180.51227036300406
INFO flwr 2024-04-17 09:47:11,877 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 09:47:11,877 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 09:47:11,877 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 09:47:11,878 | app.py:229 | app_fit: losses_centralized [(0, 2.302757740020752), (1, 2.301802396774292), (2, 2.299100637435913), (3, 2.292733907699585), (4, 2.2798101902008057), (5, 2.255626916885376), (6, 2.212839126586914), (7, 2.142650842666626), (8, 2.0450854301452637), (9, 1.9419137239456177), (10, 1.8588277101516724)]
INFO flwr 2024-04-17 09:47:11,878 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0888), (1, 0.0905), (2, 0.141), (3, 0.2763), (4, 0.5372), (5, 0.6243), (6, 0.6615), (7, 0.6698), (8, 0.6707), (9, 0.6751), (10, 0.685)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.685
wandb:     loss 1.85883
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_094350-vg2odnfu
wandb: Find logs at: ./wandb/offline-run-20240417_094350-vg2odnfu/logs
INFO flwr 2024-04-17 09:47:15,398 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 09:54:25,735 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=484585)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=484585)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 09:54:31,199	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 09:54:39,355	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 09:54:39,820	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 09:54:40,189	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_89c5ef48b106721e.zip' (37.18MiB) to Ray cluster...
2024-04-17 09:54:40,309	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_89c5ef48b106721e.zip'.
INFO flwr 2024-04-17 09:54:51,334 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77170196889.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170063792743.0, 'CPU': 64.0}
INFO flwr 2024-04-17 09:54:51,334 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 09:54:51,335 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 09:54:51,352 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 09:54:51,357 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 09:54:51,357 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 09:54:51,357 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 09:54:55,009 | server.py:94 | initial parameters (loss, other metrics): 2.3024888038635254, {'accuracy': 0.0985, 'data_size': 10000}
INFO flwr 2024-04-17 09:54:55,010 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 09:54:55,011 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=488956)[0m 2024-04-17 09:54:57.521395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=488956)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=488956)[0m 2024-04-17 09:54:59.858553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=488965)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=488965)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=488962)[0m 2024-04-17 09:54:57.755746: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=488962)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=488962)[0m 2024-04-17 09:55:00.014782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 09:55:23,887 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 09:55:25,271 | server.py:125 | fit progress: (1, 2.302417516708374, {'accuracy': 0.0998, 'data_size': 10000}, 30.261009144000127)
INFO flwr 2024-04-17 09:55:25,272 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 09:55:25,272 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:55:40,824 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 09:55:42,201 | server.py:125 | fit progress: (2, 2.302316904067993, {'accuracy': 0.1022, 'data_size': 10000}, 47.19037213100819)
INFO flwr 2024-04-17 09:55:42,201 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 09:55:42,201 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:55:56,224 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 09:55:57,423 | server.py:125 | fit progress: (3, 2.302189350128174, {'accuracy': 0.105, 'data_size': 10000}, 62.412561560006)
INFO flwr 2024-04-17 09:55:57,423 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 09:55:57,424 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:56:12,234 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 09:56:13,606 | server.py:125 | fit progress: (4, 2.3020339012145996, {'accuracy': 0.1093, 'data_size': 10000}, 78.59531034000975)
INFO flwr 2024-04-17 09:56:13,606 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 09:56:13,606 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:56:27,765 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 09:56:29,152 | server.py:125 | fit progress: (5, 2.3018479347229004, {'accuracy': 0.1164, 'data_size': 10000}, 94.14180988800945)
INFO flwr 2024-04-17 09:56:29,153 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 09:56:29,153 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:56:42,786 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 09:56:44,187 | server.py:125 | fit progress: (6, 2.301622152328491, {'accuracy': 0.1266, 'data_size': 10000}, 109.17693546100054)
INFO flwr 2024-04-17 09:56:44,188 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 09:56:44,188 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:56:59,135 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 09:57:00,539 | server.py:125 | fit progress: (7, 2.3013575077056885, {'accuracy': 0.1433, 'data_size': 10000}, 125.52810666900768)
INFO flwr 2024-04-17 09:57:00,539 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 09:57:00,539 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:57:15,493 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 09:57:16,885 | server.py:125 | fit progress: (8, 2.3010568618774414, {'accuracy': 0.1677, 'data_size': 10000}, 141.87471509200986)
INFO flwr 2024-04-17 09:57:16,885 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 09:57:16,886 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:57:31,181 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 09:57:32,550 | server.py:125 | fit progress: (9, 2.300710678100586, {'accuracy': 0.1941, 'data_size': 10000}, 157.53972185800376)
INFO flwr 2024-04-17 09:57:32,550 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 09:57:32,551 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 09:57:46,990 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 09:57:48,373 | server.py:125 | fit progress: (10, 2.300314426422119, {'accuracy': 0.22, 'data_size': 10000}, 173.36238506500376)
INFO flwr 2024-04-17 09:57:48,373 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 09:57:48,373 | server.py:153 | FL finished in 173.36295218000305
INFO flwr 2024-04-17 09:57:48,374 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 09:57:48,374 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 09:57:48,374 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 09:57:48,374 | app.py:229 | app_fit: losses_centralized [(0, 2.3024888038635254), (1, 2.302417516708374), (2, 2.302316904067993), (3, 2.302189350128174), (4, 2.3020339012145996), (5, 2.3018479347229004), (6, 2.301622152328491), (7, 2.3013575077056885), (8, 2.3010568618774414), (9, 2.300710678100586), (10, 2.300314426422119)]
INFO flwr 2024-04-17 09:57:48,374 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0985), (1, 0.0998), (2, 0.1022), (3, 0.105), (4, 0.1093), (5, 0.1164), (6, 0.1266), (7, 0.1433), (8, 0.1677), (9, 0.1941), (10, 0.22)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.22
wandb:     loss 2.30031
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_095423-u5j30msv
wandb: Find logs at: ./wandb/offline-run-20240417_095423-u5j30msv/logs
INFO flwr 2024-04-17 09:57:51,935 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 10:04:59,628 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=488953)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=488953)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 10:05:05,659	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 10:05:06,566	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 10:05:07,024	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 10:05:07,378	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_527f374aa0c1bfa6.zip' (37.19MiB) to Ray cluster...
2024-04-17 10:05:07,496	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_527f374aa0c1bfa6.zip'.
INFO flwr 2024-04-17 10:05:18,635 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77093841715.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169885630669.0, 'CPU': 64.0, 'GPU': 1.0}
INFO flwr 2024-04-17 10:05:18,635 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 10:05:18,635 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 10:05:18,653 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 10:05:18,654 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 10:05:18,655 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 10:05:18,655 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 10:05:21,854 | server.py:94 | initial parameters (loss, other metrics): 2.302672863006592, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-17 10:05:21,854 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 10:05:21,855 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=493353)[0m 2024-04-17 10:05:24.646420: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=493353)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=493353)[0m 2024-04-17 10:05:27.021712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=493355)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=493355)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=493351)[0m 2024-04-17 10:05:25.025552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=493351)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=493351)[0m 2024-04-17 10:05:27.581197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 10:05:49,727 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 10:05:51,127 | server.py:125 | fit progress: (1, 2.3026657104492188, {'accuracy': 0.0894, 'data_size': 10000}, 29.27272267000808)
INFO flwr 2024-04-17 10:05:51,128 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 10:05:51,128 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:06:06,621 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 10:06:07,782 | server.py:125 | fit progress: (2, 2.302656412124634, {'accuracy': 0.0897, 'data_size': 10000}, 45.92760359500244)
INFO flwr 2024-04-17 10:06:07,782 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 10:06:07,783 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:06:22,548 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 10:06:23,942 | server.py:125 | fit progress: (3, 2.302645444869995, {'accuracy': 0.0904, 'data_size': 10000}, 62.08715867600404)
INFO flwr 2024-04-17 10:06:23,942 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 10:06:23,942 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:06:39,879 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 10:06:41,032 | server.py:125 | fit progress: (4, 2.3026342391967773, {'accuracy': 0.0912, 'data_size': 10000}, 79.17739592400903)
INFO flwr 2024-04-17 10:06:41,032 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 10:06:41,033 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:06:55,758 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 10:06:57,130 | server.py:125 | fit progress: (5, 2.302621603012085, {'accuracy': 0.0924, 'data_size': 10000}, 95.27510341499874)
INFO flwr 2024-04-17 10:06:57,130 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 10:06:57,130 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:07:13,297 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 10:07:14,471 | server.py:125 | fit progress: (6, 2.3026084899902344, {'accuracy': 0.0937, 'data_size': 10000}, 112.61667751600908)
INFO flwr 2024-04-17 10:07:14,472 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 10:07:14,472 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:07:29,324 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 10:07:30,722 | server.py:125 | fit progress: (7, 2.3025944232940674, {'accuracy': 0.0946, 'data_size': 10000}, 128.86697116099822)
INFO flwr 2024-04-17 10:07:30,722 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 10:07:30,722 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:07:45,242 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 10:07:46,639 | server.py:125 | fit progress: (8, 2.302579164505005, {'accuracy': 0.0947, 'data_size': 10000}, 144.78440599300666)
INFO flwr 2024-04-17 10:07:46,639 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 10:07:46,639 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:08:01,249 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 10:08:02,407 | server.py:125 | fit progress: (9, 2.3025639057159424, {'accuracy': 0.0954, 'data_size': 10000}, 160.5526409390004)
INFO flwr 2024-04-17 10:08:02,408 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 10:08:02,408 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:08:17,066 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 10:08:18,244 | server.py:125 | fit progress: (10, 2.302548885345459, {'accuracy': 0.0962, 'data_size': 10000}, 176.38901761399757)
INFO flwr 2024-04-17 10:08:18,244 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 10:08:18,244 | server.py:153 | FL finished in 176.38949468100327
INFO flwr 2024-04-17 10:08:18,244 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 10:08:18,244 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 10:08:18,244 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 10:08:18,245 | app.py:229 | app_fit: losses_centralized [(0, 2.302672863006592), (1, 2.3026657104492188), (2, 2.302656412124634), (3, 2.302645444869995), (4, 2.3026342391967773), (5, 2.302621603012085), (6, 2.3026084899902344), (7, 2.3025944232940674), (8, 2.302579164505005), (9, 2.3025639057159424), (10, 2.302548885345459)]
INFO flwr 2024-04-17 10:08:18,245 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.0894), (2, 0.0897), (3, 0.0904), (4, 0.0912), (5, 0.0924), (6, 0.0937), (7, 0.0946), (8, 0.0947), (9, 0.0954), (10, 0.0962)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0962
wandb:     loss 2.30255
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_100459-1dkxutj1
wandb: Find logs at: ./wandb/offline-run-20240417_100459-1dkxutj1/logs
INFO flwr 2024-04-17 10:08:21,811 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 10:15:29,706 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=493351)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=493351)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 10:15:34,184	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 10:15:35,052	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 10:15:35,574	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 10:15:35,944	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_041120e086418d86.zip' (37.20MiB) to Ray cluster...
2024-04-17 10:15:36,065	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_041120e086418d86.zip'.
INFO flwr 2024-04-17 10:15:47,131 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'memory': 169883881677.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 77093092147.0}
INFO flwr 2024-04-17 10:15:47,131 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 10:15:47,132 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 10:15:47,150 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 10:15:47,151 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 10:15:47,152 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 10:15:47,152 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 10:15:50,559 | server.py:94 | initial parameters (loss, other metrics): 2.3027122020721436, {'accuracy': 0.0959, 'data_size': 10000}
INFO flwr 2024-04-17 10:15:50,560 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 10:15:50,560 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=498690)[0m 2024-04-17 10:15:53.208027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=498690)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=498690)[0m 2024-04-17 10:15:55.547479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=498689)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=498689)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=498692)[0m 2024-04-17 10:15:53.589018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=498692)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=498692)[0m 2024-04-17 10:15:55.924976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 10:16:20,073 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 10:16:21,477 | server.py:125 | fit progress: (1, 2.0623538494110107, {'accuracy': 0.3986, 'data_size': 10000}, 30.91684568999335)
INFO flwr 2024-04-17 10:16:21,477 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 10:16:21,477 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:16:37,426 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 10:16:38,800 | server.py:125 | fit progress: (2, 2.2303388118743896, {'accuracy': 0.2308, 'data_size': 10000}, 48.24046143599844)
INFO flwr 2024-04-17 10:16:38,801 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 10:16:38,801 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:16:53,967 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 10:16:55,333 | server.py:125 | fit progress: (3, 2.34694242477417, {'accuracy': 0.1142, 'data_size': 10000}, 64.7729552709934)
INFO flwr 2024-04-17 10:16:55,333 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 10:16:55,333 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:17:10,220 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 10:17:11,614 | server.py:125 | fit progress: (4, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 81.05457226799626)
INFO flwr 2024-04-17 10:17:11,615 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 10:17:11,615 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:17:26,301 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 10:17:27,680 | server.py:125 | fit progress: (5, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 97.11981344599917)
INFO flwr 2024-04-17 10:17:27,680 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 10:17:27,680 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:17:41,555 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 10:17:42,735 | server.py:125 | fit progress: (6, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 112.17550103800022)
INFO flwr 2024-04-17 10:17:42,736 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 10:17:42,736 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:17:55,856 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 10:17:57,259 | server.py:125 | fit progress: (7, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 126.69867722899653)
INFO flwr 2024-04-17 10:17:57,259 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 10:17:57,259 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:18:11,808 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 10:18:12,979 | server.py:125 | fit progress: (8, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 142.41879948800488)
INFO flwr 2024-04-17 10:18:12,979 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 10:18:12,979 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:18:27,028 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 10:18:28,216 | server.py:125 | fit progress: (9, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 157.6559593599959)
INFO flwr 2024-04-17 10:18:28,216 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 10:18:28,216 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:18:42,309 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 10:18:43,683 | server.py:125 | fit progress: (10, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 173.12305599100364)
INFO flwr 2024-04-17 10:18:43,683 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 10:18:43,683 | server.py:153 | FL finished in 173.12349526600156
INFO flwr 2024-04-17 10:18:43,684 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 10:18:43,684 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 10:18:43,684 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 10:18:43,684 | app.py:229 | app_fit: losses_centralized [(0, 2.3027122020721436), (1, 2.0623538494110107), (2, 2.2303388118743896), (3, 2.34694242477417), (4, 2.347642183303833), (5, 2.347642183303833), (6, 2.347642183303833), (7, 2.347642183303833), (8, 2.347642183303833), (9, 2.347642183303833), (10, 2.347642183303833)]
INFO flwr 2024-04-17 10:18:43,684 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0959), (1, 0.3986), (2, 0.2308), (3, 0.1142), (4, 0.1135), (5, 0.1135), (6, 0.1135), (7, 0.1135), (8, 0.1135), (9, 0.1135), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.34764
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_101529-u7sbpuen
wandb: Find logs at: ./wandb/offline-run-20240417_101529-u7sbpuen/logs
INFO flwr 2024-04-17 10:18:47,198 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 10:25:55,766 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=498684)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=498684)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 10:26:00,210	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 10:26:01,073	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 10:26:01,533	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 10:26:01,888	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6190251039e80792.zip' (37.21MiB) to Ray cluster...
2024-04-17 10:26:02,004	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6190251039e80792.zip'.
INFO flwr 2024-04-17 10:26:16,013 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 77092372070.0, 'accelerator_type:TITAN': 1.0, 'memory': 169882201498.0}
INFO flwr 2024-04-17 10:26:16,013 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 10:26:16,013 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 10:26:16,036 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 10:26:16,037 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 10:26:16,037 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 10:26:16,038 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 10:26:19,761 | server.py:94 | initial parameters (loss, other metrics): 2.3025059700012207, {'accuracy': 0.1114, 'data_size': 10000}
INFO flwr 2024-04-17 10:26:19,761 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 10:26:19,762 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=503070)[0m 2024-04-17 10:26:22.014755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=503070)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=503072)[0m 2024-04-17 10:26:24.368397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=503070)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=503070)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=503076)[0m 2024-04-17 10:26:22.354136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=503076)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=503076)[0m 2024-04-17 10:26:24.634086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 10:27:01,195 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 10:27:02,578 | server.py:125 | fit progress: (1, 2.2093889713287354, {'accuracy': 0.6195, 'data_size': 10000}, 42.81605712500459)
INFO flwr 2024-04-17 10:27:02,578 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 10:27:02,578 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:27:17,641 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 10:27:19,037 | server.py:125 | fit progress: (2, 1.767356038093567, {'accuracy': 0.7254, 'data_size': 10000}, 59.27548031900369)
INFO flwr 2024-04-17 10:27:19,037 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 10:27:19,037 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:27:33,608 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 10:27:35,003 | server.py:125 | fit progress: (3, 1.639000415802002, {'accuracy': 0.8184, 'data_size': 10000}, 75.24146112900053)
INFO flwr 2024-04-17 10:27:35,003 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 10:27:35,004 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:27:52,270 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 10:27:53,647 | server.py:125 | fit progress: (4, 1.5694233179092407, {'accuracy': 0.8906, 'data_size': 10000}, 93.88505974400323)
INFO flwr 2024-04-17 10:27:53,647 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 10:27:53,647 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:28:08,318 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 10:28:09,693 | server.py:125 | fit progress: (5, 1.5472793579101562, {'accuracy': 0.9135, 'data_size': 10000}, 109.93114322899783)
INFO flwr 2024-04-17 10:28:09,693 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 10:28:09,693 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:28:23,912 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 10:28:25,099 | server.py:125 | fit progress: (6, 1.5375458002090454, {'accuracy': 0.9239, 'data_size': 10000}, 125.33796994600561)
INFO flwr 2024-04-17 10:28:25,100 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 10:28:25,100 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:28:40,065 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 10:28:41,461 | server.py:125 | fit progress: (7, 1.5295782089233398, {'accuracy': 0.9312, 'data_size': 10000}, 141.6992027660017)
INFO flwr 2024-04-17 10:28:41,461 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 10:28:41,461 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:28:57,122 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 10:28:58,297 | server.py:125 | fit progress: (8, 1.52369225025177, {'accuracy': 0.9373, 'data_size': 10000}, 158.53600666800048)
INFO flwr 2024-04-17 10:28:58,298 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 10:28:58,298 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:29:11,768 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 10:29:12,940 | server.py:125 | fit progress: (9, 1.5330199003219604, {'accuracy': 0.9279, 'data_size': 10000}, 173.17818941800215)
INFO flwr 2024-04-17 10:29:12,940 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 10:29:12,940 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:29:28,046 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 10:29:29,428 | server.py:125 | fit progress: (10, 1.5353442430496216, {'accuracy': 0.9255, 'data_size': 10000}, 189.66625981600373)
INFO flwr 2024-04-17 10:29:29,428 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 10:29:29,428 | server.py:153 | FL finished in 189.66670654399786
INFO flwr 2024-04-17 10:29:29,428 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 10:29:29,428 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 10:29:29,429 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 10:29:29,429 | app.py:229 | app_fit: losses_centralized [(0, 2.3025059700012207), (1, 2.2093889713287354), (2, 1.767356038093567), (3, 1.639000415802002), (4, 1.5694233179092407), (5, 1.5472793579101562), (6, 1.5375458002090454), (7, 1.5295782089233398), (8, 1.52369225025177), (9, 1.5330199003219604), (10, 1.5353442430496216)]
INFO flwr 2024-04-17 10:29:29,429 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1114), (1, 0.6195), (2, 0.7254), (3, 0.8184), (4, 0.8906), (5, 0.9135), (6, 0.9239), (7, 0.9312), (8, 0.9373), (9, 0.9279), (10, 0.9255)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9255
wandb:     loss 1.53534
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_102555-gikq7em3
wandb: Find logs at: ./wandb/offline-run-20240417_102555-gikq7em3/logs
INFO flwr 2024-04-17 10:29:32,931 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 10:36:40,163 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=503064)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=503064)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 10:36:44,667	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 10:36:45,537	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 10:36:45,987	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 10:36:46,346	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_13282ef6e7356a3a.zip' (37.22MiB) to Ray cluster...
2024-04-17 10:36:46,468	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_13282ef6e7356a3a.zip'.
INFO flwr 2024-04-17 10:36:57,519 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 77091601612.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169880403764.0}
INFO flwr 2024-04-17 10:36:57,519 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 10:36:57,519 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 10:36:57,537 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 10:36:57,538 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 10:36:57,538 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 10:36:57,539 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 10:37:00,418 | server.py:94 | initial parameters (loss, other metrics): 2.3025736808776855, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-17 10:37:00,418 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 10:37:00,419 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=507440)[0m 2024-04-17 10:37:03.623796: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=507440)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=507440)[0m 2024-04-17 10:37:05.984436: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=507440)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=507440)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=507430)[0m 2024-04-17 10:37:03.847280: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=507430)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=507430)[0m 2024-04-17 10:37:06.173744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 10:37:29,985 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 10:37:31,379 | server.py:125 | fit progress: (1, 2.301642894744873, {'accuracy': 0.0993, 'data_size': 10000}, 30.960443657008)
INFO flwr 2024-04-17 10:37:31,379 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 10:37:31,380 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:37:46,287 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 10:37:47,434 | server.py:125 | fit progress: (2, 2.2991044521331787, {'accuracy': 0.2463, 'data_size': 10000}, 47.015231013006996)
INFO flwr 2024-04-17 10:37:47,434 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 10:37:47,434 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:38:02,304 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 10:38:03,698 | server.py:125 | fit progress: (3, 2.2931625843048096, {'accuracy': 0.3976, 'data_size': 10000}, 63.27932295300707)
INFO flwr 2024-04-17 10:38:03,698 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 10:38:03,698 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:38:18,713 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 10:38:19,872 | server.py:125 | fit progress: (4, 2.280728816986084, {'accuracy': 0.6088, 'data_size': 10000}, 79.452869878005)
INFO flwr 2024-04-17 10:38:19,872 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 10:38:19,872 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:38:35,052 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 10:38:36,469 | server.py:125 | fit progress: (5, 2.256495237350464, {'accuracy': 0.6949, 'data_size': 10000}, 96.050776796008)
INFO flwr 2024-04-17 10:38:36,470 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 10:38:36,470 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:38:52,501 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 10:38:53,647 | server.py:125 | fit progress: (6, 2.2124533653259277, {'accuracy': 0.7301, 'data_size': 10000}, 113.22857737600862)
INFO flwr 2024-04-17 10:38:53,648 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 10:38:53,648 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:39:09,190 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 10:39:10,581 | server.py:125 | fit progress: (7, 2.1393725872039795, {'accuracy': 0.7507, 'data_size': 10000}, 130.16190050200385)
INFO flwr 2024-04-17 10:39:10,581 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 10:39:10,581 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:39:26,179 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 10:39:27,361 | server.py:125 | fit progress: (8, 2.0378944873809814, {'accuracy': 0.7601, 'data_size': 10000}, 146.94273675000295)
INFO flwr 2024-04-17 10:39:27,362 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 10:39:27,362 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:39:42,752 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 10:39:44,117 | server.py:125 | fit progress: (9, 1.9279654026031494, {'accuracy': 0.7753, 'data_size': 10000}, 163.698376447006)
INFO flwr 2024-04-17 10:39:44,117 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 10:39:44,118 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:39:59,329 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 10:40:00,483 | server.py:125 | fit progress: (10, 1.8334790468215942, {'accuracy': 0.7813, 'data_size': 10000}, 180.06453442201018)
INFO flwr 2024-04-17 10:40:00,483 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 10:40:00,484 | server.py:153 | FL finished in 180.06500061100814
INFO flwr 2024-04-17 10:40:00,484 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 10:40:00,484 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 10:40:00,484 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 10:40:00,484 | app.py:229 | app_fit: losses_centralized [(0, 2.3025736808776855), (1, 2.301642894744873), (2, 2.2991044521331787), (3, 2.2931625843048096), (4, 2.280728816986084), (5, 2.256495237350464), (6, 2.2124533653259277), (7, 2.1393725872039795), (8, 2.0378944873809814), (9, 1.9279654026031494), (10, 1.8334790468215942)]
INFO flwr 2024-04-17 10:40:00,484 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.0993), (2, 0.2463), (3, 0.3976), (4, 0.6088), (5, 0.6949), (6, 0.7301), (7, 0.7507), (8, 0.7601), (9, 0.7753), (10, 0.7813)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7813
wandb:     loss 1.83348
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_103639-plo2ya73
wandb: Find logs at: ./wandb/offline-run-20240417_103639-plo2ya73/logs
INFO flwr 2024-04-17 10:40:04,014 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 10:47:12,098 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=507430)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=507430)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 10:47:16,519	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 10:47:17,335	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 10:47:17,792	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 10:47:18,147	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_09fe326bd084a382.zip' (37.24MiB) to Ray cluster...
2024-04-17 10:47:18,264	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_09fe326bd084a382.zip'.
INFO flwr 2024-04-17 10:47:29,285 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77088809779.0, 'accelerator_type:TITAN': 1.0, 'memory': 169873889485.0, 'GPU': 1.0}
INFO flwr 2024-04-17 10:47:29,285 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 10:47:29,286 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 10:47:29,312 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 10:47:29,316 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 10:47:29,316 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 10:47:29,316 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 10:47:32,449 | server.py:94 | initial parameters (loss, other metrics): 2.3022618293762207, {'accuracy': 0.152, 'data_size': 10000}
INFO flwr 2024-04-17 10:47:32,450 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 10:47:32,450 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=512182)[0m 2024-04-17 10:47:35.292750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=512182)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=512187)[0m 2024-04-17 10:47:37.659953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=512184)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=512184)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=512183)[0m 2024-04-17 10:47:35.582414: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=512183)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=512183)[0m 2024-04-17 10:47:37.890356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 10:48:02,518 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 10:48:03,922 | server.py:125 | fit progress: (1, 2.302187919616699, {'accuracy': 0.1557, 'data_size': 10000}, 31.47170409100363)
INFO flwr 2024-04-17 10:48:03,922 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 10:48:03,922 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:48:18,751 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 10:48:20,108 | server.py:125 | fit progress: (2, 2.302081823348999, {'accuracy': 0.162, 'data_size': 10000}, 47.65838905700366)
INFO flwr 2024-04-17 10:48:20,109 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 10:48:20,109 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:48:34,919 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 10:48:36,065 | server.py:125 | fit progress: (3, 2.3019425868988037, {'accuracy': 0.1714, 'data_size': 10000}, 63.61497444099223)
INFO flwr 2024-04-17 10:48:36,065 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 10:48:36,065 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:48:49,671 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 10:48:51,071 | server.py:125 | fit progress: (4, 2.3017685413360596, {'accuracy': 0.1844, 'data_size': 10000}, 78.62107763299718)
INFO flwr 2024-04-17 10:48:51,071 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 10:48:51,072 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:49:06,388 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 10:49:07,785 | server.py:125 | fit progress: (5, 2.30155611038208, {'accuracy': 0.1947, 'data_size': 10000}, 95.33477465699252)
INFO flwr 2024-04-17 10:49:07,785 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 10:49:07,786 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:49:23,138 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 10:49:24,544 | server.py:125 | fit progress: (6, 2.3013052940368652, {'accuracy': 0.2104, 'data_size': 10000}, 112.09378769400064)
INFO flwr 2024-04-17 10:49:24,544 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 10:49:24,544 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:49:40,290 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 10:49:41,661 | server.py:125 | fit progress: (7, 2.301011323928833, {'accuracy': 0.2315, 'data_size': 10000}, 129.2105682919937)
INFO flwr 2024-04-17 10:49:41,661 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 10:49:41,661 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:49:56,771 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 10:49:58,161 | server.py:125 | fit progress: (8, 2.300673723220825, {'accuracy': 0.2602, 'data_size': 10000}, 145.71104249099153)
INFO flwr 2024-04-17 10:49:58,161 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 10:49:58,161 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:50:13,300 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 10:50:14,680 | server.py:125 | fit progress: (9, 2.300288200378418, {'accuracy': 0.29, 'data_size': 10000}, 162.2303929369955)
INFO flwr 2024-04-17 10:50:14,681 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 10:50:14,681 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:50:29,962 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 10:50:31,350 | server.py:125 | fit progress: (10, 2.299851179122925, {'accuracy': 0.3239, 'data_size': 10000}, 178.90053814700514)
INFO flwr 2024-04-17 10:50:31,351 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 10:50:31,351 | server.py:153 | FL finished in 178.90099837900198
INFO flwr 2024-04-17 10:50:31,351 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 10:50:31,351 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 10:50:31,351 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 10:50:31,351 | app.py:229 | app_fit: losses_centralized [(0, 2.3022618293762207), (1, 2.302187919616699), (2, 2.302081823348999), (3, 2.3019425868988037), (4, 2.3017685413360596), (5, 2.30155611038208), (6, 2.3013052940368652), (7, 2.301011323928833), (8, 2.300673723220825), (9, 2.300288200378418), (10, 2.299851179122925)]
INFO flwr 2024-04-17 10:50:31,352 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.152), (1, 0.1557), (2, 0.162), (3, 0.1714), (4, 0.1844), (5, 0.1947), (6, 0.2104), (7, 0.2315), (8, 0.2602), (9, 0.29), (10, 0.3239)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3239
wandb:     loss 2.29985
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_104711-w658ft1n
wandb: Find logs at: ./wandb/offline-run-20240417_104711-w658ft1n/logs
INFO flwr 2024-04-17 10:50:34,849 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 10:57:43,256 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=512179)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=512179)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 10:57:47,801	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 10:57:48,663	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 10:57:49,150	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 10:57:49,507	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1c2c0ba3e50754fd.zip' (37.25MiB) to Ray cluster...
2024-04-17 10:57:49,631	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1c2c0ba3e50754fd.zip'.
INFO flwr 2024-04-17 10:58:00,661 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77088406732.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169872949044.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 10:58:00,662 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 10:58:00,662 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 10:58:00,683 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 10:58:00,685 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 10:58:00,686 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 10:58:00,686 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 10:58:05,148 | server.py:94 | initial parameters (loss, other metrics): 2.3026068210601807, {'accuracy': 0.1025, 'data_size': 10000}
INFO flwr 2024-04-17 10:58:05,149 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 10:58:05,150 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=516551)[0m 2024-04-17 10:58:06.542266: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=516551)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=516561)[0m 2024-04-17 10:58:09.283461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=516559)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=516559)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=516557)[0m 2024-04-17 10:58:07.101141: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=516557)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=516557)[0m 2024-04-17 10:58:09.473053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 10:58:33,476 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 10:58:34,931 | server.py:125 | fit progress: (1, 2.3026015758514404, {'accuracy': 0.1034, 'data_size': 10000}, 29.782170539998333)
INFO flwr 2024-04-17 10:58:34,932 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 10:58:34,932 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:58:50,395 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 10:58:51,571 | server.py:125 | fit progress: (2, 2.302593469619751, {'accuracy': 0.1051, 'data_size': 10000}, 46.42127424500359)
INFO flwr 2024-04-17 10:58:51,571 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 10:58:51,571 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:59:06,643 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 10:59:08,036 | server.py:125 | fit progress: (3, 2.302584171295166, {'accuracy': 0.1057, 'data_size': 10000}, 62.887155093005276)
INFO flwr 2024-04-17 10:59:08,037 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 10:59:08,037 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:59:23,733 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 10:59:24,923 | server.py:125 | fit progress: (4, 2.302574396133423, {'accuracy': 0.1067, 'data_size': 10000}, 79.77422235200356)
INFO flwr 2024-04-17 10:59:24,924 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 10:59:24,924 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:59:39,954 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 10:59:41,369 | server.py:125 | fit progress: (5, 2.3025636672973633, {'accuracy': 0.1086, 'data_size': 10000}, 96.21969112400257)
INFO flwr 2024-04-17 10:59:41,369 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 10:59:41,369 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 10:59:56,181 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 10:59:57,351 | server.py:125 | fit progress: (6, 2.3025522232055664, {'accuracy': 0.1098, 'data_size': 10000}, 112.20175838599971)
INFO flwr 2024-04-17 10:59:57,351 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 10:59:57,352 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:00:12,189 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 11:00:13,573 | server.py:125 | fit progress: (7, 2.3025400638580322, {'accuracy': 0.111, 'data_size': 10000}, 128.4240923299949)
INFO flwr 2024-04-17 11:00:13,574 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 11:00:13,574 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:00:29,251 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 11:00:30,407 | server.py:125 | fit progress: (8, 2.302527666091919, {'accuracy': 0.1132, 'data_size': 10000}, 145.25738637200266)
INFO flwr 2024-04-17 11:00:30,407 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 11:00:30,407 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:00:46,930 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 11:00:48,379 | server.py:125 | fit progress: (9, 2.3025145530700684, {'accuracy': 0.1143, 'data_size': 10000}, 163.2298656740022)
INFO flwr 2024-04-17 11:00:48,379 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 11:00:48,380 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:01:02,283 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 11:01:03,460 | server.py:125 | fit progress: (10, 2.302501916885376, {'accuracy': 0.1162, 'data_size': 10000}, 178.31051279400708)
INFO flwr 2024-04-17 11:01:03,460 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 11:01:03,460 | server.py:153 | FL finished in 178.3109525530017
INFO flwr 2024-04-17 11:01:03,460 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 11:01:03,461 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 11:01:03,461 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 11:01:03,461 | app.py:229 | app_fit: losses_centralized [(0, 2.3026068210601807), (1, 2.3026015758514404), (2, 2.302593469619751), (3, 2.302584171295166), (4, 2.302574396133423), (5, 2.3025636672973633), (6, 2.3025522232055664), (7, 2.3025400638580322), (8, 2.302527666091919), (9, 2.3025145530700684), (10, 2.302501916885376)]
INFO flwr 2024-04-17 11:01:03,461 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1025), (1, 0.1034), (2, 0.1051), (3, 0.1057), (4, 0.1067), (5, 0.1086), (6, 0.1098), (7, 0.111), (8, 0.1132), (9, 0.1143), (10, 0.1162)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1162
wandb:     loss 2.3025
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_105742-ms54r8n7
wandb: Find logs at: ./wandb/offline-run-20240417_105742-ms54r8n7/logs
INFO flwr 2024-04-17 11:01:06,971 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 11:08:14,748 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=516551)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=516551)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 11:08:20,219	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 11:08:21,032	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 11:08:21,550	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 11:08:21,908	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0265b95c38520dfe.zip' (37.26MiB) to Ray cluster...
2024-04-17 11:08:22,017	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0265b95c38520dfe.zip'.
INFO flwr 2024-04-17 11:08:33,028 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77103937536.0, 'GPU': 1.0, 'memory': 169909187584.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 11:08:33,028 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 11:08:33,028 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 11:08:33,044 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 11:08:33,046 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 11:08:33,046 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 11:08:33,047 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 11:08:36,285 | server.py:94 | initial parameters (loss, other metrics): 2.302543878555298, {'accuracy': 0.1025, 'data_size': 10000}
INFO flwr 2024-04-17 11:08:36,286 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 11:08:36,286 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=520945)[0m 2024-04-17 11:08:39.160248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=520945)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=520945)[0m 2024-04-17 11:08:41.574846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=520946)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=520946)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=520944)[0m 2024-04-17 11:08:39.440744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=520944)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=520944)[0m 2024-04-17 11:08:41.886604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 11:08:56,676 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 11:08:58,101 | server.py:125 | fit progress: (1, 2.262545108795166, {'accuracy': 0.1985, 'data_size': 10000}, 21.815446934997453)
INFO flwr 2024-04-17 11:08:58,102 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 11:08:58,102 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:09:07,139 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 11:09:08,314 | server.py:125 | fit progress: (2, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 32.02809993100527)
INFO flwr 2024-04-17 11:09:08,314 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 11:09:08,314 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:09:16,529 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 11:09:17,914 | server.py:125 | fit progress: (3, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 41.62805254099658)
INFO flwr 2024-04-17 11:09:17,914 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 11:09:17,914 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:09:25,968 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 11:09:27,128 | server.py:125 | fit progress: (4, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 50.84188908099895)
INFO flwr 2024-04-17 11:09:27,128 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 11:09:27,128 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:09:35,150 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 11:09:36,511 | server.py:125 | fit progress: (5, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 60.22469646700483)
INFO flwr 2024-04-17 11:09:36,511 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 11:09:36,511 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:09:44,978 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 11:09:46,353 | server.py:125 | fit progress: (6, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 70.06670153699815)
INFO flwr 2024-04-17 11:09:46,353 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 11:09:46,353 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:09:54,485 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 11:09:55,632 | server.py:125 | fit progress: (7, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 79.3458786560077)
INFO flwr 2024-04-17 11:09:55,632 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 11:09:55,632 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:10:03,729 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 11:10:05,097 | server.py:125 | fit progress: (8, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 88.81167152900889)
INFO flwr 2024-04-17 11:10:05,098 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 11:10:05,098 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:10:13,291 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 11:10:14,446 | server.py:125 | fit progress: (9, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 98.15988375800953)
INFO flwr 2024-04-17 11:10:14,446 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 11:10:14,446 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:10:22,672 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 11:10:24,064 | server.py:125 | fit progress: (10, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 107.77784485599841)
INFO flwr 2024-04-17 11:10:24,064 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 11:10:24,064 | server.py:153 | FL finished in 107.77856509599951
INFO flwr 2024-04-17 11:10:24,065 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 11:10:24,065 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 11:10:24,065 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 11:10:24,065 | app.py:229 | app_fit: losses_centralized [(0, 2.302543878555298), (1, 2.262545108795166), (2, 2.3631420135498047), (3, 2.3631420135498047), (4, 2.3631420135498047), (5, 2.3631420135498047), (6, 2.3631420135498047), (7, 2.3631420135498047), (8, 2.3631420135498047), (9, 2.3631420135498047), (10, 2.3631420135498047)]
INFO flwr 2024-04-17 11:10:24,065 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1025), (1, 0.1985), (2, 0.098), (3, 0.098), (4, 0.098), (5, 0.098), (6, 0.098), (7, 0.098), (8, 0.098), (9, 0.098), (10, 0.098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.098
wandb:     loss 2.36314
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_110814-elb4oezv
wandb: Find logs at: ./wandb/offline-run-20240417_110814-elb4oezv/logs
INFO flwr 2024-04-17 11:10:27,567 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 11:17:35,366 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=520942)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=520942)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 11:17:40,116	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 11:17:40,923	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 11:17:41,354	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 11:17:41,699	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9e4af8120cf4e8aa.zip' (37.27MiB) to Ray cluster...
2024-04-17 11:17:41,814	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9e4af8120cf4e8aa.zip'.
INFO flwr 2024-04-17 11:17:53,088 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77038013644.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169755365172.0, 'CPU': 64.0}
INFO flwr 2024-04-17 11:17:53,089 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 11:17:53,089 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 11:17:53,105 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 11:17:53,106 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 11:17:53,106 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 11:17:53,106 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 11:17:55,532 | server.py:94 | initial parameters (loss, other metrics): 2.3027327060699463, {'accuracy': 0.0684, 'data_size': 10000}
INFO flwr 2024-04-17 11:17:55,533 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 11:17:55,533 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=525669)[0m 2024-04-17 11:17:59.472470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=525669)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=525669)[0m 2024-04-17 11:18:01.774859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=525669)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=525669)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=525665)[0m 2024-04-17 11:17:59.729158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=525665)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=525668)[0m 2024-04-17 11:18:01.999430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 11:18:18,858 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 11:18:20,279 | server.py:125 | fit progress: (1, 2.257495164871216, {'accuracy': 0.1468, 'data_size': 10000}, 24.74580675600737)
INFO flwr 2024-04-17 11:18:20,279 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 11:18:20,280 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:18:29,285 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 11:18:30,456 | server.py:125 | fit progress: (2, 2.0028886795043945, {'accuracy': 0.446, 'data_size': 10000}, 34.923070602002554)
INFO flwr 2024-04-17 11:18:30,457 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 11:18:30,457 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:18:38,574 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 11:18:39,953 | server.py:125 | fit progress: (3, 1.7695783376693726, {'accuracy': 0.6925, 'data_size': 10000}, 44.419519304006826)
INFO flwr 2024-04-17 11:18:39,953 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 11:18:39,953 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:18:48,294 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 11:18:49,470 | server.py:125 | fit progress: (4, 1.7245934009552002, {'accuracy': 0.7346, 'data_size': 10000}, 53.93677637699875)
INFO flwr 2024-04-17 11:18:49,471 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 11:18:49,471 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:18:57,589 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 11:18:59,000 | server.py:125 | fit progress: (5, 1.6586997509002686, {'accuracy': 0.8022, 'data_size': 10000}, 63.46701116100303)
INFO flwr 2024-04-17 11:18:59,001 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 11:18:59,001 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:19:06,841 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 11:19:08,250 | server.py:125 | fit progress: (6, 1.6536786556243896, {'accuracy': 0.8066, 'data_size': 10000}, 72.71703468399937)
INFO flwr 2024-04-17 11:19:08,251 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 11:19:08,251 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:19:16,170 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 11:19:17,544 | server.py:125 | fit progress: (7, 1.6436975002288818, {'accuracy': 0.817, 'data_size': 10000}, 82.01121139500174)
INFO flwr 2024-04-17 11:19:17,545 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 11:19:17,545 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:19:25,246 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 11:19:26,637 | server.py:125 | fit progress: (8, 1.6304454803466797, {'accuracy': 0.8309, 'data_size': 10000}, 91.1042671030009)
INFO flwr 2024-04-17 11:19:26,638 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 11:19:26,638 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:19:34,717 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 11:19:36,109 | server.py:125 | fit progress: (9, 1.6338328123092651, {'accuracy': 0.8268, 'data_size': 10000}, 100.57612884700939)
INFO flwr 2024-04-17 11:19:36,110 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 11:19:36,110 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:19:44,344 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 11:19:45,526 | server.py:125 | fit progress: (10, 1.6300451755523682, {'accuracy': 0.8308, 'data_size': 10000}, 109.99276607800857)
INFO flwr 2024-04-17 11:19:45,526 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 11:19:45,527 | server.py:153 | FL finished in 109.99334297100722
INFO flwr 2024-04-17 11:19:45,527 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 11:19:45,527 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 11:19:45,527 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 11:19:45,527 | app.py:229 | app_fit: losses_centralized [(0, 2.3027327060699463), (1, 2.257495164871216), (2, 2.0028886795043945), (3, 1.7695783376693726), (4, 1.7245934009552002), (5, 1.6586997509002686), (6, 1.6536786556243896), (7, 1.6436975002288818), (8, 1.6304454803466797), (9, 1.6338328123092651), (10, 1.6300451755523682)]
INFO flwr 2024-04-17 11:19:45,527 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0684), (1, 0.1468), (2, 0.446), (3, 0.6925), (4, 0.7346), (5, 0.8022), (6, 0.8066), (7, 0.817), (8, 0.8309), (9, 0.8268), (10, 0.8308)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8308
wandb:     loss 1.63005
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_111734-qd8d0gf4
wandb: Find logs at: ./wandb/offline-run-20240417_111734-qd8d0gf4/logs
INFO flwr 2024-04-17 11:19:49,070 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 11:26:57,044 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=525671)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=525671)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 11:27:02,885	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 11:27:03,722	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 11:27:04,201	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 11:27:04,563	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b29e46b7484c6fd6.zip' (37.28MiB) to Ray cluster...
2024-04-17 11:27:04,679	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b29e46b7484c6fd6.zip'.
INFO flwr 2024-04-17 11:27:15,806 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 169739905229.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77031387955.0}
INFO flwr 2024-04-17 11:27:15,807 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 11:27:15,807 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 11:27:15,828 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 11:27:15,829 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 11:27:15,830 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 11:27:15,830 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 11:27:19,278 | server.py:94 | initial parameters (loss, other metrics): 2.3025317192077637, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-17 11:27:19,278 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 11:27:19,279 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=530045)[0m 2024-04-17 11:27:21.932732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=530045)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=530051)[0m 2024-04-17 11:27:24.857442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=530051)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=530051)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=530053)[0m 2024-04-17 11:27:22.192386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=530053)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=530053)[0m 2024-04-17 11:27:25.082801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 11:27:39,655 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 11:27:40,849 | server.py:125 | fit progress: (1, 2.301422119140625, {'accuracy': 0.1135, 'data_size': 10000}, 21.570190972997807)
INFO flwr 2024-04-17 11:27:40,849 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 11:27:40,849 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:27:49,627 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 11:27:50,994 | server.py:125 | fit progress: (2, 2.2984039783477783, {'accuracy': 0.1655, 'data_size': 10000}, 31.71600310699432)
INFO flwr 2024-04-17 11:27:50,995 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 11:27:50,995 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:27:59,283 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 11:28:00,459 | server.py:125 | fit progress: (3, 2.2915682792663574, {'accuracy': 0.2542, 'data_size': 10000}, 41.18021050799871)
INFO flwr 2024-04-17 11:28:00,459 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 11:28:00,459 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:28:08,448 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 11:28:09,810 | server.py:125 | fit progress: (4, 2.2779877185821533, {'accuracy': 0.3525, 'data_size': 10000}, 50.531456810000236)
INFO flwr 2024-04-17 11:28:09,810 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 11:28:09,810 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:28:17,970 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 11:28:19,407 | server.py:125 | fit progress: (5, 2.251922369003296, {'accuracy': 0.4144, 'data_size': 10000}, 60.12855333299376)
INFO flwr 2024-04-17 11:28:19,407 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 11:28:19,408 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:28:27,655 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 11:28:28,850 | server.py:125 | fit progress: (6, 2.2081260681152344, {'accuracy': 0.4664, 'data_size': 10000}, 69.57152444699022)
INFO flwr 2024-04-17 11:28:28,850 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 11:28:28,851 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:28:37,100 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 11:28:38,262 | server.py:125 | fit progress: (7, 2.1433818340301514, {'accuracy': 0.5194, 'data_size': 10000}, 78.98400474099617)
INFO flwr 2024-04-17 11:28:38,263 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 11:28:38,263 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:28:46,665 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 11:28:48,035 | server.py:125 | fit progress: (8, 2.0645558834075928, {'accuracy': 0.5563, 'data_size': 10000}, 88.75613436099957)
INFO flwr 2024-04-17 11:28:48,035 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 11:28:48,035 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:28:56,105 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 11:28:57,270 | server.py:125 | fit progress: (9, 1.9850455522537231, {'accuracy': 0.5847, 'data_size': 10000}, 97.99162871699082)
INFO flwr 2024-04-17 11:28:57,270 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 11:28:57,271 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:29:05,218 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 11:29:06,576 | server.py:125 | fit progress: (10, 1.9151538610458374, {'accuracy': 0.6097, 'data_size': 10000}, 107.29789090999111)
INFO flwr 2024-04-17 11:29:06,577 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 11:29:06,577 | server.py:153 | FL finished in 107.29836690299271
INFO flwr 2024-04-17 11:29:06,577 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 11:29:06,577 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 11:29:06,577 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 11:29:06,577 | app.py:229 | app_fit: losses_centralized [(0, 2.3025317192077637), (1, 2.301422119140625), (2, 2.2984039783477783), (3, 2.2915682792663574), (4, 2.2779877185821533), (5, 2.251922369003296), (6, 2.2081260681152344), (7, 2.1433818340301514), (8, 2.0645558834075928), (9, 1.9850455522537231), (10, 1.9151538610458374)]
INFO flwr 2024-04-17 11:29:06,578 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1135), (2, 0.1655), (3, 0.2542), (4, 0.3525), (5, 0.4144), (6, 0.4664), (7, 0.5194), (8, 0.5563), (9, 0.5847), (10, 0.6097)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6097
wandb:     loss 1.91515
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_112656-smpkxy9g
wandb: Find logs at: ./wandb/offline-run-20240417_112656-smpkxy9g/logs
INFO flwr 2024-04-17 11:29:10,130 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 11:36:18,141 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=530045)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=530045)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 11:36:22,950	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 11:36:23,949	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 11:36:24,405	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 11:36:24,763	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a3e7a0cd6e75f543.zip' (37.29MiB) to Ray cluster...
2024-04-17 11:36:24,879	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a3e7a0cd6e75f543.zip'.
INFO flwr 2024-04-17 11:36:35,895 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169710057677.0, 'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 77018596147.0}
INFO flwr 2024-04-17 11:36:35,895 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 11:36:35,895 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 11:36:35,914 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 11:36:35,915 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 11:36:35,916 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 11:36:35,916 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 11:36:39,787 | server.py:94 | initial parameters (loss, other metrics): 2.3026363849639893, {'accuracy': 0.1281, 'data_size': 10000}
INFO flwr 2024-04-17 11:36:39,788 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 11:36:39,788 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=534408)[0m 2024-04-17 11:36:42.009348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=534408)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=534408)[0m 2024-04-17 11:36:44.308275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=534405)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=534405)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=534405)[0m 2024-04-17 11:36:42.196037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=534405)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=534410)[0m 2024-04-17 11:36:44.474413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 11:36:59,212 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 11:37:00,628 | server.py:125 | fit progress: (1, 2.302554130554199, {'accuracy': 0.1389, 'data_size': 10000}, 20.840271626992035)
INFO flwr 2024-04-17 11:37:00,629 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 11:37:00,629 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:37:09,198 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 11:37:10,374 | server.py:125 | fit progress: (2, 2.302443504333496, {'accuracy': 0.1481, 'data_size': 10000}, 30.586010668994277)
INFO flwr 2024-04-17 11:37:10,374 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 11:37:10,375 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:37:18,542 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 11:37:19,702 | server.py:125 | fit progress: (3, 2.3022947311401367, {'accuracy': 0.1548, 'data_size': 10000}, 39.913766423997004)
INFO flwr 2024-04-17 11:37:19,702 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 11:37:19,702 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:37:27,768 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 11:37:29,149 | server.py:125 | fit progress: (4, 2.302109479904175, {'accuracy': 0.1595, 'data_size': 10000}, 49.36075458499545)
INFO flwr 2024-04-17 11:37:29,149 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 11:37:29,149 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:37:36,948 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 11:37:38,127 | server.py:125 | fit progress: (5, 2.301881790161133, {'accuracy': 0.1632, 'data_size': 10000}, 58.33905935699295)
INFO flwr 2024-04-17 11:37:38,127 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 11:37:38,128 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:37:46,553 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 11:37:47,930 | server.py:125 | fit progress: (6, 2.3016061782836914, {'accuracy': 0.1669, 'data_size': 10000}, 68.14201845299976)
INFO flwr 2024-04-17 11:37:47,930 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 11:37:47,931 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:37:56,062 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 11:37:57,236 | server.py:125 | fit progress: (7, 2.3012778759002686, {'accuracy': 0.1699, 'data_size': 10000}, 77.44826601599925)
INFO flwr 2024-04-17 11:37:57,237 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 11:37:57,237 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:38:05,433 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 11:38:06,612 | server.py:125 | fit progress: (8, 2.3008973598480225, {'accuracy': 0.173, 'data_size': 10000}, 86.82355294299487)
INFO flwr 2024-04-17 11:38:06,612 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 11:38:06,612 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:38:14,478 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 11:38:15,861 | server.py:125 | fit progress: (9, 2.300457239151001, {'accuracy': 0.1751, 'data_size': 10000}, 96.07343393500196)
INFO flwr 2024-04-17 11:38:15,862 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 11:38:15,862 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:38:24,134 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 11:38:25,292 | server.py:125 | fit progress: (10, 2.299947500228882, {'accuracy': 0.1775, 'data_size': 10000}, 105.5044578389934)
INFO flwr 2024-04-17 11:38:25,293 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 11:38:25,293 | server.py:153 | FL finished in 105.50492589799978
INFO flwr 2024-04-17 11:38:25,293 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 11:38:25,293 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 11:38:25,293 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 11:38:25,294 | app.py:229 | app_fit: losses_centralized [(0, 2.3026363849639893), (1, 2.302554130554199), (2, 2.302443504333496), (3, 2.3022947311401367), (4, 2.302109479904175), (5, 2.301881790161133), (6, 2.3016061782836914), (7, 2.3012778759002686), (8, 2.3008973598480225), (9, 2.300457239151001), (10, 2.299947500228882)]
INFO flwr 2024-04-17 11:38:25,294 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1281), (1, 0.1389), (2, 0.1481), (3, 0.1548), (4, 0.1595), (5, 0.1632), (6, 0.1669), (7, 0.1699), (8, 0.173), (9, 0.1751), (10, 0.1775)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1775
wandb:     loss 2.29995
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_113617-hhbvx4z1
wandb: Find logs at: ./wandb/offline-run-20240417_113617-hhbvx4z1/logs
INFO flwr 2024-04-17 11:38:28,801 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 11:45:36,722 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=534398)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=534398)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 11:45:41,328	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 11:45:42,248	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 11:45:42,719	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 11:45:43,080	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_84cb809adf933729.zip' (37.33MiB) to Ray cluster...
2024-04-17 11:45:43,197	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_84cb809adf933729.zip'.
INFO flwr 2024-04-17 11:45:54,361 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169733987328.0, 'CPU': 64.0, 'object_store_memory': 77028851712.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 11:45:54,361 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 11:45:54,362 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 11:45:54,382 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 11:45:54,384 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 11:45:54,384 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 11:45:54,385 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 11:45:57,914 | server.py:94 | initial parameters (loss, other metrics): 2.302483320236206, {'accuracy': 0.0893, 'data_size': 10000}
INFO flwr 2024-04-17 11:45:57,914 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 11:45:57,915 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=538576)[0m 2024-04-17 11:46:00.481585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=538576)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=538576)[0m 2024-04-17 11:46:02.797182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=538611)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=538611)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=538539)[0m 2024-04-17 11:46:00.908888: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=538539)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=538539)[0m 2024-04-17 11:46:03.141330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 11:46:17,488 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 11:46:18,668 | server.py:125 | fit progress: (1, 2.3024744987487793, {'accuracy': 0.0894, 'data_size': 10000}, 20.75276888201188)
INFO flwr 2024-04-17 11:46:18,668 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 11:46:18,668 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:46:27,541 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 11:46:28,917 | server.py:125 | fit progress: (2, 2.3024635314941406, {'accuracy': 0.0898, 'data_size': 10000}, 31.002086948006763)
INFO flwr 2024-04-17 11:46:28,917 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 11:46:28,917 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:46:36,943 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 11:46:38,329 | server.py:125 | fit progress: (3, 2.302449941635132, {'accuracy': 0.0908, 'data_size': 10000}, 40.41408981301356)
INFO flwr 2024-04-17 11:46:38,329 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 11:46:38,329 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:46:46,307 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 11:46:47,681 | server.py:125 | fit progress: (4, 2.3024346828460693, {'accuracy': 0.0924, 'data_size': 10000}, 49.766001745010726)
INFO flwr 2024-04-17 11:46:47,681 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 11:46:47,681 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:46:55,509 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 11:46:56,882 | server.py:125 | fit progress: (5, 2.3024184703826904, {'accuracy': 0.0935, 'data_size': 10000}, 58.96671976000653)
INFO flwr 2024-04-17 11:46:56,882 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 11:46:56,882 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:47:05,184 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 11:47:06,340 | server.py:125 | fit progress: (6, 2.302401065826416, {'accuracy': 0.0952, 'data_size': 10000}, 68.42478620700422)
INFO flwr 2024-04-17 11:47:06,340 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 11:47:06,340 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:47:14,587 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 11:47:15,748 | server.py:125 | fit progress: (7, 2.3023829460144043, {'accuracy': 0.0963, 'data_size': 10000}, 77.83285781400627)
INFO flwr 2024-04-17 11:47:15,748 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 11:47:15,748 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:47:24,051 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 11:47:25,456 | server.py:125 | fit progress: (8, 2.3023645877838135, {'accuracy': 0.097, 'data_size': 10000}, 87.54073892401357)
INFO flwr 2024-04-17 11:47:25,456 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 11:47:25,456 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:47:33,639 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 11:47:35,029 | server.py:125 | fit progress: (9, 2.3023457527160645, {'accuracy': 0.0994, 'data_size': 10000}, 97.11403536199941)
INFO flwr 2024-04-17 11:47:35,029 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 11:47:35,029 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:47:43,417 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 11:47:44,579 | server.py:125 | fit progress: (10, 2.302325487136841, {'accuracy': 0.1003, 'data_size': 10000}, 106.66385633700702)
INFO flwr 2024-04-17 11:47:44,579 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 11:47:44,579 | server.py:153 | FL finished in 106.66429393801081
INFO flwr 2024-04-17 11:47:44,579 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 11:47:44,579 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 11:47:44,580 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 11:47:44,580 | app.py:229 | app_fit: losses_centralized [(0, 2.302483320236206), (1, 2.3024744987487793), (2, 2.3024635314941406), (3, 2.302449941635132), (4, 2.3024346828460693), (5, 2.3024184703826904), (6, 2.302401065826416), (7, 2.3023829460144043), (8, 2.3023645877838135), (9, 2.3023457527160645), (10, 2.302325487136841)]
INFO flwr 2024-04-17 11:47:44,580 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0893), (1, 0.0894), (2, 0.0898), (3, 0.0908), (4, 0.0924), (5, 0.0935), (6, 0.0952), (7, 0.0963), (8, 0.097), (9, 0.0994), (10, 0.1003)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1003
wandb:     loss 2.30233
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_114536-7kytxemp
wandb: Find logs at: ./wandb/offline-run-20240417_114536-7kytxemp/logs
INFO flwr 2024-04-17 11:47:48,122 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 11:54:55,688 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=538531)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=538531)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 11:55:00,394	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 11:55:01,218	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 11:55:01,708	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 11:55:02,071	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5c64af666ef703c7.zip' (37.34MiB) to Ray cluster...
2024-04-17 11:55:02,187	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5c64af666ef703c7.zip'.
INFO flwr 2024-04-17 11:55:13,345 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169732464845.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77028199219.0, 'GPU': 1.0}
INFO flwr 2024-04-17 11:55:13,346 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 11:55:13,346 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 11:55:13,363 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 11:55:13,364 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 11:55:13,364 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 11:55:13,364 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 11:55:16,748 | server.py:94 | initial parameters (loss, other metrics): 2.302715539932251, {'accuracy': 0.1056, 'data_size': 10000}
INFO flwr 2024-04-17 11:55:16,758 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 11:55:16,759 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=542899)[0m 2024-04-17 11:55:19.458721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=542899)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=542898)[0m 2024-04-17 11:55:21.736689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=542898)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=542898)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=542896)[0m 2024-04-17 11:55:19.751486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=542896)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=542896)[0m 2024-04-17 11:55:22.191001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 11:55:36,724 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 11:55:37,889 | server.py:125 | fit progress: (1, 2.159142017364502, {'accuracy': 0.3019, 'data_size': 10000}, 21.13004726100189)
INFO flwr 2024-04-17 11:55:37,889 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 11:55:37,889 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:55:46,819 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 11:55:48,213 | server.py:125 | fit progress: (2, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 31.454723963004653)
INFO flwr 2024-04-17 11:55:48,214 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 11:55:48,214 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:55:56,498 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 11:55:57,688 | server.py:125 | fit progress: (3, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 40.92878708599892)
INFO flwr 2024-04-17 11:55:57,688 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 11:55:57,688 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:56:05,407 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 11:56:06,773 | server.py:125 | fit progress: (4, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 50.01470657499158)
INFO flwr 2024-04-17 11:56:06,774 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 11:56:06,774 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:56:14,686 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 11:56:15,858 | server.py:125 | fit progress: (5, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 59.09877453499939)
INFO flwr 2024-04-17 11:56:15,858 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 11:56:15,858 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:56:23,865 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 11:56:25,245 | server.py:125 | fit progress: (6, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 68.48656532799941)
INFO flwr 2024-04-17 11:56:25,246 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 11:56:25,246 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:56:33,491 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 11:56:34,691 | server.py:125 | fit progress: (7, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 77.93230446000234)
INFO flwr 2024-04-17 11:56:34,691 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 11:56:34,692 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:56:43,091 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 11:56:44,475 | server.py:125 | fit progress: (8, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 87.71618270900217)
INFO flwr 2024-04-17 11:56:44,475 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 11:56:44,475 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:56:52,387 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 11:56:53,782 | server.py:125 | fit progress: (9, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 97.02317375199345)
INFO flwr 2024-04-17 11:56:53,782 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 11:56:53,782 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 11:57:02,184 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 11:57:03,621 | server.py:125 | fit progress: (10, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 106.86189906099753)
INFO flwr 2024-04-17 11:57:03,621 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 11:57:03,621 | server.py:153 | FL finished in 106.86236376200395
INFO flwr 2024-04-17 11:57:03,621 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 11:57:03,621 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 11:57:03,622 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 11:57:03,622 | app.py:229 | app_fit: losses_centralized [(0, 2.302715539932251), (1, 2.159142017364502), (2, 2.365341901779175), (3, 2.365341901779175), (4, 2.365341901779175), (5, 2.365341901779175), (6, 2.365341901779175), (7, 2.365341901779175), (8, 2.365341901779175), (9, 2.365341901779175), (10, 2.365341901779175)]
INFO flwr 2024-04-17 11:57:03,622 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1056), (1, 0.3019), (2, 0.0958), (3, 0.0958), (4, 0.0958), (5, 0.0958), (6, 0.0958), (7, 0.0958), (8, 0.0958), (9, 0.0958), (10, 0.0958)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0958
wandb:     loss 2.36534
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_115455-s4yo793b
wandb: Find logs at: ./wandb/offline-run-20240417_115455-s4yo793b/logs
INFO flwr 2024-04-17 11:57:07,186 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 12:04:16,883 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=542894)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=542894)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 12:04:23,373	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 12:04:24,799	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 12:04:25,292	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 12:04:25,666	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b32e809d5f608570.zip' (37.50MiB) to Ray cluster...
2024-04-17 12:04:25,788	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b32e809d5f608570.zip'.
INFO flwr 2024-04-17 12:04:36,998 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169724949914.0, 'GPU': 1.0, 'object_store_memory': 77024978534.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 12:04:36,998 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 12:04:36,998 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 12:04:37,016 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 12:04:37,017 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 12:04:37,018 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 12:04:37,018 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 12:04:39,802 | server.py:94 | initial parameters (loss, other metrics): 2.302417039871216, {'accuracy': 0.1395, 'data_size': 10000}
INFO flwr 2024-04-17 12:04:39,802 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 12:04:39,802 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=550419)[0m 2024-04-17 12:04:45.142398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=550419)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=550419)[0m 2024-04-17 12:04:48.402077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=550419)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=550419)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=550424)[0m 2024-04-17 12:04:45.147216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=550424)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=550424)[0m 2024-04-17 12:04:48.402090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 12:05:08,616 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 12:05:10,063 | server.py:125 | fit progress: (1, 2.2007124423980713, {'accuracy': 0.356, 'data_size': 10000}, 30.260972563992254)
INFO flwr 2024-04-17 12:05:10,063 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 12:05:10,064 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:05:18,980 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 12:05:20,222 | server.py:125 | fit progress: (2, 1.9537231922149658, {'accuracy': 0.4979, 'data_size': 10000}, 40.419913623001776)
INFO flwr 2024-04-17 12:05:20,222 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 12:05:20,223 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:05:28,454 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 12:05:29,842 | server.py:125 | fit progress: (3, 1.7541515827178955, {'accuracy': 0.7092, 'data_size': 10000}, 50.040024639994954)
INFO flwr 2024-04-17 12:05:29,842 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 12:05:29,843 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:05:38,158 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 12:05:39,307 | server.py:125 | fit progress: (4, 1.673417329788208, {'accuracy': 0.7852, 'data_size': 10000}, 59.50509668099403)
INFO flwr 2024-04-17 12:05:39,308 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 12:05:39,308 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:05:47,332 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 12:05:48,514 | server.py:125 | fit progress: (5, 1.6195979118347168, {'accuracy': 0.8402, 'data_size': 10000}, 68.71236737200525)
INFO flwr 2024-04-17 12:05:48,515 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 12:05:48,515 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:05:56,613 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 12:05:58,002 | server.py:125 | fit progress: (6, 1.5914698839187622, {'accuracy': 0.8685, 'data_size': 10000}, 78.19989426099346)
INFO flwr 2024-04-17 12:05:58,002 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 12:05:58,003 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:06:06,256 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 12:06:07,659 | server.py:125 | fit progress: (7, 1.5949102640151978, {'accuracy': 0.8664, 'data_size': 10000}, 87.85648301799665)
INFO flwr 2024-04-17 12:06:07,659 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 12:06:07,659 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:06:15,736 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 12:06:16,894 | server.py:125 | fit progress: (8, 1.5926364660263062, {'accuracy': 0.8679, 'data_size': 10000}, 97.09196916800283)
INFO flwr 2024-04-17 12:06:16,894 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 12:06:16,895 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:06:24,816 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 12:06:25,976 | server.py:125 | fit progress: (9, 1.561948537826538, {'accuracy': 0.8989, 'data_size': 10000}, 106.17380935199617)
INFO flwr 2024-04-17 12:06:25,976 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 12:06:25,976 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:06:34,392 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 12:06:35,934 | server.py:125 | fit progress: (10, 1.5640580654144287, {'accuracy': 0.8974, 'data_size': 10000}, 116.13160196000536)
INFO flwr 2024-04-17 12:06:35,934 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 12:06:35,934 | server.py:153 | FL finished in 116.1320193940046
INFO flwr 2024-04-17 12:06:35,934 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 12:06:35,934 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 12:06:35,935 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 12:06:35,935 | app.py:229 | app_fit: losses_centralized [(0, 2.302417039871216), (1, 2.2007124423980713), (2, 1.9537231922149658), (3, 1.7541515827178955), (4, 1.673417329788208), (5, 1.6195979118347168), (6, 1.5914698839187622), (7, 1.5949102640151978), (8, 1.5926364660263062), (9, 1.561948537826538), (10, 1.5640580654144287)]
INFO flwr 2024-04-17 12:06:35,935 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1395), (1, 0.356), (2, 0.4979), (3, 0.7092), (4, 0.7852), (5, 0.8402), (6, 0.8685), (7, 0.8664), (8, 0.8679), (9, 0.8989), (10, 0.8974)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8974
wandb:     loss 1.56406
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_120416-88jqswwb
wandb: Find logs at: ./wandb/offline-run-20240417_120416-88jqswwb/logs
INFO flwr 2024-04-17 12:06:39,562 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 12:13:47,621 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=550424)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=550424)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 12:13:52,431	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 12:13:53,300	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 12:13:53,859	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 12:13:54,232	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d35295fab5fc6445.zip' (37.66MiB) to Ray cluster...
2024-04-17 12:13:54,347	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d35295fab5fc6445.zip'.
INFO flwr 2024-04-17 12:14:05,375 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170010511565.0, 'object_store_memory': 77147362099.0}
INFO flwr 2024-04-17 12:14:05,375 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 12:14:05,375 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 12:14:05,395 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 12:14:05,396 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 12:14:05,397 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 12:14:05,397 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 12:14:08,755 | server.py:94 | initial parameters (loss, other metrics): 2.302820920944214, {'accuracy': 0.0755, 'data_size': 10000}
INFO flwr 2024-04-17 12:14:08,755 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 12:14:08,756 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=556077)[0m 2024-04-17 12:14:11.489694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=556077)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=556086)[0m 2024-04-17 12:14:13.817173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=556086)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=556086)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=556082)[0m 2024-04-17 12:14:11.742090: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=556082)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=556082)[0m 2024-04-17 12:14:13.971939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 12:14:28,306 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 12:14:29,679 | server.py:125 | fit progress: (1, 2.301494598388672, {'accuracy': 0.1332, 'data_size': 10000}, 20.92308499700448)
INFO flwr 2024-04-17 12:14:29,679 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 12:14:29,679 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:14:38,917 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 12:14:40,415 | server.py:125 | fit progress: (2, 2.2978122234344482, {'accuracy': 0.235, 'data_size': 10000}, 31.659256283004652)
INFO flwr 2024-04-17 12:14:40,415 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 12:14:40,415 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:14:48,747 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 12:14:50,149 | server.py:125 | fit progress: (3, 2.2897756099700928, {'accuracy': 0.4158, 'data_size': 10000}, 41.39383098699909)
INFO flwr 2024-04-17 12:14:50,150 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 12:14:50,150 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:14:58,335 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 12:14:59,719 | server.py:125 | fit progress: (4, 2.2742373943328857, {'accuracy': 0.5317, 'data_size': 10000}, 50.96376766100002)
INFO flwr 2024-04-17 12:14:59,720 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 12:14:59,720 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:15:07,722 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 12:15:08,886 | server.py:125 | fit progress: (5, 2.2463696002960205, {'accuracy': 0.5974, 'data_size': 10000}, 60.129837444008444)
INFO flwr 2024-04-17 12:15:08,886 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 12:15:08,886 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:15:17,229 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 12:15:18,619 | server.py:125 | fit progress: (6, 2.199544668197632, {'accuracy': 0.6316, 'data_size': 10000}, 69.86314313100593)
INFO flwr 2024-04-17 12:15:18,619 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 12:15:18,619 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:15:26,766 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 12:15:27,945 | server.py:125 | fit progress: (7, 2.128493070602417, {'accuracy': 0.6392, 'data_size': 10000}, 79.18915639500483)
INFO flwr 2024-04-17 12:15:27,945 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 12:15:27,945 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:15:36,166 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 12:15:37,357 | server.py:125 | fit progress: (8, 2.0390963554382324, {'accuracy': 0.6469, 'data_size': 10000}, 88.60090609900362)
INFO flwr 2024-04-17 12:15:37,357 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 12:15:37,357 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:15:45,595 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 12:15:46,981 | server.py:125 | fit progress: (9, 1.9483612775802612, {'accuracy': 0.6666, 'data_size': 10000}, 98.2249421029992)
INFO flwr 2024-04-17 12:15:46,981 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 12:15:46,981 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:15:54,929 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 12:15:56,094 | server.py:125 | fit progress: (10, 1.8729695081710815, {'accuracy': 0.6874, 'data_size': 10000}, 107.3379594200087)
INFO flwr 2024-04-17 12:15:56,094 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 12:15:56,094 | server.py:153 | FL finished in 107.33843204300501
INFO flwr 2024-04-17 12:15:56,094 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 12:15:56,094 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 12:15:56,095 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 12:15:56,095 | app.py:229 | app_fit: losses_centralized [(0, 2.302820920944214), (1, 2.301494598388672), (2, 2.2978122234344482), (3, 2.2897756099700928), (4, 2.2742373943328857), (5, 2.2463696002960205), (6, 2.199544668197632), (7, 2.128493070602417), (8, 2.0390963554382324), (9, 1.9483612775802612), (10, 1.8729695081710815)]
INFO flwr 2024-04-17 12:15:56,095 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0755), (1, 0.1332), (2, 0.235), (3, 0.4158), (4, 0.5317), (5, 0.5974), (6, 0.6316), (7, 0.6392), (8, 0.6469), (9, 0.6666), (10, 0.6874)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6874
wandb:     loss 1.87297
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_121347-18ilihv5
wandb: Find logs at: ./wandb/offline-run-20240417_121347-18ilihv5/logs
INFO flwr 2024-04-17 12:15:59,646 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 12:23:08,267 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=556077)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=556077)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 12:23:18,426	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 12:23:19,820	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 12:23:20,395	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 12:23:20,776	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fb3c6d5c97e12209.zip' (37.81MiB) to Ray cluster...
2024-04-17 12:23:20,894	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fb3c6d5c97e12209.zip'.
INFO flwr 2024-04-17 12:23:31,946 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77543073792.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 170933838848.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 12:23:31,946 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 12:23:31,946 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 12:23:31,962 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 12:23:31,963 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 12:23:31,964 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 12:23:31,964 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 12:23:34,999 | server.py:94 | initial parameters (loss, other metrics): 2.3025619983673096, {'accuracy': 0.1093, 'data_size': 10000}
INFO flwr 2024-04-17 12:23:35,000 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 12:23:35,001 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=561717)[0m 2024-04-17 12:23:38.663782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=561717)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=561717)[0m 2024-04-17 12:23:41.774650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=561717)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=561717)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=561723)[0m 2024-04-17 12:23:38.709929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=561723)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=561719)[0m 2024-04-17 12:23:41.774652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 12:23:59,426 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 12:24:00,840 | server.py:125 | fit progress: (1, 2.3024773597717285, {'accuracy': 0.1233, 'data_size': 10000}, 25.838778826000635)
INFO flwr 2024-04-17 12:24:00,840 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 12:24:00,840 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:24:09,366 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 12:24:10,742 | server.py:125 | fit progress: (2, 2.3023521900177, {'accuracy': 0.1422, 'data_size': 10000}, 35.74164711499179)
INFO flwr 2024-04-17 12:24:10,743 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 12:24:10,743 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:24:19,078 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 12:24:20,452 | server.py:125 | fit progress: (3, 2.3021955490112305, {'accuracy': 0.1682, 'data_size': 10000}, 45.45168551299139)
INFO flwr 2024-04-17 12:24:20,453 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 12:24:20,453 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:24:28,279 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 12:24:29,647 | server.py:125 | fit progress: (4, 2.302009105682373, {'accuracy': 0.1911, 'data_size': 10000}, 54.64637370999844)
INFO flwr 2024-04-17 12:24:29,647 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 12:24:29,648 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:24:37,763 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 12:24:39,170 | server.py:125 | fit progress: (5, 2.3017852306365967, {'accuracy': 0.2084, 'data_size': 10000}, 64.16962613898795)
INFO flwr 2024-04-17 12:24:39,171 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 12:24:39,171 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:24:47,414 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 12:24:48,619 | server.py:125 | fit progress: (6, 2.301515817642212, {'accuracy': 0.2324, 'data_size': 10000}, 73.61804148698866)
INFO flwr 2024-04-17 12:24:48,619 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 12:24:48,619 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:24:56,629 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 12:24:58,020 | server.py:125 | fit progress: (7, 2.3011934757232666, {'accuracy': 0.2605, 'data_size': 10000}, 83.01881855700049)
INFO flwr 2024-04-17 12:24:58,020 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 12:24:58,020 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:25:06,110 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 12:25:07,264 | server.py:125 | fit progress: (8, 2.300811290740967, {'accuracy': 0.2865, 'data_size': 10000}, 92.26374624499294)
INFO flwr 2024-04-17 12:25:07,265 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 12:25:07,265 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:25:15,190 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 12:25:16,367 | server.py:125 | fit progress: (9, 2.3003721237182617, {'accuracy': 0.3077, 'data_size': 10000}, 101.36584381498687)
INFO flwr 2024-04-17 12:25:16,367 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 12:25:16,367 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:25:24,092 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 12:25:25,458 | server.py:125 | fit progress: (10, 2.299863576889038, {'accuracy': 0.32, 'data_size': 10000}, 110.45769066098728)
INFO flwr 2024-04-17 12:25:25,459 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 12:25:25,459 | server.py:153 | FL finished in 110.45819267399202
INFO flwr 2024-04-17 12:25:25,459 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 12:25:25,459 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 12:25:25,459 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 12:25:25,460 | app.py:229 | app_fit: losses_centralized [(0, 2.3025619983673096), (1, 2.3024773597717285), (2, 2.3023521900177), (3, 2.3021955490112305), (4, 2.302009105682373), (5, 2.3017852306365967), (6, 2.301515817642212), (7, 2.3011934757232666), (8, 2.300811290740967), (9, 2.3003721237182617), (10, 2.299863576889038)]
INFO flwr 2024-04-17 12:25:25,460 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1093), (1, 0.1233), (2, 0.1422), (3, 0.1682), (4, 0.1911), (5, 0.2084), (6, 0.2324), (7, 0.2605), (8, 0.2865), (9, 0.3077), (10, 0.32)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.32
wandb:     loss 2.29986
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_122307-6wlxnj8s
wandb: Find logs at: ./wandb/offline-run-20240417_122307-6wlxnj8s/logs
INFO flwr 2024-04-17 12:25:29,022 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 12:32:37,562 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=561719)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=561719)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 12:32:46,419	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 12:32:47,785	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 12:32:48,274	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 12:32:48,652	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c2ea935e49e6f850.zip' (37.97MiB) to Ray cluster...
2024-04-17 12:32:48,770	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c2ea935e49e6f850.zip'.
INFO flwr 2024-04-17 12:32:59,872 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77546234265.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 170941213287.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 12:32:59,872 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 12:32:59,872 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 12:32:59,890 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 12:32:59,891 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 12:32:59,892 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 12:32:59,892 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 12:33:03,181 | server.py:94 | initial parameters (loss, other metrics): 2.3024368286132812, {'accuracy': 0.093, 'data_size': 10000}
INFO flwr 2024-04-17 12:33:03,182 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 12:33:03,183 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=566166)[0m 2024-04-17 12:33:06.204868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=566166)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=566166)[0m 2024-04-17 12:33:08.884004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=566166)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=566166)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=566162)[0m 2024-04-17 12:33:06.353833: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=566162)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=566163)[0m 2024-04-17 12:33:08.884003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 12:33:25,860 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 12:33:27,291 | server.py:125 | fit progress: (1, 2.302428722381592, {'accuracy': 0.0932, 'data_size': 10000}, 24.108368992005126)
INFO flwr 2024-04-17 12:33:27,291 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 12:33:27,291 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:33:36,168 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 12:33:37,585 | server.py:125 | fit progress: (2, 2.3024187088012695, {'accuracy': 0.0938, 'data_size': 10000}, 34.40206251900236)
INFO flwr 2024-04-17 12:33:37,585 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 12:33:37,585 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:33:45,563 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 12:33:46,956 | server.py:125 | fit progress: (3, 2.3024065494537354, {'accuracy': 0.0945, 'data_size': 10000}, 43.77404408999428)
INFO flwr 2024-04-17 12:33:46,957 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 12:33:46,957 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:33:55,124 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 12:33:56,497 | server.py:125 | fit progress: (4, 2.3023927211761475, {'accuracy': 0.0947, 'data_size': 10000}, 53.31411840500368)
INFO flwr 2024-04-17 12:33:56,497 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 12:33:56,497 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:34:04,187 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 12:34:05,566 | server.py:125 | fit progress: (5, 2.302377939224243, {'accuracy': 0.0957, 'data_size': 10000}, 62.383744110004045)
INFO flwr 2024-04-17 12:34:05,567 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 12:34:05,567 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:34:13,942 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 12:34:15,368 | server.py:125 | fit progress: (6, 2.302363157272339, {'accuracy': 0.0969, 'data_size': 10000}, 72.1858032829914)
INFO flwr 2024-04-17 12:34:15,369 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 12:34:15,369 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:34:23,428 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 12:34:24,583 | server.py:125 | fit progress: (7, 2.3023462295532227, {'accuracy': 0.0978, 'data_size': 10000}, 81.40103804999671)
INFO flwr 2024-04-17 12:34:24,584 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 12:34:24,584 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:34:32,495 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 12:34:33,907 | server.py:125 | fit progress: (8, 2.3023295402526855, {'accuracy': 0.0986, 'data_size': 10000}, 90.7250051949959)
INFO flwr 2024-04-17 12:34:33,908 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 12:34:33,908 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:34:41,690 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 12:34:42,858 | server.py:125 | fit progress: (9, 2.3023123741149902, {'accuracy': 0.0993, 'data_size': 10000}, 99.67540897599247)
INFO flwr 2024-04-17 12:34:42,858 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 12:34:42,858 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:34:50,979 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 12:34:52,146 | server.py:125 | fit progress: (10, 2.3022937774658203, {'accuracy': 0.1007, 'data_size': 10000}, 108.96362984800362)
INFO flwr 2024-04-17 12:34:52,146 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 12:34:52,147 | server.py:153 | FL finished in 108.96409371000482
INFO flwr 2024-04-17 12:34:52,147 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 12:34:52,147 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 12:34:52,147 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 12:34:52,147 | app.py:229 | app_fit: losses_centralized [(0, 2.3024368286132812), (1, 2.302428722381592), (2, 2.3024187088012695), (3, 2.3024065494537354), (4, 2.3023927211761475), (5, 2.302377939224243), (6, 2.302363157272339), (7, 2.3023462295532227), (8, 2.3023295402526855), (9, 2.3023123741149902), (10, 2.3022937774658203)]
INFO flwr 2024-04-17 12:34:52,147 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.093), (1, 0.0932), (2, 0.0938), (3, 0.0945), (4, 0.0947), (5, 0.0957), (6, 0.0969), (7, 0.0978), (8, 0.0986), (9, 0.0993), (10, 0.1007)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1007
wandb:     loss 2.30229
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_123237-hht7fhja
wandb: Find logs at: ./wandb/offline-run-20240417_123237-hht7fhja/logs
INFO flwr 2024-04-17 12:34:55,670 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 12:42:05,604 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=566163)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=566163)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 12:42:12,687	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 12:42:14,078	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 12:42:14,571	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 12:42:14,959	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d05429e67b5c72a0.zip' (38.12MiB) to Ray cluster...
2024-04-17 12:42:15,073	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d05429e67b5c72a0.zip'.
INFO flwr 2024-04-17 12:42:26,216 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169770813645.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77044634419.0, 'GPU': 1.0}
INFO flwr 2024-04-17 12:42:26,216 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 12:42:26,216 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 12:42:26,236 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 12:42:26,237 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 12:42:26,238 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 12:42:26,238 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 12:42:29,658 | server.py:94 | initial parameters (loss, other metrics): 2.3026111125946045, {'accuracy': 0.0967, 'data_size': 10000}
INFO flwr 2024-04-17 12:42:29,658 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 12:42:29,659 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=574595)[0m 2024-04-17 12:42:32.810935: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=574595)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=574595)[0m 2024-04-17 12:42:35.693882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=574595)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=574595)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=574605)[0m 2024-04-17 12:42:32.929756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=574605)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=574605)[0m 2024-04-17 12:42:35.693882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 12:42:53,626 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 12:42:55,088 | server.py:125 | fit progress: (1, 2.3358590602874756, {'accuracy': 0.1252, 'data_size': 10000}, 25.42974236700684)
INFO flwr 2024-04-17 12:42:55,089 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 12:42:55,089 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:43:04,349 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 12:43:05,559 | server.py:125 | fit progress: (2, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 35.90022060800402)
INFO flwr 2024-04-17 12:43:05,559 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 12:43:05,559 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:43:14,008 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 12:43:15,201 | server.py:125 | fit progress: (3, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 45.542738500997075)
INFO flwr 2024-04-17 12:43:15,202 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 12:43:15,202 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:43:23,614 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 12:43:25,040 | server.py:125 | fit progress: (4, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 55.38170507400355)
INFO flwr 2024-04-17 12:43:25,041 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 12:43:25,041 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:43:33,389 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 12:43:34,797 | server.py:125 | fit progress: (5, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 65.13850144200842)
INFO flwr 2024-04-17 12:43:34,797 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 12:43:34,798 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:43:43,296 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 12:43:44,481 | server.py:125 | fit progress: (6, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 74.82280853200064)
INFO flwr 2024-04-17 12:43:44,482 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 12:43:44,482 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:43:52,947 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 12:43:54,135 | server.py:125 | fit progress: (7, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 84.47634401899995)
INFO flwr 2024-04-17 12:43:54,135 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 12:43:54,136 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:44:02,348 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 12:44:03,768 | server.py:125 | fit progress: (8, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 94.10917271299695)
INFO flwr 2024-04-17 12:44:03,768 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 12:44:03,768 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:44:12,364 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 12:44:13,786 | server.py:125 | fit progress: (9, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 104.12747306699748)
INFO flwr 2024-04-17 12:44:13,786 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 12:44:13,787 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:44:22,230 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 12:44:23,451 | server.py:125 | fit progress: (10, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 113.79248708000523)
INFO flwr 2024-04-17 12:44:23,452 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 12:44:23,452 | server.py:153 | FL finished in 113.79304643700016
INFO flwr 2024-04-17 12:44:23,452 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 12:44:23,452 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 12:44:23,452 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 12:44:23,452 | app.py:229 | app_fit: losses_centralized [(0, 2.3026111125946045), (1, 2.3358590602874756), (2, 2.3579421043395996), (3, 2.3579421043395996), (4, 2.3579421043395996), (5, 2.3579421043395996), (6, 2.3579421043395996), (7, 2.3579421043395996), (8, 2.3579421043395996), (9, 2.3579421043395996), (10, 2.3579421043395996)]
INFO flwr 2024-04-17 12:44:23,452 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0967), (1, 0.1252), (2, 0.1032), (3, 0.1032), (4, 0.1032), (5, 0.1032), (6, 0.1032), (7, 0.1032), (8, 0.1032), (9, 0.1032), (10, 0.1032)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1032
wandb:     loss 2.35794
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_124205-utf4clrn
wandb: Find logs at: ./wandb/offline-run-20240417_124205-utf4clrn/logs
INFO flwr 2024-04-17 12:44:27,023 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 12:51:35,999 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=574605)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=574605)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 12:51:40,755	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 12:51:41,586	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 12:51:42,059	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 12:51:42,429	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_058f249adf2445e2.zip' (38.28MiB) to Ray cluster...
2024-04-17 12:51:42,542	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_058f249adf2445e2.zip'.
INFO flwr 2024-04-17 12:51:53,585 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 76644849254.0, 'memory': 168837981594.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 12:51:53,586 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 12:51:53,586 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 12:51:53,603 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 12:51:53,605 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 12:51:53,605 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 12:51:53,605 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 12:51:56,966 | server.py:94 | initial parameters (loss, other metrics): 2.302750587463379, {'accuracy': 0.0967, 'data_size': 10000}
INFO flwr 2024-04-17 12:51:56,966 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 12:51:56,967 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=585308)[0m 2024-04-17 12:52:01.236824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=585308)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=585308)[0m 2024-04-17 12:52:03.590033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=585308)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=585308)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=585309)[0m 2024-04-17 12:52:01.496000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=585309)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=585309)[0m 2024-04-17 12:52:03.930561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 12:52:18,854 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 12:52:20,067 | server.py:125 | fit progress: (1, 2.1910221576690674, {'accuracy': 0.3584, 'data_size': 10000}, 23.100131040002452)
INFO flwr 2024-04-17 12:52:20,067 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 12:52:20,067 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:52:29,083 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 12:52:30,466 | server.py:125 | fit progress: (2, 1.8888477087020874, {'accuracy': 0.5832, 'data_size': 10000}, 33.499447250011144)
INFO flwr 2024-04-17 12:52:30,466 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 12:52:30,467 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:52:38,528 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 12:52:39,725 | server.py:125 | fit progress: (3, 1.7398234605789185, {'accuracy': 0.7199, 'data_size': 10000}, 42.758024720009416)
INFO flwr 2024-04-17 12:52:39,725 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 12:52:39,725 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:52:48,007 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 12:52:49,390 | server.py:125 | fit progress: (4, 1.642512321472168, {'accuracy': 0.8178, 'data_size': 10000}, 52.42371065101179)
INFO flwr 2024-04-17 12:52:49,391 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 12:52:49,391 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:52:57,263 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 12:52:58,436 | server.py:125 | fit progress: (5, 1.618922472000122, {'accuracy': 0.8414, 'data_size': 10000}, 61.469674029009184)
INFO flwr 2024-04-17 12:52:58,437 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 12:52:58,437 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:53:06,954 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 12:53:08,337 | server.py:125 | fit progress: (6, 1.5939340591430664, {'accuracy': 0.8674, 'data_size': 10000}, 71.36994310900627)
INFO flwr 2024-04-17 12:53:08,337 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 12:53:08,337 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:53:16,528 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 12:53:17,692 | server.py:125 | fit progress: (7, 1.5541549921035767, {'accuracy': 0.906, 'data_size': 10000}, 80.72548645500501)
INFO flwr 2024-04-17 12:53:17,692 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 12:53:17,693 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:53:25,803 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 12:53:27,184 | server.py:125 | fit progress: (8, 1.546172022819519, {'accuracy': 0.9151, 'data_size': 10000}, 90.21762355600367)
INFO flwr 2024-04-17 12:53:27,185 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 12:53:27,185 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:53:35,259 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 12:53:36,438 | server.py:125 | fit progress: (9, 1.5414971113204956, {'accuracy': 0.9197, 'data_size': 10000}, 99.47160792200884)
INFO flwr 2024-04-17 12:53:36,438 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 12:53:36,439 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 12:53:44,664 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 12:53:46,080 | server.py:125 | fit progress: (10, 1.5367326736450195, {'accuracy': 0.9246, 'data_size': 10000}, 109.11306324900943)
INFO flwr 2024-04-17 12:53:46,080 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 12:53:46,080 | server.py:153 | FL finished in 109.11357327600126
INFO flwr 2024-04-17 12:53:46,080 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 12:53:46,081 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 12:53:46,081 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 12:53:46,081 | app.py:229 | app_fit: losses_centralized [(0, 2.302750587463379), (1, 2.1910221576690674), (2, 1.8888477087020874), (3, 1.7398234605789185), (4, 1.642512321472168), (5, 1.618922472000122), (6, 1.5939340591430664), (7, 1.5541549921035767), (8, 1.546172022819519), (9, 1.5414971113204956), (10, 1.5367326736450195)]
INFO flwr 2024-04-17 12:53:46,081 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0967), (1, 0.3584), (2, 0.5832), (3, 0.7199), (4, 0.8178), (5, 0.8414), (6, 0.8674), (7, 0.906), (8, 0.9151), (9, 0.9197), (10, 0.9246)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9246
wandb:     loss 1.53673
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_125135-qmw8nhcn
wandb: Find logs at: ./wandb/offline-run-20240417_125135-qmw8nhcn/logs
INFO flwr 2024-04-17 12:53:49,586 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 13:00:57,617 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=585309)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=585309)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 13:01:02,368	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 13:01:03,200	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 13:01:03,675	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 13:01:04,048	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c30b0f3ce9ea884a.zip' (38.43MiB) to Ray cluster...
2024-04-17 13:01:04,171	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c30b0f3ce9ea884a.zip'.
INFO flwr 2024-04-17 13:01:15,239 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'memory': 170098070119.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77184887193.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 13:01:15,239 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 13:01:15,239 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 13:01:15,257 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 13:01:15,259 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 13:01:15,259 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 13:01:15,260 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 13:01:18,251 | server.py:94 | initial parameters (loss, other metrics): 2.302664041519165, {'accuracy': 0.1043, 'data_size': 10000}
INFO flwr 2024-04-17 13:01:18,251 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 13:01:18,252 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=589682)[0m 2024-04-17 13:01:21.349790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=589682)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=589682)[0m 2024-04-17 13:01:23.686171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=589692)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=589692)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=589689)[0m 2024-04-17 13:01:21.727127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=589689)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=589689)[0m 2024-04-17 13:01:23.940045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 13:01:38,613 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 13:01:40,033 | server.py:125 | fit progress: (1, 2.301666259765625, {'accuracy': 0.1289, 'data_size': 10000}, 21.781299416994443)
INFO flwr 2024-04-17 13:01:40,033 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 13:01:40,033 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:01:48,913 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 13:01:50,088 | server.py:125 | fit progress: (2, 2.2988169193267822, {'accuracy': 0.169, 'data_size': 10000}, 31.83593130900408)
INFO flwr 2024-04-17 13:01:50,088 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 13:01:50,088 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:01:58,332 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 13:01:59,717 | server.py:125 | fit progress: (3, 2.2921576499938965, {'accuracy': 0.331, 'data_size': 10000}, 41.4653984799952)
INFO flwr 2024-04-17 13:01:59,717 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 13:01:59,718 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:02:07,820 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 13:02:08,981 | server.py:125 | fit progress: (4, 2.2790207862854004, {'accuracy': 0.4292, 'data_size': 10000}, 50.72928940800193)
INFO flwr 2024-04-17 13:02:08,981 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 13:02:08,981 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:02:17,192 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 13:02:18,570 | server.py:125 | fit progress: (5, 2.254075050354004, {'accuracy': 0.4611, 'data_size': 10000}, 60.31869263200497)
INFO flwr 2024-04-17 13:02:18,571 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 13:02:18,571 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:02:26,771 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 13:02:27,934 | server.py:125 | fit progress: (6, 2.2104692459106445, {'accuracy': 0.4672, 'data_size': 10000}, 69.68225063399586)
INFO flwr 2024-04-17 13:02:27,934 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 13:02:27,934 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:02:35,825 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 13:02:37,225 | server.py:125 | fit progress: (7, 2.1453254222869873, {'accuracy': 0.4832, 'data_size': 10000}, 78.97385907900752)
INFO flwr 2024-04-17 13:02:37,226 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 13:02:37,226 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:02:44,827 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 13:02:46,232 | server.py:125 | fit progress: (8, 2.069749116897583, {'accuracy': 0.5205, 'data_size': 10000}, 87.97990156400192)
INFO flwr 2024-04-17 13:02:46,232 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 13:02:46,232 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:02:54,333 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 13:02:55,724 | server.py:125 | fit progress: (9, 1.9982463121414185, {'accuracy': 0.5715, 'data_size': 10000}, 97.47215737399529)
INFO flwr 2024-04-17 13:02:55,724 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 13:02:55,724 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:03:04,071 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 13:03:05,247 | server.py:125 | fit progress: (10, 1.9319037199020386, {'accuracy': 0.6262, 'data_size': 10000}, 106.9953183799953)
INFO flwr 2024-04-17 13:03:05,247 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 13:03:05,247 | server.py:153 | FL finished in 106.9958319169964
INFO flwr 2024-04-17 13:03:05,248 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 13:03:05,248 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 13:03:05,248 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 13:03:05,248 | app.py:229 | app_fit: losses_centralized [(0, 2.302664041519165), (1, 2.301666259765625), (2, 2.2988169193267822), (3, 2.2921576499938965), (4, 2.2790207862854004), (5, 2.254075050354004), (6, 2.2104692459106445), (7, 2.1453254222869873), (8, 2.069749116897583), (9, 1.9982463121414185), (10, 1.9319037199020386)]
INFO flwr 2024-04-17 13:03:05,248 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1043), (1, 0.1289), (2, 0.169), (3, 0.331), (4, 0.4292), (5, 0.4611), (6, 0.4672), (7, 0.4832), (8, 0.5205), (9, 0.5715), (10, 0.6262)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6262
wandb:     loss 1.9319
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_130057-cyt18p4o
wandb: Find logs at: ./wandb/offline-run-20240417_130057-cyt18p4o/logs
INFO flwr 2024-04-17 13:03:08,751 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 13:10:16,787 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=589680)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=589680)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 13:10:21,488	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 13:10:22,386	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 13:10:22,859	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 13:10:23,231	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ca12a0abf2539e0d.zip' (38.59MiB) to Ray cluster...
2024-04-17 13:10:23,352	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ca12a0abf2539e0d.zip'.
INFO flwr 2024-04-17 13:10:34,393 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:TITAN': 1.0, 'memory': 169906922496.0, 'object_store_memory': 77102966784.0, 'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 13:10:34,393 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 13:10:34,393 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 13:10:34,409 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 13:10:34,411 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 13:10:34,411 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 13:10:34,411 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 13:10:38,133 | server.py:94 | initial parameters (loss, other metrics): 2.3027548789978027, {'accuracy': 0.076, 'data_size': 10000}
INFO flwr 2024-04-17 13:10:38,133 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 13:10:38,134 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=594047)[0m 2024-04-17 13:10:40.459937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=594047)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=594047)[0m 2024-04-17 13:10:42.818560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=594049)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=594049)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=594056)[0m 2024-04-17 13:10:40.780249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=594056)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=594056)[0m 2024-04-17 13:10:43.196882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 13:10:57,510 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 13:10:58,714 | server.py:125 | fit progress: (1, 2.3026797771453857, {'accuracy': 0.078, 'data_size': 10000}, 20.580740962992422)
INFO flwr 2024-04-17 13:10:58,715 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 13:10:58,715 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:11:07,586 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 13:11:08,985 | server.py:125 | fit progress: (2, 2.3025686740875244, {'accuracy': 0.0827, 'data_size': 10000}, 30.851038364999113)
INFO flwr 2024-04-17 13:11:08,985 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 13:11:08,985 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:11:17,155 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 13:11:18,418 | server.py:125 | fit progress: (3, 2.3024189472198486, {'accuracy': 0.0868, 'data_size': 10000}, 40.28449952699884)
INFO flwr 2024-04-17 13:11:18,418 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 13:11:18,419 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:11:26,330 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 13:11:27,735 | server.py:125 | fit progress: (4, 2.3022265434265137, {'accuracy': 0.0906, 'data_size': 10000}, 49.60139213499497)
INFO flwr 2024-04-17 13:11:27,735 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 13:11:27,736 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:11:35,725 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 13:11:36,899 | server.py:125 | fit progress: (5, 2.301990032196045, {'accuracy': 0.0969, 'data_size': 10000}, 58.76539793299162)
INFO flwr 2024-04-17 13:11:36,899 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 13:11:36,900 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:11:44,905 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 13:11:46,072 | server.py:125 | fit progress: (6, 2.301708698272705, {'accuracy': 0.1025, 'data_size': 10000}, 67.93803862200002)
INFO flwr 2024-04-17 13:11:46,072 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 13:11:46,072 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:11:54,199 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 13:11:55,605 | server.py:125 | fit progress: (7, 2.301370859146118, {'accuracy': 0.1119, 'data_size': 10000}, 77.47107923099247)
INFO flwr 2024-04-17 13:11:55,605 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 13:11:55,605 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:12:03,856 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 13:12:05,047 | server.py:125 | fit progress: (8, 2.3009676933288574, {'accuracy': 0.1241, 'data_size': 10000}, 86.91298276199086)
INFO flwr 2024-04-17 13:12:05,047 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 13:12:05,047 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:12:13,004 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 13:12:14,382 | server.py:125 | fit progress: (9, 2.300496816635132, {'accuracy': 0.1409, 'data_size': 10000}, 96.24800968398631)
INFO flwr 2024-04-17 13:12:14,382 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 13:12:14,382 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:12:22,770 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 13:12:23,937 | server.py:125 | fit progress: (10, 2.299949884414673, {'accuracy': 0.1696, 'data_size': 10000}, 105.80329700699076)
INFO flwr 2024-04-17 13:12:23,937 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 13:12:23,938 | server.py:153 | FL finished in 105.80383794099907
INFO flwr 2024-04-17 13:12:23,938 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 13:12:23,938 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 13:12:23,938 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 13:12:23,938 | app.py:229 | app_fit: losses_centralized [(0, 2.3027548789978027), (1, 2.3026797771453857), (2, 2.3025686740875244), (3, 2.3024189472198486), (4, 2.3022265434265137), (5, 2.301990032196045), (6, 2.301708698272705), (7, 2.301370859146118), (8, 2.3009676933288574), (9, 2.300496816635132), (10, 2.299949884414673)]
INFO flwr 2024-04-17 13:12:23,939 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.076), (1, 0.078), (2, 0.0827), (3, 0.0868), (4, 0.0906), (5, 0.0969), (6, 0.1025), (7, 0.1119), (8, 0.1241), (9, 0.1409), (10, 0.1696)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1696
wandb:     loss 2.29995
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_131016-a1hxz7av
wandb: Find logs at: ./wandb/offline-run-20240417_131016-a1hxz7av/logs
INFO flwr 2024-04-17 13:12:27,457 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 13:19:35,545 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=594045)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=594045)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 13:19:40,559	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 13:19:41,409	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 13:19:41,895	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 13:19:42,265	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_db9f72693cad1662.zip' (38.74MiB) to Ray cluster...
2024-04-17 13:19:42,385	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_db9f72693cad1662.zip'.
INFO flwr 2024-04-17 13:19:53,419 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77050025164.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169783392052.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 13:19:53,419 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 13:19:53,419 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 13:19:53,441 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 13:19:53,442 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 13:19:53,442 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 13:19:53,443 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 13:19:57,305 | server.py:94 | initial parameters (loss, other metrics): 2.302499532699585, {'accuracy': 0.103, 'data_size': 10000}
INFO flwr 2024-04-17 13:19:57,310 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 13:19:57,310 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=598754)[0m 2024-04-17 13:19:59.557746: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=598754)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=598754)[0m 2024-04-17 13:20:01.832609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=598767)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=598767)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=598769)[0m 2024-04-17 13:19:59.834203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=598769)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=598769)[0m 2024-04-17 13:20:02.064840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 13:20:16,554 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 13:20:17,970 | server.py:125 | fit progress: (1, 2.302492141723633, {'accuracy': 0.103, 'data_size': 10000}, 20.659944093000377)
INFO flwr 2024-04-17 13:20:17,970 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 13:20:17,971 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:20:26,601 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 13:20:27,981 | server.py:125 | fit progress: (2, 2.302483081817627, {'accuracy': 0.103, 'data_size': 10000}, 30.671379071995034)
INFO flwr 2024-04-17 13:20:27,982 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 13:20:27,982 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:20:36,116 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 13:20:37,512 | server.py:125 | fit progress: (3, 2.3024723529815674, {'accuracy': 0.103, 'data_size': 10000}, 40.20228941399546)
INFO flwr 2024-04-17 13:20:37,513 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 13:20:37,513 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:20:45,755 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 13:20:46,912 | server.py:125 | fit progress: (4, 2.302459716796875, {'accuracy': 0.103, 'data_size': 10000}, 49.602016680000816)
INFO flwr 2024-04-17 13:20:46,912 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 13:20:46,913 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:20:55,154 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 13:20:56,323 | server.py:125 | fit progress: (5, 2.3024466037750244, {'accuracy': 0.1032, 'data_size': 10000}, 59.01339103099599)
INFO flwr 2024-04-17 13:20:56,324 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 13:20:56,324 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:21:04,604 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 13:21:05,987 | server.py:125 | fit progress: (6, 2.3024322986602783, {'accuracy': 0.1032, 'data_size': 10000}, 68.67661420399963)
INFO flwr 2024-04-17 13:21:05,987 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 13:21:05,987 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:21:13,891 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 13:21:15,074 | server.py:125 | fit progress: (7, 2.302417278289795, {'accuracy': 0.1032, 'data_size': 10000}, 77.76365632099623)
INFO flwr 2024-04-17 13:21:15,074 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 13:21:15,074 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:21:22,919 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 13:21:24,310 | server.py:125 | fit progress: (8, 2.3024020195007324, {'accuracy': 0.1032, 'data_size': 10000}, 86.9994583200023)
INFO flwr 2024-04-17 13:21:24,310 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 13:21:24,310 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:21:32,355 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 13:21:33,524 | server.py:125 | fit progress: (9, 2.3023862838745117, {'accuracy': 0.1032, 'data_size': 10000}, 96.21378458100662)
INFO flwr 2024-04-17 13:21:33,524 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 13:21:33,524 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:21:41,891 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 13:21:43,268 | server.py:125 | fit progress: (10, 2.302370309829712, {'accuracy': 0.1032, 'data_size': 10000}, 105.95805576900602)
INFO flwr 2024-04-17 13:21:43,268 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 13:21:43,269 | server.py:153 | FL finished in 105.95856269399519
INFO flwr 2024-04-17 13:21:43,269 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 13:21:43,269 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 13:21:43,269 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 13:21:43,269 | app.py:229 | app_fit: losses_centralized [(0, 2.302499532699585), (1, 2.302492141723633), (2, 2.302483081817627), (3, 2.3024723529815674), (4, 2.302459716796875), (5, 2.3024466037750244), (6, 2.3024322986602783), (7, 2.302417278289795), (8, 2.3024020195007324), (9, 2.3023862838745117), (10, 2.302370309829712)]
INFO flwr 2024-04-17 13:21:43,269 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.103), (1, 0.103), (2, 0.103), (3, 0.103), (4, 0.103), (5, 0.1032), (6, 0.1032), (7, 0.1032), (8, 0.1032), (9, 0.1032), (10, 0.1032)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1032
wandb:     loss 2.30237
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_131935-5ine72em
wandb: Find logs at: ./wandb/offline-run-20240417_131935-5ine72em/logs
INFO flwr 2024-04-17 13:21:46,822 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 13:28:55,095 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=598754)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=598754)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 13:29:00,110	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 13:29:00,983	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 13:29:01,445	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 13:29:01,813	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_bbbe2a24a7341ab6.zip' (38.90MiB) to Ray cluster...
2024-04-17 13:29:01,939	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_bbbe2a24a7341ab6.zip'.
INFO flwr 2024-04-17 13:29:13,028 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'object_store_memory': 77060713267.0, 'accelerator_type:TITAN': 1.0, 'memory': 169808330957.0}
INFO flwr 2024-04-17 13:29:13,029 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 13:29:13,029 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 13:29:13,047 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 13:29:13,050 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 13:29:13,051 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 13:29:13,051 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 13:29:16,140 | server.py:94 | initial parameters (loss, other metrics): 2.3028297424316406, {'accuracy': 0.0971, 'data_size': 10000}
INFO flwr 2024-04-17 13:29:16,141 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 13:29:16,141 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=603694)[0m 2024-04-17 13:29:19.310151: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=603694)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=603692)[0m 2024-04-17 13:29:21.682884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=603691)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=603691)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=603696)[0m 2024-04-17 13:29:19.376817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=603696)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=603697)[0m 2024-04-17 13:29:21.756462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 13:29:37,759 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 13:29:38,957 | server.py:125 | fit progress: (1, 2.2213237285614014, {'accuracy': 0.2397, 'data_size': 10000}, 22.816112908010837)
INFO flwr 2024-04-17 13:29:38,957 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 13:29:38,957 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:29:48,842 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 13:29:50,260 | server.py:125 | fit progress: (2, 2.277557611465454, {'accuracy': 0.1836, 'data_size': 10000}, 34.119290601011016)
INFO flwr 2024-04-17 13:29:50,260 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 13:29:50,261 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:29:59,241 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 13:30:00,420 | server.py:125 | fit progress: (3, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 44.27898900200671)
INFO flwr 2024-04-17 13:30:00,420 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 13:30:00,420 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:30:09,714 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 13:30:10,878 | server.py:125 | fit progress: (4, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 54.736929966005846)
INFO flwr 2024-04-17 13:30:10,878 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 13:30:10,878 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:30:20,331 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 13:30:21,704 | server.py:125 | fit progress: (5, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 65.56332470101188)
INFO flwr 2024-04-17 13:30:21,704 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 13:30:21,705 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:30:30,673 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 13:30:32,108 | server.py:125 | fit progress: (6, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 75.96705068500887)
INFO flwr 2024-04-17 13:30:32,108 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 13:30:32,108 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:30:41,106 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 13:30:42,310 | server.py:125 | fit progress: (7, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 86.16957575900597)
INFO flwr 2024-04-17 13:30:42,311 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 13:30:42,311 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:30:51,269 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 13:30:52,439 | server.py:125 | fit progress: (8, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 96.29803481700947)
INFO flwr 2024-04-17 13:30:52,439 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 13:30:52,439 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:31:01,258 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 13:31:02,648 | server.py:125 | fit progress: (9, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 106.50690594900516)
INFO flwr 2024-04-17 13:31:02,648 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 13:31:02,648 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:31:11,913 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 13:31:13,083 | server.py:125 | fit progress: (10, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 116.94272522401297)
INFO flwr 2024-04-17 13:31:13,084 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 13:31:13,084 | server.py:153 | FL finished in 116.94320010500087
INFO flwr 2024-04-17 13:31:13,084 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 13:31:13,084 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 13:31:13,084 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 13:31:13,085 | app.py:229 | app_fit: losses_centralized [(0, 2.3028297424316406), (1, 2.2213237285614014), (2, 2.277557611465454), (3, 2.365341901779175), (4, 2.365341901779175), (5, 2.365341901779175), (6, 2.365341901779175), (7, 2.365341901779175), (8, 2.365341901779175), (9, 2.365341901779175), (10, 2.365341901779175)]
INFO flwr 2024-04-17 13:31:13,085 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0971), (1, 0.2397), (2, 0.1836), (3, 0.0958), (4, 0.0958), (5, 0.0958), (6, 0.0958), (7, 0.0958), (8, 0.0958), (9, 0.0958), (10, 0.0958)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0958
wandb:     loss 2.36534
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_132854-77cqqbaz
wandb: Find logs at: ./wandb/offline-run-20240417_132854-77cqqbaz/logs
INFO flwr 2024-04-17 13:31:16,634 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 13:38:29,495 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=603687)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=603687)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 13:38:34,276	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 13:38:35,116	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 13:38:35,638	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 13:38:36,018	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b7ecc1952403445d.zip' (39.05MiB) to Ray cluster...
2024-04-17 13:38:36,136	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b7ecc1952403445d.zip'.
INFO flwr 2024-04-17 13:38:47,180 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 169803671757.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77058716467.0, 'GPU': 1.0}
INFO flwr 2024-04-17 13:38:47,180 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 13:38:47,180 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 13:38:47,200 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 13:38:47,201 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 13:38:47,201 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 13:38:47,201 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 13:38:50,578 | server.py:94 | initial parameters (loss, other metrics): 2.302584648132324, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-17 13:38:50,579 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 13:38:50,579 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=608059)[0m 2024-04-17 13:38:53.359134: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=608059)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=608059)[0m 2024-04-17 13:38:55.673106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=608058)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=608058)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=608057)[0m 2024-04-17 13:38:53.717451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=608057)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=608057)[0m 2024-04-17 13:38:55.973301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 13:39:12,200 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 13:39:13,387 | server.py:125 | fit progress: (1, 2.1988887786865234, {'accuracy': 0.4244, 'data_size': 10000}, 22.80856906599365)
INFO flwr 2024-04-17 13:39:13,388 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 13:39:13,388 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:39:23,404 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 13:39:24,601 | server.py:125 | fit progress: (2, 1.9396867752075195, {'accuracy': 0.5268, 'data_size': 10000}, 34.021669745998224)
INFO flwr 2024-04-17 13:39:24,601 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 13:39:24,601 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:39:34,183 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 13:39:35,562 | server.py:125 | fit progress: (3, 1.7552820444107056, {'accuracy': 0.7027, 'data_size': 10000}, 44.982919476999086)
INFO flwr 2024-04-17 13:39:35,562 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 13:39:35,562 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:39:44,879 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 13:39:46,300 | server.py:125 | fit progress: (4, 1.633899211883545, {'accuracy': 0.8268, 'data_size': 10000}, 55.720691519993125)
INFO flwr 2024-04-17 13:39:46,300 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 13:39:46,300 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:39:54,936 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 13:39:56,136 | server.py:125 | fit progress: (5, 1.5707818269729614, {'accuracy': 0.8898, 'data_size': 10000}, 65.55673450800532)
INFO flwr 2024-04-17 13:39:56,136 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 13:39:56,136 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:40:05,360 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 13:40:06,550 | server.py:125 | fit progress: (6, 1.5542480945587158, {'accuracy': 0.9067, 'data_size': 10000}, 75.97088940200047)
INFO flwr 2024-04-17 13:40:06,550 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 13:40:06,550 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:40:15,419 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 13:40:16,788 | server.py:125 | fit progress: (7, 1.5520641803741455, {'accuracy': 0.9091, 'data_size': 10000}, 86.20880106200639)
INFO flwr 2024-04-17 13:40:16,788 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 13:40:16,788 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:40:25,906 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 13:40:27,071 | server.py:125 | fit progress: (8, 1.54594087600708, {'accuracy': 0.9149, 'data_size': 10000}, 96.49256830199738)
INFO flwr 2024-04-17 13:40:27,072 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 13:40:27,072 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:40:35,816 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 13:40:37,246 | server.py:125 | fit progress: (9, 1.5533525943756104, {'accuracy': 0.9078, 'data_size': 10000}, 106.66674532499746)
INFO flwr 2024-04-17 13:40:37,246 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 13:40:37,246 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:40:46,215 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 13:40:47,389 | server.py:125 | fit progress: (10, 1.5479110479354858, {'accuracy': 0.9131, 'data_size': 10000}, 116.81002628899296)
INFO flwr 2024-04-17 13:40:47,389 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 13:40:47,389 | server.py:153 | FL finished in 116.81049004200031
INFO flwr 2024-04-17 13:40:47,389 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 13:40:47,390 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 13:40:47,390 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 13:40:47,390 | app.py:229 | app_fit: losses_centralized [(0, 2.302584648132324), (1, 2.1988887786865234), (2, 1.9396867752075195), (3, 1.7552820444107056), (4, 1.633899211883545), (5, 1.5707818269729614), (6, 1.5542480945587158), (7, 1.5520641803741455), (8, 1.54594087600708), (9, 1.5533525943756104), (10, 1.5479110479354858)]
INFO flwr 2024-04-17 13:40:47,390 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.4244), (2, 0.5268), (3, 0.7027), (4, 0.8268), (5, 0.8898), (6, 0.9067), (7, 0.9091), (8, 0.9149), (9, 0.9078), (10, 0.9131)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9131
wandb:     loss 1.54791
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_133829-s43xrlz2
wandb: Find logs at: ./wandb/offline-run-20240417_133829-s43xrlz2/logs
INFO flwr 2024-04-17 13:40:50,938 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 13:47:58,882 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=608048)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=608048)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 13:48:04,643	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 13:48:05,462	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 13:48:05,950	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 13:48:06,327	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_793a33ba61b2e232.zip' (39.21MiB) to Ray cluster...
2024-04-17 13:48:06,447	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_793a33ba61b2e232.zip'.
INFO flwr 2024-04-17 13:48:17,584 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77039659008.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169759204352.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 13:48:17,584 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 13:48:17,585 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 13:48:17,604 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 13:48:17,608 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 13:48:17,609 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 13:48:17,609 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 13:48:21,215 | server.py:94 | initial parameters (loss, other metrics): 2.3024637699127197, {'accuracy': 0.107, 'data_size': 10000}
INFO flwr 2024-04-17 13:48:21,215 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 13:48:21,215 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=612783)[0m 2024-04-17 13:48:23.664773: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=612783)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=612783)[0m 2024-04-17 13:48:26.012121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=612799)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=612799)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=612796)[0m 2024-04-17 13:48:23.968195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=612796)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=612796)[0m 2024-04-17 13:48:26.308653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 13:48:42,034 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 13:48:43,237 | server.py:125 | fit progress: (1, 2.3011863231658936, {'accuracy': 0.1918, 'data_size': 10000}, 22.02164102999086)
INFO flwr 2024-04-17 13:48:43,237 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 13:48:43,237 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:48:52,878 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 13:48:54,262 | server.py:125 | fit progress: (2, 2.2976889610290527, {'accuracy': 0.2913, 'data_size': 10000}, 33.04660815600073)
INFO flwr 2024-04-17 13:48:54,262 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 13:48:54,262 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:49:03,061 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 13:49:04,234 | server.py:125 | fit progress: (3, 2.2898542881011963, {'accuracy': 0.411, 'data_size': 10000}, 43.01869142499345)
INFO flwr 2024-04-17 13:49:04,234 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 13:49:04,234 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:49:13,406 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 13:49:14,792 | server.py:125 | fit progress: (4, 2.274127721786499, {'accuracy': 0.4511, 'data_size': 10000}, 53.57730251600151)
INFO flwr 2024-04-17 13:49:14,793 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 13:49:14,793 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:49:23,691 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 13:49:25,058 | server.py:125 | fit progress: (5, 2.245159387588501, {'accuracy': 0.4785, 'data_size': 10000}, 63.84283572899585)
INFO flwr 2024-04-17 13:49:25,058 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 13:49:25,058 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:49:34,146 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 13:49:35,518 | server.py:125 | fit progress: (6, 2.195592164993286, {'accuracy': 0.5068, 'data_size': 10000}, 74.30281889899925)
INFO flwr 2024-04-17 13:49:35,518 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 13:49:35,518 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:49:44,139 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 13:49:45,576 | server.py:125 | fit progress: (7, 2.1204545497894287, {'accuracy': 0.5299, 'data_size': 10000}, 84.3613279209967)
INFO flwr 2024-04-17 13:49:45,577 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 13:49:45,577 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:49:54,508 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 13:49:55,904 | server.py:125 | fit progress: (8, 2.0278162956237793, {'accuracy': 0.5641, 'data_size': 10000}, 94.68916705499578)
INFO flwr 2024-04-17 13:49:55,905 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 13:49:55,905 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:50:04,697 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 13:50:05,861 | server.py:125 | fit progress: (9, 1.938374638557434, {'accuracy': 0.6012, 'data_size': 10000}, 104.6454005449923)
INFO flwr 2024-04-17 13:50:05,861 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 13:50:05,861 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:50:15,077 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 13:50:16,469 | server.py:125 | fit progress: (10, 1.8687710762023926, {'accuracy': 0.6331, 'data_size': 10000}, 115.25402721299906)
INFO flwr 2024-04-17 13:50:16,469 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 13:50:16,470 | server.py:153 | FL finished in 115.25448753900127
INFO flwr 2024-04-17 13:50:16,470 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 13:50:16,470 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 13:50:16,470 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 13:50:16,470 | app.py:229 | app_fit: losses_centralized [(0, 2.3024637699127197), (1, 2.3011863231658936), (2, 2.2976889610290527), (3, 2.2898542881011963), (4, 2.274127721786499), (5, 2.245159387588501), (6, 2.195592164993286), (7, 2.1204545497894287), (8, 2.0278162956237793), (9, 1.938374638557434), (10, 1.8687710762023926)]
INFO flwr 2024-04-17 13:50:16,470 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.107), (1, 0.1918), (2, 0.2913), (3, 0.411), (4, 0.4511), (5, 0.4785), (6, 0.5068), (7, 0.5299), (8, 0.5641), (9, 0.6012), (10, 0.6331)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6331
wandb:     loss 1.86877
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_134758-sj1sfwz9
wandb: Find logs at: ./wandb/offline-run-20240417_134758-sj1sfwz9/logs
INFO flwr 2024-04-17 13:50:20,010 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 13:57:27,842 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=612783)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=612783)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 13:57:32,625	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 13:57:33,470	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 13:57:33,953	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 13:57:34,327	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_74e139e983b5ca21.zip' (39.37MiB) to Ray cluster...
2024-04-17 13:57:34,449	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_74e139e983b5ca21.zip'.
INFO flwr 2024-04-17 13:57:45,602 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 77040336076.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 169760784180.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 13:57:45,603 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 13:57:45,603 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 13:57:45,620 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 13:57:45,621 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 13:57:45,621 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 13:57:45,622 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 13:57:48,282 | server.py:94 | initial parameters (loss, other metrics): 2.3028573989868164, {'accuracy': 0.1004, 'data_size': 10000}
INFO flwr 2024-04-17 13:57:48,282 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 13:57:48,283 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=616550)[0m 2024-04-17 13:57:51.848058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=616550)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=616542)[0m 2024-04-17 13:57:54.232343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=616550)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=616550)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=616549)[0m 2024-04-17 13:57:52.064950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=616549)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=616549)[0m 2024-04-17 13:57:54.379023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 13:58:10,268 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 13:58:11,683 | server.py:125 | fit progress: (1, 2.3027782440185547, {'accuracy': 0.1003, 'data_size': 10000}, 23.40006169199478)
INFO flwr 2024-04-17 13:58:11,683 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 13:58:11,683 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:58:21,103 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 13:58:22,472 | server.py:125 | fit progress: (2, 2.302666664123535, {'accuracy': 0.1012, 'data_size': 10000}, 34.18993591499748)
INFO flwr 2024-04-17 13:58:22,473 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 13:58:22,473 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:58:31,343 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 13:58:32,718 | server.py:125 | fit progress: (3, 2.302525520324707, {'accuracy': 0.1018, 'data_size': 10000}, 44.43560823900043)
INFO flwr 2024-04-17 13:58:32,718 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 13:58:32,719 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:58:41,311 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 13:58:42,768 | server.py:125 | fit progress: (4, 2.3023462295532227, {'accuracy': 0.1026, 'data_size': 10000}, 54.485237412998686)
INFO flwr 2024-04-17 13:58:42,768 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 13:58:42,769 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:58:51,623 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 13:58:53,019 | server.py:125 | fit progress: (5, 2.302123546600342, {'accuracy': 0.1031, 'data_size': 10000}, 64.73687409999548)
INFO flwr 2024-04-17 13:58:53,020 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 13:58:53,020 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:59:01,694 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 13:59:02,866 | server.py:125 | fit progress: (6, 2.3018577098846436, {'accuracy': 0.1036, 'data_size': 10000}, 74.583131116)
INFO flwr 2024-04-17 13:59:02,866 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 13:59:02,866 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:59:11,754 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 13:59:13,141 | server.py:125 | fit progress: (7, 2.301546573638916, {'accuracy': 0.1052, 'data_size': 10000}, 84.85805979800352)
INFO flwr 2024-04-17 13:59:13,141 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 13:59:13,141 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:59:22,100 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 13:59:23,269 | server.py:125 | fit progress: (8, 2.30118727684021, {'accuracy': 0.1115, 'data_size': 10000}, 94.98644335300196)
INFO flwr 2024-04-17 13:59:23,269 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 13:59:23,269 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:59:31,670 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 13:59:32,843 | server.py:125 | fit progress: (9, 2.300769567489624, {'accuracy': 0.1278, 'data_size': 10000}, 104.5601761979924)
INFO flwr 2024-04-17 13:59:32,843 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 13:59:32,843 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 13:59:41,600 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 13:59:43,013 | server.py:125 | fit progress: (10, 2.300293445587158, {'accuracy': 0.148, 'data_size': 10000}, 114.73014348199649)
INFO flwr 2024-04-17 13:59:43,013 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 13:59:43,013 | server.py:153 | FL finished in 114.73067384699243
INFO flwr 2024-04-17 13:59:43,013 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 13:59:43,014 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 13:59:43,014 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 13:59:43,014 | app.py:229 | app_fit: losses_centralized [(0, 2.3028573989868164), (1, 2.3027782440185547), (2, 2.302666664123535), (3, 2.302525520324707), (4, 2.3023462295532227), (5, 2.302123546600342), (6, 2.3018577098846436), (7, 2.301546573638916), (8, 2.30118727684021), (9, 2.300769567489624), (10, 2.300293445587158)]
INFO flwr 2024-04-17 13:59:43,014 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1004), (1, 0.1003), (2, 0.1012), (3, 0.1018), (4, 0.1026), (5, 0.1031), (6, 0.1036), (7, 0.1052), (8, 0.1115), (9, 0.1278), (10, 0.148)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.148
wandb:     loss 2.30029
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_135727-565yz2o4
wandb: Find logs at: ./wandb/offline-run-20240417_135727-565yz2o4/logs
INFO flwr 2024-04-17 13:59:46,562 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 14:06:54,712 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=616542)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=616542)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 14:06:59,533	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 14:07:00,456	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 14:07:00,968	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 14:07:01,355	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b732be47a2d3bfd2.zip' (39.52MiB) to Ray cluster...
2024-04-17 14:07:01,478	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b732be47a2d3bfd2.zip'.
INFO flwr 2024-04-17 14:07:12,498 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 77049789235.0, 'memory': 169782841549.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 14:07:12,499 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 14:07:12,499 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 14:07:12,515 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 14:07:12,516 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 14:07:12,517 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 14:07:12,517 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 14:07:15,736 | server.py:94 | initial parameters (loss, other metrics): 2.302558422088623, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-17 14:07:15,736 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 14:07:15,736 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=620924)[0m 2024-04-17 14:07:22.780045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=620924)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=620924)[0m 2024-04-17 14:07:24.996777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=620924)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=620924)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=620921)[0m 2024-04-17 14:07:23.181932: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=620921)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=620921)[0m 2024-04-17 14:07:25.218511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 14:07:41,166 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 14:07:42,580 | server.py:125 | fit progress: (1, 2.3025503158569336, {'accuracy': 0.101, 'data_size': 10000}, 26.844053805994918)
INFO flwr 2024-04-17 14:07:42,581 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 14:07:42,581 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:07:52,151 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 14:07:53,543 | server.py:125 | fit progress: (2, 2.302539348602295, {'accuracy': 0.101, 'data_size': 10000}, 37.806387983000604)
INFO flwr 2024-04-17 14:07:53,543 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 14:07:53,543 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:08:02,569 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 14:08:03,976 | server.py:125 | fit progress: (3, 2.3025271892547607, {'accuracy': 0.101, 'data_size': 10000}, 48.23964271400473)
INFO flwr 2024-04-17 14:08:03,976 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 14:08:03,976 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:08:13,112 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 14:08:14,526 | server.py:125 | fit progress: (4, 2.3025128841400146, {'accuracy': 0.101, 'data_size': 10000}, 58.789646514997)
INFO flwr 2024-04-17 14:08:14,526 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 14:08:14,526 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:08:23,293 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 14:08:24,465 | server.py:125 | fit progress: (5, 2.3024981021881104, {'accuracy': 0.101, 'data_size': 10000}, 68.72921661500004)
INFO flwr 2024-04-17 14:08:24,466 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 14:08:24,466 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:08:33,842 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 14:08:35,243 | server.py:125 | fit progress: (6, 2.3024818897247314, {'accuracy': 0.101, 'data_size': 10000}, 79.50700235100521)
INFO flwr 2024-04-17 14:08:35,243 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 14:08:35,244 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:08:44,451 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 14:08:45,624 | server.py:125 | fit progress: (7, 2.3024652004241943, {'accuracy': 0.101, 'data_size': 10000}, 89.88789941799769)
INFO flwr 2024-04-17 14:08:45,624 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 14:08:45,625 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:08:55,069 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 14:08:56,236 | server.py:125 | fit progress: (8, 2.3024470806121826, {'accuracy': 0.101, 'data_size': 10000}, 100.49955911500729)
INFO flwr 2024-04-17 14:08:56,236 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 14:08:56,236 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:09:04,934 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 14:09:06,348 | server.py:125 | fit progress: (9, 2.302428960800171, {'accuracy': 0.101, 'data_size': 10000}, 110.61133350900491)
INFO flwr 2024-04-17 14:09:06,348 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 14:09:06,348 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:09:15,383 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 14:09:16,552 | server.py:125 | fit progress: (10, 2.302410840988159, {'accuracy': 0.101, 'data_size': 10000}, 120.81540448700252)
INFO flwr 2024-04-17 14:09:16,552 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 14:09:16,552 | server.py:153 | FL finished in 120.81585746600467
INFO flwr 2024-04-17 14:09:16,552 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 14:09:16,552 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 14:09:16,552 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 14:09:16,553 | app.py:229 | app_fit: losses_centralized [(0, 2.302558422088623), (1, 2.3025503158569336), (2, 2.302539348602295), (3, 2.3025271892547607), (4, 2.3025128841400146), (5, 2.3024981021881104), (6, 2.3024818897247314), (7, 2.3024652004241943), (8, 2.3024470806121826), (9, 2.302428960800171), (10, 2.302410840988159)]
INFO flwr 2024-04-17 14:09:16,553 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.101), (2, 0.101), (3, 0.101), (4, 0.101), (5, 0.101), (6, 0.101), (7, 0.101), (8, 0.101), (9, 0.101), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.30241
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_140654-gf5zkm0x
wandb: Find logs at: ./wandb/offline-run-20240417_140654-gf5zkm0x/logs
INFO flwr 2024-04-17 14:09:20,051 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 14:16:27,869 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=620918)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=620918)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 14:16:32,745	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 14:16:33,576	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 14:16:34,060	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 14:16:34,439	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_89ebc81fd3ec0a98.zip' (39.68MiB) to Ray cluster...
2024-04-17 14:16:34,558	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_89ebc81fd3ec0a98.zip'.
INFO flwr 2024-04-17 14:16:45,543 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77041920000.0, 'memory': 169764480000.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 14:16:45,543 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 14:16:45,543 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 14:16:45,562 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 14:16:45,563 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 14:16:45,563 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 14:16:45,563 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 14:16:48,656 | server.py:94 | initial parameters (loss, other metrics): 2.302537441253662, {'accuracy': 0.094, 'data_size': 10000}
INFO flwr 2024-04-17 14:16:48,656 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 14:16:48,656 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=625644)[0m 2024-04-17 14:16:51.721521: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=625644)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=625644)[0m 2024-04-17 14:16:54.097838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=625650)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=625650)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=625648)[0m 2024-04-17 14:16:52.107877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=625648)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=625642)[0m 2024-04-17 14:16:54.358326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 14:17:09,787 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 14:17:11,213 | server.py:125 | fit progress: (1, 2.095494031906128, {'accuracy': 0.3655, 'data_size': 10000}, 22.556626256002346)
INFO flwr 2024-04-17 14:17:11,213 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 14:17:11,213 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:17:20,687 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 14:17:22,069 | server.py:125 | fit progress: (2, 2.350942373275757, {'accuracy': 0.1102, 'data_size': 10000}, 33.41311636500177)
INFO flwr 2024-04-17 14:17:22,070 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 14:17:22,070 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:17:31,182 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 14:17:32,568 | server.py:125 | fit progress: (3, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 43.91155150900886)
INFO flwr 2024-04-17 14:17:32,568 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 14:17:32,568 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:17:41,360 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 14:17:42,774 | server.py:125 | fit progress: (4, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 54.118029310004204)
INFO flwr 2024-04-17 14:17:42,775 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 14:17:42,775 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:17:51,676 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 14:17:53,127 | server.py:125 | fit progress: (5, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 64.47092833500938)
INFO flwr 2024-04-17 14:17:53,127 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 14:17:53,128 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:18:02,326 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 14:18:03,522 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 74.86541270501039)
INFO flwr 2024-04-17 14:18:03,522 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 14:18:03,522 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:18:12,551 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 14:18:14,037 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 85.38055886600341)
INFO flwr 2024-04-17 14:18:14,037 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 14:18:14,037 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:18:23,418 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 14:18:24,598 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 95.94210894400021)
INFO flwr 2024-04-17 14:18:24,599 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 14:18:24,599 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:18:33,179 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 14:18:34,335 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 105.67930780400638)
INFO flwr 2024-04-17 14:18:34,336 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 14:18:34,336 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:18:43,319 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 14:18:44,699 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 116.04286312000477)
INFO flwr 2024-04-17 14:18:44,699 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 14:18:44,699 | server.py:153 | FL finished in 116.04332805800368
INFO flwr 2024-04-17 14:18:44,700 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 14:18:44,700 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 14:18:44,700 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 14:18:44,700 | app.py:229 | app_fit: losses_centralized [(0, 2.302537441253662), (1, 2.095494031906128), (2, 2.350942373275757), (3, 2.358342170715332), (4, 2.358342170715332), (5, 2.358342170715332), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-17 14:18:44,700 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.094), (1, 0.3655), (2, 0.1102), (3, 0.1028), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_141627-tvknohms
wandb: Find logs at: ./wandb/offline-run-20240417_141627-tvknohms/logs
INFO flwr 2024-04-17 14:18:48,254 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 14:25:56,562 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=625641)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=625641)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 14:26:01,648	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 14:26:02,479	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 14:26:02,941	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 14:26:03,319	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fb8ee723c0b5dd3b.zip' (39.83MiB) to Ray cluster...
2024-04-17 14:26:03,439	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fb8ee723c0b5dd3b.zip'.
INFO flwr 2024-04-17 14:26:14,566 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 77023706726.0, 'memory': 169721982362.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 14:26:14,566 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 14:26:14,567 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 14:26:14,586 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 14:26:14,587 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 14:26:14,588 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 14:26:14,588 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 14:26:18,604 | server.py:94 | initial parameters (loss, other metrics): 2.302725076675415, {'accuracy': 0.0904, 'data_size': 10000}
INFO flwr 2024-04-17 14:26:18,605 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 14:26:18,605 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=630650)[0m 2024-04-17 14:26:20.669083: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=630650)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=630650)[0m 2024-04-17 14:26:23.002999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=630650)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=630650)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=630651)[0m 2024-04-17 14:26:20.825622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=630651)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=630651)[0m 2024-04-17 14:26:23.205802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 14:26:38,562 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 14:26:39,951 | server.py:125 | fit progress: (1, 2.2231833934783936, {'accuracy': 0.3246, 'data_size': 10000}, 21.346281649006414)
INFO flwr 2024-04-17 14:26:39,951 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 14:26:39,952 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:26:49,462 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 14:26:50,870 | server.py:125 | fit progress: (2, 1.9345861673355103, {'accuracy': 0.5272, 'data_size': 10000}, 32.264793651003856)
INFO flwr 2024-04-17 14:26:50,870 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 14:26:50,870 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:26:59,245 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 14:27:00,628 | server.py:125 | fit progress: (3, 1.7355965375900269, {'accuracy': 0.7236, 'data_size': 10000}, 42.022817766002845)
INFO flwr 2024-04-17 14:27:00,628 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 14:27:00,628 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:27:09,515 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 14:27:10,887 | server.py:125 | fit progress: (4, 1.626695990562439, {'accuracy': 0.8349, 'data_size': 10000}, 52.28220062299806)
INFO flwr 2024-04-17 14:27:10,887 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 14:27:10,887 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:27:19,320 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 14:27:20,709 | server.py:125 | fit progress: (5, 1.575495958328247, {'accuracy': 0.8852, 'data_size': 10000}, 62.10404626499803)
INFO flwr 2024-04-17 14:27:20,709 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 14:27:20,709 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:27:29,750 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 14:27:31,210 | server.py:125 | fit progress: (6, 1.5799099206924438, {'accuracy': 0.8811, 'data_size': 10000}, 72.60505846599699)
INFO flwr 2024-04-17 14:27:31,210 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 14:27:31,210 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:27:40,796 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 14:27:42,006 | server.py:125 | fit progress: (7, 1.5612807273864746, {'accuracy': 0.8991, 'data_size': 10000}, 83.40137264999794)
INFO flwr 2024-04-17 14:27:42,006 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 14:27:42,007 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:27:51,196 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 14:27:52,594 | server.py:125 | fit progress: (8, 1.5575430393218994, {'accuracy': 0.9031, 'data_size': 10000}, 93.98956067900872)
INFO flwr 2024-04-17 14:27:52,595 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 14:27:52,595 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:28:01,486 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 14:28:02,688 | server.py:125 | fit progress: (9, 1.5427513122558594, {'accuracy': 0.9181, 'data_size': 10000}, 104.08288050300325)
INFO flwr 2024-04-17 14:28:02,688 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 14:28:02,688 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:28:11,670 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 14:28:12,847 | server.py:125 | fit progress: (10, 1.538469672203064, {'accuracy': 0.9224, 'data_size': 10000}, 114.24226115099736)
INFO flwr 2024-04-17 14:28:12,847 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 14:28:12,847 | server.py:153 | FL finished in 114.24273050299962
INFO flwr 2024-04-17 14:28:12,848 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 14:28:12,848 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 14:28:12,848 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 14:28:12,848 | app.py:229 | app_fit: losses_centralized [(0, 2.302725076675415), (1, 2.2231833934783936), (2, 1.9345861673355103), (3, 1.7355965375900269), (4, 1.626695990562439), (5, 1.575495958328247), (6, 1.5799099206924438), (7, 1.5612807273864746), (8, 1.5575430393218994), (9, 1.5427513122558594), (10, 1.538469672203064)]
INFO flwr 2024-04-17 14:28:12,848 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0904), (1, 0.3246), (2, 0.5272), (3, 0.7236), (4, 0.8349), (5, 0.8852), (6, 0.8811), (7, 0.8991), (8, 0.9031), (9, 0.9181), (10, 0.9224)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9224
wandb:     loss 1.53847
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_142556-xdo7itab
wandb: Find logs at: ./wandb/offline-run-20240417_142556-xdo7itab/logs
INFO flwr 2024-04-17 14:28:16,369 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 14:35:25,288 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=630646)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=630646)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 14:35:32,765	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 14:35:34,261	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 14:35:34,749	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 14:35:35,142	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_52cf365527fe8849.zip' (39.99MiB) to Ray cluster...
2024-04-17 14:35:35,262	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_52cf365527fe8849.zip'.
INFO flwr 2024-04-17 14:35:46,424 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 171093240832.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 77611388928.0}
INFO flwr 2024-04-17 14:35:46,425 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 14:35:46,425 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 14:35:46,443 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 14:35:46,444 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 14:35:46,445 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 14:35:46,445 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 14:35:49,176 | server.py:94 | initial parameters (loss, other metrics): 2.3025991916656494, {'accuracy': 0.1132, 'data_size': 10000}
INFO flwr 2024-04-17 14:35:49,176 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 14:35:49,177 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=637489)[0m 2024-04-17 14:35:53.246214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=637489)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=637489)[0m 2024-04-17 14:35:56.240951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=637489)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=637489)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=637485)[0m 2024-04-17 14:35:53.289694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=637485)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=637486)[0m 2024-04-17 14:35:56.240954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 14:36:17,852 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 14:36:19,285 | server.py:125 | fit progress: (1, 2.3012611865997314, {'accuracy': 0.203, 'data_size': 10000}, 30.108264684997266)
INFO flwr 2024-04-17 14:36:19,285 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 14:36:19,285 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:36:29,318 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 14:36:30,514 | server.py:125 | fit progress: (2, 2.2975566387176514, {'accuracy': 0.4347, 'data_size': 10000}, 41.33788471200387)
INFO flwr 2024-04-17 14:36:30,515 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 14:36:30,515 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:36:39,715 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 14:36:41,143 | server.py:125 | fit progress: (3, 2.289055109024048, {'accuracy': 0.57, 'data_size': 10000}, 51.96660700900247)
INFO flwr 2024-04-17 14:36:41,143 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 14:36:41,144 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:36:50,595 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 14:36:51,818 | server.py:125 | fit progress: (4, 2.2721004486083984, {'accuracy': 0.579, 'data_size': 10000}, 62.64126036599919)
INFO flwr 2024-04-17 14:36:51,818 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 14:36:51,818 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:37:00,794 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 14:37:02,213 | server.py:125 | fit progress: (5, 2.2403006553649902, {'accuracy': 0.5923, 'data_size': 10000}, 73.03660443199624)
INFO flwr 2024-04-17 14:37:02,213 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 14:37:02,213 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:37:11,449 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 14:37:12,651 | server.py:125 | fit progress: (6, 2.1843817234039307, {'accuracy': 0.6042, 'data_size': 10000}, 83.4744142020063)
INFO flwr 2024-04-17 14:37:12,651 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 14:37:12,651 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:37:21,661 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 14:37:23,111 | server.py:125 | fit progress: (7, 2.0984272956848145, {'accuracy': 0.6217, 'data_size': 10000}, 93.93441341500147)
INFO flwr 2024-04-17 14:37:23,111 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 14:37:23,111 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:37:32,174 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 14:37:33,598 | server.py:125 | fit progress: (8, 1.9956706762313843, {'accuracy': 0.6424, 'data_size': 10000}, 104.42160332700587)
INFO flwr 2024-04-17 14:37:33,598 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 14:37:33,598 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:37:42,759 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 14:37:44,145 | server.py:125 | fit progress: (9, 1.9031230211257935, {'accuracy': 0.6647, 'data_size': 10000}, 114.96905158300069)
INFO flwr 2024-04-17 14:37:44,146 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 14:37:44,146 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:37:53,368 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 14:37:54,738 | server.py:125 | fit progress: (10, 1.8339537382125854, {'accuracy': 0.6879, 'data_size': 10000}, 125.56177273399953)
INFO flwr 2024-04-17 14:37:54,738 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 14:37:54,739 | server.py:153 | FL finished in 125.56229340100253
INFO flwr 2024-04-17 14:37:54,739 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 14:37:54,739 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 14:37:54,739 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 14:37:54,739 | app.py:229 | app_fit: losses_centralized [(0, 2.3025991916656494), (1, 2.3012611865997314), (2, 2.2975566387176514), (3, 2.289055109024048), (4, 2.2721004486083984), (5, 2.2403006553649902), (6, 2.1843817234039307), (7, 2.0984272956848145), (8, 1.9956706762313843), (9, 1.9031230211257935), (10, 1.8339537382125854)]
INFO flwr 2024-04-17 14:37:54,740 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1132), (1, 0.203), (2, 0.4347), (3, 0.57), (4, 0.579), (5, 0.5923), (6, 0.6042), (7, 0.6217), (8, 0.6424), (9, 0.6647), (10, 0.6879)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6879
wandb:     loss 1.83395
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_143524-6iet7ig3
wandb: Find logs at: ./wandb/offline-run-20240417_143524-6iet7ig3/logs
INFO flwr 2024-04-17 14:37:58,290 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 14:45:06,211 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=637486)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=637486)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 14:45:11,023	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 14:45:12,005	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 14:45:12,480	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 14:45:12,863	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a472fe2750182dbf.zip' (40.14MiB) to Ray cluster...
2024-04-17 14:45:12,993	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a472fe2750182dbf.zip'.
INFO flwr 2024-04-17 14:45:24,176 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77044595097.0, 'accelerator_type:TITAN': 1.0, 'memory': 169770721895.0}
INFO flwr 2024-04-17 14:45:24,177 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 14:45:24,177 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 14:45:24,200 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 14:45:24,201 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 14:45:24,201 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 14:45:24,201 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 14:45:27,142 | server.py:94 | initial parameters (loss, other metrics): 2.3025283813476562, {'accuracy': 0.104, 'data_size': 10000}
INFO flwr 2024-04-17 14:45:27,143 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 14:45:27,144 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=645278)[0m 2024-04-17 14:45:30.345391: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=645278)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=645278)[0m 2024-04-17 14:45:32.622993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=645285)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=645285)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=645279)[0m 2024-04-17 14:45:30.517950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=645279)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=645279)[0m 2024-04-17 14:45:33.059485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 14:45:52,664 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 14:45:53,834 | server.py:125 | fit progress: (1, 2.3024508953094482, {'accuracy': 0.1046, 'data_size': 10000}, 26.690054514008807)
INFO flwr 2024-04-17 14:45:53,834 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 14:45:53,834 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:46:03,673 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 14:46:05,070 | server.py:125 | fit progress: (2, 2.3023459911346436, {'accuracy': 0.1051, 'data_size': 10000}, 37.92573469600757)
INFO flwr 2024-04-17 14:46:05,070 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 14:46:05,070 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:46:14,458 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 14:46:15,664 | server.py:125 | fit progress: (3, 2.302201509475708, {'accuracy': 0.1067, 'data_size': 10000}, 48.51976434199605)
INFO flwr 2024-04-17 14:46:15,664 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 14:46:15,664 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:46:24,479 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 14:46:25,878 | server.py:125 | fit progress: (4, 2.3020238876342773, {'accuracy': 0.1089, 'data_size': 10000}, 58.73461665200011)
INFO flwr 2024-04-17 14:46:25,879 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 14:46:25,879 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:46:34,648 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 14:46:36,035 | server.py:125 | fit progress: (5, 2.3018033504486084, {'accuracy': 0.1168, 'data_size': 10000}, 68.89105087300413)
INFO flwr 2024-04-17 14:46:36,035 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 14:46:36,035 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:46:44,900 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 14:46:46,298 | server.py:125 | fit progress: (6, 2.3015472888946533, {'accuracy': 0.1267, 'data_size': 10000}, 79.15386897799908)
INFO flwr 2024-04-17 14:46:46,298 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 14:46:46,298 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:46:54,855 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 14:46:56,032 | server.py:125 | fit progress: (7, 2.3012442588806152, {'accuracy': 0.1452, 'data_size': 10000}, 88.88825860100042)
INFO flwr 2024-04-17 14:46:56,032 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 14:46:56,033 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:47:04,627 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 14:47:05,794 | server.py:125 | fit progress: (8, 2.3008906841278076, {'accuracy': 0.1718, 'data_size': 10000}, 98.65014654300467)
INFO flwr 2024-04-17 14:47:05,794 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 14:47:05,794 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:47:14,751 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 14:47:16,166 | server.py:125 | fit progress: (9, 2.3004815578460693, {'accuracy': 0.211, 'data_size': 10000}, 109.02256650700292)
INFO flwr 2024-04-17 14:47:16,167 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 14:47:16,167 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:47:24,882 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 14:47:26,051 | server.py:125 | fit progress: (10, 2.300014019012451, {'accuracy': 0.2587, 'data_size': 10000}, 118.90760207900894)
INFO flwr 2024-04-17 14:47:26,052 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 14:47:26,052 | server.py:153 | FL finished in 118.9080989399954
INFO flwr 2024-04-17 14:47:26,052 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 14:47:26,052 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 14:47:26,052 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 14:47:26,053 | app.py:229 | app_fit: losses_centralized [(0, 2.3025283813476562), (1, 2.3024508953094482), (2, 2.3023459911346436), (3, 2.302201509475708), (4, 2.3020238876342773), (5, 2.3018033504486084), (6, 2.3015472888946533), (7, 2.3012442588806152), (8, 2.3008906841278076), (9, 2.3004815578460693), (10, 2.300014019012451)]
INFO flwr 2024-04-17 14:47:26,053 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.104), (1, 0.1046), (2, 0.1051), (3, 0.1067), (4, 0.1089), (5, 0.1168), (6, 0.1267), (7, 0.1452), (8, 0.1718), (9, 0.211), (10, 0.2587)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2587
wandb:     loss 2.30001
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_144505-btvffvip
wandb: Find logs at: ./wandb/offline-run-20240417_144505-btvffvip/logs
INFO flwr 2024-04-17 14:47:29,551 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 14:54:37,749 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=645279)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=645279)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 14:54:42,278	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 14:54:43,141	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 14:54:43,617	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 14:54:43,998	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d4b4f1ef36729503.zip' (40.30MiB) to Ray cluster...
2024-04-17 14:54:44,127	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d4b4f1ef36729503.zip'.
INFO flwr 2024-04-17 14:54:55,207 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77093942476.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169885865780.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 14:54:55,207 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 14:54:55,208 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 14:54:55,227 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 14:54:55,229 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 14:54:55,229 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 14:54:55,229 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 14:54:57,918 | server.py:94 | initial parameters (loss, other metrics): 2.3026959896087646, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-17 14:54:57,919 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 14:54:57,919 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=649643)[0m 2024-04-17 14:55:01.340736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=649643)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=649643)[0m 2024-04-17 14:55:03.656936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=649643)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=649643)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=649636)[0m 2024-04-17 14:55:01.534319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=649636)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=649636)[0m 2024-04-17 14:55:03.842430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 14:55:19,682 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 14:55:21,083 | server.py:125 | fit progress: (1, 2.302689790725708, {'accuracy': 0.0892, 'data_size': 10000}, 23.163546064999537)
INFO flwr 2024-04-17 14:55:21,083 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 14:55:21,083 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:55:30,754 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 14:55:32,144 | server.py:125 | fit progress: (2, 2.3026819229125977, {'accuracy': 0.0892, 'data_size': 10000}, 34.22441327100387)
INFO flwr 2024-04-17 14:55:32,144 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 14:55:32,144 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:55:41,225 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 14:55:42,599 | server.py:125 | fit progress: (3, 2.3026721477508545, {'accuracy': 0.0892, 'data_size': 10000}, 44.67993411399948)
INFO flwr 2024-04-17 14:55:42,599 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 14:55:42,600 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:55:51,470 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 14:55:52,832 | server.py:125 | fit progress: (4, 2.302661418914795, {'accuracy': 0.0892, 'data_size': 10000}, 54.91281991099822)
INFO flwr 2024-04-17 14:55:52,832 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 14:55:52,833 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:56:01,650 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 14:56:03,045 | server.py:125 | fit progress: (5, 2.3026487827301025, {'accuracy': 0.0892, 'data_size': 10000}, 65.12549219900393)
INFO flwr 2024-04-17 14:56:03,045 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 14:56:03,045 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:56:12,364 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 14:56:13,537 | server.py:125 | fit progress: (6, 2.3026363849639893, {'accuracy': 0.0892, 'data_size': 10000}, 75.61795379400428)
INFO flwr 2024-04-17 14:56:13,537 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 14:56:13,538 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:56:22,902 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 14:56:24,327 | server.py:125 | fit progress: (7, 2.3026223182678223, {'accuracy': 0.0892, 'data_size': 10000}, 86.40768132799712)
INFO flwr 2024-04-17 14:56:24,327 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 14:56:24,327 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:56:33,305 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 14:56:34,496 | server.py:125 | fit progress: (8, 2.302608013153076, {'accuracy': 0.0892, 'data_size': 10000}, 96.57696051300445)
INFO flwr 2024-04-17 14:56:34,496 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 14:56:34,497 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:56:43,376 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 14:56:44,533 | server.py:125 | fit progress: (9, 2.302593469619751, {'accuracy': 0.0892, 'data_size': 10000}, 106.61362130599446)
INFO flwr 2024-04-17 14:56:44,533 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 14:56:44,533 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 14:56:53,242 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 14:56:54,633 | server.py:125 | fit progress: (10, 2.3025784492492676, {'accuracy': 0.0892, 'data_size': 10000}, 116.7135447699984)
INFO flwr 2024-04-17 14:56:54,633 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 14:56:54,633 | server.py:153 | FL finished in 116.71417181800643
INFO flwr 2024-04-17 14:56:54,634 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 14:56:54,634 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 14:56:54,634 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 14:56:54,634 | app.py:229 | app_fit: losses_centralized [(0, 2.3026959896087646), (1, 2.302689790725708), (2, 2.3026819229125977), (3, 2.3026721477508545), (4, 2.302661418914795), (5, 2.3026487827301025), (6, 2.3026363849639893), (7, 2.3026223182678223), (8, 2.302608013153076), (9, 2.302593469619751), (10, 2.3025784492492676)]
INFO flwr 2024-04-17 14:56:54,634 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.0892), (2, 0.0892), (3, 0.0892), (4, 0.0892), (5, 0.0892), (6, 0.0892), (7, 0.0892), (8, 0.0892), (9, 0.0892), (10, 0.0892)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0892
wandb:     loss 2.30258
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_145437-qywh071x
wandb: Find logs at: ./wandb/offline-run-20240417_145437-qywh071x/logs
INFO flwr 2024-04-17 14:56:58,182 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 15:04:06,118 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=649635)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=649635)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 15:04:10,842	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 15:04:11,701	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 15:04:12,223	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 15:04:12,619	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d52c3fc4ff58485.zip' (40.45MiB) to Ray cluster...
2024-04-17 15:04:12,747	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d52c3fc4ff58485.zip'.
INFO flwr 2024-04-17 15:04:23,836 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77136368025.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169984858727.0}
INFO flwr 2024-04-17 15:04:23,837 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 15:04:23,837 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 15:04:23,859 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 15:04:23,860 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 15:04:23,860 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 15:04:23,860 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 15:04:27,408 | server.py:94 | initial parameters (loss, other metrics): 2.30275559425354, {'accuracy': 0.1047, 'data_size': 10000}
INFO flwr 2024-04-17 15:04:27,408 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 15:04:27,409 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=654010)[0m 2024-04-17 15:04:30.002798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=654010)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=654014)[0m 2024-04-17 15:04:32.361792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=654014)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=654014)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=654007)[0m 2024-04-17 15:04:30.376751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=654007)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=654007)[0m 2024-04-17 15:04:32.895788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 15:04:49,227 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 15:04:50,638 | server.py:125 | fit progress: (1, 2.1284825801849365, {'accuracy': 0.3325, 'data_size': 10000}, 23.229166819000966)
INFO flwr 2024-04-17 15:04:50,638 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 15:04:50,639 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:05:00,007 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 15:05:01,395 | server.py:125 | fit progress: (2, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 33.98591520800255)
INFO flwr 2024-04-17 15:05:01,395 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 15:05:01,395 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:05:09,923 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 15:05:11,311 | server.py:125 | fit progress: (3, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 43.9019592100085)
INFO flwr 2024-04-17 15:05:11,311 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 15:05:11,311 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:05:20,718 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 15:05:22,118 | server.py:125 | fit progress: (4, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 54.70907691400498)
INFO flwr 2024-04-17 15:05:22,118 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 15:05:22,118 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:05:30,941 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 15:05:32,109 | server.py:125 | fit progress: (5, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 64.70045591999951)
INFO flwr 2024-04-17 15:05:32,110 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 15:05:32,110 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:05:41,971 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 15:05:43,412 | server.py:125 | fit progress: (6, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 76.0032491160091)
INFO flwr 2024-04-17 15:05:43,412 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 15:05:43,413 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:05:52,597 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 15:05:53,789 | server.py:125 | fit progress: (7, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 86.38044920400716)
INFO flwr 2024-04-17 15:05:53,790 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 15:05:53,790 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:06:02,822 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 15:06:04,008 | server.py:125 | fit progress: (8, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 96.5990457660082)
INFO flwr 2024-04-17 15:06:04,008 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 15:06:04,008 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:06:13,139 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 15:06:14,521 | server.py:125 | fit progress: (9, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 107.1121341620019)
INFO flwr 2024-04-17 15:06:14,521 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 15:06:14,521 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:06:23,854 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 15:06:25,034 | server.py:125 | fit progress: (10, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 117.6253001550067)
INFO flwr 2024-04-17 15:06:25,034 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 15:06:25,035 | server.py:153 | FL finished in 117.62576059700223
INFO flwr 2024-04-17 15:06:25,035 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 15:06:25,035 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 15:06:25,035 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 15:06:25,035 | app.py:229 | app_fit: losses_centralized [(0, 2.30275559425354), (1, 2.1284825801849365), (2, 2.347642183303833), (3, 2.347642183303833), (4, 2.347642183303833), (5, 2.347642183303833), (6, 2.347642183303833), (7, 2.347642183303833), (8, 2.347642183303833), (9, 2.347642183303833), (10, 2.347642183303833)]
INFO flwr 2024-04-17 15:06:25,035 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1047), (1, 0.3325), (2, 0.1135), (3, 0.1135), (4, 0.1135), (5, 0.1135), (6, 0.1135), (7, 0.1135), (8, 0.1135), (9, 0.1135), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.34764
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_150405-uz1xoq5h
wandb: Find logs at: ./wandb/offline-run-20240417_150405-uz1xoq5h/logs
INFO flwr 2024-04-17 15:06:28,547 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 15:13:37,246 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=654004)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=654004)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 15:13:42,846	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 15:13:43,671	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 15:13:44,134	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 15:13:44,511	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ec38e94b9eb3b7e4.zip' (40.61MiB) to Ray cluster...
2024-04-17 15:13:44,642	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ec38e94b9eb3b7e4.zip'.
INFO flwr 2024-04-17 15:13:55,722 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169757036749.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'object_store_memory': 77038730035.0}
INFO flwr 2024-04-17 15:13:55,722 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 15:13:55,722 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 15:13:55,744 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 15:13:55,744 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 15:13:55,745 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 15:13:55,745 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 15:13:58,456 | server.py:94 | initial parameters (loss, other metrics): 2.3028926849365234, {'accuracy': 0.0994, 'data_size': 10000}
INFO flwr 2024-04-17 15:13:58,457 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 15:13:58,457 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=658736)[0m 2024-04-17 15:14:01.989191: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=658736)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=658736)[0m 2024-04-17 15:14:04.254673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=658731)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=658731)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=658734)[0m 2024-04-17 15:14:02.166364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=658734)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=658731)[0m 2024-04-17 15:14:04.401033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 15:14:19,865 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 15:14:21,307 | server.py:125 | fit progress: (1, 2.2121505737304688, {'accuracy': 0.4948, 'data_size': 10000}, 22.850196903003962)
INFO flwr 2024-04-17 15:14:21,308 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 15:14:21,308 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:14:30,635 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 15:14:32,017 | server.py:125 | fit progress: (2, 1.8483024835586548, {'accuracy': 0.6209, 'data_size': 10000}, 33.55944524500228)
INFO flwr 2024-04-17 15:14:32,017 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 15:14:32,017 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:14:41,006 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 15:14:42,382 | server.py:125 | fit progress: (3, 1.6630966663360596, {'accuracy': 0.7972, 'data_size': 10000}, 43.92519866899238)
INFO flwr 2024-04-17 15:14:42,383 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 15:14:42,383 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:14:50,954 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 15:14:52,339 | server.py:125 | fit progress: (4, 1.5920747518539429, {'accuracy': 0.8691, 'data_size': 10000}, 53.88135565399716)
INFO flwr 2024-04-17 15:14:52,339 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 15:14:52,339 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:15:01,143 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 15:15:02,530 | server.py:125 | fit progress: (5, 1.5730259418487549, {'accuracy': 0.8874, 'data_size': 10000}, 64.07283101600478)
INFO flwr 2024-04-17 15:15:02,530 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 15:15:02,531 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:15:11,193 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 15:15:12,359 | server.py:125 | fit progress: (6, 1.5479713678359985, {'accuracy': 0.9131, 'data_size': 10000}, 73.9017046539957)
INFO flwr 2024-04-17 15:15:12,359 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 15:15:12,359 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:15:21,366 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 15:15:22,762 | server.py:125 | fit progress: (7, 1.5395934581756592, {'accuracy': 0.9215, 'data_size': 10000}, 84.30437479399552)
INFO flwr 2024-04-17 15:15:22,762 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 15:15:22,762 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:15:31,401 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 15:15:32,585 | server.py:125 | fit progress: (8, 1.538447380065918, {'accuracy': 0.9222, 'data_size': 10000}, 94.12764633400366)
INFO flwr 2024-04-17 15:15:32,585 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 15:15:32,585 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:15:41,587 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 15:15:42,771 | server.py:125 | fit progress: (9, 1.53309965133667, {'accuracy': 0.9283, 'data_size': 10000}, 104.3136971549975)
INFO flwr 2024-04-17 15:15:42,771 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 15:15:42,771 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:15:51,868 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 15:15:53,261 | server.py:125 | fit progress: (10, 1.5353270769119263, {'accuracy': 0.9257, 'data_size': 10000}, 114.80383953399723)
INFO flwr 2024-04-17 15:15:53,261 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 15:15:53,262 | server.py:153 | FL finished in 114.80438347099698
INFO flwr 2024-04-17 15:15:53,262 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 15:15:53,262 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 15:15:53,262 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 15:15:53,262 | app.py:229 | app_fit: losses_centralized [(0, 2.3028926849365234), (1, 2.2121505737304688), (2, 1.8483024835586548), (3, 1.6630966663360596), (4, 1.5920747518539429), (5, 1.5730259418487549), (6, 1.5479713678359985), (7, 1.5395934581756592), (8, 1.538447380065918), (9, 1.53309965133667), (10, 1.5353270769119263)]
INFO flwr 2024-04-17 15:15:53,263 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0994), (1, 0.4948), (2, 0.6209), (3, 0.7972), (4, 0.8691), (5, 0.8874), (6, 0.9131), (7, 0.9215), (8, 0.9222), (9, 0.9283), (10, 0.9257)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9257
wandb:     loss 1.53533
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_151336-zzmaof5a
wandb: Find logs at: ./wandb/offline-run-20240417_151336-zzmaof5a/logs
INFO flwr 2024-04-17 15:15:56,806 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 15:23:04,528 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=658727)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=658727)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 15:23:09,253	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 15:23:10,126	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 15:23:10,613	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 15:23:10,987	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fb9dc047b5ef34dc.zip' (40.63MiB) to Ray cluster...
2024-04-17 15:23:11,112	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fb9dc047b5ef34dc.zip'.
INFO flwr 2024-04-17 15:23:22,228 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77000884224.0, 'GPU': 1.0, 'memory': 169668729856.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 15:23:22,228 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 15:23:22,228 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 15:23:22,242 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 15:23:22,243 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 15:23:22,243 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 15:23:22,244 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 15:23:25,378 | server.py:94 | initial parameters (loss, other metrics): 2.302478313446045, {'accuracy': 0.0843, 'data_size': 10000}
INFO flwr 2024-04-17 15:23:25,379 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 15:23:25,379 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=662508)[0m 2024-04-17 15:23:28.445098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=662508)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=662508)[0m 2024-04-17 15:23:30.777766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=662515)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=662515)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=662515)[0m 2024-04-17 15:23:28.727717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=662515)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=662515)[0m 2024-04-17 15:23:31.187720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 15:23:46,745 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 15:23:48,154 | server.py:125 | fit progress: (1, 2.3014767169952393, {'accuracy': 0.1196, 'data_size': 10000}, 22.775319813008537)
INFO flwr 2024-04-17 15:23:48,155 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 15:23:48,155 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:23:57,463 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 15:23:58,867 | server.py:125 | fit progress: (2, 2.298588991165161, {'accuracy': 0.2464, 'data_size': 10000}, 33.48775919100444)
INFO flwr 2024-04-17 15:23:58,867 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 15:23:58,867 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:24:08,041 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 15:24:09,428 | server.py:125 | fit progress: (3, 2.2914962768554688, {'accuracy': 0.4526, 'data_size': 10000}, 44.048961057007546)
INFO flwr 2024-04-17 15:24:09,428 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 15:24:09,428 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:24:17,638 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 15:24:19,030 | server.py:125 | fit progress: (4, 2.2767627239227295, {'accuracy': 0.6173, 'data_size': 10000}, 53.651506624999456)
INFO flwr 2024-04-17 15:24:19,031 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 15:24:19,031 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:24:27,821 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 15:24:28,993 | server.py:125 | fit progress: (5, 2.2479188442230225, {'accuracy': 0.6468, 'data_size': 10000}, 63.61449156999879)
INFO flwr 2024-04-17 15:24:28,994 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 15:24:28,994 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:24:38,112 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 15:24:39,540 | server.py:125 | fit progress: (6, 2.194809675216675, {'accuracy': 0.6687, 'data_size': 10000}, 74.16061824200733)
INFO flwr 2024-04-17 15:24:39,540 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 15:24:39,540 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:24:48,592 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 15:24:49,760 | server.py:125 | fit progress: (7, 2.1092352867126465, {'accuracy': 0.684, 'data_size': 10000}, 84.3812392730033)
INFO flwr 2024-04-17 15:24:49,760 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 15:24:49,761 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:24:58,171 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 15:24:59,344 | server.py:125 | fit progress: (8, 1.9997172355651855, {'accuracy': 0.6977, 'data_size': 10000}, 93.96515957500378)
INFO flwr 2024-04-17 15:24:59,344 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 15:24:59,345 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:25:08,509 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 15:25:09,926 | server.py:125 | fit progress: (9, 1.8965470790863037, {'accuracy': 0.7067, 'data_size': 10000}, 104.54744377400493)
INFO flwr 2024-04-17 15:25:09,927 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 15:25:09,927 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:25:19,286 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 15:25:20,694 | server.py:125 | fit progress: (10, 1.8220854997634888, {'accuracy': 0.7164, 'data_size': 10000}, 115.3148265290074)
INFO flwr 2024-04-17 15:25:20,694 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 15:25:20,694 | server.py:153 | FL finished in 115.31529358300031
INFO flwr 2024-04-17 15:25:20,694 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 15:25:20,695 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 15:25:20,695 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 15:25:20,695 | app.py:229 | app_fit: losses_centralized [(0, 2.302478313446045), (1, 2.3014767169952393), (2, 2.298588991165161), (3, 2.2914962768554688), (4, 2.2767627239227295), (5, 2.2479188442230225), (6, 2.194809675216675), (7, 2.1092352867126465), (8, 1.9997172355651855), (9, 1.8965470790863037), (10, 1.8220854997634888)]
INFO flwr 2024-04-17 15:25:20,695 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0843), (1, 0.1196), (2, 0.2464), (3, 0.4526), (4, 0.6173), (5, 0.6468), (6, 0.6687), (7, 0.684), (8, 0.6977), (9, 0.7067), (10, 0.7164)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7164
wandb:     loss 1.82209
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_152304-p7s8q48y
wandb: Find logs at: ./wandb/offline-run-20240417_152304-p7s8q48y/logs
INFO flwr 2024-04-17 15:25:24,198 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 15:32:31,965 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=662507)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=662507)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 15:32:36,707	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 15:32:37,575	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 15:32:38,086	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 15:32:38,466	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4d715d5296cf5a70.zip' (40.78MiB) to Ray cluster...
2024-04-17 15:32:38,592	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4d715d5296cf5a70.zip'.
INFO flwr 2024-04-17 15:32:51,205 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 76999310131.0, 'accelerator_type:TITAN': 1.0, 'memory': 169665056973.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 15:32:51,206 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 15:32:51,206 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 15:32:51,227 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 15:32:51,228 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 15:32:51,228 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 15:32:51,228 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 15:32:53,917 | server.py:94 | initial parameters (loss, other metrics): 2.302368402481079, {'accuracy': 0.0881, 'data_size': 10000}
INFO flwr 2024-04-17 15:32:53,918 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 15:32:53,918 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=666869)[0m 2024-04-17 15:32:57.352781: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=666869)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=666865)[0m 2024-04-17 15:32:59.633032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=666867)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=666867)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=666868)[0m 2024-04-17 15:32:57.599178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=666868)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=666868)[0m 2024-04-17 15:32:59.867976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 15:33:15,702 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 15:33:17,112 | server.py:125 | fit progress: (1, 2.302274703979492, {'accuracy': 0.0915, 'data_size': 10000}, 23.19414112699451)
INFO flwr 2024-04-17 15:33:17,112 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 15:33:17,113 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:33:26,666 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 15:33:28,072 | server.py:125 | fit progress: (2, 2.3021256923675537, {'accuracy': 0.0976, 'data_size': 10000}, 34.154120260005584)
INFO flwr 2024-04-17 15:33:28,072 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 15:33:28,072 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:33:37,202 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 15:33:38,581 | server.py:125 | fit progress: (3, 2.3019354343414307, {'accuracy': 0.1104, 'data_size': 10000}, 44.66315061200294)
INFO flwr 2024-04-17 15:33:38,581 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 15:33:38,581 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:33:47,683 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 15:33:49,063 | server.py:125 | fit progress: (4, 2.3017032146453857, {'accuracy': 0.1269, 'data_size': 10000}, 55.145666145006544)
INFO flwr 2024-04-17 15:33:49,064 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 15:33:49,064 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:33:58,188 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 15:33:59,369 | server.py:125 | fit progress: (5, 2.301419973373413, {'accuracy': 0.1473, 'data_size': 10000}, 65.45163452500128)
INFO flwr 2024-04-17 15:33:59,370 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 15:33:59,370 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:34:08,349 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 15:34:09,535 | server.py:125 | fit progress: (6, 2.3010854721069336, {'accuracy': 0.1699, 'data_size': 10000}, 75.61768020500313)
INFO flwr 2024-04-17 15:34:09,536 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 15:34:09,536 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:34:18,753 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 15:34:20,127 | server.py:125 | fit progress: (7, 2.3006982803344727, {'accuracy': 0.1925, 'data_size': 10000}, 86.20892513000581)
INFO flwr 2024-04-17 15:34:20,127 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 15:34:20,127 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:34:28,882 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 15:34:30,072 | server.py:125 | fit progress: (8, 2.3002500534057617, {'accuracy': 0.2195, 'data_size': 10000}, 96.15411686099833)
INFO flwr 2024-04-17 15:34:30,072 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 15:34:30,072 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:34:38,588 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 15:34:39,962 | server.py:125 | fit progress: (9, 2.299734592437744, {'accuracy': 0.2498, 'data_size': 10000}, 106.0445250430057)
INFO flwr 2024-04-17 15:34:39,963 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 15:34:39,963 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:34:48,904 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 15:34:50,067 | server.py:125 | fit progress: (10, 2.2991487979888916, {'accuracy': 0.2815, 'data_size': 10000}, 116.14888784200593)
INFO flwr 2024-04-17 15:34:50,067 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 15:34:50,067 | server.py:153 | FL finished in 116.1494154969987
INFO flwr 2024-04-17 15:34:50,068 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 15:34:50,068 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 15:34:50,068 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 15:34:50,068 | app.py:229 | app_fit: losses_centralized [(0, 2.302368402481079), (1, 2.302274703979492), (2, 2.3021256923675537), (3, 2.3019354343414307), (4, 2.3017032146453857), (5, 2.301419973373413), (6, 2.3010854721069336), (7, 2.3006982803344727), (8, 2.3002500534057617), (9, 2.299734592437744), (10, 2.2991487979888916)]
INFO flwr 2024-04-17 15:34:50,068 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0881), (1, 0.0915), (2, 0.0976), (3, 0.1104), (4, 0.1269), (5, 0.1473), (6, 0.1699), (7, 0.1925), (8, 0.2195), (9, 0.2498), (10, 0.2815)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2815
wandb:     loss 2.29915
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_153231-7l551jun
wandb: Find logs at: ./wandb/offline-run-20240417_153231-7l551jun/logs
INFO flwr 2024-04-17 15:34:53,622 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 15:42:01,943 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=666861)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=666861)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 15:42:06,598	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 15:42:07,491	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 15:42:07,978	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 15:42:08,355	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5c35ea6cd7ac6738.zip' (40.94MiB) to Ray cluster...
2024-04-17 15:42:08,480	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5c35ea6cd7ac6738.zip'.
INFO flwr 2024-04-17 15:42:19,497 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169659463066.0, 'object_store_memory': 76996912742.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 15:42:19,497 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 15:42:19,497 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 15:42:19,513 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 15:42:19,516 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 15:42:19,517 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 15:42:19,517 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 15:42:22,705 | server.py:94 | initial parameters (loss, other metrics): 2.3027284145355225, {'accuracy': 0.1311, 'data_size': 10000}
INFO flwr 2024-04-17 15:42:22,705 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 15:42:22,706 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=671606)[0m 2024-04-17 15:42:25.677951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=671606)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=671604)[0m 2024-04-17 15:42:27.923793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=671609)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=671609)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=671603)[0m 2024-04-17 15:42:25.968765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=671603)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=671603)[0m 2024-04-17 15:42:28.354207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 15:42:43,468 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 15:42:44,677 | server.py:125 | fit progress: (1, 2.302720546722412, {'accuracy': 0.1326, 'data_size': 10000}, 21.97137085600116)
INFO flwr 2024-04-17 15:42:44,677 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 15:42:44,678 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:42:54,323 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 15:42:55,729 | server.py:125 | fit progress: (2, 2.3027117252349854, {'accuracy': 0.1334, 'data_size': 10000}, 33.02319873298984)
INFO flwr 2024-04-17 15:42:55,729 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 15:42:55,730 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:43:04,758 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 15:43:05,927 | server.py:125 | fit progress: (3, 2.302701473236084, {'accuracy': 0.1336, 'data_size': 10000}, 43.22141782598919)
INFO flwr 2024-04-17 15:43:05,928 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 15:43:05,928 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:43:14,828 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 15:43:16,238 | server.py:125 | fit progress: (4, 2.3026890754699707, {'accuracy': 0.1339, 'data_size': 10000}, 53.53200345399091)
INFO flwr 2024-04-17 15:43:16,238 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 15:43:16,238 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:43:25,283 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 15:43:26,667 | server.py:125 | fit progress: (5, 2.302676200866699, {'accuracy': 0.1352, 'data_size': 10000}, 63.96152104999055)
INFO flwr 2024-04-17 15:43:26,668 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 15:43:26,668 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:43:35,996 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 15:43:37,404 | server.py:125 | fit progress: (6, 2.3026621341705322, {'accuracy': 0.1364, 'data_size': 10000}, 74.69868055298866)
INFO flwr 2024-04-17 15:43:37,405 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 15:43:37,405 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:43:46,110 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 15:43:47,531 | server.py:125 | fit progress: (7, 2.302647352218628, {'accuracy': 0.1372, 'data_size': 10000}, 84.82491441999446)
INFO flwr 2024-04-17 15:43:47,531 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 15:43:47,531 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:43:56,415 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 15:43:57,831 | server.py:125 | fit progress: (8, 2.3026318550109863, {'accuracy': 0.1387, 'data_size': 10000}, 95.12551795999752)
INFO flwr 2024-04-17 15:43:57,832 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 15:43:57,832 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:44:06,822 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 15:44:07,998 | server.py:125 | fit progress: (9, 2.3026161193847656, {'accuracy': 0.14, 'data_size': 10000}, 105.2925973340025)
INFO flwr 2024-04-17 15:44:07,999 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 15:44:07,999 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:44:17,158 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 15:44:18,565 | server.py:125 | fit progress: (10, 2.3025996685028076, {'accuracy': 0.1409, 'data_size': 10000}, 115.85959800299315)
INFO flwr 2024-04-17 15:44:18,566 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 15:44:18,566 | server.py:153 | FL finished in 115.86007812600292
INFO flwr 2024-04-17 15:44:18,566 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 15:44:18,566 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 15:44:18,566 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 15:44:18,566 | app.py:229 | app_fit: losses_centralized [(0, 2.3027284145355225), (1, 2.302720546722412), (2, 2.3027117252349854), (3, 2.302701473236084), (4, 2.3026890754699707), (5, 2.302676200866699), (6, 2.3026621341705322), (7, 2.302647352218628), (8, 2.3026318550109863), (9, 2.3026161193847656), (10, 2.3025996685028076)]
INFO flwr 2024-04-17 15:44:18,567 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1311), (1, 0.1326), (2, 0.1334), (3, 0.1336), (4, 0.1339), (5, 0.1352), (6, 0.1364), (7, 0.1372), (8, 0.1387), (9, 0.14), (10, 0.1409)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1409
wandb:     loss 2.3026
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_154201-6ovus1l0
wandb: Find logs at: ./wandb/offline-run-20240417_154201-6ovus1l0/logs
INFO flwr 2024-04-17 15:44:22,126 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 15:51:30,500 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=671601)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=671601)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 15:51:36,212	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 15:51:37,063	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 15:51:37,554	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 15:51:37,930	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5a780026a3c6f4b5.zip' (41.09MiB) to Ray cluster...
2024-04-17 15:51:38,061	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5a780026a3c6f4b5.zip'.
INFO flwr 2024-04-17 15:51:49,194 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77013162393.0, 'GPU': 1.0, 'memory': 169697378919.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 15:51:49,194 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 15:51:49,195 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 15:51:49,214 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 15:51:49,216 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 15:51:49,216 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 15:51:49,216 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 15:51:52,398 | server.py:94 | initial parameters (loss, other metrics): 2.3026068210601807, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-17 15:51:52,405 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 15:51:52,411 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=676489)[0m 2024-04-17 15:51:55.316921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=676489)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=676489)[0m 2024-04-17 15:51:57.675625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=676489)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=676489)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=676485)[0m 2024-04-17 15:51:55.494966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=676485)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=676485)[0m 2024-04-17 15:51:57.830515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 15:52:17,850 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 15:52:19,259 | server.py:125 | fit progress: (1, 2.2005248069763184, {'accuracy': 0.2606, 'data_size': 10000}, 26.84821669899975)
INFO flwr 2024-04-17 15:52:19,259 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 15:52:19,259 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:52:31,148 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 15:52:32,543 | server.py:125 | fit progress: (2, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 40.13271368399728)
INFO flwr 2024-04-17 15:52:32,544 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 15:52:32,544 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:52:44,331 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 15:52:45,714 | server.py:125 | fit progress: (3, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 53.30384735799453)
INFO flwr 2024-04-17 15:52:45,715 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 15:52:45,715 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:52:57,357 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 15:52:58,778 | server.py:125 | fit progress: (4, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 66.36721389100421)
INFO flwr 2024-04-17 15:52:58,778 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 15:52:58,778 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:53:08,857 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 15:53:10,266 | server.py:125 | fit progress: (5, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 77.85523697899771)
INFO flwr 2024-04-17 15:53:10,266 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 15:53:10,266 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:53:21,599 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 15:53:22,761 | server.py:125 | fit progress: (6, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 90.35066979999829)
INFO flwr 2024-04-17 15:53:22,761 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 15:53:22,762 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:53:34,646 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 15:53:36,071 | server.py:125 | fit progress: (7, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 103.66033204599808)
INFO flwr 2024-04-17 15:53:36,071 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 15:53:36,071 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:53:47,145 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 15:53:48,312 | server.py:125 | fit progress: (8, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 115.90102315699914)
INFO flwr 2024-04-17 15:53:48,312 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 15:53:48,312 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:53:59,249 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 15:54:00,439 | server.py:125 | fit progress: (9, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 128.02875829399272)
INFO flwr 2024-04-17 15:54:00,440 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 15:54:00,440 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 15:54:11,145 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 15:54:12,522 | server.py:125 | fit progress: (10, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 140.11173693899764)
INFO flwr 2024-04-17 15:54:12,523 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 15:54:12,523 | server.py:153 | FL finished in 140.11217775099794
INFO flwr 2024-04-17 15:54:12,523 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 15:54:12,523 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 15:54:12,523 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 15:54:12,523 | app.py:229 | app_fit: losses_centralized [(0, 2.3026068210601807), (1, 2.2005248069763184), (2, 2.360142230987549), (3, 2.360142230987549), (4, 2.360142230987549), (5, 2.360142230987549), (6, 2.360142230987549), (7, 2.360142230987549), (8, 2.360142230987549), (9, 2.360142230987549), (10, 2.360142230987549)]
INFO flwr 2024-04-17 15:54:12,523 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.2606), (2, 0.101), (3, 0.101), (4, 0.101), (5, 0.101), (6, 0.101), (7, 0.101), (8, 0.101), (9, 0.101), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.36014
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_155130-zjzky5xv
wandb: Find logs at: ./wandb/offline-run-20240417_155130-zjzky5xv/logs
INFO flwr 2024-04-17 15:54:16,040 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 16:01:27,101 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=676484)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=676484)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 16:01:34,732	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 16:01:45,228	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 16:01:45,703	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 16:01:46,082	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_36ed5694823572ba.zip' (41.25MiB) to Ray cluster...
2024-04-17 16:01:46,209	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_36ed5694823572ba.zip'.
INFO flwr 2024-04-17 16:01:57,633 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77012982988.0, 'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169696960308.0}
INFO flwr 2024-04-17 16:01:57,634 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 16:01:57,634 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 16:01:57,654 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 16:01:57,655 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 16:01:57,656 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 16:01:57,656 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 16:02:01,075 | server.py:94 | initial parameters (loss, other metrics): 2.30249285697937, {'accuracy': 0.1387, 'data_size': 10000}
INFO flwr 2024-04-17 16:02:01,075 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 16:02:01,076 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=680879)[0m 2024-04-17 16:02:06.333999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=680879)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=680879)[0m 2024-04-17 16:02:09.620037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=680879)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=680879)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=680881)[0m 2024-04-17 16:02:06.529253: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=680881)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=680881)[0m 2024-04-17 16:02:09.619859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 16:02:33,495 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 16:02:34,956 | server.py:125 | fit progress: (1, 2.1869306564331055, {'accuracy': 0.4322, 'data_size': 10000}, 33.88012131099822)
INFO flwr 2024-04-17 16:02:34,956 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 16:02:34,956 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:02:47,112 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 16:02:48,524 | server.py:125 | fit progress: (2, 1.8542804718017578, {'accuracy': 0.6057, 'data_size': 10000}, 47.448909789003665)
INFO flwr 2024-04-17 16:02:48,525 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 16:02:48,525 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:03:00,184 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 16:03:01,574 | server.py:125 | fit progress: (3, 1.726238489151001, {'accuracy': 0.7336, 'data_size': 10000}, 60.49877498600108)
INFO flwr 2024-04-17 16:03:01,575 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 16:03:01,575 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:03:14,908 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 16:03:16,320 | server.py:125 | fit progress: (4, 1.6201269626617432, {'accuracy': 0.8398, 'data_size': 10000}, 75.24441412699525)
INFO flwr 2024-04-17 16:03:16,320 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 16:03:16,320 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:03:26,355 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 16:03:27,523 | server.py:125 | fit progress: (5, 1.5659904479980469, {'accuracy': 0.8947, 'data_size': 10000}, 86.44705433200579)
INFO flwr 2024-04-17 16:03:27,523 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 16:03:27,523 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:03:39,519 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 16:03:40,913 | server.py:125 | fit progress: (6, 1.5537325143814087, {'accuracy': 0.9072, 'data_size': 10000}, 99.83717087599507)
INFO flwr 2024-04-17 16:03:40,913 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 16:03:40,913 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:03:52,560 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 16:03:53,750 | server.py:125 | fit progress: (7, 1.5503630638122559, {'accuracy': 0.9106, 'data_size': 10000}, 112.67437751599937)
INFO flwr 2024-04-17 16:03:53,750 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 16:03:53,751 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:04:05,848 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 16:04:07,053 | server.py:125 | fit progress: (8, 1.5408231019973755, {'accuracy': 0.9205, 'data_size': 10000}, 125.97708411500207)
INFO flwr 2024-04-17 16:04:07,053 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 16:04:07,053 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:04:19,163 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 16:04:20,554 | server.py:125 | fit progress: (9, 1.5322986841201782, {'accuracy': 0.9283, 'data_size': 10000}, 139.47833797399653)
INFO flwr 2024-04-17 16:04:20,554 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 16:04:20,554 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:04:31,865 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 16:04:33,038 | server.py:125 | fit progress: (10, 1.5293554067611694, {'accuracy': 0.9315, 'data_size': 10000}, 151.96225228700496)
INFO flwr 2024-04-17 16:04:33,038 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 16:04:33,038 | server.py:153 | FL finished in 151.96269330600626
INFO flwr 2024-04-17 16:04:33,038 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 16:04:33,038 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 16:04:33,039 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 16:04:33,039 | app.py:229 | app_fit: losses_centralized [(0, 2.30249285697937), (1, 2.1869306564331055), (2, 1.8542804718017578), (3, 1.726238489151001), (4, 1.6201269626617432), (5, 1.5659904479980469), (6, 1.5537325143814087), (7, 1.5503630638122559), (8, 1.5408231019973755), (9, 1.5322986841201782), (10, 1.5293554067611694)]
INFO flwr 2024-04-17 16:04:33,039 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1387), (1, 0.4322), (2, 0.6057), (3, 0.7336), (4, 0.8398), (5, 0.8947), (6, 0.9072), (7, 0.9106), (8, 0.9205), (9, 0.9283), (10, 0.9315)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9315
wandb:     loss 1.52936
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_160123-rph731nt
wandb: Find logs at: ./wandb/offline-run-20240417_160123-rph731nt/logs
INFO flwr 2024-04-17 16:04:36,606 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 16:11:47,044 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=680881)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=680881)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 16:11:59,766	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 16:12:04,301	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 16:12:04,803	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 16:12:05,193	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_58b3a6ca74056202.zip' (41.41MiB) to Ray cluster...
2024-04-17 16:12:05,322	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_58b3a6ca74056202.zip'.
INFO flwr 2024-04-17 16:12:16,420 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169858624512.0, 'object_store_memory': 77082267648.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 16:12:16,421 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 16:12:16,421 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 16:12:16,442 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 16:12:16,443 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 16:12:16,443 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 16:12:16,444 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 16:12:20,313 | server.py:94 | initial parameters (loss, other metrics): 2.302640676498413, {'accuracy': 0.1087, 'data_size': 10000}
INFO flwr 2024-04-17 16:12:20,314 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 16:12:20,314 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=685603)[0m 2024-04-17 16:12:25.808076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=685603)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=685610)[0m 2024-04-17 16:12:33.716570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=685605)[0m 2024-04-17 16:12:25.910332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=685605)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=685610)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=685610)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=685596)[0m 2024-04-17 16:12:33.730407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 16:13:10,733 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 16:13:12,128 | server.py:125 | fit progress: (1, 2.301682710647583, {'accuracy': 0.1702, 'data_size': 10000}, 51.813877897991915)
INFO flwr 2024-04-17 16:13:12,128 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 16:13:12,128 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:13:23,710 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 16:13:24,890 | server.py:125 | fit progress: (2, 2.2987468242645264, {'accuracy': 0.4299, 'data_size': 10000}, 64.57608945199172)
INFO flwr 2024-04-17 16:13:24,890 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 16:13:24,891 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:13:36,671 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 16:13:38,078 | server.py:125 | fit progress: (3, 2.29171085357666, {'accuracy': 0.5496, 'data_size': 10000}, 77.76368448400171)
INFO flwr 2024-04-17 16:13:38,078 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 16:13:38,078 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:13:49,026 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 16:13:50,449 | server.py:125 | fit progress: (4, 2.276906728744507, {'accuracy': 0.5811, 'data_size': 10000}, 90.13504414699855)
INFO flwr 2024-04-17 16:13:50,449 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 16:13:50,450 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:14:02,189 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 16:14:03,563 | server.py:125 | fit progress: (5, 2.2477169036865234, {'accuracy': 0.5816, 'data_size': 10000}, 103.24904275499284)
INFO flwr 2024-04-17 16:14:03,564 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 16:14:03,564 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:14:15,377 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 16:14:16,567 | server.py:125 | fit progress: (6, 2.1950795650482178, {'accuracy': 0.5855, 'data_size': 10000}, 116.25313313199149)
INFO flwr 2024-04-17 16:14:16,567 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 16:14:16,568 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:14:28,187 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 16:14:29,369 | server.py:125 | fit progress: (7, 2.117138147354126, {'accuracy': 0.5908, 'data_size': 10000}, 129.05530751698825)
INFO flwr 2024-04-17 16:14:29,370 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 16:14:29,370 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:14:41,085 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 16:14:42,458 | server.py:125 | fit progress: (8, 2.0285682678222656, {'accuracy': 0.6057, 'data_size': 10000}, 142.14391549499123)
INFO flwr 2024-04-17 16:14:42,458 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 16:14:42,459 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:14:54,602 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 16:14:56,055 | server.py:125 | fit progress: (9, 1.9493420124053955, {'accuracy': 0.6204, 'data_size': 10000}, 155.74131040299835)
INFO flwr 2024-04-17 16:14:56,056 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 16:14:56,056 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:15:07,486 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 16:15:08,663 | server.py:125 | fit progress: (10, 1.8864115476608276, {'accuracy': 0.6446, 'data_size': 10000}, 168.34906739198777)
INFO flwr 2024-04-17 16:15:08,663 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 16:15:08,664 | server.py:153 | FL finished in 168.34952469599375
INFO flwr 2024-04-17 16:15:08,664 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 16:15:08,664 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 16:15:08,664 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 16:15:08,664 | app.py:229 | app_fit: losses_centralized [(0, 2.302640676498413), (1, 2.301682710647583), (2, 2.2987468242645264), (3, 2.29171085357666), (4, 2.276906728744507), (5, 2.2477169036865234), (6, 2.1950795650482178), (7, 2.117138147354126), (8, 2.0285682678222656), (9, 1.9493420124053955), (10, 1.8864115476608276)]
INFO flwr 2024-04-17 16:15:08,664 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1087), (1, 0.1702), (2, 0.4299), (3, 0.5496), (4, 0.5811), (5, 0.5816), (6, 0.5855), (7, 0.5908), (8, 0.6057), (9, 0.6204), (10, 0.6446)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6446
wandb:     loss 1.88641
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_161145-jyehdya5
wandb: Find logs at: ./wandb/offline-run-20240417_161145-jyehdya5/logs
INFO flwr 2024-04-17 16:15:12,185 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 16:22:22,494 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=685596)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=685596)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 16:22:28,876	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 16:22:36,710	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 16:22:37,203	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 16:22:37,585	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_664beb4c2cde2efa.zip' (41.56MiB) to Ray cluster...
2024-04-17 16:22:37,718	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_664beb4c2cde2efa.zip'.
INFO flwr 2024-04-17 16:22:49,004 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77071510732.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'memory': 169833525044.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 16:22:49,005 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 16:22:49,005 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 16:22:49,027 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 16:22:49,028 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 16:22:49,028 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 16:22:49,029 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 16:22:51,639 | server.py:94 | initial parameters (loss, other metrics): 2.3026115894317627, {'accuracy': 0.1107, 'data_size': 10000}
INFO flwr 2024-04-17 16:22:51,640 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 16:22:51,640 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=689990)[0m 2024-04-17 16:22:55.702940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=689990)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=689990)[0m 2024-04-17 16:22:58.645556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=689997)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=689997)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=689992)[0m 2024-04-17 16:22:55.802026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=689992)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=689997)[0m 2024-04-17 16:22:58.657621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 16:23:26,024 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 16:23:27,426 | server.py:125 | fit progress: (1, 2.3025457859039307, {'accuracy': 0.1159, 'data_size': 10000}, 35.78538722799567)
INFO flwr 2024-04-17 16:23:27,426 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 16:23:27,426 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:23:40,956 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 16:23:42,140 | server.py:125 | fit progress: (2, 2.3024532794952393, {'accuracy': 0.123, 'data_size': 10000}, 50.499930303005385)
INFO flwr 2024-04-17 16:23:42,140 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 16:23:42,141 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:23:53,554 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 16:23:54,760 | server.py:125 | fit progress: (3, 2.3023297786712646, {'accuracy': 0.1326, 'data_size': 10000}, 63.120018104993505)
INFO flwr 2024-04-17 16:23:54,761 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 16:23:54,761 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:24:06,713 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 16:24:08,094 | server.py:125 | fit progress: (4, 2.3021769523620605, {'accuracy': 0.1463, 'data_size': 10000}, 76.45383831000072)
INFO flwr 2024-04-17 16:24:08,094 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 16:24:08,095 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:24:19,517 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 16:24:20,747 | server.py:125 | fit progress: (5, 2.3019931316375732, {'accuracy': 0.161, 'data_size': 10000}, 89.10636176499247)
INFO flwr 2024-04-17 16:24:20,747 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 16:24:20,747 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:24:32,165 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 16:24:33,562 | server.py:125 | fit progress: (6, 2.301769256591797, {'accuracy': 0.1848, 'data_size': 10000}, 101.92171888599114)
INFO flwr 2024-04-17 16:24:33,562 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 16:24:33,563 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:24:44,451 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 16:24:45,641 | server.py:125 | fit progress: (7, 2.301504373550415, {'accuracy': 0.2177, 'data_size': 10000}, 114.00065483499202)
INFO flwr 2024-04-17 16:24:45,641 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 16:24:45,641 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:24:57,075 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 16:24:58,492 | server.py:125 | fit progress: (8, 2.3011889457702637, {'accuracy': 0.2568, 'data_size': 10000}, 126.85206047599786)
INFO flwr 2024-04-17 16:24:58,493 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 16:24:58,493 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:25:09,504 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 16:25:10,888 | server.py:125 | fit progress: (9, 2.300819158554077, {'accuracy': 0.2955, 'data_size': 10000}, 139.2480884700053)
INFO flwr 2024-04-17 16:25:10,889 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 16:25:10,889 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:25:23,073 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 16:25:24,458 | server.py:125 | fit progress: (10, 2.3003897666931152, {'accuracy': 0.3298, 'data_size': 10000}, 152.8177812829963)
INFO flwr 2024-04-17 16:25:24,458 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 16:25:24,458 | server.py:153 | FL finished in 152.81825221999316
INFO flwr 2024-04-17 16:25:24,459 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 16:25:24,459 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 16:25:24,459 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 16:25:24,459 | app.py:229 | app_fit: losses_centralized [(0, 2.3026115894317627), (1, 2.3025457859039307), (2, 2.3024532794952393), (3, 2.3023297786712646), (4, 2.3021769523620605), (5, 2.3019931316375732), (6, 2.301769256591797), (7, 2.301504373550415), (8, 2.3011889457702637), (9, 2.300819158554077), (10, 2.3003897666931152)]
INFO flwr 2024-04-17 16:25:24,459 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1107), (1, 0.1159), (2, 0.123), (3, 0.1326), (4, 0.1463), (5, 0.161), (6, 0.1848), (7, 0.2177), (8, 0.2568), (9, 0.2955), (10, 0.3298)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3298
wandb:     loss 2.30039
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_162220-zjwthzif
wandb: Find logs at: ./wandb/offline-run-20240417_162220-zjwthzif/logs
INFO flwr 2024-04-17 16:25:28,023 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 16:32:40,236 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=689988)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=689988)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 16:33:03,748	ERROR services.py:1207 -- Failed to start the dashboard 
2024-04-17 16:33:03,749	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-04-17 16:33:03,749	ERROR services.py:1242 -- Couldn't read dashboard.log file. Error: [Errno 2] No such file or directory: '/local/ray/session_2024-04-17_16-32-42_164808_101421/logs/dashboard.log'. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.
2024-04-17 16:33:03,750	ERROR services.py:1276 -- Failed to read dashboard.err file: cannot mmap an empty file. It is unexpected. Please report an issue to Ray github. https://github.com/ray-project/ray/issues
2024-04-17 16:33:03,994	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 16:33:25,775	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 16:33:26,349	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 16:33:26,753	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e3cfa87af9b346d2.zip' (41.73MiB) to Ray cluster...
2024-04-17 16:33:26,887	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e3cfa87af9b346d2.zip'.
INFO flwr 2024-04-17 16:33:39,455 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 77315742105.0, 'accelerator_type:TITAN': 1.0, 'memory': 170403398247.0}
INFO flwr 2024-04-17 16:33:39,455 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 16:33:39,455 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 16:33:39,485 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 16:33:39,487 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 16:33:39,487 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 16:33:39,487 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 16:33:44,797 | server.py:94 | initial parameters (loss, other metrics): 2.3025453090667725, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-17 16:33:44,798 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 16:33:44,798 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=694526)[0m 2024-04-17 16:33:55.661776: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=694526)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=694526)[0m 2024-04-17 16:34:16.929896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=694534)[0m 2024-04-17 16:33:55.657715: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=694534)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=694526)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=694526)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=694534)[0m 2024-04-17 16:34:16.929938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 16:35:41,757 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 16:35:43,371 | server.py:125 | fit progress: (1, 2.302539110183716, {'accuracy': 0.1032, 'data_size': 10000}, 118.57293098300579)
INFO flwr 2024-04-17 16:35:43,371 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 16:35:43,371 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:35:56,618 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 16:35:57,967 | server.py:125 | fit progress: (2, 2.3025307655334473, {'accuracy': 0.1032, 'data_size': 10000}, 133.169285409007)
INFO flwr 2024-04-17 16:35:57,967 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 16:35:57,968 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:36:10,076 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 16:36:11,650 | server.py:125 | fit progress: (3, 2.302520751953125, {'accuracy': 0.1032, 'data_size': 10000}, 146.85273103301006)
INFO flwr 2024-04-17 16:36:11,651 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 16:36:11,651 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:36:23,255 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 16:36:24,709 | server.py:125 | fit progress: (4, 2.3025095462799072, {'accuracy': 0.1032, 'data_size': 10000}, 159.91167873400263)
INFO flwr 2024-04-17 16:36:24,710 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 16:36:24,710 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:36:36,401 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 16:36:37,856 | server.py:125 | fit progress: (5, 2.302497386932373, {'accuracy': 0.1032, 'data_size': 10000}, 173.05874911400315)
INFO flwr 2024-04-17 16:36:37,857 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 16:36:37,857 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:36:48,920 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 16:36:50,142 | server.py:125 | fit progress: (6, 2.3024842739105225, {'accuracy': 0.1032, 'data_size': 10000}, 185.3442141280102)
INFO flwr 2024-04-17 16:36:50,142 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 16:36:50,143 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:37:02,008 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 16:37:03,292 | server.py:125 | fit progress: (7, 2.3024702072143555, {'accuracy': 0.1032, 'data_size': 10000}, 198.4945658580109)
INFO flwr 2024-04-17 16:37:03,293 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 16:37:03,293 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:37:14,876 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 16:37:16,435 | server.py:125 | fit progress: (8, 2.3024566173553467, {'accuracy': 0.1032, 'data_size': 10000}, 211.6371853340097)
INFO flwr 2024-04-17 16:37:16,435 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 16:37:16,436 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:37:28,662 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 16:37:29,955 | server.py:125 | fit progress: (9, 2.3024420738220215, {'accuracy': 0.1032, 'data_size': 10000}, 225.1574025430018)
INFO flwr 2024-04-17 16:37:29,955 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 16:37:29,956 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:37:42,129 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 16:37:43,650 | server.py:125 | fit progress: (10, 2.302427291870117, {'accuracy': 0.1032, 'data_size': 10000}, 238.85218163700483)
INFO flwr 2024-04-17 16:37:43,650 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 16:37:43,651 | server.py:153 | FL finished in 238.8528028110013
INFO flwr 2024-04-17 16:37:43,651 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 16:37:43,651 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 16:37:43,651 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 16:37:43,651 | app.py:229 | app_fit: losses_centralized [(0, 2.3025453090667725), (1, 2.302539110183716), (2, 2.3025307655334473), (3, 2.302520751953125), (4, 2.3025095462799072), (5, 2.302497386932373), (6, 2.3024842739105225), (7, 2.3024702072143555), (8, 2.3024566173553467), (9, 2.3024420738220215), (10, 2.302427291870117)]
INFO flwr 2024-04-17 16:37:43,651 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.1032), (2, 0.1032), (3, 0.1032), (4, 0.1032), (5, 0.1032), (6, 0.1032), (7, 0.1032), (8, 0.1032), (9, 0.1032), (10, 0.1032)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1032
wandb:     loss 2.30243
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_163236-95wd2vd4
wandb: Find logs at: ./wandb/offline-run-20240417_163236-95wd2vd4/logs
INFO flwr 2024-04-17 16:37:47,277 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 16:45:06,908 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=694534)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=694534)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 16:45:12,171	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 16:45:13,674	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 16:45:14,165	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 16:45:14,566	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9ee01e78abe7c8c6.zip' (41.88MiB) to Ray cluster...
2024-04-17 16:45:14,716	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9ee01e78abe7c8c6.zip'.
INFO flwr 2024-04-17 16:45:26,748 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 73167974400.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 160725273600.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 16:45:26,749 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 16:45:26,749 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 16:45:26,767 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 16:45:26,769 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 16:45:26,769 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 16:45:26,769 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 16:45:31,434 | server.py:94 | initial parameters (loss, other metrics): 2.3026280403137207, {'accuracy': 0.1043, 'data_size': 10000}
INFO flwr 2024-04-17 16:45:31,435 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 16:45:31,435 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=701285)[0m 2024-04-17 16:45:33.259423: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=701285)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=701285)[0m 2024-04-17 16:45:35.821728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=701285)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=701285)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=701299)[0m 2024-04-17 16:45:33.347860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=701299)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=701295)[0m 2024-04-17 16:45:35.821723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 16:46:00,917 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 16:46:02,460 | server.py:125 | fit progress: (1, 2.0081090927124023, {'accuracy': 0.4529, 'data_size': 10000}, 31.024726774005103)
INFO flwr 2024-04-17 16:46:02,460 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 16:46:02,460 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:46:15,578 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 16:46:16,847 | server.py:125 | fit progress: (2, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 45.41264730399416)
INFO flwr 2024-04-17 16:46:16,848 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 16:46:16,848 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:46:28,592 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 16:46:30,107 | server.py:125 | fit progress: (3, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 58.671924020003644)
INFO flwr 2024-04-17 16:46:30,107 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 16:46:30,107 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:46:42,984 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 16:46:44,274 | server.py:125 | fit progress: (4, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 72.838752095995)
INFO flwr 2024-04-17 16:46:44,274 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 16:46:44,274 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:46:55,797 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 16:46:57,326 | server.py:125 | fit progress: (5, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 85.8914455990016)
INFO flwr 2024-04-17 16:46:57,326 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 16:46:57,327 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:47:09,755 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 16:47:11,292 | server.py:125 | fit progress: (6, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 99.85745615400083)
INFO flwr 2024-04-17 16:47:11,293 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 16:47:11,293 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:47:24,096 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 16:47:25,639 | server.py:125 | fit progress: (7, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 114.20409645300242)
INFO flwr 2024-04-17 16:47:25,639 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 16:47:25,639 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:47:37,490 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 16:47:38,784 | server.py:125 | fit progress: (8, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 127.34954715499771)
INFO flwr 2024-04-17 16:47:38,785 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 16:47:38,785 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:47:51,049 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 16:47:52,401 | server.py:125 | fit progress: (9, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 140.9657331060007)
INFO flwr 2024-04-17 16:47:52,401 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 16:47:52,401 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:48:04,284 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 16:48:05,813 | server.py:125 | fit progress: (10, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 154.37814313999843)
INFO flwr 2024-04-17 16:48:05,813 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 16:48:05,813 | server.py:153 | FL finished in 154.37856193499465
INFO flwr 2024-04-17 16:48:05,814 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 16:48:05,814 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 16:48:05,814 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 16:48:05,814 | app.py:229 | app_fit: losses_centralized [(0, 2.3026280403137207), (1, 2.0081090927124023), (2, 2.3602421283721924), (3, 2.3602421283721924), (4, 2.3602421283721924), (5, 2.3602421283721924), (6, 2.3602421283721924), (7, 2.3602421283721924), (8, 2.3602421283721924), (9, 2.3602421283721924), (10, 2.3602421283721924)]
INFO flwr 2024-04-17 16:48:05,814 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1043), (1, 0.4529), (2, 0.1009), (3, 0.1009), (4, 0.1009), (5, 0.1009), (6, 0.1009), (7, 0.1009), (8, 0.1009), (9, 0.1009), (10, 0.1009)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1009
wandb:     loss 2.36024
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_164506-6gdrt02i
wandb: Find logs at: ./wandb/offline-run-20240417_164506-6gdrt02i/logs
INFO flwr 2024-04-17 16:48:09,496 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 16:55:21,961 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=701299)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=701299)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 16:55:29,219	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 16:55:30,055	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 16:55:30,531	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 16:55:30,934	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_70af8df25f10ba2b.zip' (42.04MiB) to Ray cluster...
2024-04-17 16:55:31,077	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_70af8df25f10ba2b.zip'.
INFO flwr 2024-04-17 16:55:45,019 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 160373126964.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 73017054412.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 16:55:45,019 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 16:55:45,019 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 16:55:45,043 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 16:55:45,044 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 16:55:45,044 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 16:55:45,044 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 16:55:47,883 | server.py:94 | initial parameters (loss, other metrics): 2.3028018474578857, {'accuracy': 0.1106, 'data_size': 10000}
INFO flwr 2024-04-17 16:55:47,884 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 16:55:47,884 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=705663)[0m 2024-04-17 16:55:51.434942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=705663)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=705663)[0m 2024-04-17 16:55:54.043086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=705667)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=705667)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=705662)[0m 2024-04-17 16:55:51.784958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=705662)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=705662)[0m 2024-04-17 16:55:54.429409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 16:56:16,830 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 16:56:18,403 | server.py:125 | fit progress: (1, 2.200355291366577, {'accuracy': 0.5078, 'data_size': 10000}, 30.518434251993313)
INFO flwr 2024-04-17 16:56:18,403 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 16:56:18,403 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:56:31,182 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 16:56:32,477 | server.py:125 | fit progress: (2, 1.8612818717956543, {'accuracy': 0.6086, 'data_size': 10000}, 44.59308831200178)
INFO flwr 2024-04-17 16:56:32,478 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 16:56:32,478 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:56:44,311 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 16:56:45,861 | server.py:125 | fit progress: (3, 1.7296913862228394, {'accuracy': 0.727, 'data_size': 10000}, 57.976450305999606)
INFO flwr 2024-04-17 16:56:45,861 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 16:56:45,861 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:56:57,466 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 16:56:58,968 | server.py:125 | fit progress: (4, 1.6420663595199585, {'accuracy': 0.8187, 'data_size': 10000}, 71.08336956800485)
INFO flwr 2024-04-17 16:56:58,968 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 16:56:58,968 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:57:11,260 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 16:57:12,770 | server.py:125 | fit progress: (5, 1.5800278186798096, {'accuracy': 0.8806, 'data_size': 10000}, 84.88543363199278)
INFO flwr 2024-04-17 16:57:12,770 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 16:57:12,770 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:57:25,275 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 16:57:26,561 | server.py:125 | fit progress: (6, 1.5506794452667236, {'accuracy': 0.91, 'data_size': 10000}, 98.67680799699156)
INFO flwr 2024-04-17 16:57:26,561 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 16:57:26,562 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:57:38,121 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 16:57:39,463 | server.py:125 | fit progress: (7, 1.5496666431427002, {'accuracy': 0.9113, 'data_size': 10000}, 111.57873822200054)
INFO flwr 2024-04-17 16:57:39,463 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 16:57:39,463 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:57:52,273 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 16:57:53,776 | server.py:125 | fit progress: (8, 1.5449575185775757, {'accuracy': 0.9161, 'data_size': 10000}, 125.8916543929954)
INFO flwr 2024-04-17 16:57:53,776 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 16:57:53,776 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:58:05,733 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 16:58:07,301 | server.py:125 | fit progress: (9, 1.539815068244934, {'accuracy': 0.9214, 'data_size': 10000}, 139.41683122199902)
INFO flwr 2024-04-17 16:58:07,301 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 16:58:07,302 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 16:58:19,109 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 16:58:20,472 | server.py:125 | fit progress: (10, 1.533341646194458, {'accuracy': 0.9274, 'data_size': 10000}, 152.58760107299895)
INFO flwr 2024-04-17 16:58:20,472 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 16:58:20,472 | server.py:153 | FL finished in 152.58833212399622
INFO flwr 2024-04-17 16:58:20,473 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 16:58:20,473 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 16:58:20,473 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 16:58:20,474 | app.py:229 | app_fit: losses_centralized [(0, 2.3028018474578857), (1, 2.200355291366577), (2, 1.8612818717956543), (3, 1.7296913862228394), (4, 1.6420663595199585), (5, 1.5800278186798096), (6, 1.5506794452667236), (7, 1.5496666431427002), (8, 1.5449575185775757), (9, 1.539815068244934), (10, 1.533341646194458)]
INFO flwr 2024-04-17 16:58:20,474 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1106), (1, 0.5078), (2, 0.6086), (3, 0.727), (4, 0.8187), (5, 0.8806), (6, 0.91), (7, 0.9113), (8, 0.9161), (9, 0.9214), (10, 0.9274)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9274
wandb:     loss 1.53334
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_165521-36gzseff
wandb: Find logs at: ./wandb/offline-run-20240417_165521-36gzseff/logs
INFO flwr 2024-04-17 16:58:24,211 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 17:05:36,566 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=705665)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=705665)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 17:05:40,996	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 17:05:41,915	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 17:05:42,356	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 17:05:42,756	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_49cf8b083c452298.zip' (42.19MiB) to Ray cluster...
2024-04-17 17:05:42,899	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_49cf8b083c452298.zip'.
INFO flwr 2024-04-17 17:05:56,236 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 160317397197.0, 'GPU': 1.0, 'object_store_memory': 72993170227.0}
INFO flwr 2024-04-17 17:05:56,236 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 17:05:56,236 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 17:05:56,260 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 17:05:56,261 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 17:05:56,262 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 17:05:56,262 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 17:05:59,710 | server.py:94 | initial parameters (loss, other metrics): 2.30238676071167, {'accuracy': 0.124, 'data_size': 10000}
INFO flwr 2024-04-17 17:05:59,711 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 17:05:59,711 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=710081)[0m 2024-04-17 17:06:02.398614: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=710081)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=710081)[0m 2024-04-17 17:06:04.697499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=710080)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=710080)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=710079)[0m 2024-04-17 17:06:02.644526: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=710079)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=710079)[0m 2024-04-17 17:06:05.122581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 17:06:24,640 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 17:06:26,255 | server.py:125 | fit progress: (1, 2.301225185394287, {'accuracy': 0.2093, 'data_size': 10000}, 26.543914529000176)
INFO flwr 2024-04-17 17:06:26,255 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 17:06:26,256 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:06:38,814 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 17:06:40,137 | server.py:125 | fit progress: (2, 2.297797203063965, {'accuracy': 0.3638, 'data_size': 10000}, 40.426217086002)
INFO flwr 2024-04-17 17:06:40,138 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 17:06:40,138 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:06:52,866 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 17:06:54,408 | server.py:125 | fit progress: (3, 2.2897002696990967, {'accuracy': 0.4849, 'data_size': 10000}, 54.69638183899224)
INFO flwr 2024-04-17 17:06:54,408 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 17:06:54,408 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:07:06,578 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 17:07:08,133 | server.py:125 | fit progress: (4, 2.2731916904449463, {'accuracy': 0.5399, 'data_size': 10000}, 68.42167666798923)
INFO flwr 2024-04-17 17:07:08,133 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 17:07:08,133 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:07:20,074 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 17:07:21,641 | server.py:125 | fit progress: (5, 2.242274761199951, {'accuracy': 0.5732, 'data_size': 10000}, 81.93016322699259)
INFO flwr 2024-04-17 17:07:21,642 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 17:07:21,642 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:07:33,462 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 17:07:34,758 | server.py:125 | fit progress: (6, 2.18875789642334, {'accuracy': 0.61, 'data_size': 10000}, 95.04678816699015)
INFO flwr 2024-04-17 17:07:34,758 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 17:07:34,758 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:07:46,874 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 17:07:48,169 | server.py:125 | fit progress: (7, 2.1086807250976562, {'accuracy': 0.6453, 'data_size': 10000}, 108.45789200298896)
INFO flwr 2024-04-17 17:07:48,169 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 17:07:48,187 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:08:00,444 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 17:08:02,044 | server.py:125 | fit progress: (8, 2.0113041400909424, {'accuracy': 0.6694, 'data_size': 10000}, 122.33267013799923)
INFO flwr 2024-04-17 17:08:02,044 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 17:08:02,044 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:08:14,430 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 17:08:15,757 | server.py:125 | fit progress: (9, 1.9165292978286743, {'accuracy': 0.6951, 'data_size': 10000}, 136.04622149399074)
INFO flwr 2024-04-17 17:08:15,758 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 17:08:15,758 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:08:28,430 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 17:08:29,984 | server.py:125 | fit progress: (10, 1.8391271829605103, {'accuracy': 0.7142, 'data_size': 10000}, 150.27301636699121)
INFO flwr 2024-04-17 17:08:29,985 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 17:08:29,985 | server.py:153 | FL finished in 150.27352459100075
INFO flwr 2024-04-17 17:08:29,985 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 17:08:29,985 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 17:08:29,985 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 17:08:29,985 | app.py:229 | app_fit: losses_centralized [(0, 2.30238676071167), (1, 2.301225185394287), (2, 2.297797203063965), (3, 2.2897002696990967), (4, 2.2731916904449463), (5, 2.242274761199951), (6, 2.18875789642334), (7, 2.1086807250976562), (8, 2.0113041400909424), (9, 1.9165292978286743), (10, 1.8391271829605103)]
INFO flwr 2024-04-17 17:08:29,985 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.124), (1, 0.2093), (2, 0.3638), (3, 0.4849), (4, 0.5399), (5, 0.5732), (6, 0.61), (7, 0.6453), (8, 0.6694), (9, 0.6951), (10, 0.7142)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7142
wandb:     loss 1.83913
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_170536-763qsnnp
wandb: Find logs at: ./wandb/offline-run-20240417_170536-763qsnnp/logs
INFO flwr 2024-04-17 17:08:33,763 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 17:15:47,606 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=710072)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=710072)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 17:15:56,516	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 17:15:58,628	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 17:16:01,625	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 17:16:04,219	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a40f3ead1da4a451.zip' (42.35MiB) to Ray cluster...
2024-04-17 17:16:04,363	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a40f3ead1da4a451.zip'.
INFO flwr 2024-04-17 17:16:19,147 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72998526566.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 160329895322.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 17:16:19,147 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 17:16:19,147 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 17:16:19,164 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 17:16:19,166 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 17:16:19,166 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 17:16:19,166 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 17:16:21,897 | server.py:94 | initial parameters (loss, other metrics): 2.3025810718536377, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-17 17:16:21,898 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 17:16:21,898 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=715338)[0m 2024-04-17 17:16:25.585475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=715338)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=715338)[0m 2024-04-17 17:16:28.571323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=715341)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=715341)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=715329)[0m 2024-04-17 17:16:26.752375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=715329)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=715329)[0m 2024-04-17 17:16:29.333056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 17:16:49,001 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 17:16:50,637 | server.py:125 | fit progress: (1, 2.302521228790283, {'accuracy': 0.13, 'data_size': 10000}, 28.739050417003455)
INFO flwr 2024-04-17 17:16:50,637 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 17:16:50,638 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:17:03,178 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 17:17:04,509 | server.py:125 | fit progress: (2, 2.302427291870117, {'accuracy': 0.1349, 'data_size': 10000}, 42.61089604400331)
INFO flwr 2024-04-17 17:17:04,509 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 17:17:04,509 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:17:16,057 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 17:17:17,589 | server.py:125 | fit progress: (3, 2.302304983139038, {'accuracy': 0.1421, 'data_size': 10000}, 55.6915100079932)
INFO flwr 2024-04-17 17:17:17,590 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 17:17:17,590 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:17:29,179 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 17:17:30,699 | server.py:125 | fit progress: (4, 2.3021485805511475, {'accuracy': 0.1484, 'data_size': 10000}, 68.80127422399528)
INFO flwr 2024-04-17 17:17:30,699 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 17:17:30,700 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:17:42,784 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 17:17:44,279 | server.py:125 | fit progress: (5, 2.3019561767578125, {'accuracy': 0.1564, 'data_size': 10000}, 82.38060570399102)
INFO flwr 2024-04-17 17:17:44,279 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 17:17:44,279 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:17:56,910 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 17:17:58,239 | server.py:125 | fit progress: (6, 2.3017241954803467, {'accuracy': 0.1629, 'data_size': 10000}, 96.34088734700345)
INFO flwr 2024-04-17 17:17:58,239 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 17:17:58,239 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:18:09,971 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 17:18:11,247 | server.py:125 | fit progress: (7, 2.301447629928589, {'accuracy': 0.1711, 'data_size': 10000}, 109.3489648699906)
INFO flwr 2024-04-17 17:18:11,247 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 17:18:11,247 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:18:22,979 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 17:18:24,484 | server.py:125 | fit progress: (8, 2.3011205196380615, {'accuracy': 0.1805, 'data_size': 10000}, 122.58567911598948)
INFO flwr 2024-04-17 17:18:24,484 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 17:18:24,484 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:18:35,804 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 17:18:37,346 | server.py:125 | fit progress: (9, 2.3007373809814453, {'accuracy': 0.1904, 'data_size': 10000}, 135.44789727400348)
INFO flwr 2024-04-17 17:18:37,346 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 17:18:37,346 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:18:49,524 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 17:18:51,016 | server.py:125 | fit progress: (10, 2.300293445587158, {'accuracy': 0.2004, 'data_size': 10000}, 149.11759078699106)
INFO flwr 2024-04-17 17:18:51,016 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 17:18:51,016 | server.py:153 | FL finished in 149.11807345799753
INFO flwr 2024-04-17 17:18:51,016 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 17:18:51,016 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 17:18:51,016 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 17:18:51,017 | app.py:229 | app_fit: losses_centralized [(0, 2.3025810718536377), (1, 2.302521228790283), (2, 2.302427291870117), (3, 2.302304983139038), (4, 2.3021485805511475), (5, 2.3019561767578125), (6, 2.3017241954803467), (7, 2.301447629928589), (8, 2.3011205196380615), (9, 2.3007373809814453), (10, 2.300293445587158)]
INFO flwr 2024-04-17 17:18:51,017 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.13), (2, 0.1349), (3, 0.1421), (4, 0.1484), (5, 0.1564), (6, 0.1629), (7, 0.1711), (8, 0.1805), (9, 0.1904), (10, 0.2004)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2004
wandb:     loss 2.30029
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_171545-pln6mlfq
wandb: Find logs at: ./wandb/offline-run-20240417_171545-pln6mlfq/logs
INFO flwr 2024-04-17 17:18:54,628 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 17:26:07,030 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=715338)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=715338)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 17:26:11,574	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 17:26:12,462	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 17:26:12,973	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 17:26:13,381	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dadf5ac9baeb8082.zip' (42.50MiB) to Ray cluster...
2024-04-17 17:26:13,514	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dadf5ac9baeb8082.zip'.
INFO flwr 2024-04-17 17:26:24,760 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 72988866969.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 160307356263.0}
INFO flwr 2024-04-17 17:26:24,760 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 17:26:24,761 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 17:26:24,781 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 17:26:24,782 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 17:26:24,783 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 17:26:24,783 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 17:26:28,638 | server.py:94 | initial parameters (loss, other metrics): 2.3024919033050537, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-17 17:26:28,638 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 17:26:28,639 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=719737)[0m 2024-04-17 17:26:31.023771: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=719737)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=719754)[0m 2024-04-17 17:26:33.642905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=719754)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=719754)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=719736)[0m 2024-04-17 17:26:31.111997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=719736)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=719758)[0m 2024-04-17 17:26:33.640945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 17:26:59,032 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 17:27:00,589 | server.py:125 | fit progress: (1, 2.3024861812591553, {'accuracy': 0.0986, 'data_size': 10000}, 31.950336608002544)
INFO flwr 2024-04-17 17:27:00,589 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 17:27:00,589 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:27:13,325 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 17:27:14,893 | server.py:125 | fit progress: (2, 2.3024773597717285, {'accuracy': 0.0987, 'data_size': 10000}, 46.25413360600942)
INFO flwr 2024-04-17 17:27:14,893 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 17:27:14,893 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:27:26,988 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 17:27:28,536 | server.py:125 | fit progress: (3, 2.302466869354248, {'accuracy': 0.0992, 'data_size': 10000}, 59.89762436100864)
INFO flwr 2024-04-17 17:27:28,536 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 17:27:28,537 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:27:40,071 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 17:27:41,619 | server.py:125 | fit progress: (4, 2.302455186843872, {'accuracy': 0.0998, 'data_size': 10000}, 72.98043319499993)
INFO flwr 2024-04-17 17:27:41,619 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 17:27:41,619 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:27:53,574 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 17:27:55,133 | server.py:125 | fit progress: (5, 2.302443027496338, {'accuracy': 0.1006, 'data_size': 10000}, 86.49440956099716)
INFO flwr 2024-04-17 17:27:55,133 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 17:27:55,133 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:28:06,862 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 17:28:08,186 | server.py:125 | fit progress: (6, 2.3024299144744873, {'accuracy': 0.101, 'data_size': 10000}, 99.54766633099644)
INFO flwr 2024-04-17 17:28:08,186 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 17:28:08,187 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:28:20,116 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 17:28:21,735 | server.py:125 | fit progress: (7, 2.3024160861968994, {'accuracy': 0.1013, 'data_size': 10000}, 113.0969940549985)
INFO flwr 2024-04-17 17:28:21,736 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 17:28:21,736 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:28:33,946 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 17:28:35,294 | server.py:125 | fit progress: (8, 2.302401065826416, {'accuracy': 0.1019, 'data_size': 10000}, 126.65506491700944)
INFO flwr 2024-04-17 17:28:35,294 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 17:28:35,294 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:28:47,336 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 17:28:48,635 | server.py:125 | fit progress: (9, 2.302386999130249, {'accuracy': 0.1023, 'data_size': 10000}, 139.9967967709963)
INFO flwr 2024-04-17 17:28:48,636 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 17:28:48,636 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:29:00,692 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 17:29:02,245 | server.py:125 | fit progress: (10, 2.302372455596924, {'accuracy': 0.103, 'data_size': 10000}, 153.606889304996)
INFO flwr 2024-04-17 17:29:02,246 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 17:29:02,246 | server.py:153 | FL finished in 153.60740038999938
INFO flwr 2024-04-17 17:29:02,246 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 17:29:02,246 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 17:29:02,246 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 17:29:02,247 | app.py:229 | app_fit: losses_centralized [(0, 2.3024919033050537), (1, 2.3024861812591553), (2, 2.3024773597717285), (3, 2.302466869354248), (4, 2.302455186843872), (5, 2.302443027496338), (6, 2.3024299144744873), (7, 2.3024160861968994), (8, 2.302401065826416), (9, 2.302386999130249), (10, 2.302372455596924)]
INFO flwr 2024-04-17 17:29:02,247 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.0986), (2, 0.0987), (3, 0.0992), (4, 0.0998), (5, 0.1006), (6, 0.101), (7, 0.1013), (8, 0.1019), (9, 0.1023), (10, 0.103)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.103
wandb:     loss 2.30237
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_172606-x2tp4o4g
wandb: Find logs at: ./wandb/offline-run-20240417_172606-x2tp4o4g/logs
INFO flwr 2024-04-17 17:29:05,971 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 17:36:17,844 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=719758)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=719758)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 17:36:22,380	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 17:36:23,207	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 17:36:23,653	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 17:36:24,054	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fa3ce68ce3af82fe.zip' (42.65MiB) to Ray cluster...
2024-04-17 17:36:24,193	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fa3ce68ce3af82fe.zip'.
INFO flwr 2024-04-17 17:36:35,406 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72982965043.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'memory': 160293585101.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 17:36:35,407 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 17:36:35,407 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 17:36:35,425 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 17:36:35,426 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 17:36:35,426 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 17:36:35,426 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 17:36:38,004 | server.py:94 | initial parameters (loss, other metrics): 2.302422046661377, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-17 17:36:38,005 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 17:36:38,005 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=724112)[0m 2024-04-17 17:36:41.525184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=724112)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=724112)[0m 2024-04-17 17:36:43.879613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=724112)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=724112)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=724110)[0m 2024-04-17 17:36:41.957529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=724110)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=724110)[0m 2024-04-17 17:36:44.322953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 17:37:02,823 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 17:37:04,399 | server.py:125 | fit progress: (1, 2.080796480178833, {'accuracy': 0.3801, 'data_size': 10000}, 26.394201966002584)
INFO flwr 2024-04-17 17:37:04,399 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 17:37:04,400 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:37:16,779 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 17:37:18,070 | server.py:125 | fit progress: (2, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 40.06473866100714)
INFO flwr 2024-04-17 17:37:18,070 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 17:37:18,070 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:37:29,212 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 17:37:30,771 | server.py:125 | fit progress: (3, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 52.76573091100727)
INFO flwr 2024-04-17 17:37:30,771 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 17:37:30,771 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:37:42,794 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 17:37:44,312 | server.py:125 | fit progress: (4, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 66.30755392801075)
INFO flwr 2024-04-17 17:37:44,313 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 17:37:44,313 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:37:56,206 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 17:37:57,712 | server.py:125 | fit progress: (5, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 79.70685025100829)
INFO flwr 2024-04-17 17:37:57,712 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 17:37:57,712 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:38:09,460 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 17:38:10,750 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 92.74480048600526)
INFO flwr 2024-04-17 17:38:10,750 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 17:38:10,750 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:38:22,742 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 17:38:24,038 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 106.03268806000415)
INFO flwr 2024-04-17 17:38:24,038 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 17:38:24,038 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:38:36,187 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 17:38:37,699 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 119.69429978400876)
INFO flwr 2024-04-17 17:38:37,700 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 17:38:37,700 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:38:49,888 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 17:38:51,175 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 133.16986035500304)
INFO flwr 2024-04-17 17:38:51,175 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 17:38:51,175 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:39:03,656 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 17:39:05,176 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 147.17065258500224)
INFO flwr 2024-04-17 17:39:05,176 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 17:39:05,176 | server.py:153 | FL finished in 147.17119108101178
INFO flwr 2024-04-17 17:39:05,176 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 17:39:05,177 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 17:39:05,177 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 17:39:05,177 | app.py:229 | app_fit: losses_centralized [(0, 2.302422046661377), (1, 2.080796480178833), (2, 2.358342170715332), (3, 2.358342170715332), (4, 2.358342170715332), (5, 2.358342170715332), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-17 17:39:05,177 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.3801), (2, 0.1028), (3, 0.1028), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_173617-nkhmmaob
wandb: Find logs at: ./wandb/offline-run-20240417_173617-nkhmmaob/logs
INFO flwr 2024-04-17 17:39:08,780 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 17:46:20,024 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=724105)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=724105)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 17:46:26,115	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 17:46:26,977	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 17:46:27,459	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 17:46:27,860	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8038ad2796e0982f.zip' (42.81MiB) to Ray cluster...
2024-04-17 17:46:28,000	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8038ad2796e0982f.zip'.
INFO flwr 2024-04-17 17:46:39,194 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72887950540.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 160071884596.0}
INFO flwr 2024-04-17 17:46:39,195 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 17:46:39,195 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 17:46:39,213 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 17:46:39,214 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 17:46:39,214 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 17:46:39,214 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 17:46:42,295 | server.py:94 | initial parameters (loss, other metrics): 2.3023722171783447, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-17 17:46:42,296 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 17:46:42,296 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=728856)[0m 2024-04-17 17:46:45.429632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=728856)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=728856)[0m 2024-04-17 17:46:47.739260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=728856)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=728856)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=728853)[0m 2024-04-17 17:46:45.650824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=728853)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=728853)[0m 2024-04-17 17:46:47.883767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 17:47:07,650 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 17:47:08,974 | server.py:125 | fit progress: (1, 2.1908442974090576, {'accuracy': 0.585, 'data_size': 10000}, 26.678457295987755)
INFO flwr 2024-04-17 17:47:08,975 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 17:47:08,975 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:47:21,021 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 17:47:22,543 | server.py:125 | fit progress: (2, 1.7897189855575562, {'accuracy': 0.6775, 'data_size': 10000}, 40.24676674399234)
INFO flwr 2024-04-17 17:47:22,543 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 17:47:22,543 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:47:35,130 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 17:47:36,428 | server.py:125 | fit progress: (3, 1.605943202972412, {'accuracy': 0.8577, 'data_size': 10000}, 54.13188703999913)
INFO flwr 2024-04-17 17:47:36,428 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 17:47:36,428 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:47:48,843 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 17:47:50,417 | server.py:125 | fit progress: (4, 1.569681167602539, {'accuracy': 0.8913, 'data_size': 10000}, 68.12104340898804)
INFO flwr 2024-04-17 17:47:50,417 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 17:47:50,417 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:48:02,080 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 17:48:03,379 | server.py:125 | fit progress: (5, 1.5566942691802979, {'accuracy': 0.9039, 'data_size': 10000}, 81.08319928898709)
INFO flwr 2024-04-17 17:48:03,379 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 17:48:03,380 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:48:15,070 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 17:48:16,653 | server.py:125 | fit progress: (6, 1.5429621934890747, {'accuracy': 0.9184, 'data_size': 10000}, 94.3574161939905)
INFO flwr 2024-04-17 17:48:16,654 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 17:48:16,654 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:48:28,802 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 17:48:30,389 | server.py:125 | fit progress: (7, 1.5402326583862305, {'accuracy': 0.9206, 'data_size': 10000}, 108.0934341760003)
INFO flwr 2024-04-17 17:48:30,390 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 17:48:30,390 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:48:41,934 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 17:48:43,487 | server.py:125 | fit progress: (8, 1.5362006425857544, {'accuracy': 0.9246, 'data_size': 10000}, 121.19105819899414)
INFO flwr 2024-04-17 17:48:43,487 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 17:48:43,487 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:48:55,983 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 17:48:57,531 | server.py:125 | fit progress: (9, 1.5353389978408813, {'accuracy': 0.9252, 'data_size': 10000}, 135.2348910389992)
INFO flwr 2024-04-17 17:48:57,531 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 17:48:57,531 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:49:09,057 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 17:49:10,374 | server.py:125 | fit progress: (10, 1.5302101373672485, {'accuracy': 0.9309, 'data_size': 10000}, 148.0777488140011)
INFO flwr 2024-04-17 17:49:10,374 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 17:49:10,374 | server.py:153 | FL finished in 148.07844587998989
INFO flwr 2024-04-17 17:49:10,375 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 17:49:10,375 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 17:49:10,375 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 17:49:10,375 | app.py:229 | app_fit: losses_centralized [(0, 2.3023722171783447), (1, 2.1908442974090576), (2, 1.7897189855575562), (3, 1.605943202972412), (4, 1.569681167602539), (5, 1.5566942691802979), (6, 1.5429621934890747), (7, 1.5402326583862305), (8, 1.5362006425857544), (9, 1.5353389978408813), (10, 1.5302101373672485)]
INFO flwr 2024-04-17 17:49:10,375 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.585), (2, 0.6775), (3, 0.8577), (4, 0.8913), (5, 0.9039), (6, 0.9184), (7, 0.9206), (8, 0.9246), (9, 0.9252), (10, 0.9309)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9309
wandb:     loss 1.53021
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_174619-b27e6fxb
wandb: Find logs at: ./wandb/offline-run-20240417_174619-b27e6fxb/logs
INFO flwr 2024-04-17 17:49:14,054 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 17:56:25,208 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=728850)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=728850)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 17:56:30,025	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 17:56:30,848	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 17:56:31,311	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 17:56:31,722	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_23ebdedcadc11ae6.zip' (42.97MiB) to Ray cluster...
2024-04-17 17:56:31,867	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_23ebdedcadc11ae6.zip'.
INFO flwr 2024-04-17 17:56:43,089 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 72885502771.0, 'memory': 160066173133.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 17:56:43,089 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 17:56:43,089 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 17:56:43,106 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 17:56:43,107 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 17:56:43,108 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 17:56:43,108 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 17:56:46,693 | server.py:94 | initial parameters (loss, other metrics): 2.302631139755249, {'accuracy': 0.1017, 'data_size': 10000}
INFO flwr 2024-04-17 17:56:46,693 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 17:56:46,694 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=733205)[0m 2024-04-17 17:56:49.292703: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=733205)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=733205)[0m 2024-04-17 17:56:51.606867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=733217)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=733217)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=733214)[0m 2024-04-17 17:56:49.610239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=733214)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=733214)[0m 2024-04-17 17:56:52.150635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 17:57:11,056 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 17:57:12,351 | server.py:125 | fit progress: (1, 2.3015952110290527, {'accuracy': 0.1736, 'data_size': 10000}, 25.657674949005013)
INFO flwr 2024-04-17 17:57:12,351 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 17:57:12,352 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:57:24,736 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 17:57:26,275 | server.py:125 | fit progress: (2, 2.2982585430145264, {'accuracy': 0.2941, 'data_size': 10000}, 39.581767891999334)
INFO flwr 2024-04-17 17:57:26,276 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 17:57:26,276 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:57:38,062 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 17:57:39,340 | server.py:125 | fit progress: (3, 2.2903778553009033, {'accuracy': 0.4097, 'data_size': 10000}, 52.6461277519993)
INFO flwr 2024-04-17 17:57:39,340 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 17:57:39,340 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:57:51,026 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 17:57:52,308 | server.py:125 | fit progress: (4, 2.2750182151794434, {'accuracy': 0.4579, 'data_size': 10000}, 65.61429431800207)
INFO flwr 2024-04-17 17:57:52,308 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 17:57:52,308 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:58:04,605 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 17:58:06,126 | server.py:125 | fit progress: (5, 2.2473950386047363, {'accuracy': 0.5366, 'data_size': 10000}, 79.43281959700107)
INFO flwr 2024-04-17 17:58:06,127 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 17:58:06,127 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:58:18,413 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 17:58:19,928 | server.py:125 | fit progress: (6, 2.2010533809661865, {'accuracy': 0.6047, 'data_size': 10000}, 93.23458453700005)
INFO flwr 2024-04-17 17:58:19,928 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 17:58:19,929 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:58:31,337 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 17:58:32,617 | server.py:125 | fit progress: (7, 2.1324617862701416, {'accuracy': 0.6404, 'data_size': 10000}, 105.92335759200796)
INFO flwr 2024-04-17 17:58:32,617 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 17:58:32,617 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:58:44,341 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 17:58:45,665 | server.py:125 | fit progress: (8, 2.0441694259643555, {'accuracy': 0.6636, 'data_size': 10000}, 118.97189021100348)
INFO flwr 2024-04-17 17:58:45,666 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 17:58:45,666 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:58:57,111 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 17:58:58,655 | server.py:125 | fit progress: (9, 1.9488904476165771, {'accuracy': 0.6839, 'data_size': 10000}, 131.9614162620128)
INFO flwr 2024-04-17 17:58:58,655 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 17:58:58,655 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 17:59:10,727 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 17:59:12,043 | server.py:125 | fit progress: (10, 1.8638007640838623, {'accuracy': 0.6998, 'data_size': 10000}, 145.34909996901115)
INFO flwr 2024-04-17 17:59:12,043 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 17:59:12,043 | server.py:153 | FL finished in 145.3495407520095
INFO flwr 2024-04-17 17:59:12,043 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 17:59:12,043 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 17:59:12,043 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 17:59:12,044 | app.py:229 | app_fit: losses_centralized [(0, 2.302631139755249), (1, 2.3015952110290527), (2, 2.2982585430145264), (3, 2.2903778553009033), (4, 2.2750182151794434), (5, 2.2473950386047363), (6, 2.2010533809661865), (7, 2.1324617862701416), (8, 2.0441694259643555), (9, 1.9488904476165771), (10, 1.8638007640838623)]
INFO flwr 2024-04-17 17:59:12,044 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1017), (1, 0.1736), (2, 0.2941), (3, 0.4097), (4, 0.4579), (5, 0.5366), (6, 0.6047), (7, 0.6404), (8, 0.6636), (9, 0.6839), (10, 0.6998)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6998
wandb:     loss 1.8638
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_175624-5kmf37c3
wandb: Find logs at: ./wandb/offline-run-20240417_175624-5kmf37c3/logs
INFO flwr 2024-04-17 17:59:15,646 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 18:06:26,905 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=733205)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=733205)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 18:06:31,738	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 18:06:32,578	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 18:06:33,047	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 18:06:33,462	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e5a85b74098e3320.zip' (43.12MiB) to Ray cluster...
2024-04-17 18:06:33,605	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e5a85b74098e3320.zip'.
INFO flwr 2024-04-17 18:06:44,742 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 160065092199.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 72885039513.0, 'CPU': 64.0}
INFO flwr 2024-04-17 18:06:44,742 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 18:06:44,742 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 18:06:44,764 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 18:06:44,764 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 18:06:44,765 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 18:06:44,765 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 18:06:49,067 | server.py:94 | initial parameters (loss, other metrics): 2.30244779586792, {'accuracy': 0.0995, 'data_size': 10000}
INFO flwr 2024-04-17 18:06:49,069 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 18:06:49,073 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=737601)[0m 2024-04-17 18:06:50.876100: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=737601)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=737601)[0m 2024-04-17 18:06:53.281037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=737598)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=737598)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=737600)[0m 2024-04-17 18:06:51.180415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=737600)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=737600)[0m 2024-04-17 18:06:53.478095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 18:07:13,267 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 18:07:14,649 | server.py:125 | fit progress: (1, 2.3023598194122314, {'accuracy': 0.1008, 'data_size': 10000}, 25.576216122994083)
INFO flwr 2024-04-17 18:07:14,650 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 18:07:14,650 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:07:26,788 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 18:07:28,166 | server.py:125 | fit progress: (2, 2.3022305965423584, {'accuracy': 0.1017, 'data_size': 10000}, 39.092634435990476)
INFO flwr 2024-04-17 18:07:28,166 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 18:07:28,166 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:07:41,015 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 18:07:42,570 | server.py:125 | fit progress: (3, 2.30206561088562, {'accuracy': 0.1057, 'data_size': 10000}, 53.49644093499228)
INFO flwr 2024-04-17 18:07:42,570 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 18:07:42,570 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:07:55,357 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 18:07:56,739 | server.py:125 | fit progress: (4, 2.301859140396118, {'accuracy': 0.1104, 'data_size': 10000}, 67.66584717598744)
INFO flwr 2024-04-17 18:07:56,739 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 18:07:56,740 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:08:10,615 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 18:08:12,172 | server.py:125 | fit progress: (5, 2.3016083240509033, {'accuracy': 0.1189, 'data_size': 10000}, 83.09903831199335)
INFO flwr 2024-04-17 18:08:12,173 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 18:08:12,173 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:08:24,206 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 18:08:25,516 | server.py:125 | fit progress: (6, 2.301307201385498, {'accuracy': 0.1307, 'data_size': 10000}, 96.44268311599444)
INFO flwr 2024-04-17 18:08:25,516 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 18:08:25,516 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:08:37,402 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 18:08:38,748 | server.py:125 | fit progress: (7, 2.300952434539795, {'accuracy': 0.1442, 'data_size': 10000}, 109.67459357099142)
INFO flwr 2024-04-17 18:08:38,748 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 18:08:38,748 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:08:50,909 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 18:08:52,489 | server.py:125 | fit progress: (8, 2.300544023513794, {'accuracy': 0.1668, 'data_size': 10000}, 123.41550364499562)
INFO flwr 2024-04-17 18:08:52,489 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 18:08:52,489 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:09:04,482 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 18:09:05,804 | server.py:125 | fit progress: (9, 2.300079107284546, {'accuracy': 0.2114, 'data_size': 10000}, 136.73053926898865)
INFO flwr 2024-04-17 18:09:05,804 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 18:09:05,804 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:09:18,573 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 18:09:20,122 | server.py:125 | fit progress: (10, 2.2995433807373047, {'accuracy': 0.2498, 'data_size': 10000}, 151.0488434709987)
INFO flwr 2024-04-17 18:09:20,122 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 18:09:20,122 | server.py:153 | FL finished in 151.049314443997
INFO flwr 2024-04-17 18:09:20,123 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 18:09:20,123 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 18:09:20,123 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 18:09:20,123 | app.py:229 | app_fit: losses_centralized [(0, 2.30244779586792), (1, 2.3023598194122314), (2, 2.3022305965423584), (3, 2.30206561088562), (4, 2.301859140396118), (5, 2.3016083240509033), (6, 2.301307201385498), (7, 2.300952434539795), (8, 2.300544023513794), (9, 2.300079107284546), (10, 2.2995433807373047)]
INFO flwr 2024-04-17 18:09:20,123 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0995), (1, 0.1008), (2, 0.1017), (3, 0.1057), (4, 0.1104), (5, 0.1189), (6, 0.1307), (7, 0.1442), (8, 0.1668), (9, 0.2114), (10, 0.2498)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2498
wandb:     loss 2.29954
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_180626-wqrnyozx
wandb: Find logs at: ./wandb/offline-run-20240417_180626-wqrnyozx/logs
INFO flwr 2024-04-17 18:09:23,814 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 18:16:36,000 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=737592)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=737592)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 18:16:41,440	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 18:16:42,313	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 18:16:42,783	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 18:16:43,198	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dd9c48efc8fc0014.zip' (43.28MiB) to Ray cluster...
2024-04-17 18:16:43,345	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dd9c48efc8fc0014.zip'.
INFO flwr 2024-04-17 18:16:54,548 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 73024296960.0, 'accelerator_type:TITAN': 1.0, 'memory': 160390026240.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'GPU': 1.0}
INFO flwr 2024-04-17 18:16:54,548 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 18:16:54,548 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 18:16:54,570 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 18:16:54,571 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 18:16:54,571 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 18:16:54,571 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 18:16:57,173 | server.py:94 | initial parameters (loss, other metrics): 2.3024680614471436, {'accuracy': 0.0967, 'data_size': 10000}
INFO flwr 2024-04-17 18:16:57,173 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 18:16:57,174 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=742317)[0m 2024-04-17 18:17:00.889582: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=742317)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=742317)[0m 2024-04-17 18:17:03.143289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=742323)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=742323)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=742321)[0m 2024-04-17 18:17:01.075988: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=742321)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=742321)[0m 2024-04-17 18:17:03.391410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 18:17:22,806 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 18:17:24,393 | server.py:125 | fit progress: (1, 2.3024613857269287, {'accuracy': 0.0983, 'data_size': 10000}, 27.219119154993678)
INFO flwr 2024-04-17 18:17:24,393 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 18:17:24,394 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:17:36,577 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 18:17:37,936 | server.py:125 | fit progress: (2, 2.3024511337280273, {'accuracy': 0.0995, 'data_size': 10000}, 40.7621551159973)
INFO flwr 2024-04-17 18:17:37,936 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 18:17:37,937 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:17:49,290 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 18:17:50,825 | server.py:125 | fit progress: (3, 2.302440643310547, {'accuracy': 0.1011, 'data_size': 10000}, 53.651346072001616)
INFO flwr 2024-04-17 18:17:50,826 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 18:17:50,826 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:18:01,857 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 18:18:03,439 | server.py:125 | fit progress: (4, 2.3024277687072754, {'accuracy': 0.1027, 'data_size': 10000}, 66.26488799900108)
INFO flwr 2024-04-17 18:18:03,439 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 18:18:03,439 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:18:15,565 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 18:18:17,088 | server.py:125 | fit progress: (5, 2.302414894104004, {'accuracy': 0.1036, 'data_size': 10000}, 79.91400027299824)
INFO flwr 2024-04-17 18:18:17,088 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 18:18:17,088 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:18:29,368 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 18:18:30,673 | server.py:125 | fit progress: (6, 2.302400588989258, {'accuracy': 0.1052, 'data_size': 10000}, 93.49918966299447)
INFO flwr 2024-04-17 18:18:30,673 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 18:18:30,674 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:18:42,317 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 18:18:43,626 | server.py:125 | fit progress: (7, 2.3023860454559326, {'accuracy': 0.1065, 'data_size': 10000}, 106.4525197250041)
INFO flwr 2024-04-17 18:18:43,627 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 18:18:43,627 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:18:55,512 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 18:18:56,999 | server.py:125 | fit progress: (8, 2.30237078666687, {'accuracy': 0.1072, 'data_size': 10000}, 119.82496504900337)
INFO flwr 2024-04-17 18:18:56,999 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 18:18:57,000 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:19:08,477 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 18:19:09,744 | server.py:125 | fit progress: (9, 2.3023550510406494, {'accuracy': 0.1088, 'data_size': 10000}, 132.57027134900272)
INFO flwr 2024-04-17 18:19:09,744 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 18:19:09,745 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:19:22,140 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 18:19:23,624 | server.py:125 | fit progress: (10, 2.3023383617401123, {'accuracy': 0.1097, 'data_size': 10000}, 146.44997684299597)
INFO flwr 2024-04-17 18:19:23,624 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 18:19:23,624 | server.py:153 | FL finished in 146.45040548799443
INFO flwr 2024-04-17 18:19:23,624 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 18:19:23,625 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 18:19:23,625 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 18:19:23,625 | app.py:229 | app_fit: losses_centralized [(0, 2.3024680614471436), (1, 2.3024613857269287), (2, 2.3024511337280273), (3, 2.302440643310547), (4, 2.3024277687072754), (5, 2.302414894104004), (6, 2.302400588989258), (7, 2.3023860454559326), (8, 2.30237078666687), (9, 2.3023550510406494), (10, 2.3023383617401123)]
INFO flwr 2024-04-17 18:19:23,625 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0967), (1, 0.0983), (2, 0.0995), (3, 0.1011), (4, 0.1027), (5, 0.1036), (6, 0.1052), (7, 0.1065), (8, 0.1072), (9, 0.1088), (10, 0.1097)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1097
wandb:     loss 2.30234
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_181635-6msvb98u
wandb: Find logs at: ./wandb/offline-run-20240417_181635-6msvb98u/logs
INFO flwr 2024-04-17 18:19:27,274 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 18:26:38,442 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=742316)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=742316)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 18:26:43,492	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 18:26:44,386	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 18:26:44,857	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 18:26:45,253	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cb95f7d90d0b7df9.zip' (43.44MiB) to Ray cluster...
2024-04-17 18:26:45,404	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cb95f7d90d0b7df9.zip'.
INFO flwr 2024-04-17 18:26:56,846 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 160094515405.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 72897649459.0, 'CPU': 64.0}
INFO flwr 2024-04-17 18:26:56,846 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 18:26:56,846 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 18:26:56,864 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 18:26:56,866 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 18:26:56,866 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 18:26:56,867 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 18:26:59,874 | server.py:94 | initial parameters (loss, other metrics): 2.302729606628418, {'accuracy': 0.1182, 'data_size': 10000}
INFO flwr 2024-04-17 18:26:59,875 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 18:26:59,875 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=746700)[0m 2024-04-17 18:27:03.054034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=746700)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=746700)[0m 2024-04-17 18:27:05.388550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=746698)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=746698)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=746692)[0m 2024-04-17 18:27:03.548238: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=746692)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=746692)[0m 2024-04-17 18:27:05.891894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 18:27:19,975 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 18:27:21,309 | server.py:125 | fit progress: (1, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 21.43380664299184)
INFO flwr 2024-04-17 18:27:21,309 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 18:27:21,310 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:27:30,734 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 18:27:32,273 | server.py:125 | fit progress: (2, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 32.39792477298761)
INFO flwr 2024-04-17 18:27:32,273 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 18:27:32,274 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:27:40,781 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 18:27:42,114 | server.py:125 | fit progress: (3, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 42.23891298599483)
INFO flwr 2024-04-17 18:27:42,114 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 18:27:42,115 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:27:50,394 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 18:27:51,915 | server.py:125 | fit progress: (4, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 52.03939277298923)
INFO flwr 2024-04-17 18:27:51,915 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 18:27:51,915 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:28:00,236 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 18:28:01,525 | server.py:125 | fit progress: (5, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 61.650087236994295)
INFO flwr 2024-04-17 18:28:01,526 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 18:28:01,526 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:28:10,036 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 18:28:11,560 | server.py:125 | fit progress: (6, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 71.68486578899319)
INFO flwr 2024-04-17 18:28:11,560 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 18:28:11,561 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:28:20,002 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 18:28:21,558 | server.py:125 | fit progress: (7, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 81.6828949329938)
INFO flwr 2024-04-17 18:28:21,558 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 18:28:21,559 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:28:29,420 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 18:28:30,925 | server.py:125 | fit progress: (8, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 91.05019201998948)
INFO flwr 2024-04-17 18:28:30,926 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 18:28:30,926 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:28:39,369 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 18:28:40,875 | server.py:125 | fit progress: (9, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 100.99942992698925)
INFO flwr 2024-04-17 18:28:40,875 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 18:28:40,875 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:28:49,398 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 18:28:50,756 | server.py:125 | fit progress: (10, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 110.88072129699867)
INFO flwr 2024-04-17 18:28:50,756 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 18:28:50,756 | server.py:153 | FL finished in 110.88124599499861
INFO flwr 2024-04-17 18:28:50,757 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 18:28:50,757 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 18:28:50,757 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 18:28:50,757 | app.py:229 | app_fit: losses_centralized [(0, 2.302729606628418), (1, 2.3719422817230225), (2, 2.3719422817230225), (3, 2.3719422817230225), (4, 2.3719422817230225), (5, 2.3719422817230225), (6, 2.3719422817230225), (7, 2.3719422817230225), (8, 2.3719422817230225), (9, 2.3719422817230225), (10, 2.3719422817230225)]
INFO flwr 2024-04-17 18:28:50,757 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1182), (1, 0.0892), (2, 0.0892), (3, 0.0892), (4, 0.0892), (5, 0.0892), (6, 0.0892), (7, 0.0892), (8, 0.0892), (9, 0.0892), (10, 0.0892)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0892
wandb:     loss 2.37194
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_182638-hz8fjfpx
wandb: Find logs at: ./wandb/offline-run-20240417_182638-hz8fjfpx/logs
INFO flwr 2024-04-17 18:28:54,379 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 18:36:05,979 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=746692)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=746692)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 18:36:10,953	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 18:36:11,779	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 18:36:12,254	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 18:36:12,657	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0f202dbc970d6ae5.zip' (43.60MiB) to Ray cluster...
2024-04-17 18:36:12,793	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0f202dbc970d6ae5.zip'.
INFO flwr 2024-04-17 18:36:24,477 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 160077965927.0, 'object_store_memory': 72890556825.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 18:36:24,477 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 18:36:24,477 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 18:36:24,494 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 18:36:24,495 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 18:36:24,495 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 18:36:24,495 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 18:36:27,657 | server.py:94 | initial parameters (loss, other metrics): 2.302356719970703, {'accuracy': 0.1118, 'data_size': 10000}
INFO flwr 2024-04-17 18:36:27,657 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 18:36:27,658 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=751058)[0m 2024-04-17 18:36:30.710585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=751058)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=751058)[0m 2024-04-17 18:36:33.122353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=751060)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=751060)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=751062)[0m 2024-04-17 18:36:31.143444: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=751062)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=751062)[0m 2024-04-17 18:36:33.630932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 18:36:47,857 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 18:36:49,181 | server.py:125 | fit progress: (1, 2.255772113800049, {'accuracy': 0.1917, 'data_size': 10000}, 21.52357062100782)
INFO flwr 2024-04-17 18:36:49,182 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 18:36:49,182 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:36:58,262 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 18:36:59,779 | server.py:125 | fit progress: (2, 2.150123357772827, {'accuracy': 0.3067, 'data_size': 10000}, 32.121116610011086)
INFO flwr 2024-04-17 18:36:59,779 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 18:36:59,779 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:37:08,225 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 18:37:09,740 | server.py:125 | fit progress: (3, 2.035048484802246, {'accuracy': 0.4232, 'data_size': 10000}, 42.08190653900965)
INFO flwr 2024-04-17 18:37:09,740 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 18:37:09,740 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:37:17,923 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 18:37:19,519 | server.py:125 | fit progress: (4, 1.982160210609436, {'accuracy': 0.4687, 'data_size': 10000}, 51.86153729099897)
INFO flwr 2024-04-17 18:37:19,520 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 18:37:19,520 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:37:27,657 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 18:37:28,925 | server.py:125 | fit progress: (5, 1.9444142580032349, {'accuracy': 0.5124, 'data_size': 10000}, 61.26725485500356)
INFO flwr 2024-04-17 18:37:28,925 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 18:37:28,926 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:37:37,253 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 18:37:38,546 | server.py:125 | fit progress: (6, 1.8850412368774414, {'accuracy': 0.5744, 'data_size': 10000}, 70.88794022401271)
INFO flwr 2024-04-17 18:37:38,546 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 18:37:38,546 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:37:46,497 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 18:37:48,076 | server.py:125 | fit progress: (7, 1.8366824388504028, {'accuracy': 0.6237, 'data_size': 10000}, 80.41806448200077)
INFO flwr 2024-04-17 18:37:48,076 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 18:37:48,076 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:37:56,355 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 18:37:57,893 | server.py:125 | fit progress: (8, 1.8122546672821045, {'accuracy': 0.6482, 'data_size': 10000}, 90.23570974200265)
INFO flwr 2024-04-17 18:37:57,894 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 18:37:57,894 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:38:05,946 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 18:38:07,429 | server.py:125 | fit progress: (9, 1.808297038078308, {'accuracy': 0.6527, 'data_size': 10000}, 99.77133286499884)
INFO flwr 2024-04-17 18:38:07,429 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 18:38:07,429 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:38:15,911 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 18:38:17,441 | server.py:125 | fit progress: (10, 1.8005454540252686, {'accuracy': 0.6596, 'data_size': 10000}, 109.78340380900772)
INFO flwr 2024-04-17 18:38:17,441 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 18:38:17,441 | server.py:153 | FL finished in 109.78383058701002
INFO flwr 2024-04-17 18:38:17,442 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 18:38:17,442 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 18:38:17,443 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 18:38:17,443 | app.py:229 | app_fit: losses_centralized [(0, 2.302356719970703), (1, 2.255772113800049), (2, 2.150123357772827), (3, 2.035048484802246), (4, 1.982160210609436), (5, 1.9444142580032349), (6, 1.8850412368774414), (7, 1.8366824388504028), (8, 1.8122546672821045), (9, 1.808297038078308), (10, 1.8005454540252686)]
INFO flwr 2024-04-17 18:38:17,443 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1118), (1, 0.1917), (2, 0.3067), (3, 0.4232), (4, 0.4687), (5, 0.5124), (6, 0.5744), (7, 0.6237), (8, 0.6482), (9, 0.6527), (10, 0.6596)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6596
wandb:     loss 1.80055
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_183605-5m1gpvu6
wandb: Find logs at: ./wandb/offline-run-20240417_183605-5m1gpvu6/logs
INFO flwr 2024-04-17 18:38:21,055 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 18:45:32,607 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=751054)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=751054)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 18:45:37,518	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 18:45:38,381	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 18:45:38,840	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 18:45:39,244	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1a2b73402722e81b.zip' (43.75MiB) to Ray cluster...
2024-04-17 18:45:39,394	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1a2b73402722e81b.zip'.
INFO flwr 2024-04-17 18:45:50,671 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72813522124.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 159898218292.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 18:45:50,671 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 18:45:50,672 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 18:45:50,692 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 18:45:50,694 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 18:45:50,694 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 18:45:50,694 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 18:45:53,938 | server.py:94 | initial parameters (loss, other metrics): 2.302572011947632, {'accuracy': 0.1388, 'data_size': 10000}
INFO flwr 2024-04-17 18:45:53,938 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 18:45:53,939 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=755796)[0m 2024-04-17 18:45:56.792582: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=755796)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=755796)[0m 2024-04-17 18:45:59.115923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=755797)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=755797)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=755798)[0m 2024-04-17 18:45:57.112693: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=755798)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=755798)[0m 2024-04-17 18:45:59.615817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 18:46:13,753 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 18:46:15,309 | server.py:125 | fit progress: (1, 2.30147385597229, {'accuracy': 0.2153, 'data_size': 10000}, 21.370011663006153)
INFO flwr 2024-04-17 18:46:15,309 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 18:46:15,309 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:46:24,503 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 18:46:26,015 | server.py:125 | fit progress: (2, 2.298736810684204, {'accuracy': 0.2443, 'data_size': 10000}, 32.076424851999036)
INFO flwr 2024-04-17 18:46:26,015 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 18:46:26,016 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:46:34,430 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 18:46:35,761 | server.py:125 | fit progress: (3, 2.2928261756896973, {'accuracy': 0.2373, 'data_size': 10000}, 41.82283708199975)
INFO flwr 2024-04-17 18:46:35,762 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 18:46:35,762 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:46:44,102 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 18:46:45,424 | server.py:125 | fit progress: (4, 2.280247449874878, {'accuracy': 0.2197, 'data_size': 10000}, 51.485859815002186)
INFO flwr 2024-04-17 18:46:45,425 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 18:46:45,425 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:46:53,780 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 18:46:55,295 | server.py:125 | fit progress: (5, 2.256234645843506, {'accuracy': 0.2138, 'data_size': 10000}, 61.356247857998824)
INFO flwr 2024-04-17 18:46:55,295 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 18:46:55,296 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:47:03,672 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 18:47:05,179 | server.py:125 | fit progress: (6, 2.220797061920166, {'accuracy': 0.2362, 'data_size': 10000}, 71.24079950300802)
INFO flwr 2024-04-17 18:47:05,180 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 18:47:05,180 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:47:12,885 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 18:47:14,433 | server.py:125 | fit progress: (7, 2.183182716369629, {'accuracy': 0.2912, 'data_size': 10000}, 80.49442558700684)
INFO flwr 2024-04-17 18:47:14,433 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 18:47:14,433 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:47:22,422 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 18:47:23,983 | server.py:125 | fit progress: (8, 2.1435093879699707, {'accuracy': 0.3671, 'data_size': 10000}, 90.04412475100253)
INFO flwr 2024-04-17 18:47:23,983 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 18:47:23,983 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:47:32,570 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 18:47:33,848 | server.py:125 | fit progress: (9, 2.0993027687072754, {'accuracy': 0.4205, 'data_size': 10000}, 99.90973271500843)
INFO flwr 2024-04-17 18:47:33,849 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 18:47:33,849 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:47:42,322 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 18:47:43,589 | server.py:125 | fit progress: (10, 2.0580122470855713, {'accuracy': 0.4524, 'data_size': 10000}, 109.65009453300445)
INFO flwr 2024-04-17 18:47:43,589 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 18:47:43,589 | server.py:153 | FL finished in 109.65055121399928
INFO flwr 2024-04-17 18:47:43,589 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 18:47:43,589 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 18:47:43,590 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 18:47:43,590 | app.py:229 | app_fit: losses_centralized [(0, 2.302572011947632), (1, 2.30147385597229), (2, 2.298736810684204), (3, 2.2928261756896973), (4, 2.280247449874878), (5, 2.256234645843506), (6, 2.220797061920166), (7, 2.183182716369629), (8, 2.1435093879699707), (9, 2.0993027687072754), (10, 2.0580122470855713)]
INFO flwr 2024-04-17 18:47:43,590 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1388), (1, 0.2153), (2, 0.2443), (3, 0.2373), (4, 0.2197), (5, 0.2138), (6, 0.2362), (7, 0.2912), (8, 0.3671), (9, 0.4205), (10, 0.4524)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4524
wandb:     loss 2.05801
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_184532-xy014w2v
wandb: Find logs at: ./wandb/offline-run-20240417_184532-xy014w2v/logs
INFO flwr 2024-04-17 18:47:47,181 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 18:54:58,863 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=755787)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=755787)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 18:55:03,626	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 18:55:04,488	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 18:55:04,971	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 18:55:05,394	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_13683cb63c0ba0ab.zip' (43.91MiB) to Ray cluster...
2024-04-17 18:55:05,545	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_13683cb63c0ba0ab.zip'.
INFO flwr 2024-04-17 18:55:16,758 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 160062196327.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 72883798425.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 18:55:16,758 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 18:55:16,759 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 18:55:16,781 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 18:55:16,786 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 18:55:16,787 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 18:55:16,787 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 18:55:20,419 | server.py:94 | initial parameters (loss, other metrics): 2.302633762359619, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-17 18:55:20,419 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 18:55:20,419 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=759539)[0m 2024-04-17 18:55:22.970800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=759539)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=759539)[0m 2024-04-17 18:55:25.349029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=759543)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=759543)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=759542)[0m 2024-04-17 18:55:23.127150: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=759542)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=759542)[0m 2024-04-17 18:55:25.427066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 18:55:39,931 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 18:55:41,232 | server.py:125 | fit progress: (1, 2.302541732788086, {'accuracy': 0.0974, 'data_size': 10000}, 20.812860043006367)
INFO flwr 2024-04-17 18:55:41,232 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 18:55:41,233 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:55:50,652 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 18:55:52,177 | server.py:125 | fit progress: (2, 2.302403211593628, {'accuracy': 0.0974, 'data_size': 10000}, 31.758319056010805)
INFO flwr 2024-04-17 18:55:52,178 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 18:55:52,178 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:56:00,391 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 18:56:01,754 | server.py:125 | fit progress: (3, 2.302229404449463, {'accuracy': 0.0975, 'data_size': 10000}, 41.334934162005084)
INFO flwr 2024-04-17 18:56:01,754 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 18:56:01,755 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:56:10,116 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 18:56:11,633 | server.py:125 | fit progress: (4, 2.3020105361938477, {'accuracy': 0.0975, 'data_size': 10000}, 51.214022298008786)
INFO flwr 2024-04-17 18:56:11,633 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 18:56:11,634 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:56:20,018 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 18:56:21,282 | server.py:125 | fit progress: (5, 2.301754951477051, {'accuracy': 0.0978, 'data_size': 10000}, 60.86284166100086)
INFO flwr 2024-04-17 18:56:21,282 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 18:56:21,282 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:56:29,748 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 18:56:31,292 | server.py:125 | fit progress: (6, 2.301462173461914, {'accuracy': 0.0984, 'data_size': 10000}, 70.87274616101058)
INFO flwr 2024-04-17 18:56:31,292 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 18:56:31,292 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:56:39,320 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 18:56:40,860 | server.py:125 | fit progress: (7, 2.3011255264282227, {'accuracy': 0.0994, 'data_size': 10000}, 80.44082603500283)
INFO flwr 2024-04-17 18:56:40,860 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 18:56:40,861 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:56:49,332 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 18:56:50,852 | server.py:125 | fit progress: (8, 2.3007359504699707, {'accuracy': 0.1024, 'data_size': 10000}, 90.43234836301417)
INFO flwr 2024-04-17 18:56:50,852 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 18:56:50,852 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:56:59,090 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 18:57:00,410 | server.py:125 | fit progress: (9, 2.3002943992614746, {'accuracy': 0.107, 'data_size': 10000}, 99.99099186800595)
INFO flwr 2024-04-17 18:57:00,410 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 18:57:00,411 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 18:57:08,675 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 18:57:10,018 | server.py:125 | fit progress: (10, 2.2997944355010986, {'accuracy': 0.1136, 'data_size': 10000}, 109.59854890601127)
INFO flwr 2024-04-17 18:57:10,018 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 18:57:10,018 | server.py:153 | FL finished in 109.59906655701343
INFO flwr 2024-04-17 18:57:10,018 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 18:57:10,019 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 18:57:10,019 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 18:57:10,019 | app.py:229 | app_fit: losses_centralized [(0, 2.302633762359619), (1, 2.302541732788086), (2, 2.302403211593628), (3, 2.302229404449463), (4, 2.3020105361938477), (5, 2.301754951477051), (6, 2.301462173461914), (7, 2.3011255264282227), (8, 2.3007359504699707), (9, 2.3002943992614746), (10, 2.2997944355010986)]
INFO flwr 2024-04-17 18:57:10,019 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.0974), (2, 0.0974), (3, 0.0975), (4, 0.0975), (5, 0.0978), (6, 0.0984), (7, 0.0994), (8, 0.1024), (9, 0.107), (10, 0.1136)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1136
wandb:     loss 2.29979
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_185458-kklvxg9b
wandb: Find logs at: ./wandb/offline-run-20240417_185458-kklvxg9b/logs
INFO flwr 2024-04-17 18:57:13,746 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 19:04:25,313 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=759538)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=759538)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 19:04:30,393	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 19:04:31,247	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 19:04:31,709	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 19:04:32,118	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2a10412062d57d0d.zip' (44.06MiB) to Ray cluster...
2024-04-17 19:04:32,262	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2a10412062d57d0d.zip'.
INFO flwr 2024-04-17 19:04:45,800 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 72939390566.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 160191911322.0}
INFO flwr 2024-04-17 19:04:45,800 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 19:04:45,801 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 19:04:45,819 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 19:04:45,820 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 19:04:45,821 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 19:04:45,821 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 19:04:48,426 | server.py:94 | initial parameters (loss, other metrics): 2.3025975227355957, {'accuracy': 0.096, 'data_size': 10000}
INFO flwr 2024-04-17 19:04:48,426 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 19:04:48,427 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=763917)[0m 2024-04-17 19:04:51.854775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=763917)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=763926)[0m 2024-04-17 19:04:54.408328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=763922)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=763922)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=763927)[0m 2024-04-17 19:04:52.096282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=763927)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=763927)[0m 2024-04-17 19:04:54.611324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 19:05:09,484 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 19:05:11,092 | server.py:125 | fit progress: (1, 2.3025894165039062, {'accuracy': 0.0961, 'data_size': 10000}, 22.666013788999408)
INFO flwr 2024-04-17 19:05:11,093 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 19:05:11,093 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:05:20,150 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 19:05:21,436 | server.py:125 | fit progress: (2, 2.302579164505005, {'accuracy': 0.0962, 'data_size': 10000}, 33.00961288600229)
INFO flwr 2024-04-17 19:05:21,436 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 19:05:21,437 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:05:29,996 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 19:05:31,523 | server.py:125 | fit progress: (3, 2.3025665283203125, {'accuracy': 0.0964, 'data_size': 10000}, 43.096129798999755)
INFO flwr 2024-04-17 19:05:31,523 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 19:05:31,523 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:05:39,809 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 19:05:41,115 | server.py:125 | fit progress: (4, 2.302553176879883, {'accuracy': 0.0964, 'data_size': 10000}, 52.68884947599145)
INFO flwr 2024-04-17 19:05:41,116 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 19:05:41,116 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:05:49,540 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 19:05:51,110 | server.py:125 | fit progress: (5, 2.3025381565093994, {'accuracy': 0.0964, 'data_size': 10000}, 62.68379817200184)
INFO flwr 2024-04-17 19:05:51,111 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 19:05:51,111 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:05:59,676 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 19:06:00,974 | server.py:125 | fit progress: (6, 2.302522659301758, {'accuracy': 0.0965, 'data_size': 10000}, 72.54707591599436)
INFO flwr 2024-04-17 19:06:00,974 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 19:06:00,974 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:06:09,546 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 19:06:11,060 | server.py:125 | fit progress: (7, 2.3025059700012207, {'accuracy': 0.097, 'data_size': 10000}, 82.63403605099302)
INFO flwr 2024-04-17 19:06:11,061 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 19:06:11,061 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:06:19,517 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 19:06:21,085 | server.py:125 | fit progress: (8, 2.3024885654449463, {'accuracy': 0.097, 'data_size': 10000}, 92.65882421299466)
INFO flwr 2024-04-17 19:06:21,086 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 19:06:21,086 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:06:28,781 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 19:06:30,309 | server.py:125 | fit progress: (9, 2.3024709224700928, {'accuracy': 0.0973, 'data_size': 10000}, 101.88234502599516)
INFO flwr 2024-04-17 19:06:30,309 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 19:06:30,309 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:06:38,731 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 19:06:40,269 | server.py:125 | fit progress: (10, 2.302452325820923, {'accuracy': 0.0975, 'data_size': 10000}, 111.84279190399684)
INFO flwr 2024-04-17 19:06:40,270 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 19:06:40,270 | server.py:153 | FL finished in 111.84321787700173
INFO flwr 2024-04-17 19:06:40,270 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 19:06:40,270 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 19:06:40,270 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 19:06:40,270 | app.py:229 | app_fit: losses_centralized [(0, 2.3025975227355957), (1, 2.3025894165039062), (2, 2.302579164505005), (3, 2.3025665283203125), (4, 2.302553176879883), (5, 2.3025381565093994), (6, 2.302522659301758), (7, 2.3025059700012207), (8, 2.3024885654449463), (9, 2.3024709224700928), (10, 2.302452325820923)]
INFO flwr 2024-04-17 19:06:40,270 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.096), (1, 0.0961), (2, 0.0962), (3, 0.0964), (4, 0.0964), (5, 0.0964), (6, 0.0965), (7, 0.097), (8, 0.097), (9, 0.0973), (10, 0.0975)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0975
wandb:     loss 2.30245
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_190424-rqltfnsz
wandb: Find logs at: ./wandb/offline-run-20240417_190424-rqltfnsz/logs
INFO flwr 2024-04-17 19:06:43,934 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 19:13:55,209 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=763917)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=763917)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 19:14:01,295	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 19:14:02,126	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 19:14:02,593	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 19:14:02,998	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_27ee69111c249a32.zip' (44.21MiB) to Ray cluster...
2024-04-17 19:14:03,142	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_27ee69111c249a32.zip'.
INFO flwr 2024-04-17 19:14:14,491 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 160057898394.0, 'object_store_memory': 72881956454.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 19:14:14,492 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 19:14:14,492 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 19:14:14,512 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 19:14:14,513 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 19:14:14,514 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 19:14:14,514 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 19:14:18,791 | server.py:94 | initial parameters (loss, other metrics): 2.3024895191192627, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-17 19:14:18,791 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 19:14:18,792 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=768625)[0m 2024-04-17 19:14:20.481903: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=768625)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=768625)[0m 2024-04-17 19:14:22.796681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=768637)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=768637)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=768636)[0m 2024-04-17 19:14:20.874758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=768636)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=768636)[0m 2024-04-17 19:14:23.238725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 19:14:37,084 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 19:14:38,652 | server.py:125 | fit progress: (1, 2.2642629146575928, {'accuracy': 0.1968, 'data_size': 10000}, 19.860450218009646)
INFO flwr 2024-04-17 19:14:38,652 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 19:14:38,670 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:14:47,639 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 19:14:49,186 | server.py:125 | fit progress: (2, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 30.39427130301192)
INFO flwr 2024-04-17 19:14:49,186 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 19:14:49,186 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:14:57,089 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 19:14:58,694 | server.py:125 | fit progress: (3, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 39.9025448950124)
INFO flwr 2024-04-17 19:14:58,694 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 19:14:58,694 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:15:06,434 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 19:15:08,017 | server.py:125 | fit progress: (4, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 49.22511897000368)
INFO flwr 2024-04-17 19:15:08,017 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 19:15:08,017 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:15:16,051 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 19:15:17,549 | server.py:125 | fit progress: (5, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 58.757505479006795)
INFO flwr 2024-04-17 19:15:17,549 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 19:15:17,549 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:15:25,549 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 19:15:26,850 | server.py:125 | fit progress: (6, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 68.05848239800252)
INFO flwr 2024-04-17 19:15:26,850 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 19:15:26,850 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:15:35,498 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 19:15:37,089 | server.py:125 | fit progress: (7, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 78.29783248901367)
INFO flwr 2024-04-17 19:15:37,090 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 19:15:37,090 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:15:45,425 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 19:15:46,705 | server.py:125 | fit progress: (8, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 87.91313167400949)
INFO flwr 2024-04-17 19:15:46,705 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 19:15:46,705 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:15:54,971 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 19:15:56,243 | server.py:125 | fit progress: (9, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 97.4511557270016)
INFO flwr 2024-04-17 19:15:56,243 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 19:15:56,243 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:16:04,431 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 19:16:06,012 | server.py:125 | fit progress: (10, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 107.22079445900454)
INFO flwr 2024-04-17 19:16:06,012 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 19:16:06,013 | server.py:153 | FL finished in 107.22125065000728
INFO flwr 2024-04-17 19:16:06,013 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 19:16:06,013 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 19:16:06,013 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 19:16:06,013 | app.py:229 | app_fit: losses_centralized [(0, 2.3024895191192627), (1, 2.2642629146575928), (2, 2.3579421043395996), (3, 2.3579421043395996), (4, 2.3579421043395996), (5, 2.3579421043395996), (6, 2.3579421043395996), (7, 2.3579421043395996), (8, 2.3579421043395996), (9, 2.3579421043395996), (10, 2.3579421043395996)]
INFO flwr 2024-04-17 19:16:06,013 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1968), (2, 0.1032), (3, 0.1032), (4, 0.1032), (5, 0.1032), (6, 0.1032), (7, 0.1032), (8, 0.1032), (9, 0.1032), (10, 0.1032)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1032
wandb:     loss 2.35794
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_191354-36h4lkxk
wandb: Find logs at: ./wandb/offline-run-20240417_191354-36h4lkxk/logs
INFO flwr 2024-04-17 19:16:09,629 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 19:23:21,086 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=768625)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=768625)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 19:23:27,748	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 19:23:28,637	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 19:23:29,097	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 19:23:29,520	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_45bdd63fb651a8fc.zip' (44.36MiB) to Ray cluster...
2024-04-17 19:23:29,671	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_45bdd63fb651a8fc.zip'.
INFO flwr 2024-04-17 19:23:40,817 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72890062848.0, 'accelerator_type:TITAN': 1.0, 'memory': 160076813312.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 19:23:40,817 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 19:23:40,817 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 19:23:40,837 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 19:23:40,839 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 19:23:40,840 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 19:23:40,840 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 19:23:44,276 | server.py:94 | initial parameters (loss, other metrics): 2.3026065826416016, {'accuracy': 0.1134, 'data_size': 10000}
INFO flwr 2024-04-17 19:23:44,276 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 19:23:44,277 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=773017)[0m 2024-04-17 19:23:47.143681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=773017)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=773017)[0m 2024-04-17 19:23:49.461445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=773017)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=773017)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=773021)[0m 2024-04-17 19:23:47.480544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=773021)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=773011)[0m 2024-04-17 19:23:49.596053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 19:24:04,180 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 19:24:05,508 | server.py:125 | fit progress: (1, 2.245954751968384, {'accuracy': 0.1951, 'data_size': 10000}, 21.231412537003052)
INFO flwr 2024-04-17 19:24:05,508 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 19:24:05,509 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:24:14,457 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 19:24:15,953 | server.py:125 | fit progress: (2, 2.0075552463531494, {'accuracy': 0.4495, 'data_size': 10000}, 31.67601901201124)
INFO flwr 2024-04-17 19:24:15,953 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 19:24:15,953 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:24:24,212 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 19:24:25,511 | server.py:125 | fit progress: (3, 1.9083902835845947, {'accuracy': 0.5508, 'data_size': 10000}, 41.23451545300486)
INFO flwr 2024-04-17 19:24:25,511 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 19:24:25,512 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:24:33,938 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 19:24:35,428 | server.py:125 | fit progress: (4, 1.8741689920425415, {'accuracy': 0.5835, 'data_size': 10000}, 51.15116111500538)
INFO flwr 2024-04-17 19:24:35,428 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 19:24:35,428 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:24:43,784 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 19:24:45,070 | server.py:125 | fit progress: (5, 1.8160574436187744, {'accuracy': 0.6443, 'data_size': 10000}, 60.79344103400945)
INFO flwr 2024-04-17 19:24:45,070 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 19:24:45,071 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:24:53,606 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 19:24:54,922 | server.py:125 | fit progress: (6, 1.8108457326889038, {'accuracy': 0.6492, 'data_size': 10000}, 70.64484535800875)
INFO flwr 2024-04-17 19:24:54,922 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 19:24:54,922 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:25:03,169 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 19:25:04,654 | server.py:125 | fit progress: (7, 1.799574851989746, {'accuracy': 0.6605, 'data_size': 10000}, 80.37728664200404)
INFO flwr 2024-04-17 19:25:04,654 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 19:25:04,654 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:25:12,630 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 19:25:13,899 | server.py:125 | fit progress: (8, 1.808476209640503, {'accuracy': 0.6515, 'data_size': 10000}, 89.6221906740102)
INFO flwr 2024-04-17 19:25:13,899 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 19:25:13,899 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:25:22,211 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 19:25:23,786 | server.py:125 | fit progress: (9, 1.735766053199768, {'accuracy': 0.7244, 'data_size': 10000}, 99.50955935800448)
INFO flwr 2024-04-17 19:25:23,787 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 19:25:23,787 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:25:32,180 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 19:25:33,474 | server.py:125 | fit progress: (10, 1.7416160106658936, {'accuracy': 0.7189, 'data_size': 10000}, 109.19722075700702)
INFO flwr 2024-04-17 19:25:33,474 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 19:25:33,474 | server.py:153 | FL finished in 109.197652775998
INFO flwr 2024-04-17 19:25:33,474 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 19:25:33,475 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 19:25:33,475 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 19:25:33,475 | app.py:229 | app_fit: losses_centralized [(0, 2.3026065826416016), (1, 2.245954751968384), (2, 2.0075552463531494), (3, 1.9083902835845947), (4, 1.8741689920425415), (5, 1.8160574436187744), (6, 1.8108457326889038), (7, 1.799574851989746), (8, 1.808476209640503), (9, 1.735766053199768), (10, 1.7416160106658936)]
INFO flwr 2024-04-17 19:25:33,475 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1134), (1, 0.1951), (2, 0.4495), (3, 0.5508), (4, 0.5835), (5, 0.6443), (6, 0.6492), (7, 0.6605), (8, 0.6515), (9, 0.7244), (10, 0.7189)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7189
wandb:     loss 1.74162
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_192320-5cvz043u
wandb: Find logs at: ./wandb/offline-run-20240417_192320-5cvz043u/logs
INFO flwr 2024-04-17 19:25:37,161 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 19:32:48,239 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=773011)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=773011)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 19:32:52,981	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 19:32:53,818	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 19:32:54,287	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 19:32:54,702	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fe15013e9c1e7dbc.zip' (44.52MiB) to Ray cluster...
2024-04-17 19:32:54,842	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fe15013e9c1e7dbc.zip'.
INFO flwr 2024-04-17 19:33:06,226 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:TITAN': 1.0, 'memory': 160061840794.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 72883646054.0}
INFO flwr 2024-04-17 19:33:06,226 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 19:33:06,226 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 19:33:06,246 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 19:33:06,248 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 19:33:06,248 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 19:33:06,248 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 19:33:09,647 | server.py:94 | initial parameters (loss, other metrics): 2.302612781524658, {'accuracy': 0.0849, 'data_size': 10000}
INFO flwr 2024-04-17 19:33:09,647 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 19:33:09,648 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=777372)[0m 2024-04-17 19:33:12.313510: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=777372)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=777372)[0m 2024-04-17 19:33:14.650851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=777381)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=777381)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=777380)[0m 2024-04-17 19:33:12.609943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=777380)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=777380)[0m 2024-04-17 19:33:14.844060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 19:33:29,538 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 19:33:30,853 | server.py:125 | fit progress: (1, 2.3013522624969482, {'accuracy': 0.1539, 'data_size': 10000}, 21.205189731990686)
INFO flwr 2024-04-17 19:33:30,853 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 19:33:30,853 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:33:39,838 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 19:33:41,443 | server.py:125 | fit progress: (2, 2.2984111309051514, {'accuracy': 0.2255, 'data_size': 10000}, 31.794960918996367)
INFO flwr 2024-04-17 19:33:41,443 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 19:33:41,443 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:33:49,552 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 19:33:51,101 | server.py:125 | fit progress: (3, 2.2924842834472656, {'accuracy': 0.2928, 'data_size': 10000}, 41.45378259300196)
INFO flwr 2024-04-17 19:33:51,102 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 19:33:51,102 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:33:59,592 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 19:34:01,109 | server.py:125 | fit progress: (4, 2.2804439067840576, {'accuracy': 0.3045, 'data_size': 10000}, 51.461178359997575)
INFO flwr 2024-04-17 19:34:01,109 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 19:34:01,109 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:34:09,204 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 19:34:10,720 | server.py:125 | fit progress: (5, 2.2579777240753174, {'accuracy': 0.3408, 'data_size': 10000}, 61.07291947500198)
INFO flwr 2024-04-17 19:34:10,721 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 19:34:10,721 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:34:19,007 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 19:34:20,564 | server.py:125 | fit progress: (6, 2.219104051589966, {'accuracy': 0.393, 'data_size': 10000}, 70.91691344699939)
INFO flwr 2024-04-17 19:34:20,565 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 19:34:20,565 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:34:28,422 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 19:34:29,926 | server.py:125 | fit progress: (7, 2.1648497581481934, {'accuracy': 0.4429, 'data_size': 10000}, 80.27847659299732)
INFO flwr 2024-04-17 19:34:29,926 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 19:34:29,927 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:34:38,392 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 19:34:39,916 | server.py:125 | fit progress: (8, 2.0998826026916504, {'accuracy': 0.4687, 'data_size': 10000}, 90.26805912700365)
INFO flwr 2024-04-17 19:34:39,916 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 19:34:39,916 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:34:48,388 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 19:34:49,735 | server.py:125 | fit progress: (9, 2.027413845062256, {'accuracy': 0.5036, 'data_size': 10000}, 100.08770114499202)
INFO flwr 2024-04-17 19:34:49,736 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 19:34:49,736 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:34:57,914 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 19:34:59,425 | server.py:125 | fit progress: (10, 1.9571123123168945, {'accuracy': 0.5437, 'data_size': 10000}, 109.77722205499595)
INFO flwr 2024-04-17 19:34:59,425 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 19:34:59,425 | server.py:153 | FL finished in 109.77767829300137
INFO flwr 2024-04-17 19:34:59,425 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 19:34:59,426 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 19:34:59,426 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 19:34:59,426 | app.py:229 | app_fit: losses_centralized [(0, 2.302612781524658), (1, 2.3013522624969482), (2, 2.2984111309051514), (3, 2.2924842834472656), (4, 2.2804439067840576), (5, 2.2579777240753174), (6, 2.219104051589966), (7, 2.1648497581481934), (8, 2.0998826026916504), (9, 2.027413845062256), (10, 1.9571123123168945)]
INFO flwr 2024-04-17 19:34:59,426 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0849), (1, 0.1539), (2, 0.2255), (3, 0.2928), (4, 0.3045), (5, 0.3408), (6, 0.393), (7, 0.4429), (8, 0.4687), (9, 0.5036), (10, 0.5437)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5437
wandb:     loss 1.95711
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_193247-z2xqj01i
wandb: Find logs at: ./wandb/offline-run-20240417_193247-z2xqj01i/logs
INFO flwr 2024-04-17 19:35:03,033 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 19:42:14,386 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=777371)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=777371)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 19:42:19,363	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 19:42:20,324	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 19:42:20,796	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 19:42:21,205	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_733ba47afbe295cc.zip' (44.68MiB) to Ray cluster...
2024-04-17 19:42:21,350	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_733ba47afbe295cc.zip'.
INFO flwr 2024-04-17 19:42:32,606 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:TITAN': 1.0, 'memory': 160060088935.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'object_store_memory': 72882895257.0}
INFO flwr 2024-04-17 19:42:32,607 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 19:42:32,607 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 19:42:32,627 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 19:42:32,629 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 19:42:32,630 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 19:42:32,630 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 19:42:35,754 | server.py:94 | initial parameters (loss, other metrics): 2.302598714828491, {'accuracy': 0.1013, 'data_size': 10000}
INFO flwr 2024-04-17 19:42:35,754 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 19:42:35,755 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=782107)[0m 2024-04-17 19:42:38.703661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=782107)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=782107)[0m 2024-04-17 19:42:41.073855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=782113)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=782113)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=782110)[0m 2024-04-17 19:42:39.073426: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=782110)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=782110)[0m 2024-04-17 19:42:41.277685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 19:42:55,516 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 19:42:57,110 | server.py:125 | fit progress: (1, 2.3025169372558594, {'accuracy': 0.1014, 'data_size': 10000}, 21.355379966000328)
INFO flwr 2024-04-17 19:42:57,110 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 19:42:57,110 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:43:06,307 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 19:43:07,823 | server.py:125 | fit progress: (2, 2.302393913269043, {'accuracy': 0.1007, 'data_size': 10000}, 32.06858458700299)
INFO flwr 2024-04-17 19:43:07,823 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 19:43:07,824 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:43:16,357 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 19:43:17,630 | server.py:125 | fit progress: (3, 2.3022289276123047, {'accuracy': 0.1008, 'data_size': 10000}, 41.876017900998704)
INFO flwr 2024-04-17 19:43:17,631 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 19:43:17,631 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:43:25,960 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 19:43:27,561 | server.py:125 | fit progress: (4, 2.3020272254943848, {'accuracy': 0.1011, 'data_size': 10000}, 51.806731667005806)
INFO flwr 2024-04-17 19:43:27,562 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 19:43:27,562 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:43:35,515 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 19:43:37,013 | server.py:125 | fit progress: (5, 2.3017847537994385, {'accuracy': 0.1073, 'data_size': 10000}, 61.258910810007364)
INFO flwr 2024-04-17 19:43:37,014 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 19:43:37,014 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:43:45,573 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 19:43:47,100 | server.py:125 | fit progress: (6, 2.301497220993042, {'accuracy': 0.1199, 'data_size': 10000}, 71.34594235700206)
INFO flwr 2024-04-17 19:43:47,101 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 19:43:47,101 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:43:54,833 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 19:43:56,395 | server.py:125 | fit progress: (7, 2.3011627197265625, {'accuracy': 0.1332, 'data_size': 10000}, 80.64063016000728)
INFO flwr 2024-04-17 19:43:56,395 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 19:43:56,396 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:44:04,649 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 19:44:06,150 | server.py:125 | fit progress: (8, 2.300783157348633, {'accuracy': 0.147, 'data_size': 10000}, 90.39539357600734)
INFO flwr 2024-04-17 19:44:06,150 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 19:44:06,150 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:44:14,385 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 19:44:15,880 | server.py:125 | fit progress: (9, 2.300347089767456, {'accuracy': 0.156, 'data_size': 10000}, 100.1258728440007)
INFO flwr 2024-04-17 19:44:15,881 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 19:44:15,881 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:44:24,551 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 19:44:26,044 | server.py:125 | fit progress: (10, 2.2998461723327637, {'accuracy': 0.1625, 'data_size': 10000}, 110.28926405400853)
INFO flwr 2024-04-17 19:44:26,044 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 19:44:26,044 | server.py:153 | FL finished in 110.28969214799872
INFO flwr 2024-04-17 19:44:26,044 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 19:44:26,044 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 19:44:26,045 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 19:44:26,045 | app.py:229 | app_fit: losses_centralized [(0, 2.302598714828491), (1, 2.3025169372558594), (2, 2.302393913269043), (3, 2.3022289276123047), (4, 2.3020272254943848), (5, 2.3017847537994385), (6, 2.301497220993042), (7, 2.3011627197265625), (8, 2.300783157348633), (9, 2.300347089767456), (10, 2.2998461723327637)]
INFO flwr 2024-04-17 19:44:26,045 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1013), (1, 0.1014), (2, 0.1007), (3, 0.1008), (4, 0.1011), (5, 0.1073), (6, 0.1199), (7, 0.1332), (8, 0.147), (9, 0.156), (10, 0.1625)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1625
wandb:     loss 2.29985
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_194213-0c7cky38
wandb: Find logs at: ./wandb/offline-run-20240417_194213-0c7cky38/logs
INFO flwr 2024-04-17 19:44:29,690 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 19:51:41,156 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=782100)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=782100)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 19:51:46,153	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 19:51:47,067	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 19:51:47,508	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 19:51:47,926	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c46c59f48c83fb5f.zip' (44.84MiB) to Ray cluster...
2024-04-17 19:51:48,080	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c46c59f48c83fb5f.zip'.
INFO flwr 2024-04-17 19:51:59,271 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 72881380147.0, 'memory': 160056553677.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 19:51:59,271 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 19:51:59,271 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 19:51:59,288 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 19:51:59,289 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 19:51:59,289 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 19:51:59,290 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 19:52:03,518 | server.py:94 | initial parameters (loss, other metrics): 2.302523136138916, {'accuracy': 0.0562, 'data_size': 10000}
INFO flwr 2024-04-17 19:52:03,519 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 19:52:03,519 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=786468)[0m 2024-04-17 19:52:05.339391: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=786468)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=786471)[0m 2024-04-17 19:52:07.721451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=786476)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=786476)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=786467)[0m 2024-04-17 19:52:05.539979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=786467)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=786467)[0m 2024-04-17 19:52:07.818093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 19:52:22,152 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 19:52:23,678 | server.py:125 | fit progress: (1, 2.30251407623291, {'accuracy': 0.0575, 'data_size': 10000}, 20.15968513500411)
INFO flwr 2024-04-17 19:52:23,679 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 19:52:23,679 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:52:32,984 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 19:52:34,304 | server.py:125 | fit progress: (2, 2.3025026321411133, {'accuracy': 0.0594, 'data_size': 10000}, 30.78555949399015)
INFO flwr 2024-04-17 19:52:34,305 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 19:52:34,305 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:52:42,706 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 19:52:44,264 | server.py:125 | fit progress: (3, 2.302489757537842, {'accuracy': 0.0608, 'data_size': 10000}, 40.74564575999102)
INFO flwr 2024-04-17 19:52:44,265 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 19:52:44,265 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:52:52,364 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 19:52:53,899 | server.py:125 | fit progress: (4, 2.3024752140045166, {'accuracy': 0.0622, 'data_size': 10000}, 50.38050029599981)
INFO flwr 2024-04-17 19:52:53,900 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 19:52:53,900 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:53:02,073 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 19:53:03,598 | server.py:125 | fit progress: (5, 2.302459478378296, {'accuracy': 0.0639, 'data_size': 10000}, 60.07927774300333)
INFO flwr 2024-04-17 19:53:03,598 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 19:53:03,599 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:53:11,802 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 19:53:13,330 | server.py:125 | fit progress: (6, 2.302443027496338, {'accuracy': 0.0655, 'data_size': 10000}, 69.81164668500423)
INFO flwr 2024-04-17 19:53:13,331 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 19:53:13,331 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:53:21,593 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 19:53:23,163 | server.py:125 | fit progress: (7, 2.302424669265747, {'accuracy': 0.0675, 'data_size': 10000}, 79.64424833099474)
INFO flwr 2024-04-17 19:53:23,164 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 19:53:23,164 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:53:31,581 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 19:53:32,878 | server.py:125 | fit progress: (8, 2.3024046421051025, {'accuracy': 0.069, 'data_size': 10000}, 89.35895271699701)
INFO flwr 2024-04-17 19:53:32,878 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 19:53:32,878 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:53:41,076 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 19:53:42,598 | server.py:125 | fit progress: (9, 2.302384853363037, {'accuracy': 0.0712, 'data_size': 10000}, 99.07902434399875)
INFO flwr 2024-04-17 19:53:42,598 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 19:53:42,598 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 19:53:50,723 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 19:53:52,070 | server.py:125 | fit progress: (10, 2.3023645877838135, {'accuracy': 0.073, 'data_size': 10000}, 108.55166750399803)
INFO flwr 2024-04-17 19:53:52,071 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 19:53:52,071 | server.py:153 | FL finished in 108.55218154199247
INFO flwr 2024-04-17 19:53:52,071 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 19:53:52,071 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 19:53:52,071 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 19:53:52,072 | app.py:229 | app_fit: losses_centralized [(0, 2.302523136138916), (1, 2.30251407623291), (2, 2.3025026321411133), (3, 2.302489757537842), (4, 2.3024752140045166), (5, 2.302459478378296), (6, 2.302443027496338), (7, 2.302424669265747), (8, 2.3024046421051025), (9, 2.302384853363037), (10, 2.3023645877838135)]
INFO flwr 2024-04-17 19:53:52,072 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0562), (1, 0.0575), (2, 0.0594), (3, 0.0608), (4, 0.0622), (5, 0.0639), (6, 0.0655), (7, 0.0675), (8, 0.069), (9, 0.0712), (10, 0.073)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.073
wandb:     loss 2.30236
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_195140-2jj073a2
wandb: Find logs at: ./wandb/offline-run-20240417_195140-2jj073a2/logs
INFO flwr 2024-04-17 19:53:55,709 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 20:01:07,863 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=786467)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=786467)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 20:01:17,327	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 20:01:18,705	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 20:01:19,195	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 20:01:19,608	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3ee7b0177d418fb4.zip' (44.99MiB) to Ray cluster...
2024-04-17 20:01:19,753	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3ee7b0177d418fb4.zip'.
INFO flwr 2024-04-17 20:01:31,122 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72995540582.0, 'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 160322928026.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 20:01:31,122 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 20:01:31,122 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 20:01:31,144 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 20:01:31,145 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 20:01:31,145 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 20:01:31,145 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 20:01:34,617 | server.py:94 | initial parameters (loss, other metrics): 2.3022804260253906, {'accuracy': 0.129, 'data_size': 10000}
INFO flwr 2024-04-17 20:01:34,618 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 20:01:34,619 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=790954)[0m 2024-04-17 20:01:37.923299: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=790954)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=790945)[0m 2024-04-17 20:01:41.254959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=790945)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=790945)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=790953)[0m 2024-04-17 20:01:37.967511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=790953)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=790944)[0m 2024-04-17 20:01:41.254958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 20:02:00,757 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 20:02:02,290 | server.py:125 | fit progress: (1, 2.2814197540283203, {'accuracy': 0.1797, 'data_size': 10000}, 27.671540971001377)
INFO flwr 2024-04-17 20:02:02,290 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 20:02:02,290 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:02:11,518 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 20:02:13,046 | server.py:125 | fit progress: (2, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 38.427917477005394)
INFO flwr 2024-04-17 20:02:13,047 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 20:02:13,047 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:02:21,158 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 20:02:22,691 | server.py:125 | fit progress: (3, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 48.072928141002194)
INFO flwr 2024-04-17 20:02:22,692 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 20:02:22,692 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:02:30,927 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 20:02:32,441 | server.py:125 | fit progress: (4, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 57.82307988499815)
INFO flwr 2024-04-17 20:02:32,442 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 20:02:32,442 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:02:40,858 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 20:02:42,123 | server.py:125 | fit progress: (5, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 67.50504181299766)
INFO flwr 2024-04-17 20:02:42,124 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 20:02:42,124 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:02:50,858 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 20:02:52,399 | server.py:125 | fit progress: (6, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 77.78054052099469)
INFO flwr 2024-04-17 20:02:52,399 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 20:02:52,399 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:03:00,632 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 20:03:01,919 | server.py:125 | fit progress: (7, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 87.30019177599752)
INFO flwr 2024-04-17 20:03:01,919 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 20:03:01,919 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:03:10,040 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 20:03:11,586 | server.py:125 | fit progress: (8, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 96.96743702600361)
INFO flwr 2024-04-17 20:03:11,586 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 20:03:11,586 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:03:20,153 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 20:03:21,468 | server.py:125 | fit progress: (9, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 106.8500343929918)
INFO flwr 2024-04-17 20:03:21,469 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 20:03:21,469 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:03:29,550 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 20:03:31,047 | server.py:125 | fit progress: (10, 2.3631420135498047, {'accuracy': 0.098, 'data_size': 10000}, 116.42882468199241)
INFO flwr 2024-04-17 20:03:31,048 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 20:03:31,048 | server.py:153 | FL finished in 116.42934143000457
INFO flwr 2024-04-17 20:03:31,048 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 20:03:31,048 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 20:03:31,048 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 20:03:31,048 | app.py:229 | app_fit: losses_centralized [(0, 2.3022804260253906), (1, 2.2814197540283203), (2, 2.3631420135498047), (3, 2.3631420135498047), (4, 2.3631420135498047), (5, 2.3631420135498047), (6, 2.3631420135498047), (7, 2.3631420135498047), (8, 2.3631420135498047), (9, 2.3631420135498047), (10, 2.3631420135498047)]
INFO flwr 2024-04-17 20:03:31,049 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.129), (1, 0.1797), (2, 0.098), (3, 0.098), (4, 0.098), (5, 0.098), (6, 0.098), (7, 0.098), (8, 0.098), (9, 0.098), (10, 0.098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.098
wandb:     loss 2.36314
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_200107-27kj2864
wandb: Find logs at: ./wandb/offline-run-20240417_200107-27kj2864/logs
INFO flwr 2024-04-17 20:03:34,755 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 20:10:46,453 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=790944)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=790944)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 20:10:51,949	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 20:10:53,315	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 20:10:53,754	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 20:10:54,167	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c014ddbebd2a29a1.zip' (45.15MiB) to Ray cluster...
2024-04-17 20:10:54,315	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c014ddbebd2a29a1.zip'.
INFO flwr 2024-04-17 20:11:05,560 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 72954652262.0, 'CPU': 64.0, 'memory': 160227521946.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 20:11:05,560 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 20:11:05,560 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 20:11:05,581 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 20:11:05,582 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 20:11:05,582 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 20:11:05,583 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 20:11:08,315 | server.py:94 | initial parameters (loss, other metrics): 2.3026187419891357, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-17 20:11:08,316 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 20:11:08,317 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=795347)[0m 2024-04-17 20:11:11.822078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=795347)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=795347)[0m 2024-04-17 20:11:14.117367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=795354)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=795354)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=795355)[0m 2024-04-17 20:11:12.061922: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=795355)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=795355)[0m 2024-04-17 20:11:14.399607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 20:11:29,363 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 20:11:30,691 | server.py:125 | fit progress: (1, 2.2225329875946045, {'accuracy': 0.3064, 'data_size': 10000}, 22.374383556001703)
INFO flwr 2024-04-17 20:11:30,691 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 20:11:30,691 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:11:39,826 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 20:11:41,147 | server.py:125 | fit progress: (2, 2.0782763957977295, {'accuracy': 0.353, 'data_size': 10000}, 32.8306847789936)
INFO flwr 2024-04-17 20:11:41,147 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 20:11:41,148 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:11:49,515 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 20:11:51,085 | server.py:125 | fit progress: (3, 1.7282865047454834, {'accuracy': 0.7364, 'data_size': 10000}, 42.76833059299679)
INFO flwr 2024-04-17 20:11:51,085 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 20:11:51,085 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:11:59,152 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 20:12:00,426 | server.py:125 | fit progress: (4, 1.6837785243988037, {'accuracy': 0.7774, 'data_size': 10000}, 52.109938141991734)
INFO flwr 2024-04-17 20:12:00,427 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 20:12:00,427 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:12:08,768 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 20:12:10,258 | server.py:125 | fit progress: (5, 1.6473305225372314, {'accuracy': 0.8091, 'data_size': 10000}, 61.94147730599798)
INFO flwr 2024-04-17 20:12:10,258 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 20:12:10,258 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:12:18,268 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 20:12:19,621 | server.py:125 | fit progress: (6, 1.6415464878082275, {'accuracy': 0.8175, 'data_size': 10000}, 71.30467570100154)
INFO flwr 2024-04-17 20:12:19,621 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 20:12:19,622 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:12:27,596 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 20:12:29,092 | server.py:125 | fit progress: (7, 1.6285566091537476, {'accuracy': 0.8318, 'data_size': 10000}, 80.77599302299495)
INFO flwr 2024-04-17 20:12:29,093 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 20:12:29,093 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:12:37,754 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 20:12:39,042 | server.py:125 | fit progress: (8, 1.6013673543930054, {'accuracy': 0.8598, 'data_size': 10000}, 90.72609815999749)
INFO flwr 2024-04-17 20:12:39,043 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 20:12:39,043 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:12:47,637 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 20:12:49,198 | server.py:125 | fit progress: (9, 1.5712177753448486, {'accuracy': 0.8894, 'data_size': 10000}, 100.88195090199588)
INFO flwr 2024-04-17 20:12:49,199 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 20:12:49,199 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:12:57,263 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 20:12:58,866 | server.py:125 | fit progress: (10, 1.560701608657837, {'accuracy': 0.8999, 'data_size': 10000}, 110.5492724700016)
INFO flwr 2024-04-17 20:12:58,866 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 20:12:58,866 | server.py:153 | FL finished in 110.54970751098881
INFO flwr 2024-04-17 20:12:58,866 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 20:12:58,866 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 20:12:58,866 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 20:12:58,867 | app.py:229 | app_fit: losses_centralized [(0, 2.3026187419891357), (1, 2.2225329875946045), (2, 2.0782763957977295), (3, 1.7282865047454834), (4, 1.6837785243988037), (5, 1.6473305225372314), (6, 1.6415464878082275), (7, 1.6285566091537476), (8, 1.6013673543930054), (9, 1.5712177753448486), (10, 1.560701608657837)]
INFO flwr 2024-04-17 20:12:58,867 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.3064), (2, 0.353), (3, 0.7364), (4, 0.7774), (5, 0.8091), (6, 0.8175), (7, 0.8318), (8, 0.8598), (9, 0.8894), (10, 0.8999)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8999
wandb:     loss 1.5607
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_201046-dwuhxyvk
wandb: Find logs at: ./wandb/offline-run-20240417_201046-dwuhxyvk/logs
INFO flwr 2024-04-17 20:13:02,606 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 20:20:14,126 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=795347)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=795347)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 20:20:19,013	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 20:20:19,884	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 20:20:20,386	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 20:20:20,807	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fde2903f89de91aa.zip' (45.31MiB) to Ray cluster...
2024-04-17 20:20:20,967	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fde2903f89de91aa.zip'.
INFO flwr 2024-04-17 20:20:32,250 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 159913810125.0, 'object_store_memory': 72820204339.0}
INFO flwr 2024-04-17 20:20:32,250 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 20:20:32,250 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 20:20:32,271 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 20:20:32,273 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 20:20:32,273 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 20:20:32,273 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 20:20:35,584 | server.py:94 | initial parameters (loss, other metrics): 2.302609920501709, {'accuracy': 0.1057, 'data_size': 10000}
INFO flwr 2024-04-17 20:20:35,584 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 20:20:35,585 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=799473)[0m 2024-04-17 20:20:38.518339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=799473)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=799473)[0m 2024-04-17 20:20:40.807142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=799479)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=799479)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=799482)[0m 2024-04-17 20:20:38.655104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=799482)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=799482)[0m 2024-04-17 20:20:41.034759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 20:20:55,559 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 20:20:56,864 | server.py:125 | fit progress: (1, 2.301412343978882, {'accuracy': 0.2058, 'data_size': 10000}, 21.279187022009864)
INFO flwr 2024-04-17 20:20:56,864 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 20:20:56,864 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:21:06,081 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 20:21:07,615 | server.py:125 | fit progress: (2, 2.298309087753296, {'accuracy': 0.1971, 'data_size': 10000}, 32.0303345380089)
INFO flwr 2024-04-17 20:21:07,615 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 20:21:07,616 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:21:16,077 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 20:21:17,402 | server.py:125 | fit progress: (3, 2.291275978088379, {'accuracy': 0.2781, 'data_size': 10000}, 41.81722382600128)
INFO flwr 2024-04-17 20:21:17,402 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 20:21:17,402 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:21:25,803 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 20:21:27,323 | server.py:125 | fit progress: (4, 2.2765860557556152, {'accuracy': 0.3526, 'data_size': 10000}, 51.73807539801055)
INFO flwr 2024-04-17 20:21:27,323 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 20:21:27,323 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:21:35,439 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 20:21:36,949 | server.py:125 | fit progress: (5, 2.2480950355529785, {'accuracy': 0.4558, 'data_size': 10000}, 61.36461732900352)
INFO flwr 2024-04-17 20:21:36,949 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 20:21:36,950 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:21:45,494 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 20:21:47,068 | server.py:125 | fit progress: (6, 2.202543258666992, {'accuracy': 0.4637, 'data_size': 10000}, 71.48349547899852)
INFO flwr 2024-04-17 20:21:47,068 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 20:21:47,069 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:21:55,320 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 20:21:56,827 | server.py:125 | fit progress: (7, 2.146765947341919, {'accuracy': 0.4656, 'data_size': 10000}, 81.24279397100327)
INFO flwr 2024-04-17 20:21:56,828 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 20:21:56,828 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:22:05,365 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 20:22:06,859 | server.py:125 | fit progress: (8, 2.0897507667541504, {'accuracy': 0.4859, 'data_size': 10000}, 91.27490837100777)
INFO flwr 2024-04-17 20:22:06,860 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 20:22:06,860 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:22:14,978 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 20:22:16,513 | server.py:125 | fit progress: (9, 2.0324573516845703, {'accuracy': 0.507, 'data_size': 10000}, 100.92799923600978)
INFO flwr 2024-04-17 20:22:16,513 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 20:22:16,513 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:22:24,686 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 20:22:26,230 | server.py:125 | fit progress: (10, 1.9724081754684448, {'accuracy': 0.5469, 'data_size': 10000}, 110.64523800399911)
INFO flwr 2024-04-17 20:22:26,230 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 20:22:26,230 | server.py:153 | FL finished in 110.64591030099837
INFO flwr 2024-04-17 20:22:26,231 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 20:22:26,231 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 20:22:26,231 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 20:22:26,231 | app.py:229 | app_fit: losses_centralized [(0, 2.302609920501709), (1, 2.301412343978882), (2, 2.298309087753296), (3, 2.291275978088379), (4, 2.2765860557556152), (5, 2.2480950355529785), (6, 2.202543258666992), (7, 2.146765947341919), (8, 2.0897507667541504), (9, 2.0324573516845703), (10, 1.9724081754684448)]
INFO flwr 2024-04-17 20:22:26,231 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1057), (1, 0.2058), (2, 0.1971), (3, 0.2781), (4, 0.3526), (5, 0.4558), (6, 0.4637), (7, 0.4656), (8, 0.4859), (9, 0.507), (10, 0.5469)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5469
wandb:     loss 1.97241
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_202013-bi0gtnp4
wandb: Find logs at: ./wandb/offline-run-20240417_202013-bi0gtnp4/logs
INFO flwr 2024-04-17 20:22:29,971 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 20:29:41,178 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=799471)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=799471)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 20:29:46,132	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 20:29:46,981	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 20:29:47,439	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 20:29:47,853	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7535076270f68288.zip' (45.46MiB) to Ray cluster...
2024-04-17 20:29:47,995	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7535076270f68288.zip'.
INFO flwr 2024-04-17 20:29:59,345 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72753548083.0, 'CPU': 64.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 159758278861.0}
INFO flwr 2024-04-17 20:29:59,346 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 20:29:59,346 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 20:29:59,366 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 20:29:59,368 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 20:29:59,369 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 20:29:59,369 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 20:30:01,991 | server.py:94 | initial parameters (loss, other metrics): 2.3026788234710693, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-17 20:30:01,992 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 20:30:01,992 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=803848)[0m 2024-04-17 20:30:05.494638: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=803848)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=803848)[0m 2024-04-17 20:30:07.818458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=803848)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=803848)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=803840)[0m 2024-04-17 20:30:05.953564: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=803840)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=803840)[0m 2024-04-17 20:30:08.221406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 20:30:22,386 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 20:30:23,942 | server.py:125 | fit progress: (1, 2.3025896549224854, {'accuracy': 0.0982, 'data_size': 10000}, 21.94971338400501)
INFO flwr 2024-04-17 20:30:23,942 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 20:30:23,942 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:30:33,042 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 20:30:34,327 | server.py:125 | fit progress: (2, 2.302459716796875, {'accuracy': 0.0982, 'data_size': 10000}, 32.33487494000292)
INFO flwr 2024-04-17 20:30:34,327 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 20:30:34,327 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:30:42,962 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 20:30:44,498 | server.py:125 | fit progress: (3, 2.3023009300231934, {'accuracy': 0.0982, 'data_size': 10000}, 42.50543108199781)
INFO flwr 2024-04-17 20:30:44,498 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 20:30:44,498 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:30:52,954 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 20:30:54,301 | server.py:125 | fit progress: (4, 2.3021042346954346, {'accuracy': 0.0982, 'data_size': 10000}, 52.30919799000549)
INFO flwr 2024-04-17 20:30:54,302 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 20:30:54,302 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:31:02,353 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 20:31:03,864 | server.py:125 | fit progress: (5, 2.301875352859497, {'accuracy': 0.0982, 'data_size': 10000}, 61.871682575001614)
INFO flwr 2024-04-17 20:31:03,864 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 20:31:03,864 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:31:12,332 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 20:31:13,620 | server.py:125 | fit progress: (6, 2.301603317260742, {'accuracy': 0.0982, 'data_size': 10000}, 71.62810439600435)
INFO flwr 2024-04-17 20:31:13,620 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 20:31:13,621 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:31:21,703 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 20:31:23,256 | server.py:125 | fit progress: (7, 2.301287889480591, {'accuracy': 0.0982, 'data_size': 10000}, 81.2637846340076)
INFO flwr 2024-04-17 20:31:23,256 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 20:31:23,256 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:31:31,479 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 20:31:32,777 | server.py:125 | fit progress: (8, 2.3009281158447266, {'accuracy': 0.0982, 'data_size': 10000}, 90.78527836200374)
INFO flwr 2024-04-17 20:31:32,778 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 20:31:32,778 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:31:41,155 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 20:31:42,689 | server.py:125 | fit progress: (9, 2.300518274307251, {'accuracy': 0.1013, 'data_size': 10000}, 100.69656558800489)
INFO flwr 2024-04-17 20:31:42,689 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 20:31:42,689 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:31:51,359 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 20:31:52,717 | server.py:125 | fit progress: (10, 2.3000597953796387, {'accuracy': 0.1054, 'data_size': 10000}, 110.7244494070037)
INFO flwr 2024-04-17 20:31:52,717 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 20:31:52,717 | server.py:153 | FL finished in 110.72497174900491
INFO flwr 2024-04-17 20:31:52,717 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 20:31:52,718 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 20:31:52,718 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 20:31:52,718 | app.py:229 | app_fit: losses_centralized [(0, 2.3026788234710693), (1, 2.3025896549224854), (2, 2.302459716796875), (3, 2.3023009300231934), (4, 2.3021042346954346), (5, 2.301875352859497), (6, 2.301603317260742), (7, 2.301287889480591), (8, 2.3009281158447266), (9, 2.300518274307251), (10, 2.3000597953796387)]
INFO flwr 2024-04-17 20:31:52,718 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.0982), (2, 0.0982), (3, 0.0982), (4, 0.0982), (5, 0.0982), (6, 0.0982), (7, 0.0982), (8, 0.0982), (9, 0.1013), (10, 0.1054)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1054
wandb:     loss 2.30006
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_202940-r1rz8bo1
wandb: Find logs at: ./wandb/offline-run-20240417_202940-r1rz8bo1/logs
INFO flwr 2024-04-17 20:31:56,352 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 20:39:07,723 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=803835)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=803835)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 20:39:12,628	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 20:39:13,550	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 20:39:14,022	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 20:39:14,443	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c898d1b8f0758f4c.zip' (45.62MiB) to Ray cluster...
2024-04-17 20:39:14,592	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c898d1b8f0758f4c.zip'.
INFO flwr 2024-04-17 20:39:25,930 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 159764850484.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 72756364492.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 20:39:25,931 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 20:39:25,931 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 20:39:25,954 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 20:39:25,956 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 20:39:25,956 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 20:39:25,956 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 20:39:30,096 | server.py:94 | initial parameters (loss, other metrics): 2.3028132915496826, {'accuracy': 0.1007, 'data_size': 10000}
INFO flwr 2024-04-17 20:39:30,097 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 20:39:30,097 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=808201)[0m 2024-04-17 20:39:32.067533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=808201)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=808201)[0m 2024-04-17 20:39:34.316739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=808204)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=808204)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=808191)[0m 2024-04-17 20:39:32.196529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=808191)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=808191)[0m 2024-04-17 20:39:34.557053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 20:39:48,495 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 20:39:50,022 | server.py:125 | fit progress: (1, 2.302804946899414, {'accuracy': 0.1007, 'data_size': 10000}, 19.92510623099224)
INFO flwr 2024-04-17 20:39:50,023 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 20:39:50,023 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:39:59,015 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 20:40:00,531 | server.py:125 | fit progress: (2, 2.302794933319092, {'accuracy': 0.1007, 'data_size': 10000}, 30.433287615000154)
INFO flwr 2024-04-17 20:40:00,531 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 20:40:00,531 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:40:08,665 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 20:40:10,190 | server.py:125 | fit progress: (3, 2.302783250808716, {'accuracy': 0.1007, 'data_size': 10000}, 40.092375096995966)
INFO flwr 2024-04-17 20:40:10,190 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 20:40:10,190 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:40:18,370 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 20:40:19,921 | server.py:125 | fit progress: (4, 2.3027706146240234, {'accuracy': 0.1007, 'data_size': 10000}, 49.823645604992635)
INFO flwr 2024-04-17 20:40:19,921 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 20:40:19,921 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:40:27,878 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 20:40:29,147 | server.py:125 | fit progress: (5, 2.3027565479278564, {'accuracy': 0.1007, 'data_size': 10000}, 59.04983755199646)
INFO flwr 2024-04-17 20:40:29,147 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 20:40:29,148 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:40:37,560 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 20:40:39,065 | server.py:125 | fit progress: (6, 2.302741289138794, {'accuracy': 0.1007, 'data_size': 10000}, 68.96804972000245)
INFO flwr 2024-04-17 20:40:39,066 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 20:40:39,066 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:40:47,336 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 20:40:48,667 | server.py:125 | fit progress: (7, 2.3027255535125732, {'accuracy': 0.1007, 'data_size': 10000}, 78.56984229499358)
INFO flwr 2024-04-17 20:40:48,668 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 20:40:48,668 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:40:57,090 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 20:40:58,597 | server.py:125 | fit progress: (8, 2.302708864212036, {'accuracy': 0.1007, 'data_size': 10000}, 88.49921933699807)
INFO flwr 2024-04-17 20:40:58,597 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 20:40:58,597 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:41:06,936 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 20:41:08,231 | server.py:125 | fit progress: (9, 2.302692174911499, {'accuracy': 0.1007, 'data_size': 10000}, 98.13376642799994)
INFO flwr 2024-04-17 20:41:08,231 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 20:41:08,232 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:41:16,401 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 20:41:17,963 | server.py:125 | fit progress: (10, 2.3026745319366455, {'accuracy': 0.1007, 'data_size': 10000}, 107.865428213001)
INFO flwr 2024-04-17 20:41:17,963 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 20:41:17,963 | server.py:153 | FL finished in 107.86585244699381
INFO flwr 2024-04-17 20:41:17,963 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 20:41:17,963 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 20:41:17,964 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 20:41:17,964 | app.py:229 | app_fit: losses_centralized [(0, 2.3028132915496826), (1, 2.302804946899414), (2, 2.302794933319092), (3, 2.302783250808716), (4, 2.3027706146240234), (5, 2.3027565479278564), (6, 2.302741289138794), (7, 2.3027255535125732), (8, 2.302708864212036), (9, 2.302692174911499), (10, 2.3026745319366455)]
INFO flwr 2024-04-17 20:41:17,964 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1007), (1, 0.1007), (2, 0.1007), (3, 0.1007), (4, 0.1007), (5, 0.1007), (6, 0.1007), (7, 0.1007), (8, 0.1007), (9, 0.1007), (10, 0.1007)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1007
wandb:     loss 2.30267
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_203907-h0uew64v
wandb: Find logs at: ./wandb/offline-run-20240417_203907-h0uew64v/logs
INFO flwr 2024-04-17 20:41:21,615 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 20:48:32,844 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=808191)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=808191)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 20:48:37,741	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 20:48:38,612	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 20:48:39,074	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 20:48:39,508	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ec4d94fe58e24e25.zip' (45.77MiB) to Ray cluster...
2024-04-17 20:48:39,663	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ec4d94fe58e24e25.zip'.
INFO flwr 2024-04-17 20:48:50,895 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 159645001524.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'object_store_memory': 72705000652.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 20:48:50,896 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 20:48:50,896 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 20:48:50,917 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 20:48:50,919 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 20:48:50,919 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 20:48:50,919 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 20:48:54,034 | server.py:94 | initial parameters (loss, other metrics): 2.30283260345459, {'accuracy': 0.0881, 'data_size': 10000}
INFO flwr 2024-04-17 20:48:54,034 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 20:48:54,035 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=812926)[0m 2024-04-17 20:48:57.129734: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=812926)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=812923)[0m 2024-04-17 20:48:59.478277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=812924)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=812924)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=812929)[0m 2024-04-17 20:48:57.405335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=812929)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=812929)[0m 2024-04-17 20:48:59.779825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 20:49:14,725 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 20:49:16,058 | server.py:125 | fit progress: (1, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 22.023667296991334)
INFO flwr 2024-04-17 20:49:16,058 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 20:49:16,059 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:49:25,532 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 20:49:26,845 | server.py:125 | fit progress: (2, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 32.810334927999065)
INFO flwr 2024-04-17 20:49:26,845 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 20:49:26,845 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:49:35,532 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 20:49:37,103 | server.py:125 | fit progress: (3, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 43.06881994199648)
INFO flwr 2024-04-17 20:49:37,104 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 20:49:37,104 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:49:45,753 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 20:49:47,040 | server.py:125 | fit progress: (4, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 53.00525605099392)
INFO flwr 2024-04-17 20:49:47,040 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 20:49:47,040 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:49:55,767 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 20:49:57,270 | server.py:125 | fit progress: (5, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 63.235641964987735)
INFO flwr 2024-04-17 20:49:57,270 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 20:49:57,271 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:50:06,087 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 20:50:07,364 | server.py:125 | fit progress: (6, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 73.32906488698791)
INFO flwr 2024-04-17 20:50:07,364 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 20:50:07,364 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:50:15,842 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 20:50:17,194 | server.py:125 | fit progress: (7, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 83.15942702899338)
INFO flwr 2024-04-17 20:50:17,194 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 20:50:17,194 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:50:25,494 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 20:50:27,024 | server.py:125 | fit progress: (8, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 92.98972725299245)
INFO flwr 2024-04-17 20:50:27,024 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 20:50:27,025 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:50:35,926 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 20:50:37,220 | server.py:125 | fit progress: (9, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 103.18512485199608)
INFO flwr 2024-04-17 20:50:37,220 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 20:50:37,220 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:50:46,098 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 20:50:47,609 | server.py:125 | fit progress: (10, 2.3637421131134033, {'accuracy': 0.0974, 'data_size': 10000}, 113.57445172299049)
INFO flwr 2024-04-17 20:50:47,609 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 20:50:47,609 | server.py:153 | FL finished in 113.57490521398722
INFO flwr 2024-04-17 20:50:47,609 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 20:50:47,610 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 20:50:47,610 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 20:50:47,610 | app.py:229 | app_fit: losses_centralized [(0, 2.30283260345459), (1, 2.3637421131134033), (2, 2.3637421131134033), (3, 2.3637421131134033), (4, 2.3637421131134033), (5, 2.3637421131134033), (6, 2.3637421131134033), (7, 2.3637421131134033), (8, 2.3637421131134033), (9, 2.3637421131134033), (10, 2.3637421131134033)]
INFO flwr 2024-04-17 20:50:47,610 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0881), (1, 0.0974), (2, 0.0974), (3, 0.0974), (4, 0.0974), (5, 0.0974), (6, 0.0974), (7, 0.0974), (8, 0.0974), (9, 0.0974), (10, 0.0974)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0974
wandb:     loss 2.36374
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_204832-vk8r75z0
wandb: Find logs at: ./wandb/offline-run-20240417_204832-vk8r75z0/logs
INFO flwr 2024-04-17 20:50:51,197 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 20:58:02,623 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=812918)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=812918)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 20:58:08,580	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 20:58:09,524	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 20:58:09,979	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 20:58:10,392	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_273e698a4b9a0b1a.zip' (45.93MiB) to Ray cluster...
2024-04-17 20:58:10,543	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_273e698a4b9a0b1a.zip'.
INFO flwr 2024-04-17 20:58:22,376 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 159611957044.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 72690838732.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 20:58:22,376 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 20:58:22,376 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 20:58:22,397 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 20:58:22,398 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 20:58:22,399 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 20:58:22,399 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 20:58:25,651 | server.py:94 | initial parameters (loss, other metrics): 2.3027424812316895, {'accuracy': 0.0905, 'data_size': 10000}
INFO flwr 2024-04-17 20:58:25,651 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 20:58:25,651 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=817295)[0m 2024-04-17 20:58:28.727614: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=817295)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=817295)[0m 2024-04-17 20:58:31.144637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=817295)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=817295)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=817290)[0m 2024-04-17 20:58:29.054034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=817290)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=817290)[0m 2024-04-17 20:58:31.317909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 20:58:46,227 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 20:58:47,771 | server.py:125 | fit progress: (1, 2.253953218460083, {'accuracy': 0.1803, 'data_size': 10000}, 22.11944485398999)
INFO flwr 2024-04-17 20:58:47,771 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 20:58:47,771 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:58:57,633 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 20:58:58,914 | server.py:125 | fit progress: (2, 2.058873176574707, {'accuracy': 0.3984, 'data_size': 10000}, 33.262629213990294)
INFO flwr 2024-04-17 20:58:58,914 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 20:58:58,914 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:59:07,801 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 20:59:09,384 | server.py:125 | fit progress: (3, 1.7692177295684814, {'accuracy': 0.6947, 'data_size': 10000}, 43.732682884990936)
INFO flwr 2024-04-17 20:59:09,384 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 20:59:09,385 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:59:18,120 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 20:59:19,400 | server.py:125 | fit progress: (4, 1.7033109664916992, {'accuracy': 0.7564, 'data_size': 10000}, 53.74838456399448)
INFO flwr 2024-04-17 20:59:19,400 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 20:59:19,400 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:59:28,234 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 20:59:29,532 | server.py:125 | fit progress: (5, 1.662846326828003, {'accuracy': 0.7972, 'data_size': 10000}, 63.88066668198735)
INFO flwr 2024-04-17 20:59:29,532 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 20:59:29,533 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:59:38,443 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 20:59:40,043 | server.py:125 | fit progress: (6, 1.648529291152954, {'accuracy': 0.812, 'data_size': 10000}, 74.3917662849999)
INFO flwr 2024-04-17 20:59:40,043 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 20:59:40,044 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:59:48,738 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 20:59:50,057 | server.py:125 | fit progress: (7, 1.642818570137024, {'accuracy': 0.8184, 'data_size': 10000}, 84.40610074799042)
INFO flwr 2024-04-17 20:59:50,058 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 20:59:50,058 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 20:59:58,652 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 21:00:00,153 | server.py:125 | fit progress: (8, 1.6434240341186523, {'accuracy': 0.8173, 'data_size': 10000}, 94.50197262100119)
INFO flwr 2024-04-17 21:00:00,154 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 21:00:00,154 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:00:08,776 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 21:00:10,282 | server.py:125 | fit progress: (9, 1.6491096019744873, {'accuracy': 0.8116, 'data_size': 10000}, 104.63040357398859)
INFO flwr 2024-04-17 21:00:10,282 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 21:00:10,282 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:00:18,927 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 21:00:20,469 | server.py:125 | fit progress: (10, 1.638762354850769, {'accuracy': 0.8223, 'data_size': 10000}, 114.81758630799595)
INFO flwr 2024-04-17 21:00:20,469 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 21:00:20,469 | server.py:153 | FL finished in 114.81802907800011
INFO flwr 2024-04-17 21:00:20,469 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 21:00:20,470 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 21:00:20,470 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 21:00:20,470 | app.py:229 | app_fit: losses_centralized [(0, 2.3027424812316895), (1, 2.253953218460083), (2, 2.058873176574707), (3, 1.7692177295684814), (4, 1.7033109664916992), (5, 1.662846326828003), (6, 1.648529291152954), (7, 1.642818570137024), (8, 1.6434240341186523), (9, 1.6491096019744873), (10, 1.638762354850769)]
INFO flwr 2024-04-17 21:00:20,470 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0905), (1, 0.1803), (2, 0.3984), (3, 0.6947), (4, 0.7564), (5, 0.7972), (6, 0.812), (7, 0.8184), (8, 0.8173), (9, 0.8116), (10, 0.8223)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8223
wandb:     loss 1.63876
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_205802-nux9oj2k
wandb: Find logs at: ./wandb/offline-run-20240417_205802-nux9oj2k/logs
INFO flwr 2024-04-17 21:00:24,063 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 21:07:35,559 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=817286)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=817286)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 21:07:40,488	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 21:07:41,471	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 21:07:41,931	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 21:07:42,361	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b22f3cd7d0dddc78.zip' (46.09MiB) to Ray cluster...
2024-04-17 21:07:42,512	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b22f3cd7d0dddc78.zip'.
INFO flwr 2024-04-17 21:07:53,694 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 74201927270.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 163137830298.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 21:07:53,694 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 21:07:53,695 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 21:07:53,714 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 21:07:53,715 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 21:07:53,715 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 21:07:53,716 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 21:07:56,781 | server.py:94 | initial parameters (loss, other metrics): 2.3026514053344727, {'accuracy': 0.1111, 'data_size': 10000}
INFO flwr 2024-04-17 21:07:56,781 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 21:07:56,782 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=827015)[0m 2024-04-17 21:07:59.894574: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=827015)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=827016)[0m 2024-04-17 21:08:02.275188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=827016)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=827016)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=826997)[0m 2024-04-17 21:08:00.209321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=826997)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=826997)[0m 2024-04-17 21:08:02.526046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 21:08:17,404 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 21:08:18,921 | server.py:125 | fit progress: (1, 2.3013834953308105, {'accuracy': 0.2186, 'data_size': 10000}, 22.139431033996516)
INFO flwr 2024-04-17 21:08:18,921 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 21:08:18,922 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:08:28,316 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 21:08:29,609 | server.py:125 | fit progress: (2, 2.2980995178222656, {'accuracy': 0.4137, 'data_size': 10000}, 32.82765450699662)
INFO flwr 2024-04-17 21:08:29,610 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 21:08:29,610 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:08:38,777 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 21:08:40,293 | server.py:125 | fit progress: (3, 2.290602445602417, {'accuracy': 0.4288, 'data_size': 10000}, 43.51180131899309)
INFO flwr 2024-04-17 21:08:40,294 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 21:08:40,294 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:08:48,431 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 21:08:49,988 | server.py:125 | fit progress: (4, 2.274714231491089, {'accuracy': 0.4241, 'data_size': 10000}, 53.20651940199605)
INFO flwr 2024-04-17 21:08:49,988 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 21:08:49,989 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:08:58,516 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 21:09:00,047 | server.py:125 | fit progress: (5, 2.2428641319274902, {'accuracy': 0.424, 'data_size': 10000}, 63.264908594996086)
INFO flwr 2024-04-17 21:09:00,047 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 21:09:00,047 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:09:08,582 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 21:09:09,858 | server.py:125 | fit progress: (6, 2.191998243331909, {'accuracy': 0.4258, 'data_size': 10000}, 73.07669372500095)
INFO flwr 2024-04-17 21:09:09,859 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 21:09:09,859 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:09:18,713 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 21:09:20,031 | server.py:125 | fit progress: (7, 2.1316699981689453, {'accuracy': 0.4363, 'data_size': 10000}, 83.2498761099996)
INFO flwr 2024-04-17 21:09:20,032 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 21:09:20,032 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:09:28,176 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 21:09:29,696 | server.py:125 | fit progress: (8, 2.073096752166748, {'accuracy': 0.4565, 'data_size': 10000}, 92.91443579600309)
INFO flwr 2024-04-17 21:09:29,696 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 21:09:29,697 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:09:38,342 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 21:09:39,662 | server.py:125 | fit progress: (9, 2.0168581008911133, {'accuracy': 0.4942, 'data_size': 10000}, 102.88015387300402)
INFO flwr 2024-04-17 21:09:39,662 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 21:09:39,662 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:09:48,614 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 21:09:50,123 | server.py:125 | fit progress: (10, 1.9604889154434204, {'accuracy': 0.5282, 'data_size': 10000}, 113.34124059000169)
INFO flwr 2024-04-17 21:09:50,123 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 21:09:50,123 | server.py:153 | FL finished in 113.341711467001
INFO flwr 2024-04-17 21:09:50,124 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 21:09:50,124 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 21:09:50,124 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 21:09:50,124 | app.py:229 | app_fit: losses_centralized [(0, 2.3026514053344727), (1, 2.3013834953308105), (2, 2.2980995178222656), (3, 2.290602445602417), (4, 2.274714231491089), (5, 2.2428641319274902), (6, 2.191998243331909), (7, 2.1316699981689453), (8, 2.073096752166748), (9, 2.0168581008911133), (10, 1.9604889154434204)]
INFO flwr 2024-04-17 21:09:50,124 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1111), (1, 0.2186), (2, 0.4137), (3, 0.4288), (4, 0.4241), (5, 0.424), (6, 0.4258), (7, 0.4363), (8, 0.4565), (9, 0.4942), (10, 0.5282)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5282
wandb:     loss 1.96049
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_210735-9d4k2l7p
wandb: Find logs at: ./wandb/offline-run-20240417_210735-9d4k2l7p/logs
INFO flwr 2024-04-17 21:09:53,733 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 21:17:06,216 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=826997)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=826997)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 21:17:15,070	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 21:17:16,560	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 21:17:16,991	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 21:17:17,400	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a6516c57cbf49a3c.zip' (46.24MiB) to Ray cluster...
2024-04-17 21:17:17,553	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a6516c57cbf49a3c.zip'.
INFO flwr 2024-04-17 21:17:29,975 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 163831873332.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 74499374284.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 21:17:29,975 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 21:17:29,995 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 21:17:30,012 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 21:17:30,013 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 21:17:30,013 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 21:17:30,013 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 21:17:33,126 | server.py:94 | initial parameters (loss, other metrics): 2.302612543106079, {'accuracy': 0.0868, 'data_size': 10000}
INFO flwr 2024-04-17 21:17:33,127 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 21:17:33,127 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=832079)[0m 2024-04-17 21:17:36.638903: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=832079)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=832088)[0m 2024-04-17 21:17:39.737984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=832088)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=832088)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=832078)[0m 2024-04-17 21:17:36.809060: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=832078)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=832078)[0m 2024-04-17 21:17:39.737984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 21:17:58,501 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 21:17:59,846 | server.py:125 | fit progress: (1, 2.3025383949279785, {'accuracy': 0.1067, 'data_size': 10000}, 26.718948570996872)
INFO flwr 2024-04-17 21:17:59,846 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 21:17:59,846 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:18:09,759 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 21:18:11,104 | server.py:125 | fit progress: (2, 2.3024344444274902, {'accuracy': 0.1346, 'data_size': 10000}, 37.97694211899943)
INFO flwr 2024-04-17 21:18:11,104 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 21:18:11,104 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:18:20,013 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 21:18:21,563 | server.py:125 | fit progress: (3, 2.3022985458374023, {'accuracy': 0.1667, 'data_size': 10000}, 48.435793021999416)
INFO flwr 2024-04-17 21:18:21,563 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 21:18:21,563 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:18:30,453 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 21:18:31,776 | server.py:125 | fit progress: (4, 2.3021247386932373, {'accuracy': 0.1979, 'data_size': 10000}, 58.64967800900922)
INFO flwr 2024-04-17 21:18:31,777 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 21:18:31,777 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:18:40,579 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 21:18:42,151 | server.py:125 | fit progress: (5, 2.301915407180786, {'accuracy': 0.232, 'data_size': 10000}, 69.02380060100404)
INFO flwr 2024-04-17 21:18:42,151 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 21:18:42,151 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:18:51,184 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 21:18:52,487 | server.py:125 | fit progress: (6, 2.30165958404541, {'accuracy': 0.2587, 'data_size': 10000}, 79.35978754599637)
INFO flwr 2024-04-17 21:18:52,487 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 21:18:52,487 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:19:01,341 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 21:19:02,646 | server.py:125 | fit progress: (7, 2.3013558387756348, {'accuracy': 0.2821, 'data_size': 10000}, 89.51944685800117)
INFO flwr 2024-04-17 21:19:02,647 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 21:19:02,647 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:19:11,592 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 21:19:13,135 | server.py:125 | fit progress: (8, 2.3010058403015137, {'accuracy': 0.303, 'data_size': 10000}, 100.00811966700712)
INFO flwr 2024-04-17 21:19:13,135 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 21:19:13,136 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:19:21,803 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 21:19:23,087 | server.py:125 | fit progress: (9, 2.3005993366241455, {'accuracy': 0.3176, 'data_size': 10000}, 109.96020855000825)
INFO flwr 2024-04-17 21:19:23,087 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 21:19:23,088 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:19:31,974 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 21:19:33,540 | server.py:125 | fit progress: (10, 2.300140142440796, {'accuracy': 0.3281, 'data_size': 10000}, 120.41340754200064)
INFO flwr 2024-04-17 21:19:33,540 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 21:19:33,541 | server.py:153 | FL finished in 120.41383986400615
INFO flwr 2024-04-17 21:19:33,541 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 21:19:33,541 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 21:19:33,541 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 21:19:33,541 | app.py:229 | app_fit: losses_centralized [(0, 2.302612543106079), (1, 2.3025383949279785), (2, 2.3024344444274902), (3, 2.3022985458374023), (4, 2.3021247386932373), (5, 2.301915407180786), (6, 2.30165958404541), (7, 2.3013558387756348), (8, 2.3010058403015137), (9, 2.3005993366241455), (10, 2.300140142440796)]
INFO flwr 2024-04-17 21:19:33,541 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0868), (1, 0.1067), (2, 0.1346), (3, 0.1667), (4, 0.1979), (5, 0.232), (6, 0.2587), (7, 0.2821), (8, 0.303), (9, 0.3176), (10, 0.3281)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3281
wandb:     loss 2.30014
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_211705-jmf9ptnf
wandb: Find logs at: ./wandb/offline-run-20240417_211705-jmf9ptnf/logs
INFO flwr 2024-04-17 21:19:37,177 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 21:26:49,058 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=832078)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=832078)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 21:26:57,989	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 21:26:59,399	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 21:26:59,880	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 21:27:00,316	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_89bd2faafe8c9c53.zip' (46.40MiB) to Ray cluster...
2024-04-17 21:27:00,465	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_89bd2faafe8c9c53.zip'.
INFO flwr 2024-04-17 21:27:12,482 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 160274807808.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'object_store_memory': 72974917632.0}
INFO flwr 2024-04-17 21:27:12,482 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 21:27:12,482 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 21:27:12,503 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 21:27:12,504 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 21:27:12,504 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 21:27:12,504 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 21:27:15,414 | server.py:94 | initial parameters (loss, other metrics): 2.302556276321411, {'accuracy': 0.1008, 'data_size': 10000}
INFO flwr 2024-04-17 21:27:15,415 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 21:27:15,416 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=842892)[0m 2024-04-17 21:27:19.237422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=842892)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=842892)[0m 2024-04-17 21:27:22.246650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=842892)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=842892)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=842898)[0m 2024-04-17 21:27:19.296408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=842898)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=842902)[0m 2024-04-17 21:27:22.242750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 21:27:40,768 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 21:27:42,293 | server.py:125 | fit progress: (1, 2.3025476932525635, {'accuracy': 0.102, 'data_size': 10000}, 26.87741626999923)
INFO flwr 2024-04-17 21:27:42,293 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 21:27:42,293 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:27:51,573 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 21:27:52,911 | server.py:125 | fit progress: (2, 2.302535057067871, {'accuracy': 0.1036, 'data_size': 10000}, 37.49569316700217)
INFO flwr 2024-04-17 21:27:52,911 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 21:27:52,911 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:28:01,891 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 21:28:03,444 | server.py:125 | fit progress: (3, 2.3025214672088623, {'accuracy': 0.1056, 'data_size': 10000}, 48.02859441800683)
INFO flwr 2024-04-17 21:28:03,444 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 21:28:03,444 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:28:11,812 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 21:28:13,115 | server.py:125 | fit progress: (4, 2.3025059700012207, {'accuracy': 0.1075, 'data_size': 10000}, 57.699835564999375)
INFO flwr 2024-04-17 21:28:13,115 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 21:28:13,116 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:28:21,986 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 21:28:23,306 | server.py:125 | fit progress: (5, 2.3024892807006836, {'accuracy': 0.1105, 'data_size': 10000}, 67.89126496200333)
INFO flwr 2024-04-17 21:28:23,307 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 21:28:23,307 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:28:32,002 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 21:28:33,606 | server.py:125 | fit progress: (6, 2.302471399307251, {'accuracy': 0.1124, 'data_size': 10000}, 78.19079694400716)
INFO flwr 2024-04-17 21:28:33,606 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 21:28:33,607 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:28:42,405 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 21:28:43,714 | server.py:125 | fit progress: (7, 2.302452802658081, {'accuracy': 0.115, 'data_size': 10000}, 88.29855000200041)
INFO flwr 2024-04-17 21:28:43,714 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 21:28:43,714 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:28:52,623 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 21:28:54,142 | server.py:125 | fit progress: (8, 2.3024325370788574, {'accuracy': 0.1178, 'data_size': 10000}, 98.72694073800812)
INFO flwr 2024-04-17 21:28:54,142 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 21:28:54,143 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:29:02,943 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 21:29:04,230 | server.py:125 | fit progress: (9, 2.302412748336792, {'accuracy': 0.12, 'data_size': 10000}, 108.81515869800933)
INFO flwr 2024-04-17 21:29:04,231 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 21:29:04,231 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:29:13,425 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 21:29:14,974 | server.py:125 | fit progress: (10, 2.302391767501831, {'accuracy': 0.1218, 'data_size': 10000}, 119.55832484000712)
INFO flwr 2024-04-17 21:29:14,974 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 21:29:14,974 | server.py:153 | FL finished in 119.558823364001
INFO flwr 2024-04-17 21:29:14,974 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 21:29:14,974 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 21:29:14,975 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 21:29:14,975 | app.py:229 | app_fit: losses_centralized [(0, 2.302556276321411), (1, 2.3025476932525635), (2, 2.302535057067871), (3, 2.3025214672088623), (4, 2.3025059700012207), (5, 2.3024892807006836), (6, 2.302471399307251), (7, 2.302452802658081), (8, 2.3024325370788574), (9, 2.302412748336792), (10, 2.302391767501831)]
INFO flwr 2024-04-17 21:29:14,975 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1008), (1, 0.102), (2, 0.1036), (3, 0.1056), (4, 0.1075), (5, 0.1105), (6, 0.1124), (7, 0.115), (8, 0.1178), (9, 0.12), (10, 0.1218)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1218
wandb:     loss 2.30239
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_212648-0jtz4qnd
wandb: Find logs at: ./wandb/offline-run-20240417_212648-0jtz4qnd/logs
INFO flwr 2024-04-17 21:29:18,622 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 21:36:29,910 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=842902)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=842902)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 21:36:34,580	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 21:36:35,426	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 21:36:35,874	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 21:36:36,303	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ca4342e4f6345b30.zip' (46.55MiB) to Ray cluster...
2024-04-17 21:36:36,460	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ca4342e4f6345b30.zip'.
INFO flwr 2024-04-17 21:36:47,713 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 72874442342.0, 'GPU': 1.0, 'memory': 160040365466.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 21:36:47,714 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 21:36:47,714 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 21:36:47,731 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 21:36:47,732 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 21:36:47,733 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 21:36:47,733 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 21:36:51,514 | server.py:94 | initial parameters (loss, other metrics): 2.3026905059814453, {'accuracy': 0.0965, 'data_size': 10000}
INFO flwr 2024-04-17 21:36:51,515 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 21:36:51,515 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=847319)[0m 2024-04-17 21:36:53.885861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=847319)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=847319)[0m 2024-04-17 21:36:56.214136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=847302)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=847302)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=847265)[0m 2024-04-17 21:36:54.158289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=847265)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=847263)[0m 2024-04-17 21:36:56.706829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 21:37:11,176 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 21:37:12,716 | server.py:125 | fit progress: (1, 2.2827165126800537, {'accuracy': 0.1784, 'data_size': 10000}, 21.20123543200316)
INFO flwr 2024-04-17 21:37:12,717 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 21:37:12,717 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:37:22,509 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 21:37:23,804 | server.py:125 | fit progress: (2, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 32.28933114500251)
INFO flwr 2024-04-17 21:37:23,805 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 21:37:23,805 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:37:32,475 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 21:37:34,026 | server.py:125 | fit progress: (3, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 42.510756906005554)
INFO flwr 2024-04-17 21:37:34,026 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 21:37:34,026 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:37:42,560 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 21:37:43,837 | server.py:125 | fit progress: (4, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 52.32224051600497)
INFO flwr 2024-04-17 21:37:43,838 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 21:37:43,838 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:37:52,586 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 21:37:54,081 | server.py:125 | fit progress: (5, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 62.56550227200205)
INFO flwr 2024-04-17 21:37:54,081 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 21:37:54,081 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:38:02,648 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 21:38:04,162 | server.py:125 | fit progress: (6, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 72.64726967000752)
INFO flwr 2024-04-17 21:38:04,163 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 21:38:04,163 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:38:13,093 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 21:38:14,711 | server.py:125 | fit progress: (7, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 83.19565343600698)
INFO flwr 2024-04-17 21:38:14,711 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 21:38:14,711 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:38:22,789 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 21:38:24,332 | server.py:125 | fit progress: (8, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 92.81651042000158)
INFO flwr 2024-04-17 21:38:24,332 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 21:38:24,332 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:38:32,815 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 21:38:34,320 | server.py:125 | fit progress: (9, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 102.8046295660024)
INFO flwr 2024-04-17 21:38:34,320 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 21:38:34,320 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:38:43,036 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 21:38:44,314 | server.py:125 | fit progress: (10, 2.3579421043395996, {'accuracy': 0.1032, 'data_size': 10000}, 112.79860676900716)
INFO flwr 2024-04-17 21:38:44,314 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 21:38:44,314 | server.py:153 | FL finished in 112.79902426300396
INFO flwr 2024-04-17 21:38:44,314 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 21:38:44,314 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 21:38:44,315 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 21:38:44,315 | app.py:229 | app_fit: losses_centralized [(0, 2.3026905059814453), (1, 2.2827165126800537), (2, 2.3579421043395996), (3, 2.3579421043395996), (4, 2.3579421043395996), (5, 2.3579421043395996), (6, 2.3579421043395996), (7, 2.3579421043395996), (8, 2.3579421043395996), (9, 2.3579421043395996), (10, 2.3579421043395996)]
INFO flwr 2024-04-17 21:38:44,315 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0965), (1, 0.1784), (2, 0.1032), (3, 0.1032), (4, 0.1032), (5, 0.1032), (6, 0.1032), (7, 0.1032), (8, 0.1032), (9, 0.1032), (10, 0.1032)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1032
wandb:     loss 2.35794
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_213629-z3zsj5kf
wandb: Find logs at: ./wandb/offline-run-20240417_213629-z3zsj5kf/logs
INFO flwr 2024-04-17 21:38:47,946 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 21:45:59,350 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=847262)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=847262)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 21:46:05,278	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 21:46:06,188	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 21:46:06,634	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 21:46:07,051	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9ef9858b78a46361.zip' (46.71MiB) to Ray cluster...
2024-04-17 21:46:07,193	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9ef9858b78a46361.zip'.
INFO flwr 2024-04-17 21:46:18,805 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 72902036275.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 160104751309.0}
INFO flwr 2024-04-17 21:46:18,805 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 21:46:18,805 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 21:46:18,823 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 21:46:18,824 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 21:46:18,824 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 21:46:18,825 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 21:46:22,066 | server.py:94 | initial parameters (loss, other metrics): 2.3027865886688232, {'accuracy': 0.1137, 'data_size': 10000}
INFO flwr 2024-04-17 21:46:22,066 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 21:46:22,067 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=851400)[0m 2024-04-17 21:46:25.384937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=851400)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=851400)[0m 2024-04-17 21:46:27.740346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=851408)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=851408)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=851406)[0m 2024-04-17 21:46:25.497372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=851406)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=851406)[0m 2024-04-17 21:46:27.864100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 21:46:43,241 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 21:46:44,546 | server.py:125 | fit progress: (1, 2.218975305557251, {'accuracy': 0.2001, 'data_size': 10000}, 22.47958969400497)
INFO flwr 2024-04-17 21:46:44,547 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 21:46:44,547 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:46:53,963 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 21:46:55,518 | server.py:125 | fit progress: (2, 2.1079020500183105, {'accuracy': 0.3431, 'data_size': 10000}, 33.451361472005374)
INFO flwr 2024-04-17 21:46:55,518 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 21:46:55,519 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:47:03,944 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 21:47:05,525 | server.py:125 | fit progress: (3, 1.8118559122085571, {'accuracy': 0.6428, 'data_size': 10000}, 43.457984565000515)
INFO flwr 2024-04-17 21:47:05,525 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 21:47:05,525 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:47:14,576 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 21:47:16,084 | server.py:125 | fit progress: (4, 1.6245427131652832, {'accuracy': 0.8371, 'data_size': 10000}, 54.017020746003254)
INFO flwr 2024-04-17 21:47:16,084 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 21:47:16,084 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:47:24,553 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 21:47:26,060 | server.py:125 | fit progress: (5, 1.6033252477645874, {'accuracy': 0.858, 'data_size': 10000}, 63.993434358009836)
INFO flwr 2024-04-17 21:47:26,060 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 21:47:26,061 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:47:35,009 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 21:47:36,533 | server.py:125 | fit progress: (6, 1.5856448411941528, {'accuracy': 0.8754, 'data_size': 10000}, 74.46658073501021)
INFO flwr 2024-04-17 21:47:36,534 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 21:47:36,534 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:47:45,064 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 21:47:46,583 | server.py:125 | fit progress: (7, 1.5643194913864136, {'accuracy': 0.8956, 'data_size': 10000}, 84.5163788140053)
INFO flwr 2024-04-17 21:47:46,583 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 21:47:46,584 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:47:55,937 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 21:47:57,448 | server.py:125 | fit progress: (8, 1.5588966608047485, {'accuracy': 0.9019, 'data_size': 10000}, 95.38131098799931)
INFO flwr 2024-04-17 21:47:57,448 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 21:47:57,448 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:48:06,018 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 21:48:07,322 | server.py:125 | fit progress: (9, 1.5560907125473022, {'accuracy': 0.9044, 'data_size': 10000}, 105.25483874900965)
INFO flwr 2024-04-17 21:48:07,322 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 21:48:07,322 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:48:15,691 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 21:48:17,283 | server.py:125 | fit progress: (10, 1.5529004335403442, {'accuracy': 0.9082, 'data_size': 10000}, 115.21620395799982)
INFO flwr 2024-04-17 21:48:17,283 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 21:48:17,283 | server.py:153 | FL finished in 115.21665096200013
INFO flwr 2024-04-17 21:48:17,284 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 21:48:17,284 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 21:48:17,284 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 21:48:17,284 | app.py:229 | app_fit: losses_centralized [(0, 2.3027865886688232), (1, 2.218975305557251), (2, 2.1079020500183105), (3, 1.8118559122085571), (4, 1.6245427131652832), (5, 1.6033252477645874), (6, 1.5856448411941528), (7, 1.5643194913864136), (8, 1.5588966608047485), (9, 1.5560907125473022), (10, 1.5529004335403442)]
INFO flwr 2024-04-17 21:48:17,284 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1137), (1, 0.2001), (2, 0.3431), (3, 0.6428), (4, 0.8371), (5, 0.858), (6, 0.8754), (7, 0.8956), (8, 0.9019), (9, 0.9044), (10, 0.9082)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9082
wandb:     loss 1.5529
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_214558-ewntjvf9
wandb: Find logs at: ./wandb/offline-run-20240417_214558-ewntjvf9/logs
INFO flwr 2024-04-17 21:48:20,875 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 21:55:32,084 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=851400)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=851400)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 21:55:37,325	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 21:55:38,188	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 21:55:38,624	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 21:55:39,050	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f7134637b466b585.zip' (46.86MiB) to Ray cluster...
2024-04-17 21:55:39,206	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f7134637b466b585.zip'.
INFO flwr 2024-04-17 21:55:51,499 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 72896171212.0, 'accelerator_type:TITAN': 1.0, 'memory': 160091066164.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 21:55:51,500 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 21:55:51,500 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 21:55:51,524 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 21:55:51,526 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 21:55:51,526 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 21:55:51,527 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 21:55:53,846 | server.py:94 | initial parameters (loss, other metrics): 2.30256724357605, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-17 21:55:53,847 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 21:55:53,847 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=855768)[0m 2024-04-17 21:55:58.345495: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=855768)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=855768)[0m 2024-04-17 21:56:00.840821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=855768)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=855768)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=855762)[0m 2024-04-17 21:55:58.653283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=855762)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=855769)[0m 2024-04-17 21:56:01.091652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 21:56:16,481 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 21:56:18,033 | server.py:125 | fit progress: (1, 2.3014116287231445, {'accuracy': 0.1249, 'data_size': 10000}, 24.186299958993914)
INFO flwr 2024-04-17 21:56:18,034 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 21:56:18,034 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:56:27,315 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 21:56:28,846 | server.py:125 | fit progress: (2, 2.2982137203216553, {'accuracy': 0.1651, 'data_size': 10000}, 34.99940689599316)
INFO flwr 2024-04-17 21:56:28,847 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 21:56:28,847 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:56:37,555 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 21:56:39,048 | server.py:125 | fit progress: (3, 2.2911839485168457, {'accuracy': 0.2466, 'data_size': 10000}, 45.20129348199407)
INFO flwr 2024-04-17 21:56:39,048 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 21:56:39,049 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:56:47,478 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 21:56:49,044 | server.py:125 | fit progress: (4, 2.2770042419433594, {'accuracy': 0.4992, 'data_size': 10000}, 55.19730116199935)
INFO flwr 2024-04-17 21:56:49,044 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 21:56:49,045 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:56:57,457 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 21:56:58,978 | server.py:125 | fit progress: (5, 2.2497503757476807, {'accuracy': 0.6329, 'data_size': 10000}, 65.13145054099732)
INFO flwr 2024-04-17 21:56:58,979 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 21:56:58,979 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:57:07,708 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 21:57:09,013 | server.py:125 | fit progress: (6, 2.2005488872528076, {'accuracy': 0.6662, 'data_size': 10000}, 75.16604228499637)
INFO flwr 2024-04-17 21:57:09,013 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 21:57:09,013 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:57:17,682 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 21:57:19,227 | server.py:125 | fit progress: (7, 2.1245672702789307, {'accuracy': 0.677, 'data_size': 10000}, 85.37969793200318)
INFO flwr 2024-04-17 21:57:19,227 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 21:57:19,227 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:57:28,209 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 21:57:29,476 | server.py:125 | fit progress: (8, 2.027965545654297, {'accuracy': 0.6796, 'data_size': 10000}, 95.62905429500097)
INFO flwr 2024-04-17 21:57:29,476 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 21:57:29,476 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:57:37,919 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 21:57:39,424 | server.py:125 | fit progress: (9, 1.9344207048416138, {'accuracy': 0.6757, 'data_size': 10000}, 105.57719196699327)
INFO flwr 2024-04-17 21:57:39,424 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 21:57:39,425 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 21:57:48,070 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 21:57:49,337 | server.py:125 | fit progress: (10, 1.861810326576233, {'accuracy': 0.6773, 'data_size': 10000}, 115.4898792640015)
INFO flwr 2024-04-17 21:57:49,337 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 21:57:49,337 | server.py:153 | FL finished in 115.49033162300475
INFO flwr 2024-04-17 21:57:49,337 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 21:57:49,338 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 21:57:49,338 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 21:57:49,338 | app.py:229 | app_fit: losses_centralized [(0, 2.30256724357605), (1, 2.3014116287231445), (2, 2.2982137203216553), (3, 2.2911839485168457), (4, 2.2770042419433594), (5, 2.2497503757476807), (6, 2.2005488872528076), (7, 2.1245672702789307), (8, 2.027965545654297), (9, 1.9344207048416138), (10, 1.861810326576233)]
INFO flwr 2024-04-17 21:57:49,338 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.1249), (2, 0.1651), (3, 0.2466), (4, 0.4992), (5, 0.6329), (6, 0.6662), (7, 0.677), (8, 0.6796), (9, 0.6757), (10, 0.6773)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6773
wandb:     loss 1.86181
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_215531-khr31ad4
wandb: Find logs at: ./wandb/offline-run-20240417_215531-khr31ad4/logs
INFO flwr 2024-04-17 21:57:52,998 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 22:05:04,364 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=855762)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=855762)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 22:05:09,392	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 22:05:10,403	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 22:05:10,874	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 22:05:11,303	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_936518445bb5f744.zip' (46.88MiB) to Ray cluster...
2024-04-17 22:05:11,452	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_936518445bb5f744.zip'.
INFO flwr 2024-04-17 22:05:22,672 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'object_store_memory': 72895760793.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 160090108519.0}
INFO flwr 2024-04-17 22:05:22,673 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 22:05:22,673 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 22:05:22,694 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 22:05:22,696 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 22:05:22,696 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 22:05:22,697 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 22:05:25,986 | server.py:94 | initial parameters (loss, other metrics): 2.302631139755249, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-17 22:05:25,987 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 22:05:25,987 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=860153)[0m 2024-04-17 22:05:29.097078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=860153)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=860156)[0m 2024-04-17 22:05:31.475382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=860156)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=860156)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=860155)[0m 2024-04-17 22:05:29.378402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=860155)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=860155)[0m 2024-04-17 22:05:31.703480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 22:05:46,918 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 22:05:48,250 | server.py:125 | fit progress: (1, 2.3025479316711426, {'accuracy': 0.098, 'data_size': 10000}, 22.263332804999663)
INFO flwr 2024-04-17 22:05:48,250 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 22:05:48,251 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:05:57,636 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 22:05:59,167 | server.py:125 | fit progress: (2, 2.302434206008911, {'accuracy': 0.0983, 'data_size': 10000}, 33.17982713400852)
INFO flwr 2024-04-17 22:05:59,167 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 22:05:59,167 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:06:08,293 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 22:06:09,588 | server.py:125 | fit progress: (3, 2.3022847175598145, {'accuracy': 0.0991, 'data_size': 10000}, 43.601012912011356)
INFO flwr 2024-04-17 22:06:09,588 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 22:06:09,588 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:06:18,618 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 22:06:20,131 | server.py:125 | fit progress: (4, 2.302103042602539, {'accuracy': 0.1, 'data_size': 10000}, 54.14428782300092)
INFO flwr 2024-04-17 22:06:20,131 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 22:06:20,132 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:06:28,149 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 22:06:29,694 | server.py:125 | fit progress: (5, 2.3018765449523926, {'accuracy': 0.1036, 'data_size': 10000}, 63.707265974007896)
INFO flwr 2024-04-17 22:06:29,694 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 22:06:29,695 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:06:38,511 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 22:06:40,029 | server.py:125 | fit progress: (6, 2.301605224609375, {'accuracy': 0.1167, 'data_size': 10000}, 74.04189702800068)
INFO flwr 2024-04-17 22:06:40,029 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 22:06:40,029 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:06:48,703 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 22:06:50,006 | server.py:125 | fit progress: (7, 2.301284074783325, {'accuracy': 0.1357, 'data_size': 10000}, 84.01927711399912)
INFO flwr 2024-04-17 22:06:50,007 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 22:06:50,007 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:06:59,114 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 22:07:00,440 | server.py:125 | fit progress: (8, 2.3009033203125, {'accuracy': 0.1626, 'data_size': 10000}, 94.45343470200896)
INFO flwr 2024-04-17 22:07:00,441 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 22:07:00,441 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:07:08,860 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 22:07:10,438 | server.py:125 | fit progress: (9, 2.3004543781280518, {'accuracy': 0.1987, 'data_size': 10000}, 104.4514085000119)
INFO flwr 2024-04-17 22:07:10,438 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 22:07:10,439 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:07:19,167 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 22:07:20,445 | server.py:125 | fit progress: (10, 2.2999367713928223, {'accuracy': 0.2517, 'data_size': 10000}, 114.45833663801022)
INFO flwr 2024-04-17 22:07:20,445 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 22:07:20,446 | server.py:153 | FL finished in 114.45882284200343
INFO flwr 2024-04-17 22:07:20,446 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 22:07:20,446 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 22:07:20,446 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 22:07:20,446 | app.py:229 | app_fit: losses_centralized [(0, 2.302631139755249), (1, 2.3025479316711426), (2, 2.302434206008911), (3, 2.3022847175598145), (4, 2.302103042602539), (5, 2.3018765449523926), (6, 2.301605224609375), (7, 2.301284074783325), (8, 2.3009033203125), (9, 2.3004543781280518), (10, 2.2999367713928223)]
INFO flwr 2024-04-17 22:07:20,446 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.098), (2, 0.0983), (3, 0.0991), (4, 0.1), (5, 0.1036), (6, 0.1167), (7, 0.1357), (8, 0.1626), (9, 0.1987), (10, 0.2517)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2517
wandb:     loss 2.29994
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_220503-9so3rp6b
wandb: Find logs at: ./wandb/offline-run-20240417_220503-9so3rp6b/logs
INFO flwr 2024-04-17 22:07:24,059 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 22:14:35,489 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=860149)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=860149)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 22:14:40,519	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 22:14:41,443	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 22:14:41,911	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 22:14:42,345	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9bf03099bbb4d1ea.zip' (47.04MiB) to Ray cluster...
2024-04-17 22:14:42,496	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9bf03099bbb4d1ea.zip'.
INFO flwr 2024-04-17 22:14:53,803 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 72858549043.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 160003281101.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-17 22:14:53,803 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 22:14:53,803 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 22:14:53,827 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 22:14:53,828 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 22:14:53,828 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 22:14:53,828 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 22:14:57,941 | server.py:94 | initial parameters (loss, other metrics): 2.302825689315796, {'accuracy': 0.1007, 'data_size': 10000}
INFO flwr 2024-04-17 22:14:57,953 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 22:14:57,953 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=864845)[0m 2024-04-17 22:14:59.852684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=864845)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=864845)[0m 2024-04-17 22:15:02.214439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=864850)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=864850)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=864853)[0m 2024-04-17 22:15:00.296649: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=864853)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=864853)[0m 2024-04-17 22:15:02.729679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 22:15:17,014 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 22:15:18,575 | server.py:125 | fit progress: (1, 2.302818775177002, {'accuracy': 0.1013, 'data_size': 10000}, 20.62159709399566)
INFO flwr 2024-04-17 22:15:18,575 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 22:15:18,575 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:15:27,981 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 22:15:29,303 | server.py:125 | fit progress: (2, 2.3028082847595215, {'accuracy': 0.1016, 'data_size': 10000}, 31.349355464000837)
INFO flwr 2024-04-17 22:15:29,303 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 22:15:29,303 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:15:38,096 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 22:15:39,372 | server.py:125 | fit progress: (3, 2.3027970790863037, {'accuracy': 0.102, 'data_size': 10000}, 41.418450497003505)
INFO flwr 2024-04-17 22:15:39,372 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 22:15:39,372 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:15:48,264 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 22:15:49,765 | server.py:125 | fit progress: (4, 2.3027849197387695, {'accuracy': 0.1027, 'data_size': 10000}, 51.81176568599767)
INFO flwr 2024-04-17 22:15:49,765 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 22:15:49,765 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:15:58,656 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 22:15:59,962 | server.py:125 | fit progress: (5, 2.3027727603912354, {'accuracy': 0.1034, 'data_size': 10000}, 62.00879658400663)
INFO flwr 2024-04-17 22:15:59,962 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 22:15:59,963 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:16:08,652 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 22:16:10,228 | server.py:125 | fit progress: (6, 2.3027591705322266, {'accuracy': 0.1041, 'data_size': 10000}, 72.27484603099583)
INFO flwr 2024-04-17 22:16:10,228 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 22:16:10,229 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:16:18,979 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 22:16:20,328 | server.py:125 | fit progress: (7, 2.302743911743164, {'accuracy': 0.1048, 'data_size': 10000}, 82.37520675000269)
INFO flwr 2024-04-17 22:16:20,329 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 22:16:20,329 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:16:28,895 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 22:16:30,447 | server.py:125 | fit progress: (8, 2.3027284145355225, {'accuracy': 0.1056, 'data_size': 10000}, 92.49368585699995)
INFO flwr 2024-04-17 22:16:30,447 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 22:16:30,447 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:16:39,268 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 22:16:40,784 | server.py:125 | fit progress: (9, 2.3027122020721436, {'accuracy': 0.1068, 'data_size': 10000}, 102.83045129200036)
INFO flwr 2024-04-17 22:16:40,784 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 22:16:40,784 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:16:49,314 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 22:16:50,890 | server.py:125 | fit progress: (10, 2.3026955127716064, {'accuracy': 0.108, 'data_size': 10000}, 112.93689105300291)
INFO flwr 2024-04-17 22:16:50,890 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 22:16:50,890 | server.py:153 | FL finished in 112.93732585800171
INFO flwr 2024-04-17 22:16:50,891 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 22:16:50,891 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 22:16:50,891 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 22:16:50,891 | app.py:229 | app_fit: losses_centralized [(0, 2.302825689315796), (1, 2.302818775177002), (2, 2.3028082847595215), (3, 2.3027970790863037), (4, 2.3027849197387695), (5, 2.3027727603912354), (6, 2.3027591705322266), (7, 2.302743911743164), (8, 2.3027284145355225), (9, 2.3027122020721436), (10, 2.3026955127716064)]
INFO flwr 2024-04-17 22:16:50,891 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1007), (1, 0.1013), (2, 0.1016), (3, 0.102), (4, 0.1027), (5, 0.1034), (6, 0.1041), (7, 0.1048), (8, 0.1056), (9, 0.1068), (10, 0.108)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.108
wandb:     loss 2.3027
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_221435-jrrraoct
wandb: Find logs at: ./wandb/offline-run-20240417_221435-jrrraoct/logs
INFO flwr 2024-04-17 22:16:54,582 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 22:24:05,640 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=864839)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=864839)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 22:24:10,414	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 22:24:11,329	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 22:24:11,784	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 22:24:12,214	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a53a511b8f4fe412.zip' (47.19MiB) to Ray cluster...
2024-04-17 22:24:12,369	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a53a511b8f4fe412.zip'.
INFO flwr 2024-04-17 22:24:23,697 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 72879171993.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 160051401319.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-17 22:24:23,697 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 22:24:23,698 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 22:24:23,715 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 22:24:23,716 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 22:24:23,716 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 22:24:23,716 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 22:24:26,512 | server.py:94 | initial parameters (loss, other metrics): 2.3027002811431885, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-17 22:24:26,512 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 22:24:26,512 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=869223)[0m 2024-04-17 22:24:29.788013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=869223)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=869223)[0m 2024-04-17 22:24:32.246827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=869225)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=869225)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=869218)[0m 2024-04-17 22:24:30.034864: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=869218)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=869218)[0m 2024-04-17 22:24:32.391699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 22:24:47,549 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 22:24:49,085 | server.py:125 | fit progress: (1, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 22.57310911398963)
INFO flwr 2024-04-17 22:24:49,086 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 22:24:49,086 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:24:58,484 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 22:24:59,815 | server.py:125 | fit progress: (2, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 33.30307685099251)
INFO flwr 2024-04-17 22:24:59,816 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 22:24:59,816 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:25:08,626 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 22:25:10,127 | server.py:125 | fit progress: (3, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 43.614539084999706)
INFO flwr 2024-04-17 22:25:10,127 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 22:25:10,127 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:25:18,837 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 22:25:20,142 | server.py:125 | fit progress: (4, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 53.629422109996085)
INFO flwr 2024-04-17 22:25:20,142 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 22:25:20,142 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:25:29,011 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 22:25:30,565 | server.py:125 | fit progress: (5, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 64.05240815599973)
INFO flwr 2024-04-17 22:25:30,565 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 22:25:30,565 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:25:39,491 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 22:25:40,814 | server.py:125 | fit progress: (6, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 74.3019118529919)
INFO flwr 2024-04-17 22:25:40,815 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 22:25:40,815 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:25:49,475 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 22:25:51,005 | server.py:125 | fit progress: (7, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 84.49276810699666)
INFO flwr 2024-04-17 22:25:51,005 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 22:25:51,006 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:25:59,749 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 22:26:01,048 | server.py:125 | fit progress: (8, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 94.53555997199146)
INFO flwr 2024-04-17 22:26:01,048 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 22:26:01,048 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:26:09,890 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 22:26:11,218 | server.py:125 | fit progress: (9, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 104.70549637899967)
INFO flwr 2024-04-17 22:26:11,218 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 22:26:11,218 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:26:19,634 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 22:26:21,195 | server.py:125 | fit progress: (10, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 114.6826787149912)
INFO flwr 2024-04-17 22:26:21,195 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 22:26:21,195 | server.py:153 | FL finished in 114.68310408698744
INFO flwr 2024-04-17 22:26:21,196 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 22:26:21,196 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 22:26:21,196 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 22:26:21,196 | app.py:229 | app_fit: losses_centralized [(0, 2.3027002811431885), (1, 2.3629422187805176), (2, 2.3629422187805176), (3, 2.3629422187805176), (4, 2.3629422187805176), (5, 2.3629422187805176), (6, 2.3629422187805176), (7, 2.3629422187805176), (8, 2.3629422187805176), (9, 2.3629422187805176), (10, 2.3629422187805176)]
INFO flwr 2024-04-17 22:26:21,196 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.0982), (2, 0.0982), (3, 0.0982), (4, 0.0982), (5, 0.0982), (6, 0.0982), (7, 0.0982), (8, 0.0982), (9, 0.0982), (10, 0.0982)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0982
wandb:     loss 2.36294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_222405-ws0zksgd
wandb: Find logs at: ./wandb/offline-run-20240417_222405-ws0zksgd/logs
INFO flwr 2024-04-17 22:26:24,843 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 22:33:36,210 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=869218)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=869218)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 22:33:41,023	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 22:33:41,938	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 22:33:42,388	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 22:33:42,820	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9b18b1f66a9dda10.zip' (47.35MiB) to Ray cluster...
2024-04-17 22:33:42,971	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9b18b1f66a9dda10.zip'.
INFO flwr 2024-04-17 22:33:55,969 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 72865225113.0, 'accelerator_type:TITAN': 1.0, 'memory': 160018858599.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-17 22:33:55,969 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 22:33:55,970 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 22:33:55,988 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 22:33:55,989 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 22:33:55,990 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 22:33:55,990 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 22:34:00,578 | server.py:94 | initial parameters (loss, other metrics): 2.3026888370513916, {'accuracy': 0.1003, 'data_size': 10000}
INFO flwr 2024-04-17 22:34:00,578 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 22:34:00,579 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=873594)[0m 2024-04-17 22:34:01.993077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=873594)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=873594)[0m 2024-04-17 22:34:04.387889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=873595)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=873595)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=873586)[0m 2024-04-17 22:34:02.299892: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=873586)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=873592)[0m 2024-04-17 22:34:04.537470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 22:34:19,226 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 22:34:20,536 | server.py:125 | fit progress: (1, 2.1881799697875977, {'accuracy': 0.4086, 'data_size': 10000}, 19.95715943700634)
INFO flwr 2024-04-17 22:34:20,536 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 22:34:20,536 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:34:29,907 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 22:34:31,203 | server.py:125 | fit progress: (2, 1.9726662635803223, {'accuracy': 0.5002, 'data_size': 10000}, 30.62477352800488)
INFO flwr 2024-04-17 22:34:31,204 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 22:34:31,204 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:34:40,146 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 22:34:41,653 | server.py:125 | fit progress: (3, 1.8321006298065186, {'accuracy': 0.6267, 'data_size': 10000}, 41.07473158600624)
INFO flwr 2024-04-17 22:34:41,654 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 22:34:41,654 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:34:50,297 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 22:34:51,637 | server.py:125 | fit progress: (4, 1.7487021684646606, {'accuracy': 0.7092, 'data_size': 10000}, 51.05804328700469)
INFO flwr 2024-04-17 22:34:51,637 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 22:34:51,637 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:35:00,337 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 22:35:01,838 | server.py:125 | fit progress: (5, 1.6392691135406494, {'accuracy': 0.8208, 'data_size': 10000}, 61.25967265800864)
INFO flwr 2024-04-17 22:35:01,838 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 22:35:01,839 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:35:10,683 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 22:35:11,964 | server.py:125 | fit progress: (6, 1.6001183986663818, {'accuracy': 0.8601, 'data_size': 10000}, 71.38545716200315)
INFO flwr 2024-04-17 22:35:11,964 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 22:35:11,964 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:35:21,066 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 22:35:22,588 | server.py:125 | fit progress: (7, 1.5767472982406616, {'accuracy': 0.8835, 'data_size': 10000}, 82.00927763000072)
INFO flwr 2024-04-17 22:35:22,588 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 22:35:22,588 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:35:30,830 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 22:35:32,397 | server.py:125 | fit progress: (8, 1.5691230297088623, {'accuracy': 0.8912, 'data_size': 10000}, 91.81857852400572)
INFO flwr 2024-04-17 22:35:32,397 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 22:35:32,398 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:35:41,075 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 22:35:42,598 | server.py:125 | fit progress: (9, 1.5589511394500732, {'accuracy': 0.9017, 'data_size': 10000}, 102.01931473400327)
INFO flwr 2024-04-17 22:35:42,598 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 22:35:42,598 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:35:51,845 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 22:35:53,154 | server.py:125 | fit progress: (10, 1.5572888851165771, {'accuracy': 0.9036, 'data_size': 10000}, 112.57591082100407)
INFO flwr 2024-04-17 22:35:53,155 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 22:35:53,155 | server.py:153 | FL finished in 112.57637585600605
INFO flwr 2024-04-17 22:35:53,155 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 22:35:53,155 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 22:35:53,155 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 22:35:53,155 | app.py:229 | app_fit: losses_centralized [(0, 2.3026888370513916), (1, 2.1881799697875977), (2, 1.9726662635803223), (3, 1.8321006298065186), (4, 1.7487021684646606), (5, 1.6392691135406494), (6, 1.6001183986663818), (7, 1.5767472982406616), (8, 1.5691230297088623), (9, 1.5589511394500732), (10, 1.5572888851165771)]
INFO flwr 2024-04-17 22:35:53,156 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1003), (1, 0.4086), (2, 0.5002), (3, 0.6267), (4, 0.7092), (5, 0.8208), (6, 0.8601), (7, 0.8835), (8, 0.8912), (9, 0.9017), (10, 0.9036)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9036
wandb:     loss 1.55729
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_223335-r03pcih5
wandb: Find logs at: ./wandb/offline-run-20240417_223335-r03pcih5/logs
INFO flwr 2024-04-17 22:35:56,790 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 22:43:08,242 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=873585)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=873585)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 22:43:13,139	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 22:43:14,255	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 22:43:14,710	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 22:43:15,129	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d6dd8525ad0db868.zip' (47.51MiB) to Ray cluster...
2024-04-17 22:43:15,285	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d6dd8525ad0db868.zip'.
INFO flwr 2024-04-17 22:43:26,668 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 72862830182.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 160013270426.0}
INFO flwr 2024-04-17 22:43:26,669 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 22:43:26,669 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 22:43:26,691 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 22:43:26,691 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 22:43:26,692 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 22:43:26,692 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 22:43:30,028 | server.py:94 | initial parameters (loss, other metrics): 2.302476644515991, {'accuracy': 0.1033, 'data_size': 10000}
INFO flwr 2024-04-17 22:43:30,029 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 22:43:30,029 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=878334)[0m 2024-04-17 22:43:32.869336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=878334)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=878334)[0m 2024-04-17 22:43:35.169492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=878336)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=878336)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=878332)[0m 2024-04-17 22:43:33.134735: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=878332)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=878332)[0m 2024-04-17 22:43:35.398046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 22:43:50,440 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 22:43:52,019 | server.py:125 | fit progress: (1, 2.3013484477996826, {'accuracy': 0.1651, 'data_size': 10000}, 21.989961056009633)
INFO flwr 2024-04-17 22:43:52,019 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 22:43:52,019 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:44:01,338 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 22:44:02,878 | server.py:125 | fit progress: (2, 2.2980828285217285, {'accuracy': 0.3988, 'data_size': 10000}, 32.84866695001256)
INFO flwr 2024-04-17 22:44:02,878 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 22:44:02,878 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:44:11,707 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 22:44:13,211 | server.py:125 | fit progress: (3, 2.2905499935150146, {'accuracy': 0.5843, 'data_size': 10000}, 43.18212363500788)
INFO flwr 2024-04-17 22:44:13,211 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 22:44:13,212 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:44:21,864 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 22:44:23,374 | server.py:125 | fit progress: (4, 2.274726152420044, {'accuracy': 0.6347, 'data_size': 10000}, 53.34469528400223)
INFO flwr 2024-04-17 22:44:23,374 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 22:44:23,374 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:44:31,815 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 22:44:33,143 | server.py:125 | fit progress: (5, 2.2439844608306885, {'accuracy': 0.6574, 'data_size': 10000}, 63.1143251050089)
INFO flwr 2024-04-17 22:44:33,144 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 22:44:33,144 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:44:41,624 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 22:44:43,147 | server.py:125 | fit progress: (6, 2.1883819103240967, {'accuracy': 0.674, 'data_size': 10000}, 73.11823460301093)
INFO flwr 2024-04-17 22:44:43,147 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 22:44:43,148 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:44:51,814 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 22:44:53,092 | server.py:125 | fit progress: (7, 2.104297161102295, {'accuracy': 0.6741, 'data_size': 10000}, 83.06354907900095)
INFO flwr 2024-04-17 22:44:53,093 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 22:44:53,093 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:45:01,582 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 22:45:03,143 | server.py:125 | fit progress: (8, 2.0072500705718994, {'accuracy': 0.6809, 'data_size': 10000}, 93.11446507301298)
INFO flwr 2024-04-17 22:45:03,144 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 22:45:03,144 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:45:11,504 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 22:45:12,830 | server.py:125 | fit progress: (9, 1.9216042757034302, {'accuracy': 0.6969, 'data_size': 10000}, 102.80069197500416)
INFO flwr 2024-04-17 22:45:12,830 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 22:45:12,830 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:45:21,653 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 22:45:23,161 | server.py:125 | fit progress: (10, 1.8566854000091553, {'accuracy': 0.7085, 'data_size': 10000}, 113.13261032100127)
INFO flwr 2024-04-17 22:45:23,162 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 22:45:23,162 | server.py:153 | FL finished in 113.13306373200612
INFO flwr 2024-04-17 22:45:23,162 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 22:45:23,162 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 22:45:23,162 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 22:45:23,162 | app.py:229 | app_fit: losses_centralized [(0, 2.302476644515991), (1, 2.3013484477996826), (2, 2.2980828285217285), (3, 2.2905499935150146), (4, 2.274726152420044), (5, 2.2439844608306885), (6, 2.1883819103240967), (7, 2.104297161102295), (8, 2.0072500705718994), (9, 1.9216042757034302), (10, 1.8566854000091553)]
INFO flwr 2024-04-17 22:45:23,163 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1033), (1, 0.1651), (2, 0.3988), (3, 0.5843), (4, 0.6347), (5, 0.6574), (6, 0.674), (7, 0.6741), (8, 0.6809), (9, 0.6969), (10, 0.7085)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7085
wandb:     loss 1.85669
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_224307-biorm92l
wandb: Find logs at: ./wandb/offline-run-20240417_224307-biorm92l/logs
INFO flwr 2024-04-17 22:45:26,791 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 22:52:38,099 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=878329)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=878329)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 22:52:43,037	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 22:52:43,942	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 22:52:44,394	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 22:52:44,828	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_84150b7d885e4852.zip' (47.66MiB) to Ray cluster...
2024-04-17 22:52:44,993	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_84150b7d885e4852.zip'.
INFO flwr 2024-04-17 22:52:56,310 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72745189785.0, 'memory': 159738776167.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 22:52:56,311 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 22:52:56,311 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 22:52:56,335 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 22:52:56,336 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 22:52:56,336 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 22:52:56,336 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 22:52:59,604 | server.py:94 | initial parameters (loss, other metrics): 2.302635431289673, {'accuracy': 0.0944, 'data_size': 10000}
INFO flwr 2024-04-17 22:52:59,604 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 22:52:59,605 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=882687)[0m 2024-04-17 22:53:02.381345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=882687)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=882684)[0m 2024-04-17 22:53:04.719279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=882691)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=882691)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=882690)[0m 2024-04-17 22:53:02.624938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=882690)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=882688)[0m 2024-04-17 22:53:04.883675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 22:53:20,101 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 22:53:21,446 | server.py:125 | fit progress: (1, 2.3025615215301514, {'accuracy': 0.0931, 'data_size': 10000}, 21.84167316800449)
INFO flwr 2024-04-17 22:53:21,447 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 22:53:21,447 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:53:30,892 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 22:53:32,233 | server.py:125 | fit progress: (2, 2.302443742752075, {'accuracy': 0.0993, 'data_size': 10000}, 32.62823269600631)
INFO flwr 2024-04-17 22:53:32,233 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 22:53:32,234 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:53:41,278 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 22:53:42,768 | server.py:125 | fit progress: (3, 2.3022897243499756, {'accuracy': 0.1154, 'data_size': 10000}, 43.16331594900112)
INFO flwr 2024-04-17 22:53:42,768 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 22:53:42,769 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:53:51,348 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 22:53:52,650 | server.py:125 | fit progress: (4, 2.302090644836426, {'accuracy': 0.1429, 'data_size': 10000}, 53.04484104900621)
INFO flwr 2024-04-17 22:53:52,650 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 22:53:52,650 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:54:01,317 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 22:54:02,877 | server.py:125 | fit progress: (5, 2.3018534183502197, {'accuracy': 0.1742, 'data_size': 10000}, 63.27189246600028)
INFO flwr 2024-04-17 22:54:02,877 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 22:54:02,877 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:54:11,481 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 22:54:12,793 | server.py:125 | fit progress: (6, 2.3015666007995605, {'accuracy': 0.2091, 'data_size': 10000}, 73.18819732600241)
INFO flwr 2024-04-17 22:54:12,793 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 22:54:12,793 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:54:21,274 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 22:54:22,581 | server.py:125 | fit progress: (7, 2.3012146949768066, {'accuracy': 0.2375, 'data_size': 10000}, 82.97610369400354)
INFO flwr 2024-04-17 22:54:22,581 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 22:54:22,582 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:54:31,722 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 22:54:33,243 | server.py:125 | fit progress: (8, 2.3008065223693848, {'accuracy': 0.2582, 'data_size': 10000}, 93.63809922200744)
INFO flwr 2024-04-17 22:54:33,243 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 22:54:33,243 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:54:42,088 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 22:54:43,434 | server.py:125 | fit progress: (9, 2.30033540725708, {'accuracy': 0.2725, 'data_size': 10000}, 103.82873075200769)
INFO flwr 2024-04-17 22:54:43,434 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 22:54:43,434 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 22:54:52,229 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 22:54:53,803 | server.py:125 | fit progress: (10, 2.299790382385254, {'accuracy': 0.2838, 'data_size': 10000}, 114.19856960700417)
INFO flwr 2024-04-17 22:54:53,804 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 22:54:53,804 | server.py:153 | FL finished in 114.19901718600886
INFO flwr 2024-04-17 22:54:53,804 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 22:54:53,804 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 22:54:53,804 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 22:54:53,804 | app.py:229 | app_fit: losses_centralized [(0, 2.302635431289673), (1, 2.3025615215301514), (2, 2.302443742752075), (3, 2.3022897243499756), (4, 2.302090644836426), (5, 2.3018534183502197), (6, 2.3015666007995605), (7, 2.3012146949768066), (8, 2.3008065223693848), (9, 2.30033540725708), (10, 2.299790382385254)]
INFO flwr 2024-04-17 22:54:53,805 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0944), (1, 0.0931), (2, 0.0993), (3, 0.1154), (4, 0.1429), (5, 0.1742), (6, 0.2091), (7, 0.2375), (8, 0.2582), (9, 0.2725), (10, 0.2838)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2838
wandb:     loss 2.29979
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_225237-d6u3h37s
wandb: Find logs at: ./wandb/offline-run-20240417_225237-d6u3h37s/logs
INFO flwr 2024-04-17 22:54:57,518 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 23:02:08,569 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=882684)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=882684)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 23:02:13,505	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 23:02:14,401	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 23:02:14,874	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 23:02:15,305	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0a03cdc20125d64d.zip' (47.82MiB) to Ray cluster...
2024-04-17 23:02:15,497	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0a03cdc20125d64d.zip'.
INFO flwr 2024-04-17 23:02:26,801 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'memory': 159844498432.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 72790499328.0}
INFO flwr 2024-04-17 23:02:26,802 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 23:02:26,802 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 23:02:26,823 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 23:02:26,824 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 23:02:26,824 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 23:02:26,824 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 23:02:29,746 | server.py:94 | initial parameters (loss, other metrics): 2.3026087284088135, {'accuracy': 0.0694, 'data_size': 10000}
INFO flwr 2024-04-17 23:02:29,746 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 23:02:29,747 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=887069)[0m 2024-04-17 23:02:32.938660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=887069)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=887062)[0m 2024-04-17 23:02:35.270590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=887066)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=887066)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=887071)[0m 2024-04-17 23:02:33.289624: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=887071)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=887071)[0m 2024-04-17 23:02:35.606717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 23:02:50,603 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 23:02:52,195 | server.py:125 | fit progress: (1, 2.3026020526885986, {'accuracy': 0.0707, 'data_size': 10000}, 22.448484358988935)
INFO flwr 2024-04-17 23:02:52,195 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 23:02:52,196 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:03:01,608 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 23:03:02,920 | server.py:125 | fit progress: (2, 2.3025922775268555, {'accuracy': 0.0719, 'data_size': 10000}, 33.1733942199935)
INFO flwr 2024-04-17 23:03:02,920 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 23:03:02,921 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:03:11,727 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 23:03:13,250 | server.py:125 | fit progress: (3, 2.3025803565979004, {'accuracy': 0.0733, 'data_size': 10000}, 43.50323482300155)
INFO flwr 2024-04-17 23:03:13,250 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 23:03:13,251 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:03:21,947 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 23:03:23,232 | server.py:125 | fit progress: (4, 2.302568197250366, {'accuracy': 0.0752, 'data_size': 10000}, 53.48568196799897)
INFO flwr 2024-04-17 23:03:23,233 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 23:03:23,233 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:03:32,092 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 23:03:33,418 | server.py:125 | fit progress: (5, 2.3025546073913574, {'accuracy': 0.0773, 'data_size': 10000}, 63.67110641099862)
INFO flwr 2024-04-17 23:03:33,418 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 23:03:33,418 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:03:42,087 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 23:03:43,656 | server.py:125 | fit progress: (6, 2.3025412559509277, {'accuracy': 0.0785, 'data_size': 10000}, 73.90952181899047)
INFO flwr 2024-04-17 23:03:43,656 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 23:03:43,657 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:03:52,317 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 23:03:53,604 | server.py:125 | fit progress: (7, 2.3025259971618652, {'accuracy': 0.0804, 'data_size': 10000}, 83.85772302999976)
INFO flwr 2024-04-17 23:03:53,605 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 23:03:53,605 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:04:02,495 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 23:04:03,999 | server.py:125 | fit progress: (8, 2.3025104999542236, {'accuracy': 0.0817, 'data_size': 10000}, 94.25268241099548)
INFO flwr 2024-04-17 23:04:04,000 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 23:04:04,000 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:04:12,652 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 23:04:13,979 | server.py:125 | fit progress: (9, 2.302495002746582, {'accuracy': 0.0836, 'data_size': 10000}, 104.23249377099273)
INFO flwr 2024-04-17 23:04:13,979 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 23:04:13,980 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:04:22,467 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 23:04:24,018 | server.py:125 | fit progress: (10, 2.3024790287017822, {'accuracy': 0.085, 'data_size': 10000}, 114.27179017099843)
INFO flwr 2024-04-17 23:04:24,019 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 23:04:24,019 | server.py:153 | FL finished in 114.27225455999724
INFO flwr 2024-04-17 23:04:24,019 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 23:04:24,019 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 23:04:24,019 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 23:04:24,019 | app.py:229 | app_fit: losses_centralized [(0, 2.3026087284088135), (1, 2.3026020526885986), (2, 2.3025922775268555), (3, 2.3025803565979004), (4, 2.302568197250366), (5, 2.3025546073913574), (6, 2.3025412559509277), (7, 2.3025259971618652), (8, 2.3025104999542236), (9, 2.302495002746582), (10, 2.3024790287017822)]
INFO flwr 2024-04-17 23:04:24,020 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0694), (1, 0.0707), (2, 0.0719), (3, 0.0733), (4, 0.0752), (5, 0.0773), (6, 0.0785), (7, 0.0804), (8, 0.0817), (9, 0.0836), (10, 0.085)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.085
wandb:     loss 2.30248
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_230208-uophwzre
wandb: Find logs at: ./wandb/offline-run-20240417_230208-uophwzre/logs
INFO flwr 2024-04-17 23:04:27,699 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 23:11:38,754 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=887062)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=887062)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 23:11:43,557	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 23:11:44,483	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 23:11:44,927	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 23:11:45,358	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8f9be310a395adf7.zip' (47.97MiB) to Ray cluster...
2024-04-17 23:11:45,552	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8f9be310a395adf7.zip'.
INFO flwr 2024-04-17 23:11:56,892 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 159909414708.0, 'object_store_memory': 72818320588.0}
INFO flwr 2024-04-17 23:11:56,893 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 23:11:56,893 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 23:11:56,915 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 23:11:56,917 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 23:11:56,918 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 23:11:56,918 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 23:12:00,147 | server.py:94 | initial parameters (loss, other metrics): 2.302739143371582, {'accuracy': 0.0891, 'data_size': 10000}
INFO flwr 2024-04-17 23:12:00,147 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 23:12:00,148 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=891181)[0m 2024-04-17 23:12:02.953719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=891181)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=891181)[0m 2024-04-17 23:12:05.240793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=891186)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=891186)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=891190)[0m 2024-04-17 23:12:03.268990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=891190)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=891190)[0m 2024-04-17 23:12:05.783944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 23:12:21,626 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 23:12:23,221 | server.py:125 | fit progress: (1, 2.350478172302246, {'accuracy': 0.1106, 'data_size': 10000}, 23.0734937209927)
INFO flwr 2024-04-17 23:12:23,221 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 23:12:23,222 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:12:34,142 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 23:12:35,469 | server.py:125 | fit progress: (2, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 35.3215899420029)
INFO flwr 2024-04-17 23:12:35,470 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 23:12:35,470 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:12:46,378 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 23:12:47,957 | server.py:125 | fit progress: (3, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 47.80938448899542)
INFO flwr 2024-04-17 23:12:47,957 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 23:12:47,958 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:12:58,259 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 23:12:59,796 | server.py:125 | fit progress: (4, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 59.648473908993765)
INFO flwr 2024-04-17 23:12:59,796 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 23:12:59,797 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:13:09,474 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 23:13:10,993 | server.py:125 | fit progress: (5, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 70.84573919799004)
INFO flwr 2024-04-17 23:13:10,994 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 23:13:10,994 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:13:21,099 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 23:13:22,606 | server.py:125 | fit progress: (6, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 82.4585443889955)
INFO flwr 2024-04-17 23:13:22,606 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 23:13:22,607 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:13:32,687 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 23:13:34,004 | server.py:125 | fit progress: (7, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 93.85588712900062)
INFO flwr 2024-04-17 23:13:34,004 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 23:13:34,004 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:13:43,461 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 23:13:44,783 | server.py:125 | fit progress: (8, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 104.63513904499996)
INFO flwr 2024-04-17 23:13:44,783 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 23:13:44,783 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:13:54,682 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 23:13:56,241 | server.py:125 | fit progress: (9, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 116.09319584199693)
INFO flwr 2024-04-17 23:13:56,241 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 23:13:56,241 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:14:06,939 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 23:14:08,229 | server.py:125 | fit progress: (10, 2.3719422817230225, {'accuracy': 0.0892, 'data_size': 10000}, 128.08143670199206)
INFO flwr 2024-04-17 23:14:08,229 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 23:14:08,229 | server.py:153 | FL finished in 128.0818726229918
INFO flwr 2024-04-17 23:14:08,230 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 23:14:08,230 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 23:14:08,230 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 23:14:08,230 | app.py:229 | app_fit: losses_centralized [(0, 2.302739143371582), (1, 2.350478172302246), (2, 2.3719422817230225), (3, 2.3719422817230225), (4, 2.3719422817230225), (5, 2.3719422817230225), (6, 2.3719422817230225), (7, 2.3719422817230225), (8, 2.3719422817230225), (9, 2.3719422817230225), (10, 2.3719422817230225)]
INFO flwr 2024-04-17 23:14:08,230 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0891), (1, 0.1106), (2, 0.0892), (3, 0.0892), (4, 0.0892), (5, 0.0892), (6, 0.0892), (7, 0.0892), (8, 0.0892), (9, 0.0892), (10, 0.0892)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0892
wandb:     loss 2.37194
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_231138-56czg3q2
wandb: Find logs at: ./wandb/offline-run-20240417_231138-56czg3q2/logs
INFO flwr 2024-04-17 23:14:11,824 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 23:21:22,937 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=891181)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=891181)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 23:21:27,779	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 23:21:28,651	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 23:21:29,120	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 23:21:29,547	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_179f099499bcad59.zip' (48.13MiB) to Ray cluster...
2024-04-17 23:21:29,752	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_179f099499bcad59.zip'.
INFO flwr 2024-04-17 23:21:41,118 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 160159265383.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 72925399449.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-17 23:21:41,119 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 23:21:41,119 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 23:21:41,139 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 23:21:41,140 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 23:21:41,141 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 23:21:41,141 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 23:21:44,727 | server.py:94 | initial parameters (loss, other metrics): 2.3027589321136475, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-17 23:21:44,727 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 23:21:44,728 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=895584)[0m 2024-04-17 23:21:47.253298: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=895584)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=895576)[0m 2024-04-17 23:21:49.737473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=895584)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=895584)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=895583)[0m 2024-04-17 23:21:47.512095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=895583)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=895585)[0m 2024-04-17 23:21:49.874055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 23:22:06,899 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 23:22:08,221 | server.py:125 | fit progress: (1, 2.210305690765381, {'accuracy': 0.4909, 'data_size': 10000}, 23.49362450200715)
INFO flwr 2024-04-17 23:22:08,222 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 23:22:08,222 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:22:18,979 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 23:22:20,318 | server.py:125 | fit progress: (2, 1.9014517068862915, {'accuracy': 0.5684, 'data_size': 10000}, 35.59039256699907)
INFO flwr 2024-04-17 23:22:20,318 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 23:22:20,319 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:22:30,285 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 23:22:31,850 | server.py:125 | fit progress: (3, 1.7639566659927368, {'accuracy': 0.694, 'data_size': 10000}, 47.122505567007465)
INFO flwr 2024-04-17 23:22:31,850 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 23:22:31,851 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:22:41,958 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 23:22:43,483 | server.py:125 | fit progress: (4, 1.6236063241958618, {'accuracy': 0.8362, 'data_size': 10000}, 58.7555792540079)
INFO flwr 2024-04-17 23:22:43,484 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 23:22:43,484 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:22:53,195 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 23:22:54,733 | server.py:125 | fit progress: (5, 1.5758394002914429, {'accuracy': 0.8848, 'data_size': 10000}, 70.0055381340062)
INFO flwr 2024-04-17 23:22:54,734 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 23:22:54,734 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:23:05,436 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 23:23:06,970 | server.py:125 | fit progress: (6, 1.6139155626296997, {'accuracy': 0.8464, 'data_size': 10000}, 82.24212319900107)
INFO flwr 2024-04-17 23:23:06,970 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 23:23:06,970 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:23:16,835 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 23:23:18,127 | server.py:125 | fit progress: (7, 1.6947309970855713, {'accuracy': 0.7663, 'data_size': 10000}, 93.3997842999961)
INFO flwr 2024-04-17 23:23:18,128 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 23:23:18,128 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:23:28,471 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 23:23:29,751 | server.py:125 | fit progress: (8, 1.616047739982605, {'accuracy': 0.8452, 'data_size': 10000}, 105.02290709500085)
INFO flwr 2024-04-17 23:23:29,751 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 23:23:29,751 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:23:39,532 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 23:23:41,091 | server.py:125 | fit progress: (9, 1.571989893913269, {'accuracy': 0.889, 'data_size': 10000}, 116.36356739600888)
INFO flwr 2024-04-17 23:23:41,091 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 23:23:41,092 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:23:50,783 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 23:23:52,087 | server.py:125 | fit progress: (10, 1.5914628505706787, {'accuracy': 0.8696, 'data_size': 10000}, 127.35896019899519)
INFO flwr 2024-04-17 23:23:52,087 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 23:23:52,087 | server.py:153 | FL finished in 127.35939066699939
INFO flwr 2024-04-17 23:23:52,087 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 23:23:52,087 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 23:23:52,087 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 23:23:52,088 | app.py:229 | app_fit: losses_centralized [(0, 2.3027589321136475), (1, 2.210305690765381), (2, 1.9014517068862915), (3, 1.7639566659927368), (4, 1.6236063241958618), (5, 1.5758394002914429), (6, 1.6139155626296997), (7, 1.6947309970855713), (8, 1.616047739982605), (9, 1.571989893913269), (10, 1.5914628505706787)]
INFO flwr 2024-04-17 23:23:52,088 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.4909), (2, 0.5684), (3, 0.694), (4, 0.8362), (5, 0.8848), (6, 0.8464), (7, 0.7663), (8, 0.8452), (9, 0.889), (10, 0.8696)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8696
wandb:     loss 1.59146
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_232122-exi1k3lx
wandb: Find logs at: ./wandb/offline-run-20240417_232122-exi1k3lx/logs
INFO flwr 2024-04-17 23:23:55,815 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 23:31:07,040 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=895576)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=895576)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 23:31:11,903	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 23:31:12,814	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 23:31:13,277	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 23:31:13,436	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (10.10MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-17 23:31:13,704	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_332e28b8c8714382.zip' (48.28MiB) to Ray cluster...
2024-04-17 23:31:13,903	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_332e28b8c8714382.zip'.
INFO flwr 2024-04-17 23:31:25,160 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 72855743692.0, 'accelerator_type:TITAN': 1.0, 'memory': 159996735284.0}
INFO flwr 2024-04-17 23:31:25,160 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 23:31:25,160 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 23:31:25,176 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 23:31:25,177 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 23:31:25,178 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 23:31:25,178 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 23:31:28,633 | server.py:94 | initial parameters (loss, other metrics): 2.302478790283203, {'accuracy': 0.1095, 'data_size': 10000}
INFO flwr 2024-04-17 23:31:28,637 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 23:31:28,639 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=899942)[0m 2024-04-17 23:31:31.362685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=899942)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=899939)[0m 2024-04-17 23:31:33.763838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=899943)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=899943)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=899941)[0m 2024-04-17 23:31:31.568960: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=899941)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=899945)[0m 2024-04-17 23:31:33.939576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 23:31:50,659 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 23:31:51,953 | server.py:125 | fit progress: (1, 2.301147699356079, {'accuracy': 0.202, 'data_size': 10000}, 23.31565651699202)
INFO flwr 2024-04-17 23:31:51,953 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 23:31:51,954 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:32:02,326 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 23:32:03,622 | server.py:125 | fit progress: (2, 2.2973039150238037, {'accuracy': 0.2952, 'data_size': 10000}, 34.98454616499657)
INFO flwr 2024-04-17 23:32:03,622 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 23:32:03,623 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:32:13,593 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 23:32:15,091 | server.py:125 | fit progress: (3, 2.2887182235717773, {'accuracy': 0.4555, 'data_size': 10000}, 46.45337109499087)
INFO flwr 2024-04-17 23:32:15,091 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 23:32:15,091 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:32:25,572 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 23:32:27,147 | server.py:125 | fit progress: (4, 2.2717063426971436, {'accuracy': 0.5038, 'data_size': 10000}, 58.50988700799644)
INFO flwr 2024-04-17 23:32:27,148 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 23:32:27,148 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:32:37,040 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 23:32:38,599 | server.py:125 | fit progress: (5, 2.2400519847869873, {'accuracy': 0.5431, 'data_size': 10000}, 69.96115406299941)
INFO flwr 2024-04-17 23:32:38,599 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 23:32:38,599 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:32:48,693 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 23:32:50,229 | server.py:125 | fit progress: (6, 2.1860854625701904, {'accuracy': 0.5657, 'data_size': 10000}, 81.59160814899951)
INFO flwr 2024-04-17 23:32:50,229 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 23:32:50,230 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:33:00,648 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 23:33:01,922 | server.py:125 | fit progress: (7, 2.10750412940979, {'accuracy': 0.5987, 'data_size': 10000}, 93.28435711399652)
INFO flwr 2024-04-17 23:33:01,922 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 23:33:01,922 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:33:11,553 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 23:33:12,888 | server.py:125 | fit progress: (8, 2.012941598892212, {'accuracy': 0.6411, 'data_size': 10000}, 104.25039255799493)
INFO flwr 2024-04-17 23:33:12,888 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 23:33:12,888 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:33:23,291 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 23:33:24,806 | server.py:125 | fit progress: (9, 1.9181984663009644, {'accuracy': 0.6825, 'data_size': 10000}, 116.16846309199173)
INFO flwr 2024-04-17 23:33:24,806 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 23:33:24,806 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:33:35,024 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 23:33:36,304 | server.py:125 | fit progress: (10, 1.8338935375213623, {'accuracy': 0.7296, 'data_size': 10000}, 127.6666873619979)
INFO flwr 2024-04-17 23:33:36,304 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 23:33:36,305 | server.py:153 | FL finished in 127.66711570500047
INFO flwr 2024-04-17 23:33:36,305 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 23:33:36,305 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 23:33:36,305 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 23:33:36,306 | app.py:229 | app_fit: losses_centralized [(0, 2.302478790283203), (1, 2.301147699356079), (2, 2.2973039150238037), (3, 2.2887182235717773), (4, 2.2717063426971436), (5, 2.2400519847869873), (6, 2.1860854625701904), (7, 2.10750412940979), (8, 2.012941598892212), (9, 1.9181984663009644), (10, 1.8338935375213623)]
INFO flwr 2024-04-17 23:33:36,306 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1095), (1, 0.202), (2, 0.2952), (3, 0.4555), (4, 0.5038), (5, 0.5431), (6, 0.5657), (7, 0.5987), (8, 0.6411), (9, 0.6825), (10, 0.7296)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7296
wandb:     loss 1.83389
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_233106-mb45wkbe
wandb: Find logs at: ./wandb/offline-run-20240417_233106-mb45wkbe/logs
INFO flwr 2024-04-17 23:33:39,946 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 23:40:51,211 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=899935)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=899935)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 23:40:57,107	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 23:40:57,995	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 23:40:58,455	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 23:40:58,612	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (10.25MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-17 23:40:58,889	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_bb230821fc7c659b.zip' (48.44MiB) to Ray cluster...
2024-04-17 23:40:59,075	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_bb230821fc7c659b.zip'.
INFO flwr 2024-04-17 23:41:10,268 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 72842815488.0, 'accelerator_type:TITAN': 1.0, 'memory': 159966569472.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-17 23:41:10,268 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 23:41:10,269 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 23:41:10,290 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 23:41:10,291 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 23:41:10,291 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 23:41:10,291 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 23:41:14,482 | server.py:94 | initial parameters (loss, other metrics): 2.3026034832000732, {'accuracy': 0.0868, 'data_size': 10000}
INFO flwr 2024-04-17 23:41:14,483 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 23:41:14,483 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=904330)[0m 2024-04-17 23:41:16.459854: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=904330)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=904330)[0m 2024-04-17 23:41:18.730723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=904331)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=904331)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=904323)[0m 2024-04-17 23:41:16.706586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=904323)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=904328)[0m 2024-04-17 23:41:18.878156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 23:41:35,794 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 23:41:37,135 | server.py:125 | fit progress: (1, 2.3025288581848145, {'accuracy': 0.0942, 'data_size': 10000}, 22.652089626004454)
INFO flwr 2024-04-17 23:41:37,136 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 23:41:37,136 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:41:48,487 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 23:41:49,821 | server.py:125 | fit progress: (2, 2.302419662475586, {'accuracy': 0.1047, 'data_size': 10000}, 35.33731842201087)
INFO flwr 2024-04-17 23:41:49,821 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 23:41:49,821 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:41:59,980 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 23:42:01,486 | server.py:125 | fit progress: (3, 2.3022713661193848, {'accuracy': 0.1177, 'data_size': 10000}, 47.00269940201542)
INFO flwr 2024-04-17 23:42:01,486 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 23:42:01,487 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:42:11,805 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 23:42:13,331 | server.py:125 | fit progress: (4, 2.302088975906372, {'accuracy': 0.132, 'data_size': 10000}, 58.847739804012235)
INFO flwr 2024-04-17 23:42:13,331 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 23:42:13,332 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:42:22,706 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 23:42:24,224 | server.py:125 | fit progress: (5, 2.3018641471862793, {'accuracy': 0.1457, 'data_size': 10000}, 69.74052336902241)
INFO flwr 2024-04-17 23:42:24,224 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 23:42:24,224 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:42:34,349 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 23:42:35,908 | server.py:125 | fit progress: (6, 2.301593065261841, {'accuracy': 0.1599, 'data_size': 10000}, 81.42475188002572)
INFO flwr 2024-04-17 23:42:35,908 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 23:42:35,909 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:42:46,160 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 23:42:47,515 | server.py:125 | fit progress: (7, 2.3012733459472656, {'accuracy': 0.1761, 'data_size': 10000}, 93.03163688600762)
INFO flwr 2024-04-17 23:42:47,515 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 23:42:47,515 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:42:57,105 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 23:42:58,441 | server.py:125 | fit progress: (8, 2.300896406173706, {'accuracy': 0.1942, 'data_size': 10000}, 103.9576169270149)
INFO flwr 2024-04-17 23:42:58,441 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 23:42:58,441 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:43:08,575 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 23:43:10,088 | server.py:125 | fit progress: (9, 2.3004610538482666, {'accuracy': 0.2095, 'data_size': 10000}, 115.60502616802114)
INFO flwr 2024-04-17 23:43:10,089 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 23:43:10,089 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:43:19,740 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 23:43:21,035 | server.py:125 | fit progress: (10, 2.2999660968780518, {'accuracy': 0.2236, 'data_size': 10000}, 126.55137058201944)
INFO flwr 2024-04-17 23:43:21,035 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 23:43:21,035 | server.py:153 | FL finished in 126.5519428300031
INFO flwr 2024-04-17 23:43:21,035 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 23:43:21,036 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 23:43:21,036 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 23:43:21,036 | app.py:229 | app_fit: losses_centralized [(0, 2.3026034832000732), (1, 2.3025288581848145), (2, 2.302419662475586), (3, 2.3022713661193848), (4, 2.302088975906372), (5, 2.3018641471862793), (6, 2.301593065261841), (7, 2.3012733459472656), (8, 2.300896406173706), (9, 2.3004610538482666), (10, 2.2999660968780518)]
INFO flwr 2024-04-17 23:43:21,036 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0868), (1, 0.0942), (2, 0.1047), (3, 0.1177), (4, 0.132), (5, 0.1457), (6, 0.1599), (7, 0.1761), (8, 0.1942), (9, 0.2095), (10, 0.2236)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2236
wandb:     loss 2.29997
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_234050-h2il5oxv
wandb: Find logs at: ./wandb/offline-run-20240417_234050-h2il5oxv/logs
INFO flwr 2024-04-17 23:43:24,662 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.15}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-17 23:50:35,759 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=904323)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=904323)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-17 23:50:41,519	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-17 23:50:42,394	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-17 23:50:42,890	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-17 23:50:43,052	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (10.39MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-17 23:50:43,320	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9137a6446e2bb3f9.zip' (48.59MiB) to Ray cluster...
2024-04-17 23:50:43,513	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9137a6446e2bb3f9.zip'.
INFO flwr 2024-04-17 23:50:54,775 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72836236492.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 159951218484.0, 'CPU': 64.0}
INFO flwr 2024-04-17 23:50:54,775 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-17 23:50:54,775 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-17 23:50:54,797 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-17 23:50:54,797 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-17 23:50:54,798 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-17 23:50:54,798 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-17 23:50:58,572 | server.py:94 | initial parameters (loss, other metrics): 2.302717924118042, {'accuracy': 0.0764, 'data_size': 10000}
INFO flwr 2024-04-17 23:50:58,572 | server.py:104 | FL starting
DEBUG flwr 2024-04-17 23:50:58,573 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=909045)[0m 2024-04-17 23:51:00.906096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=909045)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=909048)[0m 2024-04-17 23:51:03.206142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=909045)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=909045)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=909041)[0m 2024-04-17 23:51:01.094565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=909041)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=909041)[0m 2024-04-17 23:51:03.417059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-17 23:51:20,284 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-17 23:51:21,610 | server.py:125 | fit progress: (1, 2.3027100563049316, {'accuracy': 0.0767, 'data_size': 10000}, 23.037215155985905)
INFO flwr 2024-04-17 23:51:21,610 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-17 23:51:21,611 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:51:32,473 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-17 23:51:33,802 | server.py:125 | fit progress: (2, 2.302699565887451, {'accuracy': 0.0776, 'data_size': 10000}, 35.2291776810016)
INFO flwr 2024-04-17 23:51:33,802 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-17 23:51:33,803 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:51:44,262 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-17 23:51:45,823 | server.py:125 | fit progress: (3, 2.302687883377075, {'accuracy': 0.0786, 'data_size': 10000}, 47.250520032976056)
INFO flwr 2024-04-17 23:51:45,824 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-17 23:51:45,824 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:51:55,887 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-17 23:51:57,435 | server.py:125 | fit progress: (4, 2.3026745319366455, {'accuracy': 0.0792, 'data_size': 10000}, 58.861953808984254)
INFO flwr 2024-04-17 23:51:57,435 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-17 23:51:57,435 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:52:07,515 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-17 23:52:09,029 | server.py:125 | fit progress: (5, 2.3026602268218994, {'accuracy': 0.0797, 'data_size': 10000}, 70.45634837198304)
INFO flwr 2024-04-17 23:52:09,030 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-17 23:52:09,030 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:52:19,191 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-17 23:52:20,734 | server.py:125 | fit progress: (6, 2.302644729614258, {'accuracy': 0.0804, 'data_size': 10000}, 82.16160002799006)
INFO flwr 2024-04-17 23:52:20,735 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-17 23:52:20,735 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:52:30,538 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-17 23:52:32,063 | server.py:125 | fit progress: (7, 2.3026294708251953, {'accuracy': 0.082, 'data_size': 10000}, 93.49025408099988)
INFO flwr 2024-04-17 23:52:32,063 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-17 23:52:32,064 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:52:41,921 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-17 23:52:43,434 | server.py:125 | fit progress: (8, 2.3026123046875, {'accuracy': 0.0836, 'data_size': 10000}, 104.8609247150016)
INFO flwr 2024-04-17 23:52:43,434 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-17 23:52:43,434 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:52:52,797 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-17 23:52:54,344 | server.py:125 | fit progress: (9, 2.3025946617126465, {'accuracy': 0.0845, 'data_size': 10000}, 115.77113824698608)
INFO flwr 2024-04-17 23:52:54,344 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-17 23:52:54,344 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-17 23:53:05,695 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-17 23:53:07,254 | server.py:125 | fit progress: (10, 2.3025765419006348, {'accuracy': 0.0855, 'data_size': 10000}, 128.6814220919914)
INFO flwr 2024-04-17 23:53:07,255 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-17 23:53:07,255 | server.py:153 | FL finished in 128.6818917329947
INFO flwr 2024-04-17 23:53:07,255 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-17 23:53:07,255 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-17 23:53:07,255 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-17 23:53:07,255 | app.py:229 | app_fit: losses_centralized [(0, 2.302717924118042), (1, 2.3027100563049316), (2, 2.302699565887451), (3, 2.302687883377075), (4, 2.3026745319366455), (5, 2.3026602268218994), (6, 2.302644729614258), (7, 2.3026294708251953), (8, 2.3026123046875), (9, 2.3025946617126465), (10, 2.3025765419006348)]
INFO flwr 2024-04-17 23:53:07,255 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0764), (1, 0.0767), (2, 0.0776), (3, 0.0786), (4, 0.0792), (5, 0.0797), (6, 0.0804), (7, 0.082), (8, 0.0836), (9, 0.0845), (10, 0.0855)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0855
wandb:     loss 2.30258
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240417_235035-0ir91h8t
wandb: Find logs at: ./wandb/offline-run-20240417_235035-0ir91h8t/logs
INFO flwr 2024-04-17 23:53:10,931 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 00:00:22,059 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=909041)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=909041)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 00:00:27,069	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 00:00:28,168	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 00:00:28,623	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 00:00:28,784	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (10.54MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 00:00:29,056	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9e6d32a415b5b81e.zip' (48.75MiB) to Ray cluster...
2024-04-18 00:00:29,251	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9e6d32a415b5b81e.zip'.
INFO flwr 2024-04-18 00:00:40,550 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 72832075776.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 159941510144.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-18 00:00:40,550 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 00:00:40,550 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 00:00:40,576 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 00:00:40,578 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 00:00:40,579 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 00:00:40,579 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 00:00:43,799 | server.py:94 | initial parameters (loss, other metrics): 2.3023974895477295, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-18 00:00:43,799 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 00:00:43,799 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=913454)[0m 2024-04-18 00:00:46.767164: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=913454)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=913454)[0m 2024-04-18 00:00:49.136260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=913458)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=913458)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=913462)[0m 2024-04-18 00:00:46.987434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=913462)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=913459)[0m 2024-04-18 00:00:49.198976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 00:01:05,898 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 00:01:07,473 | server.py:125 | fit progress: (1, 2.1551754474639893, {'accuracy': 0.3059, 'data_size': 10000}, 23.673570308979833)
INFO flwr 2024-04-18 00:01:07,473 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 00:01:07,473 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:01:17,982 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 00:01:19,267 | server.py:125 | fit progress: (2, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 35.46766603499418)
INFO flwr 2024-04-18 00:01:19,267 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 00:01:19,267 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:01:29,221 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 00:01:30,753 | server.py:125 | fit progress: (3, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 46.953970845992444)
INFO flwr 2024-04-18 00:01:30,754 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 00:01:30,754 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:01:40,780 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 00:01:42,368 | server.py:125 | fit progress: (4, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 58.569097572995815)
INFO flwr 2024-04-18 00:01:42,369 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 00:01:42,369 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:01:51,721 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 00:01:53,289 | server.py:125 | fit progress: (5, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 69.48977315600496)
INFO flwr 2024-04-18 00:01:53,289 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 00:01:53,289 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:02:03,337 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 00:02:04,661 | server.py:125 | fit progress: (6, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 80.86158412398072)
INFO flwr 2024-04-18 00:02:04,661 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 00:02:04,661 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:02:14,719 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 00:02:15,997 | server.py:125 | fit progress: (7, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 92.19729321400519)
INFO flwr 2024-04-18 00:02:15,997 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 00:02:15,997 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:02:25,936 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 00:02:27,449 | server.py:125 | fit progress: (8, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 103.64964122700621)
INFO flwr 2024-04-18 00:02:27,449 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 00:02:27,449 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:02:37,533 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 00:02:38,821 | server.py:125 | fit progress: (9, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 115.02204525499837)
INFO flwr 2024-04-18 00:02:38,822 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 00:02:38,822 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:02:49,006 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 00:02:50,548 | server.py:125 | fit progress: (10, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 126.74883046699688)
INFO flwr 2024-04-18 00:02:50,548 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 00:02:50,549 | server.py:153 | FL finished in 126.7494206620031
INFO flwr 2024-04-18 00:02:50,549 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 00:02:50,549 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 00:02:50,549 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 00:02:50,549 | app.py:229 | app_fit: losses_centralized [(0, 2.3023974895477295), (1, 2.1551754474639893), (2, 2.347642183303833), (3, 2.347642183303833), (4, 2.347642183303833), (5, 2.347642183303833), (6, 2.347642183303833), (7, 2.347642183303833), (8, 2.347642183303833), (9, 2.347642183303833), (10, 2.347642183303833)]
INFO flwr 2024-04-18 00:02:50,550 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.3059), (2, 0.1135), (3, 0.1135), (4, 0.1135), (5, 0.1135), (6, 0.1135), (7, 0.1135), (8, 0.1135), (9, 0.1135), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.34764
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_000021-q507069n
wandb: Find logs at: ./wandb/offline-run-20240418_000021-q507069n/logs
INFO flwr 2024-04-18 00:02:54,169 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 00:10:05,311 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=913454)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=913454)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 00:10:10,322	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 00:10:11,325	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 00:10:11,771	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 00:10:11,933	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (10.68MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 00:10:12,205	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ffa17eb658184b0b.zip' (48.91MiB) to Ray cluster...
2024-04-18 00:10:12,405	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ffa17eb658184b0b.zip'.
INFO flwr 2024-04-18 00:10:23,636 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 72829618176.0, 'accelerator_type:TITAN': 1.0, 'memory': 159935775744.0}
INFO flwr 2024-04-18 00:10:23,636 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 00:10:23,637 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 00:10:23,653 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 00:10:23,655 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 00:10:23,656 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 00:10:23,656 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 00:10:26,411 | server.py:94 | initial parameters (loss, other metrics): 2.3028337955474854, {'accuracy': 0.0983, 'data_size': 10000}
INFO flwr 2024-04-18 00:10:26,416 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 00:10:26,416 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=917890)[0m 2024-04-18 00:10:29.836909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=917890)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=917889)[0m 2024-04-18 00:10:32.107103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=917890)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=917890)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=917895)[0m 2024-04-18 00:10:30.112617: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=917895)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=917895)[0m 2024-04-18 00:10:32.423083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 00:10:49,272 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 00:10:50,624 | server.py:125 | fit progress: (1, 2.1911308765411377, {'accuracy': 0.4887, 'data_size': 10000}, 24.207627021009102)
INFO flwr 2024-04-18 00:10:50,624 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 00:10:50,624 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:11:02,018 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 00:11:03,370 | server.py:125 | fit progress: (2, 1.9148098230361938, {'accuracy': 0.5508, 'data_size': 10000}, 36.95382838399382)
INFO flwr 2024-04-18 00:11:03,370 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 00:11:03,371 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:11:13,650 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 00:11:15,145 | server.py:125 | fit progress: (3, 1.720944881439209, {'accuracy': 0.7353, 'data_size': 10000}, 48.72903290501563)
INFO flwr 2024-04-18 00:11:15,146 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 00:11:15,146 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:11:25,746 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 00:11:27,024 | server.py:125 | fit progress: (4, 1.5962964296340942, {'accuracy': 0.8646, 'data_size': 10000}, 60.60815827400074)
INFO flwr 2024-04-18 00:11:27,025 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 00:11:27,025 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:11:37,537 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 00:11:39,080 | server.py:125 | fit progress: (5, 1.5676709413528442, {'accuracy': 0.8925, 'data_size': 10000}, 72.66339143301593)
INFO flwr 2024-04-18 00:11:39,080 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 00:11:39,080 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:11:49,406 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 00:11:50,917 | server.py:125 | fit progress: (6, 1.5566056966781616, {'accuracy': 0.9043, 'data_size': 10000}, 84.50048527598847)
INFO flwr 2024-04-18 00:11:50,917 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 00:11:50,917 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:12:00,887 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 00:12:02,387 | server.py:125 | fit progress: (7, 1.5611505508422852, {'accuracy': 0.9003, 'data_size': 10000}, 95.97053663100814)
INFO flwr 2024-04-18 00:12:02,387 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 00:12:02,387 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:12:12,871 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 00:12:14,174 | server.py:125 | fit progress: (8, 1.5581406354904175, {'accuracy': 0.9029, 'data_size': 10000}, 107.75779761499143)
INFO flwr 2024-04-18 00:12:14,174 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 00:12:14,175 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:12:24,113 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 00:12:25,671 | server.py:125 | fit progress: (9, 1.5457894802093506, {'accuracy': 0.9154, 'data_size': 10000}, 119.25471656399895)
INFO flwr 2024-04-18 00:12:25,671 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 00:12:25,671 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:12:35,752 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 00:12:37,078 | server.py:125 | fit progress: (10, 1.5497812032699585, {'accuracy': 0.9113, 'data_size': 10000}, 130.66186660301173)
INFO flwr 2024-04-18 00:12:37,078 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 00:12:37,079 | server.py:153 | FL finished in 130.66231538000284
INFO flwr 2024-04-18 00:12:37,079 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 00:12:37,079 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 00:12:37,079 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 00:12:37,079 | app.py:229 | app_fit: losses_centralized [(0, 2.3028337955474854), (1, 2.1911308765411377), (2, 1.9148098230361938), (3, 1.720944881439209), (4, 1.5962964296340942), (5, 1.5676709413528442), (6, 1.5566056966781616), (7, 1.5611505508422852), (8, 1.5581406354904175), (9, 1.5457894802093506), (10, 1.5497812032699585)]
INFO flwr 2024-04-18 00:12:37,079 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0983), (1, 0.4887), (2, 0.5508), (3, 0.7353), (4, 0.8646), (5, 0.8925), (6, 0.9043), (7, 0.9003), (8, 0.9029), (9, 0.9154), (10, 0.9113)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9113
wandb:     loss 1.54978
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_001004-dkow6s69
wandb: Find logs at: ./wandb/offline-run-20240418_001004-dkow6s69/logs
INFO flwr 2024-04-18 00:12:40,811 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 00:19:52,132 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=917883)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=917883)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 00:19:56,927	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 00:19:57,860	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 00:19:58,307	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 00:19:58,479	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (10.83MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 00:19:58,752	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c0e9f6ebbaa6604d.zip' (49.07MiB) to Ray cluster...
2024-04-18 00:19:58,955	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c0e9f6ebbaa6604d.zip'.
INFO flwr 2024-04-18 00:20:10,243 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 72828017049.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 159932039783.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 00:20:10,243 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 00:20:10,243 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 00:20:10,269 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 00:20:10,270 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 00:20:10,271 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 00:20:10,271 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 00:20:13,893 | server.py:94 | initial parameters (loss, other metrics): 2.302658796310425, {'accuracy': 0.0955, 'data_size': 10000}
INFO flwr 2024-04-18 00:20:13,894 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 00:20:13,894 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=922632)[0m 2024-04-18 00:20:16.405590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=922632)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=922637)[0m 2024-04-18 00:20:18.779906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=922637)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=922637)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=922634)[0m 2024-04-18 00:20:16.577219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=922634)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=922634)[0m 2024-04-18 00:20:18.949604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 00:20:35,699 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 00:20:37,045 | server.py:125 | fit progress: (1, 2.3016138076782227, {'accuracy': 0.1006, 'data_size': 10000}, 23.150607517018216)
INFO flwr 2024-04-18 00:20:37,045 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 00:20:37,045 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:20:47,890 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 00:20:49,382 | server.py:125 | fit progress: (2, 2.298189163208008, {'accuracy': 0.2349, 'data_size': 10000}, 35.48802186900866)
INFO flwr 2024-04-18 00:20:49,383 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 00:20:49,383 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:20:59,339 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 00:21:00,882 | server.py:125 | fit progress: (3, 2.289715528488159, {'accuracy': 0.5738, 'data_size': 10000}, 46.987595963000786)
INFO flwr 2024-04-18 00:21:00,882 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 00:21:00,882 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:21:10,891 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 00:21:12,249 | server.py:125 | fit progress: (4, 2.2719955444335938, {'accuracy': 0.5962, 'data_size': 10000}, 58.354709245992126)
INFO flwr 2024-04-18 00:21:12,249 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 00:21:12,250 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:21:22,535 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 00:21:23,897 | server.py:125 | fit progress: (5, 2.2373430728912354, {'accuracy': 0.5693, 'data_size': 10000}, 70.00313022101182)
INFO flwr 2024-04-18 00:21:23,898 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 00:21:23,898 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:21:34,160 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 00:21:35,671 | server.py:125 | fit progress: (6, 2.177619457244873, {'accuracy': 0.5609, 'data_size': 10000}, 81.77684765600134)
INFO flwr 2024-04-18 00:21:35,671 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 00:21:35,672 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:21:45,578 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 00:21:46,866 | server.py:125 | fit progress: (7, 2.094151258468628, {'accuracy': 0.5651, 'data_size': 10000}, 92.9712316270161)
INFO flwr 2024-04-18 00:21:46,866 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 00:21:46,866 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:21:56,418 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 00:21:57,953 | server.py:125 | fit progress: (8, 2.006772756576538, {'accuracy': 0.5786, 'data_size': 10000}, 104.05830454701209)
INFO flwr 2024-04-18 00:21:57,953 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 00:21:57,953 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:22:07,945 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 00:22:09,238 | server.py:125 | fit progress: (9, 1.934794306755066, {'accuracy': 0.6003, 'data_size': 10000}, 115.34399320199736)
INFO flwr 2024-04-18 00:22:09,239 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 00:22:09,239 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:22:19,051 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 00:22:20,433 | server.py:125 | fit progress: (10, 1.8811982870101929, {'accuracy': 0.6234, 'data_size': 10000}, 126.53874593198998)
INFO flwr 2024-04-18 00:22:20,433 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 00:22:20,433 | server.py:153 | FL finished in 126.53918211799464
INFO flwr 2024-04-18 00:22:20,434 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 00:22:20,434 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 00:22:20,434 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 00:22:20,434 | app.py:229 | app_fit: losses_centralized [(0, 2.302658796310425), (1, 2.3016138076782227), (2, 2.298189163208008), (3, 2.289715528488159), (4, 2.2719955444335938), (5, 2.2373430728912354), (6, 2.177619457244873), (7, 2.094151258468628), (8, 2.006772756576538), (9, 1.934794306755066), (10, 1.8811982870101929)]
INFO flwr 2024-04-18 00:22:20,434 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0955), (1, 0.1006), (2, 0.2349), (3, 0.5738), (4, 0.5962), (5, 0.5693), (6, 0.5609), (7, 0.5651), (8, 0.5786), (9, 0.6003), (10, 0.6234)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6234
wandb:     loss 1.8812
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_001951-nyaf41en
wandb: Find logs at: ./wandb/offline-run-20240418_001951-nyaf41en/logs
INFO flwr 2024-04-18 00:22:24,072 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 00:29:35,264 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=922631)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=922631)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 00:29:40,023	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 00:29:40,894	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 00:29:41,346	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 00:29:41,515	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (10.97MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 00:29:41,806	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a764a231c7fa0728.zip' (49.22MiB) to Ray cluster...
2024-04-18 00:29:41,984	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a764a231c7fa0728.zip'.
INFO flwr 2024-04-18 00:29:53,304 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'object_store_memory': 72842373120.0, 'memory': 159965537280.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 00:29:53,305 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 00:29:53,305 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 00:29:53,327 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 00:29:53,328 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 00:29:53,328 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 00:29:53,328 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 00:29:56,467 | server.py:94 | initial parameters (loss, other metrics): 2.3025951385498047, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-18 00:29:56,467 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 00:29:56,467 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=926990)[0m 2024-04-18 00:29:59.606143: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=926990)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=926990)[0m 2024-04-18 00:30:01.892179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=926995)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=926995)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=926994)[0m 2024-04-18 00:30:00.068346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=926994)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=926994)[0m 2024-04-18 00:30:02.376862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 00:30:18,872 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 00:30:20,192 | server.py:125 | fit progress: (1, 2.302525520324707, {'accuracy': 0.101, 'data_size': 10000}, 23.724421630002325)
INFO flwr 2024-04-18 00:30:20,192 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 00:30:20,192 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:30:30,801 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 00:30:32,329 | server.py:125 | fit progress: (2, 2.302424907684326, {'accuracy': 0.101, 'data_size': 10000}, 35.86146881998866)
INFO flwr 2024-04-18 00:30:32,329 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 00:30:32,329 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:30:42,852 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 00:30:44,127 | server.py:125 | fit progress: (3, 2.3022916316986084, {'accuracy': 0.101, 'data_size': 10000}, 47.66002058799495)
INFO flwr 2024-04-18 00:30:44,128 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 00:30:44,128 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:30:53,667 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 00:30:55,175 | server.py:125 | fit progress: (4, 2.30212140083313, {'accuracy': 0.101, 'data_size': 10000}, 58.70811835897621)
INFO flwr 2024-04-18 00:30:55,176 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 00:30:55,176 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:31:05,589 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 00:31:06,918 | server.py:125 | fit progress: (5, 2.3019134998321533, {'accuracy': 0.101, 'data_size': 10000}, 70.45102804899216)
INFO flwr 2024-04-18 00:31:06,919 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 00:31:06,919 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:31:16,657 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 00:31:18,254 | server.py:125 | fit progress: (6, 2.3016576766967773, {'accuracy': 0.101, 'data_size': 10000}, 81.78688009700272)
INFO flwr 2024-04-18 00:31:18,254 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 00:31:18,255 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:31:28,469 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 00:31:29,795 | server.py:125 | fit progress: (7, 2.3013522624969482, {'accuracy': 0.101, 'data_size': 10000}, 93.32760197698371)
INFO flwr 2024-04-18 00:31:29,795 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 00:31:29,795 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:31:40,002 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 00:31:41,525 | server.py:125 | fit progress: (8, 2.3009934425354004, {'accuracy': 0.1013, 'data_size': 10000}, 105.05782001800253)
INFO flwr 2024-04-18 00:31:41,525 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 00:31:41,526 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:31:51,460 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 00:31:52,750 | server.py:125 | fit progress: (9, 2.3005776405334473, {'accuracy': 0.1015, 'data_size': 10000}, 116.2829862019862)
INFO flwr 2024-04-18 00:31:52,751 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 00:31:52,751 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:32:03,271 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 00:32:04,794 | server.py:125 | fit progress: (10, 2.3000969886779785, {'accuracy': 0.1047, 'data_size': 10000}, 128.32652654897538)
INFO flwr 2024-04-18 00:32:04,794 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 00:32:04,794 | server.py:153 | FL finished in 128.32710917299846
INFO flwr 2024-04-18 00:32:04,795 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 00:32:04,795 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 00:32:04,795 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 00:32:04,795 | app.py:229 | app_fit: losses_centralized [(0, 2.3025951385498047), (1, 2.302525520324707), (2, 2.302424907684326), (3, 2.3022916316986084), (4, 2.30212140083313), (5, 2.3019134998321533), (6, 2.3016576766967773), (7, 2.3013522624969482), (8, 2.3009934425354004), (9, 2.3005776405334473), (10, 2.3000969886779785)]
INFO flwr 2024-04-18 00:32:04,795 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.101), (2, 0.101), (3, 0.101), (4, 0.101), (5, 0.101), (6, 0.101), (7, 0.101), (8, 0.1013), (9, 0.1015), (10, 0.1047)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1047
wandb:     loss 2.3001
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_002934-fhgfx6hc
wandb: Find logs at: ./wandb/offline-run-20240418_002934-fhgfx6hc/logs
INFO flwr 2024-04-18 00:32:08,434 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 00:39:19,642 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=926989)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=926989)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 00:39:25,378	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 00:39:26,315	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 00:39:26,764	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 00:39:26,931	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (11.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 00:39:27,213	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7bdb1876e48ec111.zip' (49.38MiB) to Ray cluster...
2024-04-18 00:39:27,383	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7bdb1876e48ec111.zip'.
INFO flwr 2024-04-18 00:39:38,685 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 159953827636.0, 'CPU': 64.0, 'object_store_memory': 72837354700.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 00:39:38,686 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 00:39:38,686 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 00:39:38,704 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 00:39:38,705 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 00:39:38,705 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 00:39:38,705 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 00:39:41,307 | server.py:94 | initial parameters (loss, other metrics): 2.302574872970581, {'accuracy': 0.0742, 'data_size': 10000}
INFO flwr 2024-04-18 00:39:41,308 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 00:39:41,308 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=931340)[0m 2024-04-18 00:39:44.916705: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=931340)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=931340)[0m 2024-04-18 00:39:47.164271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=931345)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=931345)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=931342)[0m 2024-04-18 00:39:45.244558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=931342)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=931342)[0m 2024-04-18 00:39:47.644737: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 00:40:04,109 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 00:40:05,687 | server.py:125 | fit progress: (1, 2.302567481994629, {'accuracy': 0.0751, 'data_size': 10000}, 24.37865619699005)
INFO flwr 2024-04-18 00:40:05,687 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 00:40:05,687 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:40:16,403 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 00:40:17,976 | server.py:125 | fit progress: (2, 2.302558183670044, {'accuracy': 0.0769, 'data_size': 10000}, 36.667572508973535)
INFO flwr 2024-04-18 00:40:17,976 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 00:40:17,976 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:40:28,561 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 00:40:30,141 | server.py:125 | fit progress: (3, 2.302546977996826, {'accuracy': 0.0782, 'data_size': 10000}, 48.83242505197995)
INFO flwr 2024-04-18 00:40:30,141 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 00:40:30,141 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:40:39,781 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 00:40:41,321 | server.py:125 | fit progress: (4, 2.302534580230713, {'accuracy': 0.0813, 'data_size': 10000}, 60.012629412987735)
INFO flwr 2024-04-18 00:40:41,321 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 00:40:41,321 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:40:52,296 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 00:40:53,890 | server.py:125 | fit progress: (5, 2.3025217056274414, {'accuracy': 0.0836, 'data_size': 10000}, 72.58173440798419)
INFO flwr 2024-04-18 00:40:53,890 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 00:40:53,890 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:41:03,660 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 00:41:04,948 | server.py:125 | fit progress: (6, 2.3025076389312744, {'accuracy': 0.085, 'data_size': 10000}, 83.6396606049966)
INFO flwr 2024-04-18 00:41:04,948 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 00:41:04,948 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:41:14,736 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 00:41:16,262 | server.py:125 | fit progress: (7, 2.3024935722351074, {'accuracy': 0.0876, 'data_size': 10000}, 94.95430038199993)
INFO flwr 2024-04-18 00:41:16,263 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 00:41:16,263 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:41:26,566 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 00:41:27,852 | server.py:125 | fit progress: (8, 2.302478551864624, {'accuracy': 0.09, 'data_size': 10000}, 106.54365412099287)
INFO flwr 2024-04-18 00:41:27,852 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 00:41:27,852 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:41:38,074 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 00:41:39,379 | server.py:125 | fit progress: (9, 2.3024630546569824, {'accuracy': 0.0925, 'data_size': 10000}, 118.07088387297699)
INFO flwr 2024-04-18 00:41:39,379 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 00:41:39,380 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:41:49,170 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 00:41:50,746 | server.py:125 | fit progress: (10, 2.3024466037750244, {'accuracy': 0.0944, 'data_size': 10000}, 129.4378116549924)
INFO flwr 2024-04-18 00:41:50,746 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 00:41:50,747 | server.py:153 | FL finished in 129.43852780098678
INFO flwr 2024-04-18 00:41:50,747 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 00:41:50,747 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 00:41:50,747 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 00:41:50,747 | app.py:229 | app_fit: losses_centralized [(0, 2.302574872970581), (1, 2.302567481994629), (2, 2.302558183670044), (3, 2.302546977996826), (4, 2.302534580230713), (5, 2.3025217056274414), (6, 2.3025076389312744), (7, 2.3024935722351074), (8, 2.302478551864624), (9, 2.3024630546569824), (10, 2.3024466037750244)]
INFO flwr 2024-04-18 00:41:50,748 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0742), (1, 0.0751), (2, 0.0769), (3, 0.0782), (4, 0.0813), (5, 0.0836), (6, 0.085), (7, 0.0876), (8, 0.09), (9, 0.0925), (10, 0.0944)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0944
wandb:     loss 2.30245
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_003919-sg51tgc6
wandb: Find logs at: ./wandb/offline-run-20240418_003919-sg51tgc6/logs
INFO flwr 2024-04-18 00:41:54,430 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 00:49:05,682 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=931340)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=931340)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 00:49:10,505	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 00:49:11,513	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 00:49:11,979	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 00:49:12,144	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (11.26MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 00:49:12,437	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fda7f7c3df464b88.zip' (49.53MiB) to Ray cluster...
2024-04-18 00:49:12,623	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fda7f7c3df464b88.zip'.
INFO flwr 2024-04-18 00:49:23,944 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 159947617280.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'object_store_memory': 72834693120.0}
INFO flwr 2024-04-18 00:49:23,945 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 00:49:23,945 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 00:49:23,961 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 00:49:23,962 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 00:49:23,962 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 00:49:23,962 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 00:49:27,738 | server.py:94 | initial parameters (loss, other metrics): 2.3024778366088867, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-18 00:49:27,739 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 00:49:27,739 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=936095)[0m 2024-04-18 00:49:30.110226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=936095)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=936095)[0m 2024-04-18 00:49:32.426328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=936095)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=936095)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=936087)[0m 2024-04-18 00:49:30.562154: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=936087)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=936087)[0m 2024-04-18 00:49:32.969140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 00:49:49,087 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 00:49:50,404 | server.py:125 | fit progress: (1, 2.1618051528930664, {'accuracy': 0.2992, 'data_size': 10000}, 22.664968725002836)
INFO flwr 2024-04-18 00:49:50,404 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 00:49:50,404 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:50:01,211 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 00:50:02,509 | server.py:125 | fit progress: (2, 2.360142230987549, {'accuracy': 0.101, 'data_size': 10000}, 34.769659408018924)
INFO flwr 2024-04-18 00:50:02,509 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 00:50:02,509 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:50:12,565 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 00:50:14,072 | server.py:125 | fit progress: (3, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 46.33322612801567)
INFO flwr 2024-04-18 00:50:14,072 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 00:50:14,073 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:50:23,995 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 00:50:25,550 | server.py:125 | fit progress: (4, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 57.81127555100829)
INFO flwr 2024-04-18 00:50:25,550 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 00:50:25,551 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:50:35,140 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 00:50:36,421 | server.py:125 | fit progress: (5, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 68.6817591440049)
INFO flwr 2024-04-18 00:50:36,421 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 00:50:36,421 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:50:46,017 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 00:50:47,595 | server.py:125 | fit progress: (6, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 79.85617936300696)
INFO flwr 2024-04-18 00:50:47,595 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 00:50:47,596 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:50:57,906 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 00:50:59,190 | server.py:125 | fit progress: (7, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 91.45090705601615)
INFO flwr 2024-04-18 00:50:59,190 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 00:50:59,190 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:51:09,250 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 00:51:10,594 | server.py:125 | fit progress: (8, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 102.85476435700548)
INFO flwr 2024-04-18 00:51:10,594 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 00:51:10,594 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:51:20,701 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 00:51:22,258 | server.py:125 | fit progress: (9, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 114.51869891901151)
INFO flwr 2024-04-18 00:51:22,258 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 00:51:22,258 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:51:32,677 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 00:51:34,294 | server.py:125 | fit progress: (10, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 126.5546894230065)
INFO flwr 2024-04-18 00:51:34,294 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 00:51:34,294 | server.py:153 | FL finished in 126.55521615000907
INFO flwr 2024-04-18 00:51:34,294 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 00:51:34,295 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 00:51:34,295 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 00:51:34,295 | app.py:229 | app_fit: losses_centralized [(0, 2.3024778366088867), (1, 2.1618051528930664), (2, 2.360142230987549), (3, 2.3602421283721924), (4, 2.3602421283721924), (5, 2.3602421283721924), (6, 2.3602421283721924), (7, 2.3602421283721924), (8, 2.3602421283721924), (9, 2.3602421283721924), (10, 2.3602421283721924)]
INFO flwr 2024-04-18 00:51:34,295 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.2992), (2, 0.101), (3, 0.1009), (4, 0.1009), (5, 0.1009), (6, 0.1009), (7, 0.1009), (8, 0.1009), (9, 0.1009), (10, 0.1009)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1009
wandb:     loss 2.36024
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_004905-94jccxdk
wandb: Find logs at: ./wandb/offline-run-20240418_004905-94jccxdk/logs
INFO flwr 2024-04-18 00:51:37,968 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 00:58:49,092 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=936086)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=936086)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 00:58:54,033	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 00:58:54,949	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 00:58:55,404	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 00:58:55,571	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (11.40MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 00:58:55,861	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ca1a1b23570fd119.zip' (49.69MiB) to Ray cluster...
2024-04-18 00:58:56,043	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ca1a1b23570fd119.zip'.
INFO flwr 2024-04-18 00:59:07,380 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 72833508556.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 159944853300.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-18 00:59:07,380 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 00:59:07,380 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 00:59:07,401 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 00:59:07,403 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 00:59:07,404 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 00:59:07,404 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 00:59:10,597 | server.py:94 | initial parameters (loss, other metrics): 2.3026692867279053, {'accuracy': 0.0876, 'data_size': 10000}
INFO flwr 2024-04-18 00:59:10,598 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 00:59:10,598 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=940459)[0m 2024-04-18 00:59:13.494443: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=940459)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=940459)[0m 2024-04-18 00:59:15.881567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=940459)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=940459)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=940453)[0m 2024-04-18 00:59:13.708115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=940453)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=940453)[0m 2024-04-18 00:59:15.991227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 00:59:32,299 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 00:59:33,868 | server.py:125 | fit progress: (1, 2.1995420455932617, {'accuracy': 0.3781, 'data_size': 10000}, 23.270201646984788)
INFO flwr 2024-04-18 00:59:33,869 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 00:59:33,869 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:59:44,249 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 00:59:45,566 | server.py:125 | fit progress: (2, 1.9174396991729736, {'accuracy': 0.556, 'data_size': 10000}, 34.967958248977084)
INFO flwr 2024-04-18 00:59:45,566 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 00:59:45,567 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 00:59:55,625 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 00:59:57,216 | server.py:125 | fit progress: (3, 1.7070963382720947, {'accuracy': 0.7498, 'data_size': 10000}, 46.61758099999861)
INFO flwr 2024-04-18 00:59:57,216 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 00:59:57,216 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:00:07,524 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 01:00:08,854 | server.py:125 | fit progress: (4, 1.5720702409744263, {'accuracy': 0.8886, 'data_size': 10000}, 58.25642734899884)
INFO flwr 2024-04-18 01:00:08,855 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 01:00:08,855 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:00:18,825 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 01:00:20,333 | server.py:125 | fit progress: (5, 1.5667190551757812, {'accuracy': 0.8943, 'data_size': 10000}, 69.7348103679833)
INFO flwr 2024-04-18 01:00:20,333 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 01:00:20,333 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:00:30,571 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 01:00:32,070 | server.py:125 | fit progress: (6, 1.5517675876617432, {'accuracy': 0.9091, 'data_size': 10000}, 81.4724167859822)
INFO flwr 2024-04-18 01:00:32,071 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 01:00:32,071 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:00:42,262 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 01:00:43,843 | server.py:125 | fit progress: (7, 1.5452147722244263, {'accuracy': 0.9154, 'data_size': 10000}, 93.2446918769856)
INFO flwr 2024-04-18 01:00:43,843 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 01:00:43,843 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:00:52,953 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 01:00:54,450 | server.py:125 | fit progress: (8, 1.5425769090652466, {'accuracy': 0.9185, 'data_size': 10000}, 103.85195408199797)
INFO flwr 2024-04-18 01:00:54,450 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 01:00:54,451 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:01:04,455 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 01:01:05,969 | server.py:125 | fit progress: (9, 1.541521668434143, {'accuracy': 0.9193, 'data_size': 10000}, 115.37101951800287)
INFO flwr 2024-04-18 01:01:05,969 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 01:01:05,970 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:01:15,764 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 01:01:17,079 | server.py:125 | fit progress: (10, 1.547066569328308, {'accuracy': 0.9141, 'data_size': 10000}, 126.48103795098723)
INFO flwr 2024-04-18 01:01:17,079 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 01:01:17,080 | server.py:153 | FL finished in 126.48154154300573
INFO flwr 2024-04-18 01:01:17,080 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 01:01:17,080 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 01:01:17,080 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 01:01:17,080 | app.py:229 | app_fit: losses_centralized [(0, 2.3026692867279053), (1, 2.1995420455932617), (2, 1.9174396991729736), (3, 1.7070963382720947), (4, 1.5720702409744263), (5, 1.5667190551757812), (6, 1.5517675876617432), (7, 1.5452147722244263), (8, 1.5425769090652466), (9, 1.541521668434143), (10, 1.547066569328308)]
INFO flwr 2024-04-18 01:01:17,081 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0876), (1, 0.3781), (2, 0.556), (3, 0.7498), (4, 0.8886), (5, 0.8943), (6, 0.9091), (7, 0.9154), (8, 0.9185), (9, 0.9193), (10, 0.9141)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9141
wandb:     loss 1.54707
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_005848-x43kfzta
wandb: Find logs at: ./wandb/offline-run-20240418_005848-x43kfzta/logs
INFO flwr 2024-04-18 01:01:20,757 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 01:08:31,818 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=940450)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=940450)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 01:08:36,646	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 01:08:37,556	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 01:08:37,999	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 01:08:38,158	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (11.55MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 01:08:38,450	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d799313cb763a97e.zip' (49.85MiB) to Ray cluster...
2024-04-18 01:08:38,633	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d799313cb763a97e.zip'.
INFO flwr 2024-04-18 01:08:49,859 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 72719063040.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 159677813760.0}
INFO flwr 2024-04-18 01:08:49,859 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 01:08:49,859 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 01:08:49,877 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 01:08:49,878 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 01:08:49,878 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 01:08:49,878 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 01:08:53,829 | server.py:94 | initial parameters (loss, other metrics): 2.3026018142700195, {'accuracy': 0.1163, 'data_size': 10000}
INFO flwr 2024-04-18 01:08:53,830 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 01:08:53,832 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=944837)[0m 2024-04-18 01:08:56.244068: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=944837)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=944837)[0m 2024-04-18 01:08:58.606015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=944837)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=944837)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=944833)[0m 2024-04-18 01:08:56.493826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=944833)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=944833)[0m 2024-04-18 01:08:58.817149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 01:09:16,669 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 01:09:17,987 | server.py:125 | fit progress: (1, 2.301543951034546, {'accuracy': 0.1565, 'data_size': 10000}, 24.156049092009198)
INFO flwr 2024-04-18 01:09:17,988 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 01:09:17,988 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:09:28,750 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 01:09:30,296 | server.py:125 | fit progress: (2, 2.298652172088623, {'accuracy': 0.2069, 'data_size': 10000}, 36.46461174401338)
INFO flwr 2024-04-18 01:09:30,296 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 01:09:30,296 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:09:40,545 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 01:09:42,059 | server.py:125 | fit progress: (3, 2.291853427886963, {'accuracy': 0.3602, 'data_size': 10000}, 48.227342010010034)
INFO flwr 2024-04-18 01:09:42,059 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 01:09:42,059 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:09:52,153 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 01:09:53,738 | server.py:125 | fit progress: (4, 2.277346134185791, {'accuracy': 0.5417, 'data_size': 10000}, 59.90687187801814)
INFO flwr 2024-04-18 01:09:53,738 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 01:09:53,739 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:10:03,440 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 01:10:04,989 | server.py:125 | fit progress: (5, 2.249945640563965, {'accuracy': 0.7012, 'data_size': 10000}, 71.15803267000592)
INFO flwr 2024-04-18 01:10:04,989 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 01:10:04,990 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:10:15,207 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 01:10:16,787 | server.py:125 | fit progress: (6, 2.2015328407287598, {'accuracy': 0.7586, 'data_size': 10000}, 82.95622098300373)
INFO flwr 2024-04-18 01:10:16,788 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 01:10:16,788 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:10:26,728 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 01:10:28,078 | server.py:125 | fit progress: (7, 2.1230812072753906, {'accuracy': 0.7768, 'data_size': 10000}, 94.24633168501896)
INFO flwr 2024-04-18 01:10:28,078 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 01:10:28,078 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:10:38,658 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 01:10:40,198 | server.py:125 | fit progress: (8, 2.01670503616333, {'accuracy': 0.778, 'data_size': 10000}, 106.36703767199651)
INFO flwr 2024-04-18 01:10:40,198 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 01:10:40,199 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:10:50,084 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 01:10:51,628 | server.py:125 | fit progress: (9, 1.908052682876587, {'accuracy': 0.7671, 'data_size': 10000}, 117.7963413069956)
INFO flwr 2024-04-18 01:10:51,628 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 01:10:51,628 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:11:01,822 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 01:11:03,403 | server.py:125 | fit progress: (10, 1.824939250946045, {'accuracy': 0.7578, 'data_size': 10000}, 129.57179993001046)
INFO flwr 2024-04-18 01:11:03,403 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 01:11:03,404 | server.py:153 | FL finished in 129.5723915020062
INFO flwr 2024-04-18 01:11:03,404 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 01:11:03,404 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 01:11:03,404 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 01:11:03,404 | app.py:229 | app_fit: losses_centralized [(0, 2.3026018142700195), (1, 2.301543951034546), (2, 2.298652172088623), (3, 2.291853427886963), (4, 2.277346134185791), (5, 2.249945640563965), (6, 2.2015328407287598), (7, 2.1230812072753906), (8, 2.01670503616333), (9, 1.908052682876587), (10, 1.824939250946045)]
INFO flwr 2024-04-18 01:11:03,405 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1163), (1, 0.1565), (2, 0.2069), (3, 0.3602), (4, 0.5417), (5, 0.7012), (6, 0.7586), (7, 0.7768), (8, 0.778), (9, 0.7671), (10, 0.7578)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7578
wandb:     loss 1.82494
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_010831-tb75p9ht
wandb: Find logs at: ./wandb/offline-run-20240418_010831-tb75p9ht/logs
INFO flwr 2024-04-18 01:11:07,090 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 01:18:18,357 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=944828)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=944828)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 01:18:23,122	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 01:18:23,998	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 01:18:24,495	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 01:18:24,662	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (11.69MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 01:18:24,956	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1a95724abb242024.zip' (50.00MiB) to Ray cluster...
2024-04-18 01:18:25,137	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1a95724abb242024.zip'.
INFO flwr 2024-04-18 01:18:36,464 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 74961266688.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'memory': 164909622272.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 01:18:36,464 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 01:18:36,464 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 01:18:36,484 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 01:18:36,485 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 01:18:36,485 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 01:18:36,486 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 01:18:40,377 | server.py:94 | initial parameters (loss, other metrics): 2.302818536758423, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-18 01:18:40,379 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 01:18:40,384 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=954876)[0m 2024-04-18 01:18:42.909123: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=954876)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=954871)[0m 2024-04-18 01:18:45.222917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=954876)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=954876)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=954868)[0m 2024-04-18 01:18:43.103112: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=954868)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=954868)[0m 2024-04-18 01:18:45.440199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 01:19:02,408 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 01:19:04,001 | server.py:125 | fit progress: (1, 2.302741289138794, {'accuracy': 0.1009, 'data_size': 10000}, 23.62014874498709)
INFO flwr 2024-04-18 01:19:04,001 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 01:19:04,002 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:19:15,073 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 01:19:16,409 | server.py:125 | fit progress: (2, 2.3026273250579834, {'accuracy': 0.1009, 'data_size': 10000}, 36.02827357998467)
INFO flwr 2024-04-18 01:19:16,410 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 01:19:16,410 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:19:26,677 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 01:19:28,235 | server.py:125 | fit progress: (3, 2.3024775981903076, {'accuracy': 0.1009, 'data_size': 10000}, 47.853434276999906)
INFO flwr 2024-04-18 01:19:28,235 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 01:19:28,235 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:19:38,600 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 01:19:39,907 | server.py:125 | fit progress: (4, 2.3022921085357666, {'accuracy': 0.1009, 'data_size': 10000}, 59.525711634982144)
INFO flwr 2024-04-18 01:19:39,907 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 01:19:39,907 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:19:49,684 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 01:19:51,211 | server.py:125 | fit progress: (5, 2.302067518234253, {'accuracy': 0.1009, 'data_size': 10000}, 70.82975109800464)
INFO flwr 2024-04-18 01:19:51,211 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 01:19:51,211 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:20:01,394 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 01:20:02,719 | server.py:125 | fit progress: (6, 2.301795721054077, {'accuracy': 0.1009, 'data_size': 10000}, 82.33752910100156)
INFO flwr 2024-04-18 01:20:02,719 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 01:20:02,719 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:20:12,549 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 01:20:14,070 | server.py:125 | fit progress: (7, 2.30147647857666, {'accuracy': 0.1009, 'data_size': 10000}, 93.68859602499288)
INFO flwr 2024-04-18 01:20:14,070 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 01:20:14,070 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:20:24,238 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 01:20:25,547 | server.py:125 | fit progress: (8, 2.301100492477417, {'accuracy': 0.1011, 'data_size': 10000}, 105.16583349899156)
INFO flwr 2024-04-18 01:20:25,563 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 01:20:25,564 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:20:35,635 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 01:20:37,031 | server.py:125 | fit progress: (9, 2.3006672859191895, {'accuracy': 0.1029, 'data_size': 10000}, 116.65024612098932)
INFO flwr 2024-04-18 01:20:37,032 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 01:20:37,032 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:20:47,641 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 01:20:49,227 | server.py:125 | fit progress: (10, 2.300170421600342, {'accuracy': 0.1099, 'data_size': 10000}, 128.84637608600315)
INFO flwr 2024-04-18 01:20:49,228 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 01:20:49,228 | server.py:153 | FL finished in 128.84680884500267
INFO flwr 2024-04-18 01:20:49,228 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 01:20:49,228 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 01:20:49,228 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 01:20:49,228 | app.py:229 | app_fit: losses_centralized [(0, 2.302818536758423), (1, 2.302741289138794), (2, 2.3026273250579834), (3, 2.3024775981903076), (4, 2.3022921085357666), (5, 2.302067518234253), (6, 2.301795721054077), (7, 2.30147647857666), (8, 2.301100492477417), (9, 2.3006672859191895), (10, 2.300170421600342)]
INFO flwr 2024-04-18 01:20:49,229 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.1009), (2, 0.1009), (3, 0.1009), (4, 0.1009), (5, 0.1009), (6, 0.1009), (7, 0.1009), (8, 0.1011), (9, 0.1029), (10, 0.1099)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1099
wandb:     loss 2.30017
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_011817-wnnh5ijv
wandb: Find logs at: ./wandb/offline-run-20240418_011817-wnnh5ijv/logs
INFO flwr 2024-04-18 01:20:52,904 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 01:28:04,369 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=954864)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=954864)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 01:28:09,152	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 01:28:10,045	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 01:28:10,531	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 01:28:10,703	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (12.17MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 01:28:11,000	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_79adf4454807efd3.zip' (50.50MiB) to Ray cluster...
2024-04-18 01:28:11,183	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_79adf4454807efd3.zip'.
INFO flwr 2024-04-18 01:28:22,427 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 74999643340.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 164999167796.0}
INFO flwr 2024-04-18 01:28:22,428 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 01:28:22,428 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 01:28:22,451 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 01:28:22,452 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 01:28:22,452 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 01:28:22,453 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 01:28:26,055 | server.py:94 | initial parameters (loss, other metrics): 2.302603244781494, {'accuracy': 0.0907, 'data_size': 10000}
INFO flwr 2024-04-18 01:28:26,055 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 01:28:26,056 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=959250)[0m 2024-04-18 01:28:28.683377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=959250)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=959248)[0m 2024-04-18 01:28:31.009740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=959249)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=959249)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=959249)[0m 2024-04-18 01:28:28.819384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=959249)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=959255)[0m 2024-04-18 01:28:31.369998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 01:28:48,059 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 01:28:49,377 | server.py:125 | fit progress: (1, 2.302596092224121, {'accuracy': 0.0913, 'data_size': 10000}, 23.320995154994307)
INFO flwr 2024-04-18 01:28:49,377 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 01:28:49,377 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:29:00,149 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 01:29:01,490 | server.py:125 | fit progress: (2, 2.302586793899536, {'accuracy': 0.0916, 'data_size': 10000}, 35.434204602992395)
INFO flwr 2024-04-18 01:29:01,490 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 01:29:01,490 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:29:11,372 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 01:29:12,861 | server.py:125 | fit progress: (3, 2.3025758266448975, {'accuracy': 0.0926, 'data_size': 10000}, 46.80530786598683)
INFO flwr 2024-04-18 01:29:12,861 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 01:29:12,862 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:29:22,656 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 01:29:24,156 | server.py:125 | fit progress: (4, 2.3025641441345215, {'accuracy': 0.0934, 'data_size': 10000}, 58.10011517399107)
INFO flwr 2024-04-18 01:29:24,156 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 01:29:24,156 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:29:34,100 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 01:29:35,417 | server.py:125 | fit progress: (5, 2.30255126953125, {'accuracy': 0.0943, 'data_size': 10000}, 69.36164150299737)
INFO flwr 2024-04-18 01:29:35,418 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 01:29:35,418 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:29:45,245 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 01:29:46,811 | server.py:125 | fit progress: (6, 2.302537441253662, {'accuracy': 0.0952, 'data_size': 10000}, 80.75526785000693)
INFO flwr 2024-04-18 01:29:46,811 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 01:29:46,811 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:29:56,553 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 01:29:57,826 | server.py:125 | fit progress: (7, 2.302523136138916, {'accuracy': 0.0962, 'data_size': 10000}, 91.77073153399397)
INFO flwr 2024-04-18 01:29:57,827 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 01:29:57,827 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:30:07,977 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 01:30:09,257 | server.py:125 | fit progress: (8, 2.302509069442749, {'accuracy': 0.0974, 'data_size': 10000}, 103.20189059298718)
INFO flwr 2024-04-18 01:30:09,258 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 01:30:09,258 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:30:19,238 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 01:30:20,768 | server.py:125 | fit progress: (9, 2.3024933338165283, {'accuracy': 0.0986, 'data_size': 10000}, 114.71211749099893)
INFO flwr 2024-04-18 01:30:20,768 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 01:30:20,768 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:30:31,095 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 01:30:32,407 | server.py:125 | fit progress: (10, 2.302478313446045, {'accuracy': 0.0989, 'data_size': 10000}, 126.3515837189916)
INFO flwr 2024-04-18 01:30:32,407 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 01:30:32,408 | server.py:153 | FL finished in 126.35200944900862
INFO flwr 2024-04-18 01:30:32,408 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 01:30:32,408 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 01:30:32,408 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 01:30:32,408 | app.py:229 | app_fit: losses_centralized [(0, 2.302603244781494), (1, 2.302596092224121), (2, 2.302586793899536), (3, 2.3025758266448975), (4, 2.3025641441345215), (5, 2.30255126953125), (6, 2.302537441253662), (7, 2.302523136138916), (8, 2.302509069442749), (9, 2.3024933338165283), (10, 2.302478313446045)]
INFO flwr 2024-04-18 01:30:32,408 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0907), (1, 0.0913), (2, 0.0916), (3, 0.0926), (4, 0.0934), (5, 0.0943), (6, 0.0952), (7, 0.0962), (8, 0.0974), (9, 0.0986), (10, 0.0989)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0989
wandb:     loss 2.30248
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_012804-ve07et77
wandb: Find logs at: ./wandb/offline-run-20240418_012804-ve07et77/logs
[2m[36m(DefaultActor pid=959245)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=959245)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 01:31:40.021570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-18 01:31:41.322315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 01:31:51,035 | batch_run_simulation.py:80 | Loaded 180 configs with name MINST-2NN-FEDADAM, running...
INFO flwr 2024-04-18 01:31:51,035 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 01:39:20,834 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 01:39:23,649	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 01:39:24,623	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 01:39:25,072	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 01:39:25,228	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (12.66MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 01:39:25,531	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e1313016614577a0.zip' (50.99MiB) to Ray cluster...
2024-04-18 01:39:25,716	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e1313016614577a0.zip'.
INFO flwr 2024-04-18 01:39:37,046 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 75037913088.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 165088463872.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 01:39:37,046 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 01:39:37,047 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 01:39:37,065 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 01:39:37,066 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 01:39:37,066 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 01:39:37,066 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=963682)[0m 2024-04-18 01:39:43.053033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=963682)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=963682)[0m 2024-04-18 01:39:45.392057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 01:39:48,374 | server.py:94 | initial parameters (loss, other metrics): 2.302238702774048, {'accuracy': 0.1222, 'data_size': 10000}
INFO flwr 2024-04-18 01:39:48,375 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 01:39:48,375 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=963689)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=963689)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=963684)[0m 2024-04-18 01:39:43.431413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=963684)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=963684)[0m 2024-04-18 01:39:45.867685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=963682)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=963682)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 01:40:07,480 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 01:40:11,811 | server.py:125 | fit progress: (1, 1.743434190750122, {'accuracy': 0.7178, 'data_size': 10000}, 23.436569109995617)
INFO flwr 2024-04-18 01:40:11,812 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 01:40:11,812 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:40:24,419 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 01:40:29,052 | server.py:125 | fit progress: (2, 2.0447120666503906, {'accuracy': 0.4164, 'data_size': 10000}, 40.67674159401213)
INFO flwr 2024-04-18 01:40:29,052 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 01:40:29,052 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:40:40,570 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 01:40:45,176 | server.py:125 | fit progress: (3, 2.248443126678467, {'accuracy': 0.2127, 'data_size': 10000}, 56.80148198400275)
INFO flwr 2024-04-18 01:40:45,177 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 01:40:45,177 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:40:56,670 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 01:41:01,153 | server.py:125 | fit progress: (4, 2.301142692565918, {'accuracy': 0.16, 'data_size': 10000}, 72.77797252999153)
INFO flwr 2024-04-18 01:41:01,153 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 01:41:01,154 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:41:13,242 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 01:41:17,564 | server.py:125 | fit progress: (5, 2.3285422325134277, {'accuracy': 0.1326, 'data_size': 10000}, 89.18947986699641)
INFO flwr 2024-04-18 01:41:17,565 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 01:41:17,565 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:41:29,043 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 01:41:33,335 | server.py:125 | fit progress: (6, 2.3443422317504883, {'accuracy': 0.1168, 'data_size': 10000}, 104.9605188590067)
INFO flwr 2024-04-18 01:41:33,336 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 01:41:33,336 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:41:45,509 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 01:41:50,100 | server.py:125 | fit progress: (7, 2.351842164993286, {'accuracy': 0.1093, 'data_size': 10000}, 121.72508960301639)
INFO flwr 2024-04-18 01:41:50,100 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 01:41:50,101 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:42:01,911 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 01:42:06,270 | server.py:125 | fit progress: (8, 2.3559422492980957, {'accuracy': 0.1052, 'data_size': 10000}, 137.8954394000175)
INFO flwr 2024-04-18 01:42:06,271 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 01:42:06,271 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:42:18,376 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 01:42:22,933 | server.py:125 | fit progress: (9, 2.3585422039031982, {'accuracy': 0.1026, 'data_size': 10000}, 154.5579615480092)
INFO flwr 2024-04-18 01:42:22,933 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 01:42:22,934 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:42:35,672 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 01:42:40,004 | server.py:125 | fit progress: (10, 2.3599421977996826, {'accuracy': 0.1012, 'data_size': 10000}, 171.62888748699334)
INFO flwr 2024-04-18 01:42:40,004 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 01:42:40,004 | server.py:153 | FL finished in 171.62940416199854
INFO flwr 2024-04-18 01:42:40,007 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 01:42:40,008 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 01:42:40,008 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 01:42:40,008 | app.py:229 | app_fit: losses_centralized [(0, 2.302238702774048), (1, 1.743434190750122), (2, 2.0447120666503906), (3, 2.248443126678467), (4, 2.301142692565918), (5, 2.3285422325134277), (6, 2.3443422317504883), (7, 2.351842164993286), (8, 2.3559422492980957), (9, 2.3585422039031982), (10, 2.3599421977996826)]
INFO flwr 2024-04-18 01:42:40,008 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1222), (1, 0.7178), (2, 0.4164), (3, 0.2127), (4, 0.16), (5, 0.1326), (6, 0.1168), (7, 0.1093), (8, 0.1052), (9, 0.1026), (10, 0.1012)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1012
wandb:     loss 2.35994
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_013920-c44maq2d
wandb: Find logs at: ./wandb/offline-run-20240418_013920-c44maq2d/logs
INFO flwr 2024-04-18 01:42:43,668 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 01:50:12,254 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=963673)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=963673)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 01:50:17,252	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 01:50:18,296	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 01:50:18,756	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 01:50:18,912	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (13.14MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 01:50:19,200	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_43d8233bc8e59743.zip' (51.49MiB) to Ray cluster...
2024-04-18 01:50:19,382	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_43d8233bc8e59743.zip'.
INFO flwr 2024-04-18 01:50:30,657 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 74775382425.0, 'memory': 164475892327.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 01:50:30,657 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 01:50:30,658 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 01:50:30,677 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 01:50:30,678 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 01:50:30,678 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 01:50:30,679 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=968463)[0m 2024-04-18 01:50:36.812689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=968463)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=968460)[0m 2024-04-18 01:50:39.090583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 01:50:40,304 | server.py:94 | initial parameters (loss, other metrics): 2.3024935722351074, {'accuracy': 0.1005, 'data_size': 10000}
INFO flwr 2024-04-18 01:50:40,304 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 01:50:40,304 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=968470)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=968470)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=968461)[0m 2024-04-18 01:50:37.001895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=968461)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=968461)[0m 2024-04-18 01:50:39.270596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=968463)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=968463)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 01:50:58,995 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 01:51:03,629 | server.py:125 | fit progress: (1, 1.910457968711853, {'accuracy': 0.5409, 'data_size': 10000}, 23.324774337001145)
INFO flwr 2024-04-18 01:51:03,629 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 01:51:03,630 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:51:15,585 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 01:51:19,893 | server.py:125 | fit progress: (2, 1.6235936880111694, {'accuracy': 0.8368, 'data_size': 10000}, 39.588926834985614)
INFO flwr 2024-04-18 01:51:19,893 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 01:51:19,894 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:51:31,371 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 01:51:35,891 | server.py:125 | fit progress: (3, 1.5144284963607788, {'accuracy': 0.949, 'data_size': 10000}, 55.58695256599458)
INFO flwr 2024-04-18 01:51:35,892 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 01:51:35,892 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:51:47,595 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 01:51:52,164 | server.py:125 | fit progress: (4, 1.4972715377807617, {'accuracy': 0.9638, 'data_size': 10000}, 71.85980362998089)
INFO flwr 2024-04-18 01:51:52,164 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 01:51:52,165 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:52:03,625 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 01:52:07,999 | server.py:125 | fit progress: (5, 1.489043116569519, {'accuracy': 0.9724, 'data_size': 10000}, 87.69488129200181)
INFO flwr 2024-04-18 01:52:07,999 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 01:52:08,000 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:52:20,068 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 01:52:24,553 | server.py:125 | fit progress: (6, 1.4896514415740967, {'accuracy': 0.9717, 'data_size': 10000}, 104.24891539500095)
INFO flwr 2024-04-18 01:52:24,553 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 01:52:24,554 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:52:36,213 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 01:52:40,500 | server.py:125 | fit progress: (7, 1.4880962371826172, {'accuracy': 0.9732, 'data_size': 10000}, 120.1958074309805)
INFO flwr 2024-04-18 01:52:40,501 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 01:52:40,501 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:52:52,956 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 01:52:57,517 | server.py:125 | fit progress: (8, 1.4863359928131104, {'accuracy': 0.9745, 'data_size': 10000}, 137.2123343849962)
INFO flwr 2024-04-18 01:52:57,517 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 01:52:57,517 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:53:08,935 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 01:53:13,537 | server.py:125 | fit progress: (9, 1.4859565496444702, {'accuracy': 0.9751, 'data_size': 10000}, 153.23269093199633)
INFO flwr 2024-04-18 01:53:13,537 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 01:53:13,538 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 01:53:25,622 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 01:53:30,214 | server.py:125 | fit progress: (10, 1.4874515533447266, {'accuracy': 0.9736, 'data_size': 10000}, 169.91001278100885)
INFO flwr 2024-04-18 01:53:30,215 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 01:53:30,215 | server.py:153 | FL finished in 169.91049922199454
INFO flwr 2024-04-18 01:53:30,217 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 01:53:30,217 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 01:53:30,218 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 01:53:30,218 | app.py:229 | app_fit: losses_centralized [(0, 2.3024935722351074), (1, 1.910457968711853), (2, 1.6235936880111694), (3, 1.5144284963607788), (4, 1.4972715377807617), (5, 1.489043116569519), (6, 1.4896514415740967), (7, 1.4880962371826172), (8, 1.4863359928131104), (9, 1.4859565496444702), (10, 1.4874515533447266)]
INFO flwr 2024-04-18 01:53:30,218 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1005), (1, 0.5409), (2, 0.8368), (3, 0.949), (4, 0.9638), (5, 0.9724), (6, 0.9717), (7, 0.9732), (8, 0.9745), (9, 0.9751), (10, 0.9736)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9736
wandb:     loss 1.48745
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_015011-o7gtmn3p
wandb: Find logs at: ./wandb/offline-run-20240418_015011-o7gtmn3p/logs
INFO flwr 2024-04-18 01:53:33,858 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 02:01:02,602 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=968460)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=968460)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 02:01:07,315	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 02:01:08,278	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 02:01:08,746	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 02:01:08,904	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (13.63MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 02:01:09,279	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8a065a26d1428bfe.zip' (51.99MiB) to Ray cluster...
2024-04-18 02:01:09,462	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8a065a26d1428bfe.zip'.
INFO flwr 2024-04-18 02:01:20,802 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 164472741274.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 74774031974.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 02:01:20,802 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 02:01:20,802 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 02:01:20,820 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 02:01:20,822 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 02:01:20,822 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 02:01:20,822 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=973359)[0m 2024-04-18 02:01:26.913335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=973359)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=973365)[0m 2024-04-18 02:01:29.196792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 02:01:29,773 | server.py:94 | initial parameters (loss, other metrics): 2.3026363849639893, {'accuracy': 0.096, 'data_size': 10000}
INFO flwr 2024-04-18 02:01:29,773 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 02:01:29,773 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=973370)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=973370)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=973361)[0m 2024-04-18 02:01:27.160079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=973361)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=973361)[0m 2024-04-18 02:01:29.391854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=973363)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=973363)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 02:01:49,555 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 02:01:54,096 | server.py:125 | fit progress: (1, 2.2930283546447754, {'accuracy': 0.4815, 'data_size': 10000}, 24.3232759120001)
INFO flwr 2024-04-18 02:01:54,097 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 02:01:54,097 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:02:06,758 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 02:02:11,249 | server.py:125 | fit progress: (2, 2.2713420391082764, {'accuracy': 0.669, 'data_size': 10000}, 41.47538946699933)
INFO flwr 2024-04-18 02:02:11,249 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 02:02:11,249 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:02:21,702 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 02:02:25,986 | server.py:125 | fit progress: (3, 2.226874589920044, {'accuracy': 0.7885, 'data_size': 10000}, 56.21275829098886)
INFO flwr 2024-04-18 02:02:25,986 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 02:02:25,986 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:02:37,912 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 02:02:42,286 | server.py:125 | fit progress: (4, 2.1467134952545166, {'accuracy': 0.8595, 'data_size': 10000}, 72.51290152099682)
INFO flwr 2024-04-18 02:02:42,286 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 02:02:42,287 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:02:54,965 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 02:02:59,404 | server.py:125 | fit progress: (5, 2.0216245651245117, {'accuracy': 0.9012, 'data_size': 10000}, 89.63073477201397)
INFO flwr 2024-04-18 02:02:59,404 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 02:02:59,405 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:03:11,412 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 02:03:15,989 | server.py:125 | fit progress: (6, 1.8663164377212524, {'accuracy': 0.9278, 'data_size': 10000}, 106.2153494140075)
INFO flwr 2024-04-18 02:03:15,989 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 02:03:15,989 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:03:28,931 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 02:03:33,367 | server.py:125 | fit progress: (7, 1.717841625213623, {'accuracy': 0.9415, 'data_size': 10000}, 123.59399868201581)
INFO flwr 2024-04-18 02:03:33,368 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 02:03:33,368 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:03:44,992 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 02:03:49,465 | server.py:125 | fit progress: (8, 1.6159030199050903, {'accuracy': 0.9505, 'data_size': 10000}, 139.69215451501077)
INFO flwr 2024-04-18 02:03:49,466 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 02:03:49,466 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:04:01,498 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 02:04:06,161 | server.py:125 | fit progress: (9, 1.5572161674499512, {'accuracy': 0.956, 'data_size': 10000}, 156.3879182279925)
INFO flwr 2024-04-18 02:04:06,162 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 02:04:06,162 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:04:17,354 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 02:04:21,902 | server.py:125 | fit progress: (10, 1.5270686149597168, {'accuracy': 0.9607, 'data_size': 10000}, 172.12876375499764)
INFO flwr 2024-04-18 02:04:21,902 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 02:04:21,902 | server.py:153 | FL finished in 172.1293162050133
INFO flwr 2024-04-18 02:04:21,907 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 02:04:21,907 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 02:04:21,908 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 02:04:21,908 | app.py:229 | app_fit: losses_centralized [(0, 2.3026363849639893), (1, 2.2930283546447754), (2, 2.2713420391082764), (3, 2.226874589920044), (4, 2.1467134952545166), (5, 2.0216245651245117), (6, 1.8663164377212524), (7, 1.717841625213623), (8, 1.6159030199050903), (9, 1.5572161674499512), (10, 1.5270686149597168)]
INFO flwr 2024-04-18 02:04:21,908 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.096), (1, 0.4815), (2, 0.669), (3, 0.7885), (4, 0.8595), (5, 0.9012), (6, 0.9278), (7, 0.9415), (8, 0.9505), (9, 0.956), (10, 0.9607)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9607
wandb:     loss 1.52707
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_020102-1ft7twce
wandb: Find logs at: ./wandb/offline-run-20240418_020102-1ft7twce/logs
INFO flwr 2024-04-18 02:04:25,715 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 02:11:49,512 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=973359)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=973359)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 02:11:54,153	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 02:11:55,052	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 02:11:55,570	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 02:11:55,722	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (14.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 02:11:55,997	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5637eefcd80b1f97.zip' (52.49MiB) to Ray cluster...
2024-04-18 02:11:56,173	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5637eefcd80b1f97.zip'.
INFO flwr 2024-04-18 02:12:07,299 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169546363495.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 76948441497.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-18 02:12:07,299 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 02:12:07,300 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 02:12:07,320 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 02:12:07,323 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 02:12:07,323 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 02:12:07,323 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=983410)[0m 2024-04-18 02:12:13.241641: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=983410)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-18 02:12:15,984 | server.py:94 | initial parameters (loss, other metrics): 2.302727460861206, {'accuracy': 0.1025, 'data_size': 10000}
INFO flwr 2024-04-18 02:12:15,984 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 02:12:15,985 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=983410)[0m 2024-04-18 02:12:15.996284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=983415)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=983415)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=983406)[0m 2024-04-18 02:12:13.697423: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=983406)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=983403)[0m 2024-04-18 02:12:16.045395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=983404)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=983404)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-18 02:12:35,338 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 02:12:39,556 | server.py:125 | fit progress: (1, 2.3019814491271973, {'accuracy': 0.1465, 'data_size': 10000}, 23.57085752498824)
INFO flwr 2024-04-18 02:12:39,556 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 02:12:39,556 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:12:51,902 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 02:12:55,995 | server.py:125 | fit progress: (2, 2.300848960876465, {'accuracy': 0.26, 'data_size': 10000}, 40.009887637977954)
INFO flwr 2024-04-18 02:12:55,995 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 02:12:55,995 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:13:06,968 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 02:13:11,162 | server.py:125 | fit progress: (3, 2.299345016479492, {'accuracy': 0.3679, 'data_size': 10000}, 55.17753120898851)
INFO flwr 2024-04-18 02:13:11,163 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 02:13:11,163 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:13:21,955 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 02:13:25,905 | server.py:125 | fit progress: (4, 2.2974772453308105, {'accuracy': 0.4967, 'data_size': 10000}, 69.91984996999963)
INFO flwr 2024-04-18 02:13:25,905 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 02:13:25,905 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:13:37,557 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 02:13:41,429 | server.py:125 | fit progress: (5, 2.2952494621276855, {'accuracy': 0.5855, 'data_size': 10000}, 85.44417195700225)
INFO flwr 2024-04-18 02:13:41,429 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 02:13:41,429 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:13:53,264 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 02:13:57,393 | server.py:125 | fit progress: (6, 2.2926414012908936, {'accuracy': 0.6524, 'data_size': 10000}, 101.40792541997507)
INFO flwr 2024-04-18 02:13:57,393 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 02:13:57,393 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:14:08,808 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 02:14:12,757 | server.py:125 | fit progress: (7, 2.2896604537963867, {'accuracy': 0.7026, 'data_size': 10000}, 116.7724353359954)
INFO flwr 2024-04-18 02:14:12,758 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 02:14:12,758 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:14:23,785 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 02:14:27,888 | server.py:125 | fit progress: (8, 2.286280870437622, {'accuracy': 0.7349, 'data_size': 10000}, 131.90285503998166)
INFO flwr 2024-04-18 02:14:27,888 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 02:14:27,888 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:14:38,509 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 02:14:42,596 | server.py:125 | fit progress: (9, 2.2824833393096924, {'accuracy': 0.7573, 'data_size': 10000}, 146.6114618309948)
INFO flwr 2024-04-18 02:14:42,596 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 02:14:42,597 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:14:55,102 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 02:14:59,223 | server.py:125 | fit progress: (10, 2.2782559394836426, {'accuracy': 0.7819, 'data_size': 10000}, 163.23825898498762)
INFO flwr 2024-04-18 02:14:59,223 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 02:14:59,223 | server.py:153 | FL finished in 163.23876259400276
INFO flwr 2024-04-18 02:14:59,228 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 02:14:59,228 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 02:14:59,228 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 02:14:59,228 | app.py:229 | app_fit: losses_centralized [(0, 2.302727460861206), (1, 2.3019814491271973), (2, 2.300848960876465), (3, 2.299345016479492), (4, 2.2974772453308105), (5, 2.2952494621276855), (6, 2.2926414012908936), (7, 2.2896604537963867), (8, 2.286280870437622), (9, 2.2824833393096924), (10, 2.2782559394836426)]
INFO flwr 2024-04-18 02:14:59,229 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1025), (1, 0.1465), (2, 0.26), (3, 0.3679), (4, 0.4967), (5, 0.5855), (6, 0.6524), (7, 0.7026), (8, 0.7349), (9, 0.7573), (10, 0.7819)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7819
wandb:     loss 2.27826
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_021149-s46hvmoy
wandb: Find logs at: ./wandb/offline-run-20240418_021149-s46hvmoy/logs
INFO flwr 2024-04-18 02:15:02,758 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 02:22:26,901 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=983403)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=983403)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-18 02:22:31,425	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 02:22:32,467	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 02:22:32,927	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 02:22:33,080	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (14.60MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 02:22:33,375	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5ea091711aa23715.zip' (52.99MiB) to Ray cluster...
2024-04-18 02:22:33,557	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5ea091711aa23715.zip'.
INFO flwr 2024-04-18 02:22:44,768 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169582940365.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 76964117299.0}
INFO flwr 2024-04-18 02:22:44,768 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 02:22:44,768 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 02:22:44,786 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 02:22:44,788 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 02:22:44,789 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 02:22:44,789 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=987854)[0m 2024-04-18 02:22:50.751658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=987854)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=987854)[0m 2024-04-18 02:22:53.039349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 02:22:54,646 | server.py:94 | initial parameters (loss, other metrics): 2.302785634994507, {'accuracy': 0.0847, 'data_size': 10000}
INFO flwr 2024-04-18 02:22:54,647 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 02:22:54,647 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=987867)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=987867)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=987862)[0m 2024-04-18 02:22:51.039539: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=987862)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=987862)[0m 2024-04-18 02:22:53.393125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=987857)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=987857)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 02:23:13,842 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 02:23:17,935 | server.py:125 | fit progress: (1, 2.3027288913726807, {'accuracy': 0.087, 'data_size': 10000}, 23.28791527400608)
INFO flwr 2024-04-18 02:23:17,935 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 02:23:17,935 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:23:30,529 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 02:23:34,719 | server.py:125 | fit progress: (2, 2.302647829055786, {'accuracy': 0.0899, 'data_size': 10000}, 40.07196561098681)
INFO flwr 2024-04-18 02:23:34,719 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 02:23:34,719 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:23:45,185 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 02:23:49,292 | server.py:125 | fit progress: (3, 2.3025529384613037, {'accuracy': 0.0957, 'data_size': 10000}, 54.64534315900528)
INFO flwr 2024-04-18 02:23:49,292 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 02:23:49,293 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:24:00,775 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 02:24:04,715 | server.py:125 | fit progress: (4, 2.302452325820923, {'accuracy': 0.1035, 'data_size': 10000}, 70.06843737399322)
INFO flwr 2024-04-18 02:24:04,715 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 02:24:04,716 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:24:15,486 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 02:24:19,386 | server.py:125 | fit progress: (5, 2.3023452758789062, {'accuracy': 0.1104, 'data_size': 10000}, 84.73898382799234)
INFO flwr 2024-04-18 02:24:19,386 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 02:24:19,386 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:24:31,154 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 02:24:35,258 | server.py:125 | fit progress: (6, 2.3022327423095703, {'accuracy': 0.1213, 'data_size': 10000}, 100.61076557799242)
INFO flwr 2024-04-18 02:24:35,258 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 02:24:35,258 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:24:46,705 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 02:24:50,584 | server.py:125 | fit progress: (7, 2.3021109104156494, {'accuracy': 0.1321, 'data_size': 10000}, 115.93748450200655)
INFO flwr 2024-04-18 02:24:50,585 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 02:24:50,585 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:25:02,088 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 02:25:06,184 | server.py:125 | fit progress: (8, 2.30198335647583, {'accuracy': 0.1418, 'data_size': 10000}, 131.53745138298837)
INFO flwr 2024-04-18 02:25:06,185 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 02:25:06,185 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:25:17,085 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 02:25:21,161 | server.py:125 | fit progress: (9, 2.301849126815796, {'accuracy': 0.1505, 'data_size': 10000}, 146.51440242299577)
INFO flwr 2024-04-18 02:25:21,162 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 02:25:21,162 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:25:32,065 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 02:25:36,159 | server.py:125 | fit progress: (10, 2.301710844039917, {'accuracy': 0.1604, 'data_size': 10000}, 161.5122085409821)
INFO flwr 2024-04-18 02:25:36,159 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 02:25:36,159 | server.py:153 | FL finished in 161.5126773029915
INFO flwr 2024-04-18 02:25:36,166 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 02:25:36,166 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 02:25:36,167 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 02:25:36,167 | app.py:229 | app_fit: losses_centralized [(0, 2.302785634994507), (1, 2.3027288913726807), (2, 2.302647829055786), (3, 2.3025529384613037), (4, 2.302452325820923), (5, 2.3023452758789062), (6, 2.3022327423095703), (7, 2.3021109104156494), (8, 2.30198335647583), (9, 2.301849126815796), (10, 2.301710844039917)]
INFO flwr 2024-04-18 02:25:36,167 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0847), (1, 0.087), (2, 0.0899), (3, 0.0957), (4, 0.1035), (5, 0.1104), (6, 0.1213), (7, 0.1321), (8, 0.1418), (9, 0.1505), (10, 0.1604)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1604
wandb:     loss 2.30171
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_022226-9iakho46
wandb: Find logs at: ./wandb/offline-run-20240418_022226-9iakho46/logs
INFO flwr 2024-04-18 02:25:39,680 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 02:33:03,446 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=987854)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=987854)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 02:33:08,063	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 02:33:09,064	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 02:33:09,527	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 02:33:09,686	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (15.09MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 02:33:09,964	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_de623d4ceb85299e.zip' (53.49MiB) to Ray cluster...
2024-04-18 02:33:10,144	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_de623d4ceb85299e.zip'.
INFO flwr 2024-04-18 02:33:21,278 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 76948989542.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169547642266.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-18 02:33:21,279 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 02:33:21,279 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 02:33:21,302 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 02:33:21,304 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 02:33:21,304 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 02:33:21,304 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=992220)[0m 2024-04-18 02:33:27.239303: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=992220)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=992222)[0m 2024-04-18 02:33:29.582171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 02:33:31,373 | server.py:94 | initial parameters (loss, other metrics): 2.302922487258911, {'accuracy': 0.1152, 'data_size': 10000}
INFO flwr 2024-04-18 02:33:31,373 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 02:33:31,373 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=992230)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=992230)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=992225)[0m 2024-04-18 02:33:27.497337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=992225)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=992225)[0m 2024-04-18 02:33:29.792363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=992225)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=992225)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 02:33:50,743 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 02:33:54,868 | server.py:125 | fit progress: (1, 1.711664080619812, {'accuracy': 0.7494, 'data_size': 10000}, 23.495168207999086)
INFO flwr 2024-04-18 02:33:54,869 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 02:33:54,869 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:34:06,617 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 02:34:10,782 | server.py:125 | fit progress: (2, 1.9970406293869019, {'accuracy': 0.4641, 'data_size': 10000}, 39.40887310000835)
INFO flwr 2024-04-18 02:34:10,782 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 02:34:10,783 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:34:21,479 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 02:34:25,561 | server.py:125 | fit progress: (3, 2.1118438243865967, {'accuracy': 0.3493, 'data_size': 10000}, 54.18810222801403)
INFO flwr 2024-04-18 02:34:25,562 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 02:34:25,562 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:34:36,684 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 02:34:40,589 | server.py:125 | fit progress: (4, 2.172243118286133, {'accuracy': 0.2889, 'data_size': 10000}, 69.21578910699463)
INFO flwr 2024-04-18 02:34:40,589 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 02:34:40,590 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:34:51,248 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 02:34:55,101 | server.py:125 | fit progress: (5, 2.236743450164795, {'accuracy': 0.2244, 'data_size': 10000}, 83.72748255301849)
INFO flwr 2024-04-18 02:34:55,101 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 02:34:55,101 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:35:06,986 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 02:35:11,096 | server.py:125 | fit progress: (6, 2.2843430042266846, {'accuracy': 0.1768, 'data_size': 10000}, 99.72277924400987)
INFO flwr 2024-04-18 02:35:11,096 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 02:35:11,096 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:35:21,828 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 02:35:25,830 | server.py:125 | fit progress: (7, 2.303642749786377, {'accuracy': 0.1575, 'data_size': 10000}, 114.45665058400482)
INFO flwr 2024-04-18 02:35:25,830 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 02:35:25,831 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:35:36,460 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 02:35:40,642 | server.py:125 | fit progress: (8, 2.3169426918029785, {'accuracy': 0.1442, 'data_size': 10000}, 129.26927866099868)
INFO flwr 2024-04-18 02:35:40,643 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 02:35:40,643 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:35:51,866 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 02:35:56,007 | server.py:125 | fit progress: (9, 2.3246426582336426, {'accuracy': 0.1365, 'data_size': 10000}, 144.63407876901329)
INFO flwr 2024-04-18 02:35:56,008 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 02:35:56,008 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:36:07,183 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 02:36:11,376 | server.py:125 | fit progress: (10, 2.3304426670074463, {'accuracy': 0.1307, 'data_size': 10000}, 160.00285448599607)
INFO flwr 2024-04-18 02:36:11,376 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 02:36:11,376 | server.py:153 | FL finished in 160.003346426005
INFO flwr 2024-04-18 02:36:11,379 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 02:36:11,379 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 02:36:11,379 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 02:36:11,379 | app.py:229 | app_fit: losses_centralized [(0, 2.302922487258911), (1, 1.711664080619812), (2, 1.9970406293869019), (3, 2.1118438243865967), (4, 2.172243118286133), (5, 2.236743450164795), (6, 2.2843430042266846), (7, 2.303642749786377), (8, 2.3169426918029785), (9, 2.3246426582336426), (10, 2.3304426670074463)]
INFO flwr 2024-04-18 02:36:11,379 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1152), (1, 0.7494), (2, 0.4641), (3, 0.3493), (4, 0.2889), (5, 0.2244), (6, 0.1768), (7, 0.1575), (8, 0.1442), (9, 0.1365), (10, 0.1307)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1307
wandb:     loss 2.33044
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_023303-mnv4prat
wandb: Find logs at: ./wandb/offline-run-20240418_023303-mnv4prat/logs
INFO flwr 2024-04-18 02:36:14,900 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 02:43:38,979 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=992218)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=992218)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 02:43:43,581	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 02:43:44,583	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 02:43:45,069	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 02:43:45,223	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (15.57MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 02:43:45,501	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3a1e017eb83053da.zip' (53.98MiB) to Ray cluster...
2024-04-18 02:43:45,698	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3a1e017eb83053da.zip'.
INFO flwr 2024-04-18 02:43:56,755 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169650351104.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'object_store_memory': 76993007616.0}
INFO flwr 2024-04-18 02:43:56,756 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 02:43:56,756 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 02:43:56,771 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 02:43:56,774 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 02:43:56,775 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 02:43:56,775 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=997463)[0m 2024-04-18 02:44:02.913872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=997463)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=997463)[0m 2024-04-18 02:44:05.226528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 02:44:06,011 | server.py:94 | initial parameters (loss, other metrics): 2.3025691509246826, {'accuracy': 0.1088, 'data_size': 10000}
INFO flwr 2024-04-18 02:44:06,011 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 02:44:06,011 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=997466)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=997466)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=997464)[0m 2024-04-18 02:44:03.253885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=997464)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=997464)[0m 2024-04-18 02:44:05.438110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=997461)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=997461)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 02:44:25,063 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 02:44:29,247 | server.py:125 | fit progress: (1, 1.90352201461792, {'accuracy': 0.7904, 'data_size': 10000}, 23.23548442698666)
INFO flwr 2024-04-18 02:44:29,247 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 02:44:29,247 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:44:40,757 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 02:44:44,907 | server.py:125 | fit progress: (2, 1.5300090312957764, {'accuracy': 0.9433, 'data_size': 10000}, 38.89536114901421)
INFO flwr 2024-04-18 02:44:44,907 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 02:44:44,907 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:44:55,545 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 02:44:59,680 | server.py:125 | fit progress: (3, 1.4989327192306519, {'accuracy': 0.9626, 'data_size': 10000}, 53.66908066399628)
INFO flwr 2024-04-18 02:44:59,681 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 02:44:59,681 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:45:10,494 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 02:45:14,462 | server.py:125 | fit progress: (4, 1.4943602085113525, {'accuracy': 0.9666, 'data_size': 10000}, 68.45129777499824)
INFO flwr 2024-04-18 02:45:14,463 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 02:45:14,463 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:45:25,975 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 02:45:29,869 | server.py:125 | fit progress: (5, 1.495357632637024, {'accuracy': 0.9658, 'data_size': 10000}, 83.85741080500884)
INFO flwr 2024-04-18 02:45:29,869 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 02:45:29,869 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:45:40,955 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 02:45:45,091 | server.py:125 | fit progress: (6, 1.4985613822937012, {'accuracy': 0.9622, 'data_size': 10000}, 99.07972120400518)
INFO flwr 2024-04-18 02:45:45,091 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 02:45:45,092 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:45:56,548 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 02:46:00,450 | server.py:125 | fit progress: (7, 1.5016181468963623, {'accuracy': 0.9592, 'data_size': 10000}, 114.43869868799811)
INFO flwr 2024-04-18 02:46:00,450 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 02:46:00,451 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:46:11,845 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 02:46:16,140 | server.py:125 | fit progress: (8, 1.4960705041885376, {'accuracy': 0.965, 'data_size': 10000}, 130.1290846740012)
INFO flwr 2024-04-18 02:46:16,141 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 02:46:16,141 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:46:27,504 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 02:46:31,613 | server.py:125 | fit progress: (9, 1.4937505722045898, {'accuracy': 0.9673, 'data_size': 10000}, 145.60129727100139)
INFO flwr 2024-04-18 02:46:31,613 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 02:46:31,613 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:46:42,789 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 02:46:47,001 | server.py:125 | fit progress: (10, 1.4956008195877075, {'accuracy': 0.9656, 'data_size': 10000}, 160.98947649198817)
INFO flwr 2024-04-18 02:46:47,001 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 02:46:47,001 | server.py:153 | FL finished in 160.99003283600905
INFO flwr 2024-04-18 02:46:47,006 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 02:46:47,006 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 02:46:47,006 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 02:46:47,006 | app.py:229 | app_fit: losses_centralized [(0, 2.3025691509246826), (1, 1.90352201461792), (2, 1.5300090312957764), (3, 1.4989327192306519), (4, 1.4943602085113525), (5, 1.495357632637024), (6, 1.4985613822937012), (7, 1.5016181468963623), (8, 1.4960705041885376), (9, 1.4937505722045898), (10, 1.4956008195877075)]
INFO flwr 2024-04-18 02:46:47,007 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1088), (1, 0.7904), (2, 0.9433), (3, 0.9626), (4, 0.9666), (5, 0.9658), (6, 0.9622), (7, 0.9592), (8, 0.965), (9, 0.9673), (10, 0.9656)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9656
wandb:     loss 1.4956
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_024338-ixhinjjw
wandb: Find logs at: ./wandb/offline-run-20240418_024338-ixhinjjw/logs
INFO flwr 2024-04-18 02:46:50,523 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 02:54:14,755 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=997458)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=997458)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 02:54:19,300	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 02:54:20,199	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 02:54:20,670	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 02:54:20,826	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (16.05MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 02:54:21,112	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cf54eadb8a212a57.zip' (54.48MiB) to Ray cluster...
2024-04-18 02:54:21,307	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cf54eadb8a212a57.zip'.
INFO flwr 2024-04-18 02:54:32,351 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 76912018636.0, 'memory': 169461376820.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-18 02:54:32,351 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 02:54:32,351 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 02:54:32,376 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 02:54:32,379 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 02:54:32,379 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 02:54:32,379 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1001949)[0m 2024-04-18 02:54:38.436086: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1001949)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1001949)[0m 2024-04-18 02:54:40.804242: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 02:54:42,172 | server.py:94 | initial parameters (loss, other metrics): 2.302553415298462, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-18 02:54:42,173 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 02:54:42,173 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1001952)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1001952)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1001947)[0m 2024-04-18 02:54:38.760088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1001947)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1001947)[0m 2024-04-18 02:54:41.028826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1001948)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1001948)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 02:55:01,850 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 02:55:06,022 | server.py:125 | fit progress: (1, 2.2933335304260254, {'accuracy': 0.5382, 'data_size': 10000}, 23.848809485003585)
INFO flwr 2024-04-18 02:55:06,022 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 02:55:06,022 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:55:17,979 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 02:55:22,088 | server.py:125 | fit progress: (2, 2.272763967514038, {'accuracy': 0.8189, 'data_size': 10000}, 39.91533049600548)
INFO flwr 2024-04-18 02:55:22,089 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 02:55:22,089 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:55:33,272 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 02:55:37,427 | server.py:125 | fit progress: (3, 2.2350072860717773, {'accuracy': 0.8866, 'data_size': 10000}, 55.25350075800088)
INFO flwr 2024-04-18 02:55:37,427 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 02:55:37,427 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:55:48,940 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 02:55:52,883 | server.py:125 | fit progress: (4, 2.1676244735717773, {'accuracy': 0.9087, 'data_size': 10000}, 70.7102270819887)
INFO flwr 2024-04-18 02:55:52,884 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 02:55:52,884 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:56:04,214 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 02:56:08,119 | server.py:125 | fit progress: (5, 2.0572257041931152, {'accuracy': 0.9223, 'data_size': 10000}, 85.94549553698744)
INFO flwr 2024-04-18 02:56:08,119 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 02:56:08,119 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:56:18,530 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 02:56:22,697 | server.py:125 | fit progress: (6, 1.8973445892333984, {'accuracy': 0.9359, 'data_size': 10000}, 100.52405134899891)
INFO flwr 2024-04-18 02:56:22,697 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 02:56:22,698 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:56:34,467 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 02:56:38,404 | server.py:125 | fit progress: (7, 1.737287163734436, {'accuracy': 0.943, 'data_size': 10000}, 116.23135341401212)
INFO flwr 2024-04-18 02:56:38,405 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 02:56:38,405 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:56:49,324 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 02:56:53,493 | server.py:125 | fit progress: (8, 1.6231125593185425, {'accuracy': 0.9493, 'data_size': 10000}, 131.3203377519967)
INFO flwr 2024-04-18 02:56:53,494 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 02:56:53,494 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:57:04,803 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 02:57:08,888 | server.py:125 | fit progress: (9, 1.5592527389526367, {'accuracy': 0.9542, 'data_size': 10000}, 146.71478912999737)
INFO flwr 2024-04-18 02:57:08,888 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 02:57:08,888 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 02:57:19,668 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 02:57:23,786 | server.py:125 | fit progress: (10, 1.5271635055541992, {'accuracy': 0.9575, 'data_size': 10000}, 161.61311048798962)
INFO flwr 2024-04-18 02:57:23,786 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 02:57:23,787 | server.py:153 | FL finished in 161.6135790630069
INFO flwr 2024-04-18 02:57:23,794 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 02:57:23,794 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 02:57:23,794 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 02:57:23,794 | app.py:229 | app_fit: losses_centralized [(0, 2.302553415298462), (1, 2.2933335304260254), (2, 2.272763967514038), (3, 2.2350072860717773), (4, 2.1676244735717773), (5, 2.0572257041931152), (6, 1.8973445892333984), (7, 1.737287163734436), (8, 1.6231125593185425), (9, 1.5592527389526367), (10, 1.5271635055541992)]
INFO flwr 2024-04-18 02:57:23,795 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.5382), (2, 0.8189), (3, 0.8866), (4, 0.9087), (5, 0.9223), (6, 0.9359), (7, 0.943), (8, 0.9493), (9, 0.9542), (10, 0.9575)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9575
wandb:     loss 1.52716
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_025414-6l4bmxj6
wandb: Find logs at: ./wandb/offline-run-20240418_025414-6l4bmxj6/logs
INFO flwr 2024-04-18 02:57:27,334 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 03:04:51,360 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1001941)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1001941)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 03:04:56,397	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 03:04:57,281	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 03:04:57,754	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 03:04:57,908	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (16.54MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 03:04:58,196	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_45193ab9a0a87b18.zip' (54.98MiB) to Ray cluster...
2024-04-18 03:04:58,384	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_45193ab9a0a87b18.zip'.
INFO flwr 2024-04-18 03:05:09,453 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169365274010.0, 'object_store_memory': 76870831718.0}
INFO flwr 2024-04-18 03:05:09,453 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 03:05:09,453 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 03:05:09,470 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 03:05:09,473 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 03:05:09,473 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 03:05:09,473 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1006347)[0m 2024-04-18 03:05:15.493425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1006347)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1006347)[0m 2024-04-18 03:05:17.802315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 03:05:19,322 | server.py:94 | initial parameters (loss, other metrics): 2.302480459213257, {'accuracy': 0.0621, 'data_size': 10000}
INFO flwr 2024-04-18 03:05:19,323 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 03:05:19,323 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1006347)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1006347)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1006340)[0m 2024-04-18 03:05:15.621480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1006340)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1006339)[0m 2024-04-18 03:05:18.007700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1006343)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1006343)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 03:05:38,924 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 03:05:43,149 | server.py:125 | fit progress: (1, 2.301778554916382, {'accuracy': 0.1469, 'data_size': 10000}, 23.826475636014948)
INFO flwr 2024-04-18 03:05:43,150 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 03:05:43,150 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:05:55,229 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 03:05:59,339 | server.py:125 | fit progress: (2, 2.3007760047912598, {'accuracy': 0.2592, 'data_size': 10000}, 40.0166009440145)
INFO flwr 2024-04-18 03:05:59,342 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 03:05:59,342 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:06:10,667 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 03:06:14,793 | server.py:125 | fit progress: (3, 2.299499988555908, {'accuracy': 0.3348, 'data_size': 10000}, 55.46997377401567)
INFO flwr 2024-04-18 03:06:14,793 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 03:06:14,793 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:06:25,942 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 03:06:29,791 | server.py:125 | fit progress: (4, 2.297931432723999, {'accuracy': 0.381, 'data_size': 10000}, 70.46862983101164)
INFO flwr 2024-04-18 03:06:29,792 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 03:06:29,792 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:06:41,080 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 03:06:44,970 | server.py:125 | fit progress: (5, 2.296090602874756, {'accuracy': 0.4665, 'data_size': 10000}, 85.64712830999633)
INFO flwr 2024-04-18 03:06:44,970 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 03:06:44,970 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:06:55,916 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 03:07:00,019 | server.py:125 | fit progress: (6, 2.293961524963379, {'accuracy': 0.5555, 'data_size': 10000}, 100.69618870099657)
INFO flwr 2024-04-18 03:07:00,019 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 03:07:00,019 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:07:11,167 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 03:07:15,063 | server.py:125 | fit progress: (7, 2.2915518283843994, {'accuracy': 0.6242, 'data_size': 10000}, 115.7399871259986)
INFO flwr 2024-04-18 03:07:15,063 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 03:07:15,063 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:07:26,795 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 03:07:30,988 | server.py:125 | fit progress: (8, 2.2888569831848145, {'accuracy': 0.6786, 'data_size': 10000}, 131.6654437740217)
INFO flwr 2024-04-18 03:07:30,989 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 03:07:30,989 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:07:41,803 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 03:07:45,982 | server.py:125 | fit progress: (9, 2.2858481407165527, {'accuracy': 0.7209, 'data_size': 10000}, 146.65939607701148)
INFO flwr 2024-04-18 03:07:45,982 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 03:07:45,983 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:07:57,495 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 03:08:01,556 | server.py:125 | fit progress: (10, 2.28247332572937, {'accuracy': 0.7557, 'data_size': 10000}, 162.23336767600267)
INFO flwr 2024-04-18 03:08:01,556 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 03:08:01,557 | server.py:153 | FL finished in 162.2338023899938
INFO flwr 2024-04-18 03:08:01,560 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 03:08:01,560 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 03:08:01,560 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 03:08:01,560 | app.py:229 | app_fit: losses_centralized [(0, 2.302480459213257), (1, 2.301778554916382), (2, 2.3007760047912598), (3, 2.299499988555908), (4, 2.297931432723999), (5, 2.296090602874756), (6, 2.293961524963379), (7, 2.2915518283843994), (8, 2.2888569831848145), (9, 2.2858481407165527), (10, 2.28247332572937)]
INFO flwr 2024-04-18 03:08:01,560 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0621), (1, 0.1469), (2, 0.2592), (3, 0.3348), (4, 0.381), (5, 0.4665), (6, 0.5555), (7, 0.6242), (8, 0.6786), (9, 0.7209), (10, 0.7557)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7557
wandb:     loss 2.28247
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_030451-4hcuj288
wandb: Find logs at: ./wandb/offline-run-20240418_030451-4hcuj288/logs
INFO flwr 2024-04-18 03:08:05,122 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 03:15:29,654 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1006338)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1006338)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 03:15:34,197	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 03:15:35,161	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 03:15:35,625	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 03:15:35,778	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (17.02MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 03:15:36,062	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fb53ee4bdada1c32.zip' (55.48MiB) to Ray cluster...
2024-04-18 03:15:36,255	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fb53ee4bdada1c32.zip'.
INFO flwr 2024-04-18 03:15:47,330 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 76845106790.0, 'CPU': 64.0, 'memory': 169305249178.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 03:15:47,330 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 03:15:47,330 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 03:15:47,347 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 03:15:47,350 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 03:15:47,350 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 03:15:47,351 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1011069)[0m 2024-04-18 03:15:53.384433: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1011069)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1011069)[0m 2024-04-18 03:15:55.675821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 03:15:57,110 | server.py:94 | initial parameters (loss, other metrics): 2.3025336265563965, {'accuracy': 0.1267, 'data_size': 10000}
INFO flwr 2024-04-18 03:15:57,111 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 03:15:57,111 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1011071)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1011071)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1011058)[0m 2024-04-18 03:15:53.715166: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1011058)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1011058)[0m 2024-04-18 03:15:56.144872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1011064)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1011064)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 03:16:15,284 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 03:16:19,473 | server.py:125 | fit progress: (1, 2.3024659156799316, {'accuracy': 0.1305, 'data_size': 10000}, 22.361712408979656)
INFO flwr 2024-04-18 03:16:19,473 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 03:16:19,473 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:16:31,583 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 03:16:35,786 | server.py:125 | fit progress: (2, 2.3023736476898193, {'accuracy': 0.134, 'data_size': 10000}, 38.675171614973806)
INFO flwr 2024-04-18 03:16:35,786 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 03:16:35,787 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:16:47,041 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 03:16:51,157 | server.py:125 | fit progress: (3, 2.302266836166382, {'accuracy': 0.138, 'data_size': 10000}, 54.046130630973494)
INFO flwr 2024-04-18 03:16:51,157 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 03:16:51,157 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:17:02,104 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 03:17:06,052 | server.py:125 | fit progress: (4, 2.3021461963653564, {'accuracy': 0.1436, 'data_size': 10000}, 68.94130337898969)
INFO flwr 2024-04-18 03:17:06,052 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 03:17:06,053 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:17:17,400 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 03:17:21,293 | server.py:125 | fit progress: (5, 2.302016019821167, {'accuracy': 0.1504, 'data_size': 10000}, 84.18250894697849)
INFO flwr 2024-04-18 03:17:21,294 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 03:17:21,294 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:17:32,038 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 03:17:36,217 | server.py:125 | fit progress: (6, 2.3018791675567627, {'accuracy': 0.1595, 'data_size': 10000}, 99.10647456697188)
INFO flwr 2024-04-18 03:17:36,218 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 03:17:36,218 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:17:47,635 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 03:17:51,510 | server.py:125 | fit progress: (7, 2.301734447479248, {'accuracy': 0.1711, 'data_size': 10000}, 114.39886206199299)
INFO flwr 2024-04-18 03:17:51,510 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 03:17:51,510 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:18:02,665 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 03:18:06,790 | server.py:125 | fit progress: (8, 2.3015849590301514, {'accuracy': 0.1813, 'data_size': 10000}, 129.6790741229779)
INFO flwr 2024-04-18 03:18:06,790 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 03:18:06,790 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:18:17,637 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 03:18:21,758 | server.py:125 | fit progress: (9, 2.30143141746521, {'accuracy': 0.1928, 'data_size': 10000}, 144.6474815349793)
INFO flwr 2024-04-18 03:18:21,759 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 03:18:21,760 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:18:33,331 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 03:18:37,485 | server.py:125 | fit progress: (10, 2.3012726306915283, {'accuracy': 0.2069, 'data_size': 10000}, 160.3742345609935)
INFO flwr 2024-04-18 03:18:37,485 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 03:18:37,485 | server.py:153 | FL finished in 160.37467527599074
INFO flwr 2024-04-18 03:18:37,488 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 03:18:37,488 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 03:18:37,488 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 03:18:37,488 | app.py:229 | app_fit: losses_centralized [(0, 2.3025336265563965), (1, 2.3024659156799316), (2, 2.3023736476898193), (3, 2.302266836166382), (4, 2.3021461963653564), (5, 2.302016019821167), (6, 2.3018791675567627), (7, 2.301734447479248), (8, 2.3015849590301514), (9, 2.30143141746521), (10, 2.3012726306915283)]
INFO flwr 2024-04-18 03:18:37,489 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1267), (1, 0.1305), (2, 0.134), (3, 0.138), (4, 0.1436), (5, 0.1504), (6, 0.1595), (7, 0.1711), (8, 0.1813), (9, 0.1928), (10, 0.2069)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2069
wandb:     loss 2.30127
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_031529-iidq6enz
wandb: Find logs at: ./wandb/offline-run-20240418_031529-iidq6enz/logs
INFO flwr 2024-04-18 03:18:41,003 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 03:26:05,268 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1011058)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1011058)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 03:26:09,912	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 03:26:10,928	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 03:26:11,400	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 03:26:11,560	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (17.51MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 03:26:11,851	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9cdbe1baf1677a7b.zip' (55.98MiB) to Ray cluster...
2024-04-18 03:26:12,044	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9cdbe1baf1677a7b.zip'.
INFO flwr 2024-04-18 03:26:23,101 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 76824610406.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 169257424282.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 03:26:23,102 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 03:26:23,102 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 03:26:23,119 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 03:26:23,122 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 03:26:23,122 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 03:26:23,122 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1015464)[0m 2024-04-18 03:26:29.175608: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1015464)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1015464)[0m 2024-04-18 03:26:31.562273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 03:26:31,863 | server.py:94 | initial parameters (loss, other metrics): 2.3025062084198, {'accuracy': 0.0793, 'data_size': 10000}
INFO flwr 2024-04-18 03:26:31,863 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 03:26:31,864 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1015466)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1015466)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1015454)[0m 2024-04-18 03:26:29.494878: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1015454)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1015454)[0m 2024-04-18 03:26:31.716684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1015453)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1015453)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-18 03:26:51,740 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 03:26:55,995 | server.py:125 | fit progress: (1, 1.59914231300354, {'accuracy': 0.8619, 'data_size': 10000}, 24.13123521101079)
INFO flwr 2024-04-18 03:26:55,995 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 03:26:55,995 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:27:07,645 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 03:27:11,816 | server.py:125 | fit progress: (2, 1.8384239673614502, {'accuracy': 0.6227, 'data_size': 10000}, 39.95201440999517)
INFO flwr 2024-04-18 03:27:11,816 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 03:27:11,816 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:27:23,011 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 03:27:27,173 | server.py:125 | fit progress: (3, 1.8555495738983154, {'accuracy': 0.6056, 'data_size': 10000}, 55.309178310009884)
INFO flwr 2024-04-18 03:27:27,173 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 03:27:27,173 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:27:38,334 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 03:27:42,248 | server.py:125 | fit progress: (4, 1.9230462312698364, {'accuracy': 0.5381, 'data_size': 10000}, 70.38418615801493)
INFO flwr 2024-04-18 03:27:42,248 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 03:27:42,248 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:27:53,220 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 03:27:57,093 | server.py:125 | fit progress: (5, 2.0420303344726562, {'accuracy': 0.4191, 'data_size': 10000}, 85.22918284501066)
INFO flwr 2024-04-18 03:27:57,093 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 03:27:57,094 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:28:08,815 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 03:28:12,961 | server.py:125 | fit progress: (6, 2.250643253326416, {'accuracy': 0.2105, 'data_size': 10000}, 101.09692976399674)
INFO flwr 2024-04-18 03:28:12,961 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 03:28:12,961 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:28:24,411 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 03:28:28,309 | server.py:125 | fit progress: (7, 2.2793421745300293, {'accuracy': 0.1818, 'data_size': 10000}, 116.44567887001904)
INFO flwr 2024-04-18 03:28:28,310 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 03:28:28,310 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:28:40,444 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 03:28:44,553 | server.py:125 | fit progress: (8, 2.3131425380706787, {'accuracy': 0.148, 'data_size': 10000}, 132.6891215580108)
INFO flwr 2024-04-18 03:28:44,553 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 03:28:44,553 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:28:56,058 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 03:29:00,183 | server.py:125 | fit progress: (9, 2.345442295074463, {'accuracy': 0.1157, 'data_size': 10000}, 148.31927673501195)
INFO flwr 2024-04-18 03:29:00,184 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 03:29:00,184 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:29:11,810 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 03:29:16,089 | server.py:125 | fit progress: (10, 2.3562421798706055, {'accuracy': 0.1049, 'data_size': 10000}, 164.2258030899975)
INFO flwr 2024-04-18 03:29:16,090 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 03:29:16,090 | server.py:153 | FL finished in 164.2262505260005
INFO flwr 2024-04-18 03:29:16,098 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 03:29:16,098 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 03:29:16,098 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 03:29:16,098 | app.py:229 | app_fit: losses_centralized [(0, 2.3025062084198), (1, 1.59914231300354), (2, 1.8384239673614502), (3, 1.8555495738983154), (4, 1.9230462312698364), (5, 2.0420303344726562), (6, 2.250643253326416), (7, 2.2793421745300293), (8, 2.3131425380706787), (9, 2.345442295074463), (10, 2.3562421798706055)]
INFO flwr 2024-04-18 03:29:16,098 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0793), (1, 0.8619), (2, 0.6227), (3, 0.6056), (4, 0.5381), (5, 0.4191), (6, 0.2105), (7, 0.1818), (8, 0.148), (9, 0.1157), (10, 0.1049)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1049
wandb:     loss 2.35624
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_032604-mnf42eyf
wandb: Find logs at: ./wandb/offline-run-20240418_032604-mnf42eyf/logs
INFO flwr 2024-04-18 03:29:19,621 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 03:36:43,504 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1015449)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1015449)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-18 03:36:48,510	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 03:36:49,559	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 03:36:50,020	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 03:36:50,174	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (18.00MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 03:36:50,459	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_17ab4855be6557c6.zip' (56.47MiB) to Ray cluster...
2024-04-18 03:36:50,653	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_17ab4855be6557c6.zip'.
INFO flwr 2024-04-18 03:37:01,687 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 76875190272.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169375443968.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 03:37:01,688 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 03:37:01,688 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 03:37:01,707 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 03:37:01,711 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 03:37:01,711 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 03:37:01,711 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1019820)[0m 2024-04-18 03:37:07.892589: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1019820)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1019827)[0m 2024-04-18 03:37:10.140296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 03:37:10,440 | server.py:94 | initial parameters (loss, other metrics): 2.3027071952819824, {'accuracy': 0.0544, 'data_size': 10000}
INFO flwr 2024-04-18 03:37:10,441 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 03:37:10,441 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1019830)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1019830)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1019826)[0m 2024-04-18 03:37:07.951581: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1019826)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1019826)[0m 2024-04-18 03:37:10.224969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1019822)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1019822)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 03:37:30,991 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 03:37:35,181 | server.py:125 | fit progress: (1, 2.0175840854644775, {'accuracy': 0.6236, 'data_size': 10000}, 24.739470244996483)
INFO flwr 2024-04-18 03:37:35,181 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 03:37:35,181 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:37:46,837 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 03:37:51,049 | server.py:125 | fit progress: (2, 1.5501208305358887, {'accuracy': 0.9334, 'data_size': 10000}, 40.6073966389813)
INFO flwr 2024-04-18 03:37:51,049 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 03:37:51,049 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:38:02,295 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 03:38:06,454 | server.py:125 | fit progress: (3, 1.5018280744552612, {'accuracy': 0.96, 'data_size': 10000}, 56.01316974699148)
INFO flwr 2024-04-18 03:38:06,455 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 03:38:06,455 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:38:17,699 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 03:38:21,653 | server.py:125 | fit progress: (4, 1.492281198501587, {'accuracy': 0.9687, 'data_size': 10000}, 71.21148836499196)
INFO flwr 2024-04-18 03:38:21,653 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 03:38:21,653 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:38:32,913 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 03:38:36,785 | server.py:125 | fit progress: (5, 1.4910320043563843, {'accuracy': 0.9702, 'data_size': 10000}, 86.34334284099168)
INFO flwr 2024-04-18 03:38:36,785 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 03:38:36,785 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:38:48,293 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 03:38:52,434 | server.py:125 | fit progress: (6, 1.4905197620391846, {'accuracy': 0.9708, 'data_size': 10000}, 101.99233393900795)
INFO flwr 2024-04-18 03:38:52,434 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 03:38:52,434 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:39:03,982 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 03:39:07,896 | server.py:125 | fit progress: (7, 1.4926273822784424, {'accuracy': 0.9685, 'data_size': 10000}, 117.45463471100084)
INFO flwr 2024-04-18 03:39:07,896 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 03:39:07,896 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:39:19,105 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 03:39:23,257 | server.py:125 | fit progress: (8, 1.4946635961532593, {'accuracy': 0.9666, 'data_size': 10000}, 132.81581911098328)
INFO flwr 2024-04-18 03:39:23,257 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 03:39:23,258 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:39:34,500 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 03:39:38,644 | server.py:125 | fit progress: (9, 1.4937540292739868, {'accuracy': 0.9671, 'data_size': 10000}, 148.20314198700362)
INFO flwr 2024-04-18 03:39:38,645 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 03:39:38,645 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:39:50,311 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 03:39:54,473 | server.py:125 | fit progress: (10, 1.4928432703018188, {'accuracy': 0.9682, 'data_size': 10000}, 164.03221593500348)
INFO flwr 2024-04-18 03:39:54,474 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 03:39:54,474 | server.py:153 | FL finished in 164.03265831698081
INFO flwr 2024-04-18 03:39:54,481 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 03:39:54,481 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 03:39:54,481 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 03:39:54,482 | app.py:229 | app_fit: losses_centralized [(0, 2.3027071952819824), (1, 2.0175840854644775), (2, 1.5501208305358887), (3, 1.5018280744552612), (4, 1.492281198501587), (5, 1.4910320043563843), (6, 1.4905197620391846), (7, 1.4926273822784424), (8, 1.4946635961532593), (9, 1.4937540292739868), (10, 1.4928432703018188)]
INFO flwr 2024-04-18 03:39:54,482 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0544), (1, 0.6236), (2, 0.9334), (3, 0.96), (4, 0.9687), (5, 0.9702), (6, 0.9708), (7, 0.9685), (8, 0.9666), (9, 0.9671), (10, 0.9682)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9682
wandb:     loss 1.49284
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_033643-fvkff4yo
wandb: Find logs at: ./wandb/offline-run-20240418_033643-fvkff4yo/logs
INFO flwr 2024-04-18 03:39:58,004 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 03:47:21,838 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1019820)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1019820)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 03:47:26,743	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 03:47:27,671	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 03:47:28,172	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 03:47:28,327	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (18.48MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 03:47:28,622	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5a82cbefc3fe1ada.zip' (56.97MiB) to Ray cluster...
2024-04-18 03:47:28,816	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5a82cbefc3fe1ada.zip'.
INFO flwr 2024-04-18 03:47:39,886 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 76903595212.0, 'GPU': 1.0, 'memory': 169441722164.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 03:47:39,887 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 03:47:39,887 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 03:47:39,903 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 03:47:39,905 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 03:47:39,905 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 03:47:39,906 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1024568)[0m 2024-04-18 03:47:46.045979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1024568)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1024568)[0m 2024-04-18 03:47:48.356888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 03:47:48,957 | server.py:94 | initial parameters (loss, other metrics): 2.3026270866394043, {'accuracy': 0.1035, 'data_size': 10000}
INFO flwr 2024-04-18 03:47:48,958 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 03:47:48,958 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1024568)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1024568)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1024566)[0m 2024-04-18 03:47:46.329083: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1024566)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1024566)[0m 2024-04-18 03:47:48.650585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1024563)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1024563)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 03:48:07,977 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 03:48:12,157 | server.py:125 | fit progress: (1, 2.2928991317749023, {'accuracy': 0.59, 'data_size': 10000}, 23.198940381000284)
INFO flwr 2024-04-18 03:48:12,157 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 03:48:12,158 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:48:23,810 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 03:48:27,917 | server.py:125 | fit progress: (2, 2.274902820587158, {'accuracy': 0.7657, 'data_size': 10000}, 38.95860666700173)
INFO flwr 2024-04-18 03:48:27,917 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 03:48:27,917 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:48:38,545 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 03:48:42,706 | server.py:125 | fit progress: (3, 2.243619918823242, {'accuracy': 0.8725, 'data_size': 10000}, 53.74757950598723)
INFO flwr 2024-04-18 03:48:42,706 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 03:48:42,707 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:48:53,581 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 03:48:57,577 | server.py:125 | fit progress: (4, 2.185549736022949, {'accuracy': 0.9058, 'data_size': 10000}, 68.61873640498379)
INFO flwr 2024-04-18 03:48:57,577 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 03:48:57,577 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:49:08,730 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 03:49:12,625 | server.py:125 | fit progress: (5, 2.0859124660491943, {'accuracy': 0.9239, 'data_size': 10000}, 83.66654676798498)
INFO flwr 2024-04-18 03:49:12,625 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 03:49:12,625 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:49:23,880 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 03:49:28,041 | server.py:125 | fit progress: (6, 1.9441076517105103, {'accuracy': 0.9335, 'data_size': 10000}, 99.08256370999152)
INFO flwr 2024-04-18 03:49:28,041 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 03:49:28,042 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:49:38,977 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 03:49:42,957 | server.py:125 | fit progress: (7, 1.7879599332809448, {'accuracy': 0.9418, 'data_size': 10000}, 113.99920631799614)
INFO flwr 2024-04-18 03:49:42,958 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 03:49:42,958 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:49:54,161 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 03:49:58,290 | server.py:125 | fit progress: (8, 1.6617720127105713, {'accuracy': 0.9477, 'data_size': 10000}, 129.3314472209895)
INFO flwr 2024-04-18 03:49:58,290 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 03:49:58,290 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:50:09,542 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 03:50:13,628 | server.py:125 | fit progress: (9, 1.581440806388855, {'accuracy': 0.9508, 'data_size': 10000}, 144.67038626398426)
INFO flwr 2024-04-18 03:50:13,629 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 03:50:13,629 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:50:24,926 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 03:50:29,149 | server.py:125 | fit progress: (10, 1.539682149887085, {'accuracy': 0.9548, 'data_size': 10000}, 160.19068585999776)
INFO flwr 2024-04-18 03:50:29,149 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 03:50:29,149 | server.py:153 | FL finished in 160.1912085239892
INFO flwr 2024-04-18 03:50:29,150 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 03:50:29,150 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 03:50:29,150 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 03:50:29,150 | app.py:229 | app_fit: losses_centralized [(0, 2.3026270866394043), (1, 2.2928991317749023), (2, 2.274902820587158), (3, 2.243619918823242), (4, 2.185549736022949), (5, 2.0859124660491943), (6, 1.9441076517105103), (7, 1.7879599332809448), (8, 1.6617720127105713), (9, 1.581440806388855), (10, 1.539682149887085)]
INFO flwr 2024-04-18 03:50:29,150 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1035), (1, 0.59), (2, 0.7657), (3, 0.8725), (4, 0.9058), (5, 0.9239), (6, 0.9335), (7, 0.9418), (8, 0.9477), (9, 0.9508), (10, 0.9548)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9548
wandb:     loss 1.53968
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_034721-sqoyvq2f
wandb: Find logs at: ./wandb/offline-run-20240418_034721-sqoyvq2f/logs
INFO flwr 2024-04-18 03:50:32,727 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 03:57:56,309 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1024554)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1024554)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 03:58:00,921	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 03:58:01,955	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 03:58:02,465	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 03:58:02,621	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (18.97MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 03:58:02,915	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1acef80c8a7114d6.zip' (57.47MiB) to Ray cluster...
2024-04-18 03:58:03,109	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1acef80c8a7114d6.zip'.
INFO flwr 2024-04-18 03:58:14,327 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169444790068.0, 'GPU': 1.0, 'object_store_memory': 76904910028.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 03:58:14,328 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 03:58:14,328 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 03:58:14,346 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 03:58:14,349 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 03:58:14,350 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 03:58:14,350 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1028944)[0m 2024-04-18 03:58:20.458415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1028944)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1028945)[0m 2024-04-18 03:58:22.782655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 03:58:23,632 | server.py:94 | initial parameters (loss, other metrics): 2.302152156829834, {'accuracy': 0.1248, 'data_size': 10000}
INFO flwr 2024-04-18 03:58:23,633 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 03:58:23,633 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1028947)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1028947)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1028943)[0m 2024-04-18 03:58:20.575613: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1028943)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1028943)[0m 2024-04-18 03:58:22.866106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1028922)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1028922)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 03:58:43,570 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 03:58:47,732 | server.py:125 | fit progress: (1, 2.3015859127044678, {'accuracy': 0.2434, 'data_size': 10000}, 24.09869887001696)
INFO flwr 2024-04-18 03:58:47,732 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 03:58:47,732 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:58:59,980 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 03:59:04,063 | server.py:125 | fit progress: (2, 2.3007330894470215, {'accuracy': 0.3535, 'data_size': 10000}, 40.42984820602578)
INFO flwr 2024-04-18 03:59:04,063 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 03:59:04,064 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:59:15,005 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 03:59:19,139 | server.py:125 | fit progress: (3, 2.299642562866211, {'accuracy': 0.4255, 'data_size': 10000}, 55.5057738830219)
INFO flwr 2024-04-18 03:59:19,139 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 03:59:19,139 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:59:30,505 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 03:59:34,396 | server.py:125 | fit progress: (4, 2.298368453979492, {'accuracy': 0.4653, 'data_size': 10000}, 70.76290447401698)
INFO flwr 2024-04-18 03:59:34,396 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 03:59:34,396 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 03:59:45,623 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 03:59:49,522 | server.py:125 | fit progress: (5, 2.296881914138794, {'accuracy': 0.5021, 'data_size': 10000}, 85.88953526201658)
INFO flwr 2024-04-18 03:59:49,523 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 03:59:49,523 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:00:01,764 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 04:00:05,915 | server.py:125 | fit progress: (6, 2.2952024936676025, {'accuracy': 0.5462, 'data_size': 10000}, 102.28188251302345)
INFO flwr 2024-04-18 04:00:05,915 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 04:00:05,915 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:00:17,259 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 04:00:21,364 | server.py:125 | fit progress: (7, 2.2932941913604736, {'accuracy': 0.586, 'data_size': 10000}, 117.73167718900368)
INFO flwr 2024-04-18 04:00:21,365 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 04:00:21,365 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:00:31,860 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 04:00:35,760 | server.py:125 | fit progress: (8, 2.2911324501037598, {'accuracy': 0.62, 'data_size': 10000}, 132.1272346379992)
INFO flwr 2024-04-18 04:00:35,760 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 04:00:35,761 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:00:47,308 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 04:00:51,477 | server.py:125 | fit progress: (9, 2.2887306213378906, {'accuracy': 0.6664, 'data_size': 10000}, 147.84383428801084)
INFO flwr 2024-04-18 04:00:51,477 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 04:00:51,477 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:01:02,841 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 04:01:06,746 | server.py:125 | fit progress: (10, 2.2861199378967285, {'accuracy': 0.7266, 'data_size': 10000}, 163.11271852900973)
INFO flwr 2024-04-18 04:01:06,746 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 04:01:06,746 | server.py:153 | FL finished in 163.11322454802576
INFO flwr 2024-04-18 04:01:06,751 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 04:01:06,751 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 04:01:06,751 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 04:01:06,751 | app.py:229 | app_fit: losses_centralized [(0, 2.302152156829834), (1, 2.3015859127044678), (2, 2.3007330894470215), (3, 2.299642562866211), (4, 2.298368453979492), (5, 2.296881914138794), (6, 2.2952024936676025), (7, 2.2932941913604736), (8, 2.2911324501037598), (9, 2.2887306213378906), (10, 2.2861199378967285)]
INFO flwr 2024-04-18 04:01:06,751 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1248), (1, 0.2434), (2, 0.3535), (3, 0.4255), (4, 0.4653), (5, 0.5021), (6, 0.5462), (7, 0.586), (8, 0.62), (9, 0.6664), (10, 0.7266)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7266
wandb:     loss 2.28612
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_035755-k3vo9cu8
wandb: Find logs at: ./wandb/offline-run-20240418_035755-k3vo9cu8/logs
INFO flwr 2024-04-18 04:01:10,276 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 04:08:34,455 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1028919)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1028919)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 04:08:39,257	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 04:08:40,344	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 04:08:40,830	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 04:08:40,994	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (19.45MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 04:08:41,287	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cb739afe4d474f06.zip' (57.97MiB) to Ray cluster...
2024-04-18 04:08:41,484	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cb739afe4d474f06.zip'.
INFO flwr 2024-04-18 04:08:52,604 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 169445736244.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 76905315532.0}
INFO flwr 2024-04-18 04:08:52,605 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 04:08:52,605 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 04:08:52,620 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 04:08:52,622 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 04:08:52,622 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 04:08:52,622 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1033308)[0m 2024-04-18 04:08:58.620245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1033308)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1033308)[0m 2024-04-18 04:09:00.985506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 04:09:02,126 | server.py:94 | initial parameters (loss, other metrics): 2.302633762359619, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-18 04:09:02,126 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 04:09:02,127 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1033315)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1033315)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1033315)[0m 2024-04-18 04:08:58.957765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1033315)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1033315)[0m 2024-04-18 04:09:01.175172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1033311)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1033311)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 04:09:21,098 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 04:09:25,336 | server.py:125 | fit progress: (1, 2.30257511138916, {'accuracy': 0.0974, 'data_size': 10000}, 23.209390682983212)
INFO flwr 2024-04-18 04:09:25,336 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 04:09:25,336 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:09:37,586 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 04:09:41,797 | server.py:125 | fit progress: (2, 2.302499771118164, {'accuracy': 0.0974, 'data_size': 10000}, 39.6703087099886)
INFO flwr 2024-04-18 04:09:41,797 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 04:09:41,797 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:09:52,880 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 04:09:57,013 | server.py:125 | fit progress: (3, 2.302410125732422, {'accuracy': 0.0976, 'data_size': 10000}, 54.88675337997847)
INFO flwr 2024-04-18 04:09:57,013 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 04:09:57,014 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:10:08,371 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 04:10:12,369 | server.py:125 | fit progress: (4, 2.3023128509521484, {'accuracy': 0.0976, 'data_size': 10000}, 70.24280245398404)
INFO flwr 2024-04-18 04:10:12,369 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 04:10:12,370 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:10:23,389 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 04:10:27,301 | server.py:125 | fit progress: (5, 2.3022100925445557, {'accuracy': 0.098, 'data_size': 10000}, 85.17461342999013)
INFO flwr 2024-04-18 04:10:27,301 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 04:10:27,302 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:10:38,875 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 04:10:42,991 | server.py:125 | fit progress: (6, 2.30210018157959, {'accuracy': 0.0986, 'data_size': 10000}, 100.86473154299892)
INFO flwr 2024-04-18 04:10:42,991 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 04:10:42,992 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:10:54,509 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 04:10:58,418 | server.py:125 | fit progress: (7, 2.3019859790802, {'accuracy': 0.0989, 'data_size': 10000}, 116.291528095986)
INFO flwr 2024-04-18 04:10:58,418 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 04:10:58,418 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:11:09,279 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 04:11:13,427 | server.py:125 | fit progress: (8, 2.301868438720703, {'accuracy': 0.1002, 'data_size': 10000}, 131.30088254300063)
INFO flwr 2024-04-18 04:11:13,428 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 04:11:13,428 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:11:25,112 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 04:11:29,074 | server.py:125 | fit progress: (9, 2.3017497062683105, {'accuracy': 0.1013, 'data_size': 10000}, 146.94757235399447)
INFO flwr 2024-04-18 04:11:29,074 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 04:11:29,074 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:11:40,877 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 04:11:44,837 | server.py:125 | fit progress: (10, 2.3016281127929688, {'accuracy': 0.1028, 'data_size': 10000}, 162.71031479499652)
INFO flwr 2024-04-18 04:11:44,837 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 04:11:44,837 | server.py:153 | FL finished in 162.71084040097776
INFO flwr 2024-04-18 04:11:44,842 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 04:11:44,842 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 04:11:44,842 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 04:11:44,842 | app.py:229 | app_fit: losses_centralized [(0, 2.302633762359619), (1, 2.30257511138916), (2, 2.302499771118164), (3, 2.302410125732422), (4, 2.3023128509521484), (5, 2.3022100925445557), (6, 2.30210018157959), (7, 2.3019859790802), (8, 2.301868438720703), (9, 2.3017497062683105), (10, 2.3016281127929688)]
INFO flwr 2024-04-18 04:11:44,842 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.0974), (2, 0.0974), (3, 0.0976), (4, 0.0976), (5, 0.098), (6, 0.0986), (7, 0.0989), (8, 0.1002), (9, 0.1013), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.30163
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_040834-86fjpn3f
wandb: Find logs at: ./wandb/offline-run-20240418_040834-86fjpn3f/logs
INFO flwr 2024-04-18 04:11:48,406 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 04:19:12,469 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1033307)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1033307)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 04:19:17,084	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 04:19:18,077	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 04:19:18,541	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 04:19:18,692	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (19.94MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 04:19:18,987	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_773db72b2b7ac002.zip' (58.47MiB) to Ray cluster...
2024-04-18 04:19:19,190	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_773db72b2b7ac002.zip'.
INFO flwr 2024-04-18 04:19:30,389 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 169517384704.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'object_store_memory': 76936022016.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 04:19:30,390 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 04:19:30,390 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 04:19:30,406 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 04:19:30,409 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 04:19:30,409 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 04:19:30,409 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1038044)[0m 2024-04-18 04:19:36.552415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1038044)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1038043)[0m 2024-04-18 04:19:38.799084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 04:19:39,641 | server.py:94 | initial parameters (loss, other metrics): 2.302518606185913, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-18 04:19:39,641 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 04:19:39,642 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1038047)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1038047)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1038046)[0m 2024-04-18 04:19:36.721309: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1038046)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1038046)[0m 2024-04-18 04:19:39.318775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1038039)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1038039)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 04:20:04,472 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 04:20:08,700 | server.py:125 | fit progress: (1, 1.6582180261611938, {'accuracy': 0.803, 'data_size': 10000}, 29.05881845598924)
INFO flwr 2024-04-18 04:20:08,701 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 04:20:08,701 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:20:25,543 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 04:20:29,717 | server.py:125 | fit progress: (2, 1.888365626335144, {'accuracy': 0.5728, 'data_size': 10000}, 50.075004905986134)
INFO flwr 2024-04-18 04:20:29,717 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 04:20:29,717 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:20:46,460 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 04:20:50,644 | server.py:125 | fit progress: (3, 2.0325446128845215, {'accuracy': 0.4286, 'data_size': 10000}, 71.00270169100258)
INFO flwr 2024-04-18 04:20:50,645 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 04:20:50,645 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:21:08,309 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 04:21:12,274 | server.py:125 | fit progress: (4, 2.2570340633392334, {'accuracy': 0.2041, 'data_size': 10000}, 92.63256464299047)
INFO flwr 2024-04-18 04:21:12,275 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 04:21:12,275 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:21:29,214 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 04:21:33,178 | server.py:125 | fit progress: (5, 2.2763428688049316, {'accuracy': 0.1848, 'data_size': 10000}, 113.53613150099409)
INFO flwr 2024-04-18 04:21:33,178 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 04:21:33,178 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:21:50,350 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 04:21:54,592 | server.py:125 | fit progress: (6, 2.2836430072784424, {'accuracy': 0.1775, 'data_size': 10000}, 134.95095144800143)
INFO flwr 2024-04-18 04:21:54,593 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 04:21:54,593 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:22:10,312 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 04:22:14,273 | server.py:125 | fit progress: (7, 2.290742874145508, {'accuracy': 0.1704, 'data_size': 10000}, 154.6314508689975)
INFO flwr 2024-04-18 04:22:14,273 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 04:22:14,273 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:22:31,606 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 04:22:35,783 | server.py:125 | fit progress: (8, 2.295942783355713, {'accuracy': 0.1652, 'data_size': 10000}, 176.14108334798948)
INFO flwr 2024-04-18 04:22:35,783 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 04:22:35,783 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:22:50,627 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 04:22:54,781 | server.py:125 | fit progress: (9, 2.301342725753784, {'accuracy': 0.1598, 'data_size': 10000}, 195.13917057798244)
INFO flwr 2024-04-18 04:22:54,781 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 04:22:54,781 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:23:11,077 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 04:23:15,241 | server.py:125 | fit progress: (10, 2.307142496109009, {'accuracy': 0.154, 'data_size': 10000}, 215.59926363397972)
INFO flwr 2024-04-18 04:23:15,241 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 04:23:15,241 | server.py:153 | FL finished in 215.5997195179807
INFO flwr 2024-04-18 04:23:15,248 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 04:23:15,248 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 04:23:15,248 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 04:23:15,248 | app.py:229 | app_fit: losses_centralized [(0, 2.302518606185913), (1, 1.6582180261611938), (2, 1.888365626335144), (3, 2.0325446128845215), (4, 2.2570340633392334), (5, 2.2763428688049316), (6, 2.2836430072784424), (7, 2.290742874145508), (8, 2.295942783355713), (9, 2.301342725753784), (10, 2.307142496109009)]
INFO flwr 2024-04-18 04:23:15,249 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.803), (2, 0.5728), (3, 0.4286), (4, 0.2041), (5, 0.1848), (6, 0.1775), (7, 0.1704), (8, 0.1652), (9, 0.1598), (10, 0.154)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.154
wandb:     loss 2.30714
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_041912-16q3jhcm
wandb: Find logs at: ./wandb/offline-run-20240418_041912-16q3jhcm/logs
INFO flwr 2024-04-18 04:23:18,810 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 04:30:41,849 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1038035)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1038035)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 04:30:46,642	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 04:30:47,620	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 04:30:48,102	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 04:30:48,255	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (20.42MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 04:30:48,549	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_033537a11b1a3b23.zip' (58.97MiB) to Ray cluster...
2024-04-18 04:30:48,747	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_033537a11b1a3b23.zip'.
INFO flwr 2024-04-18 04:30:59,826 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 76953372672.0, 'memory': 169557869568.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-18 04:30:59,826 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 04:30:59,827 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 04:30:59,845 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 04:30:59,846 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 04:30:59,847 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 04:30:59,847 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1043047)[0m 2024-04-18 04:31:05.866001: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1043047)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1043053)[0m 2024-04-18 04:31:08.247639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 04:31:09,043 | server.py:94 | initial parameters (loss, other metrics): 2.302527904510498, {'accuracy': 0.0673, 'data_size': 10000}
INFO flwr 2024-04-18 04:31:09,043 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 04:31:09,044 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1043058)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1043058)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1043044)[0m 2024-04-18 04:31:06.010499: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1043044)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1043054)[0m 2024-04-18 04:31:08.348147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1043053)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1043053)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 04:31:33,540 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 04:31:37,845 | server.py:125 | fit progress: (1, 1.9566625356674194, {'accuracy': 0.8119, 'data_size': 10000}, 28.8015755089873)
INFO flwr 2024-04-18 04:31:37,845 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 04:31:37,846 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:31:53,778 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 04:31:57,709 | server.py:125 | fit progress: (2, 1.5424083471298218, {'accuracy': 0.9251, 'data_size': 10000}, 48.66516567597864)
INFO flwr 2024-04-18 04:31:57,709 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 04:31:57,709 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:32:14,302 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 04:32:18,456 | server.py:125 | fit progress: (3, 1.498085379600525, {'accuracy': 0.9636, 'data_size': 10000}, 69.41209070797777)
INFO flwr 2024-04-18 04:32:18,456 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 04:32:18,456 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:32:34,755 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 04:32:38,881 | server.py:125 | fit progress: (4, 1.494055986404419, {'accuracy': 0.9671, 'data_size': 10000}, 89.83713791199261)
INFO flwr 2024-04-18 04:32:38,881 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 04:32:38,881 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:32:54,858 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 04:32:58,765 | server.py:125 | fit progress: (5, 1.4885334968566895, {'accuracy': 0.9726, 'data_size': 10000}, 109.72139436998987)
INFO flwr 2024-04-18 04:32:58,765 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 04:32:58,766 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:33:15,170 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 04:33:19,328 | server.py:125 | fit progress: (6, 1.4868886470794678, {'accuracy': 0.9741, 'data_size': 10000}, 130.2840532679984)
INFO flwr 2024-04-18 04:33:19,328 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 04:33:19,328 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:33:36,428 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 04:33:40,356 | server.py:125 | fit progress: (7, 1.4861772060394287, {'accuracy': 0.9748, 'data_size': 10000}, 151.31286395399366)
INFO flwr 2024-04-18 04:33:40,357 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 04:33:40,357 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:33:56,203 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 04:34:00,381 | server.py:125 | fit progress: (8, 1.4901411533355713, {'accuracy': 0.9708, 'data_size': 10000}, 171.33765224399394)
INFO flwr 2024-04-18 04:34:00,382 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 04:34:00,382 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:34:17,455 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 04:34:21,601 | server.py:125 | fit progress: (9, 1.4880118370056152, {'accuracy': 0.973, 'data_size': 10000}, 192.5572590069787)
INFO flwr 2024-04-18 04:34:21,601 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 04:34:21,601 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:34:38,779 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 04:34:43,004 | server.py:125 | fit progress: (10, 1.4910367727279663, {'accuracy': 0.97, 'data_size': 10000}, 213.96036578400526)
INFO flwr 2024-04-18 04:34:43,004 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 04:34:43,004 | server.py:153 | FL finished in 213.9609377529996
INFO flwr 2024-04-18 04:34:43,005 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 04:34:43,005 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 04:34:43,005 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 04:34:43,005 | app.py:229 | app_fit: losses_centralized [(0, 2.302527904510498), (1, 1.9566625356674194), (2, 1.5424083471298218), (3, 1.498085379600525), (4, 1.494055986404419), (5, 1.4885334968566895), (6, 1.4868886470794678), (7, 1.4861772060394287), (8, 1.4901411533355713), (9, 1.4880118370056152), (10, 1.4910367727279663)]
INFO flwr 2024-04-18 04:34:43,006 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0673), (1, 0.8119), (2, 0.9251), (3, 0.9636), (4, 0.9671), (5, 0.9726), (6, 0.9741), (7, 0.9748), (8, 0.9708), (9, 0.973), (10, 0.97)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.97
wandb:     loss 1.49104
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_043041-pg57v441
wandb: Find logs at: ./wandb/offline-run-20240418_043041-pg57v441/logs
INFO flwr 2024-04-18 04:34:46,528 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 04:42:09,519 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1043042)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1043042)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 04:42:14,246	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 04:42:15,191	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 04:42:15,648	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 04:42:15,814	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (20.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 04:42:16,112	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_27349c804a3f1982.zip' (59.46MiB) to Ray cluster...
2024-04-18 04:42:16,318	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_27349c804a3f1982.zip'.
INFO flwr 2024-04-18 04:42:27,335 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 76867482009.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169357458023.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 04:42:27,336 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 04:42:27,336 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 04:42:27,356 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 04:42:27,357 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 04:42:27,357 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 04:42:27,358 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1047799)[0m 2024-04-18 04:42:33.458571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1047799)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1047806)[0m 2024-04-18 04:42:35.705082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 04:42:37,099 | server.py:94 | initial parameters (loss, other metrics): 2.3026843070983887, {'accuracy': 0.1055, 'data_size': 10000}
INFO flwr 2024-04-18 04:42:37,099 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 04:42:37,100 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1047806)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1047806)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1047795)[0m 2024-04-18 04:42:33.627580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1047795)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1047795)[0m 2024-04-18 04:42:35.889328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1047797)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1047797)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 04:43:02,377 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 04:43:06,581 | server.py:125 | fit progress: (1, 2.293633222579956, {'accuracy': 0.5192, 'data_size': 10000}, 29.481178877991624)
INFO flwr 2024-04-18 04:43:06,581 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 04:43:06,581 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:43:25,020 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 04:43:28,987 | server.py:125 | fit progress: (2, 2.270620346069336, {'accuracy': 0.6986, 'data_size': 10000}, 51.88753891500528)
INFO flwr 2024-04-18 04:43:28,987 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 04:43:28,987 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:43:45,038 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 04:43:49,204 | server.py:125 | fit progress: (3, 2.2215795516967773, {'accuracy': 0.8219, 'data_size': 10000}, 72.10458353901049)
INFO flwr 2024-04-18 04:43:49,204 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 04:43:49,205 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:44:06,122 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 04:44:10,297 | server.py:125 | fit progress: (4, 2.126415491104126, {'accuracy': 0.8665, 'data_size': 10000}, 93.19723053101916)
INFO flwr 2024-04-18 04:44:10,297 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 04:44:10,297 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:44:26,648 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 04:44:30,590 | server.py:125 | fit progress: (5, 1.974144697189331, {'accuracy': 0.9121, 'data_size': 10000}, 113.49106801499147)
INFO flwr 2024-04-18 04:44:30,591 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 04:44:30,591 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:44:46,920 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 04:44:51,142 | server.py:125 | fit progress: (6, 1.8021533489227295, {'accuracy': 0.9386, 'data_size': 10000}, 134.04299112400622)
INFO flwr 2024-04-18 04:44:51,143 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 04:44:51,143 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:45:06,468 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 04:45:10,463 | server.py:125 | fit progress: (7, 1.6602152585983276, {'accuracy': 0.951, 'data_size': 10000}, 153.36316740399343)
INFO flwr 2024-04-18 04:45:10,463 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 04:45:10,463 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:45:27,591 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 04:45:31,737 | server.py:125 | fit progress: (8, 1.5740045309066772, {'accuracy': 0.9569, 'data_size': 10000}, 174.6375138500007)
INFO flwr 2024-04-18 04:45:31,737 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 04:45:31,737 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:45:46,962 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 04:45:51,160 | server.py:125 | fit progress: (9, 1.5314515829086304, {'accuracy': 0.9595, 'data_size': 10000}, 194.0608547150041)
INFO flwr 2024-04-18 04:45:51,161 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 04:45:51,161 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:46:07,161 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 04:46:11,375 | server.py:125 | fit progress: (10, 1.5128334760665894, {'accuracy': 0.9612, 'data_size': 10000}, 214.275749249995)
INFO flwr 2024-04-18 04:46:11,376 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 04:46:11,376 | server.py:153 | FL finished in 214.2763772479957
INFO flwr 2024-04-18 04:46:11,381 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 04:46:11,381 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 04:46:11,381 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 04:46:11,381 | app.py:229 | app_fit: losses_centralized [(0, 2.3026843070983887), (1, 2.293633222579956), (2, 2.270620346069336), (3, 2.2215795516967773), (4, 2.126415491104126), (5, 1.974144697189331), (6, 1.8021533489227295), (7, 1.6602152585983276), (8, 1.5740045309066772), (9, 1.5314515829086304), (10, 1.5128334760665894)]
INFO flwr 2024-04-18 04:46:11,382 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1055), (1, 0.5192), (2, 0.6986), (3, 0.8219), (4, 0.8665), (5, 0.9121), (6, 0.9386), (7, 0.951), (8, 0.9569), (9, 0.9595), (10, 0.9612)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9612
wandb:     loss 1.51283
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_044209-4j4r5r2q
wandb: Find logs at: ./wandb/offline-run-20240418_044209-4j4r5r2q/logs
INFO flwr 2024-04-18 04:46:14,970 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 04:53:38,065 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1047792)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1047792)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 04:53:42,611	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 04:53:43,589	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 04:53:44,074	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 04:53:44,228	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (21.39MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 04:53:44,524	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e0d62bfd5591923d.zip' (59.96MiB) to Ray cluster...
2024-04-18 04:53:44,720	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e0d62bfd5591923d.zip'.
INFO flwr 2024-04-18 04:53:55,864 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 169368579892.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'object_store_memory': 76872248524.0}
INFO flwr 2024-04-18 04:53:55,864 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 04:53:55,864 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 04:53:55,895 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 04:53:55,897 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 04:53:55,897 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 04:53:55,898 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1052158)[0m 2024-04-18 04:54:02.029782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1052158)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1052159)[0m 2024-04-18 04:54:04.344013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 04:54:04,936 | server.py:94 | initial parameters (loss, other metrics): 2.302581548690796, {'accuracy': 0.0973, 'data_size': 10000}
INFO flwr 2024-04-18 04:54:04,936 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 04:54:04,937 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1052177)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1052177)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1052176)[0m 2024-04-18 04:54:02.133695: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1052176)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1052176)[0m 2024-04-18 04:54:04.475224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1052159)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1052159)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 04:54:28,827 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 04:54:33,101 | server.py:125 | fit progress: (1, 2.3020057678222656, {'accuracy': 0.0974, 'data_size': 10000}, 28.164102879993152)
INFO flwr 2024-04-18 04:54:33,101 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 04:54:33,101 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:54:49,666 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 04:54:53,599 | server.py:125 | fit progress: (2, 2.3011887073516846, {'accuracy': 0.1008, 'data_size': 10000}, 48.66249348499696)
INFO flwr 2024-04-18 04:54:53,599 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 04:54:53,600 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:55:10,244 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 04:55:14,389 | server.py:125 | fit progress: (3, 2.3001279830932617, {'accuracy': 0.1677, 'data_size': 10000}, 69.45250949601177)
INFO flwr 2024-04-18 04:55:14,389 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 04:55:14,390 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:55:30,664 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 04:55:34,931 | server.py:125 | fit progress: (4, 2.2988269329071045, {'accuracy': 0.2928, 'data_size': 10000}, 89.99451303901151)
INFO flwr 2024-04-18 04:55:34,931 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 04:55:34,932 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:55:50,528 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 04:55:54,455 | server.py:125 | fit progress: (5, 2.297257423400879, {'accuracy': 0.3773, 'data_size': 10000}, 109.51860861701425)
INFO flwr 2024-04-18 04:55:54,455 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 04:55:54,456 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:56:10,621 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 04:56:14,818 | server.py:125 | fit progress: (6, 2.2954185009002686, {'accuracy': 0.4597, 'data_size': 10000}, 129.88132467799005)
INFO flwr 2024-04-18 04:56:14,818 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 04:56:14,818 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:56:29,603 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 04:56:33,564 | server.py:125 | fit progress: (7, 2.2932751178741455, {'accuracy': 0.5354, 'data_size': 10000}, 148.62793596100528)
INFO flwr 2024-04-18 04:56:33,565 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 04:56:33,565 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:56:52,406 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 04:56:56,605 | server.py:125 | fit progress: (8, 2.2908995151519775, {'accuracy': 0.5992, 'data_size': 10000}, 171.66873545799172)
INFO flwr 2024-04-18 04:56:56,606 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 04:56:56,606 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:57:15,666 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 04:57:19,818 | server.py:125 | fit progress: (9, 2.28826642036438, {'accuracy': 0.6551, 'data_size': 10000}, 194.88129856300657)
INFO flwr 2024-04-18 04:57:19,818 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 04:57:19,818 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 04:57:35,780 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 04:57:39,972 | server.py:125 | fit progress: (10, 2.285377264022827, {'accuracy': 0.6998, 'data_size': 10000}, 215.03573076598695)
INFO flwr 2024-04-18 04:57:39,973 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 04:57:39,973 | server.py:153 | FL finished in 215.03644410701236
INFO flwr 2024-04-18 04:57:39,973 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 04:57:39,973 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 04:57:39,973 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 04:57:39,973 | app.py:229 | app_fit: losses_centralized [(0, 2.302581548690796), (1, 2.3020057678222656), (2, 2.3011887073516846), (3, 2.3001279830932617), (4, 2.2988269329071045), (5, 2.297257423400879), (6, 2.2954185009002686), (7, 2.2932751178741455), (8, 2.2908995151519775), (9, 2.28826642036438), (10, 2.285377264022827)]
INFO flwr 2024-04-18 04:57:39,974 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0973), (1, 0.0974), (2, 0.1008), (3, 0.1677), (4, 0.2928), (5, 0.3773), (6, 0.4597), (7, 0.5354), (8, 0.5992), (9, 0.6551), (10, 0.6998)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6998
wandb:     loss 2.28538
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_045337-0urpml1s
wandb: Find logs at: ./wandb/offline-run-20240418_045337-0urpml1s/logs
INFO flwr 2024-04-18 04:57:43,551 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 05:05:06,958 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1052157)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1052157)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 05:05:11,442	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 05:05:12,423	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 05:05:12,898	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 05:05:13,062	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (21.88MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 05:05:13,371	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c0ad2d000fe4e647.zip' (60.46MiB) to Ray cluster...
2024-04-18 05:05:13,579	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c0ad2d000fe4e647.zip'.
INFO flwr 2024-04-18 05:05:24,636 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 169273268429.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 76831400755.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 05:05:24,636 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 05:05:24,636 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 05:05:24,654 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 05:05:24,655 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 05:05:24,656 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 05:05:24,656 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1057173)[0m 2024-04-18 05:05:30.722202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1057173)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1057173)[0m 2024-04-18 05:05:33.015965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 05:05:33,676 | server.py:94 | initial parameters (loss, other metrics): 2.302260637283325, {'accuracy': 0.1244, 'data_size': 10000}
INFO flwr 2024-04-18 05:05:33,677 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 05:05:33,677 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1057178)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1057178)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1057175)[0m 2024-04-18 05:05:31.206766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1057175)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1057175)[0m 2024-04-18 05:05:33.559565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1057174)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1057174)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 05:05:58,717 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 05:06:02,944 | server.py:125 | fit progress: (1, 2.3022000789642334, {'accuracy': 0.134, 'data_size': 10000}, 29.266795745003037)
INFO flwr 2024-04-18 05:06:02,944 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 05:06:02,944 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:06:19,597 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 05:06:23,623 | server.py:125 | fit progress: (2, 2.3021183013916016, {'accuracy': 0.1439, 'data_size': 10000}, 49.94604015798541)
INFO flwr 2024-04-18 05:06:23,623 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 05:06:23,624 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:06:41,874 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 05:06:46,037 | server.py:125 | fit progress: (3, 2.302021026611328, {'accuracy': 0.1557, 'data_size': 10000}, 72.3594440779998)
INFO flwr 2024-04-18 05:06:46,037 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 05:06:46,037 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:07:02,876 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 05:07:07,037 | server.py:125 | fit progress: (4, 2.3019118309020996, {'accuracy': 0.1685, 'data_size': 10000}, 93.35952508298215)
INFO flwr 2024-04-18 05:07:07,037 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 05:07:07,037 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:07:22,341 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 05:07:26,280 | server.py:125 | fit progress: (5, 2.301792860031128, {'accuracy': 0.1831, 'data_size': 10000}, 112.60249426198425)
INFO flwr 2024-04-18 05:07:26,280 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 05:07:26,280 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:07:42,164 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 05:07:46,329 | server.py:125 | fit progress: (6, 2.301666498184204, {'accuracy': 0.1973, 'data_size': 10000}, 132.6518794009753)
INFO flwr 2024-04-18 05:07:46,329 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 05:07:46,330 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:08:02,836 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 05:08:06,780 | server.py:125 | fit progress: (7, 2.301534414291382, {'accuracy': 0.2091, 'data_size': 10000}, 153.1031789589906)
INFO flwr 2024-04-18 05:08:06,781 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 05:08:06,781 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:08:23,226 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 05:08:27,442 | server.py:125 | fit progress: (8, 2.3013975620269775, {'accuracy': 0.2211, 'data_size': 10000}, 173.76497209997615)
INFO flwr 2024-04-18 05:08:27,442 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 05:08:27,443 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:08:45,613 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 05:08:49,819 | server.py:125 | fit progress: (9, 2.3012545108795166, {'accuracy': 0.2321, 'data_size': 10000}, 196.14155714699882)
INFO flwr 2024-04-18 05:08:49,819 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 05:08:49,819 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:09:06,320 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 05:09:10,542 | server.py:125 | fit progress: (10, 2.3011090755462646, {'accuracy': 0.2384, 'data_size': 10000}, 216.8647015679744)
INFO flwr 2024-04-18 05:09:10,542 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 05:09:10,542 | server.py:153 | FL finished in 216.86522363999393
INFO flwr 2024-04-18 05:09:10,547 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 05:09:10,547 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 05:09:10,547 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 05:09:10,548 | app.py:229 | app_fit: losses_centralized [(0, 2.302260637283325), (1, 2.3022000789642334), (2, 2.3021183013916016), (3, 2.302021026611328), (4, 2.3019118309020996), (5, 2.301792860031128), (6, 2.301666498184204), (7, 2.301534414291382), (8, 2.3013975620269775), (9, 2.3012545108795166), (10, 2.3011090755462646)]
INFO flwr 2024-04-18 05:09:10,548 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1244), (1, 0.134), (2, 0.1439), (3, 0.1557), (4, 0.1685), (5, 0.1831), (6, 0.1973), (7, 0.2091), (8, 0.2211), (9, 0.2321), (10, 0.2384)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2384
wandb:     loss 2.30111
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_050506-q1jw6gyl
wandb: Find logs at: ./wandb/offline-run-20240418_050506-q1jw6gyl/logs
INFO flwr 2024-04-18 05:09:14,077 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 05:16:37,215 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1057169)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1057169)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 05:16:42,088	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 05:16:43,030	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 05:16:43,502	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 05:16:43,656	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (22.84MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 05:16:43,959	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e207ff233bf7b441.zip' (61.43MiB) to Ray cluster...
2024-04-18 05:16:44,161	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e207ff233bf7b441.zip'.
INFO flwr 2024-04-18 05:16:55,250 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 76902824755.0, 'accelerator_type:TITAN': 1.0, 'memory': 169439924429.0}
INFO flwr 2024-04-18 05:16:55,250 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 05:16:55,250 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 05:16:55,270 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 05:16:55,271 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 05:16:55,271 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 05:16:55,271 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1061907)[0m 2024-04-18 05:17:01.375996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1061907)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1061907)[0m 2024-04-18 05:17:03.708393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 05:17:04,270 | server.py:94 | initial parameters (loss, other metrics): 2.3028411865234375, {'accuracy': 0.0599, 'data_size': 10000}
INFO flwr 2024-04-18 05:17:04,271 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 05:17:04,271 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1061914)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1061914)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1061913)[0m 2024-04-18 05:17:01.520579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1061913)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1061913)[0m 2024-04-18 05:17:03.765459: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1061907)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1061907)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 05:17:28,833 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 05:17:33,125 | server.py:125 | fit progress: (1, 1.6607794761657715, {'accuracy': 0.8004, 'data_size': 10000}, 28.85398209898267)
INFO flwr 2024-04-18 05:17:33,125 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 05:17:33,125 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:17:50,593 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 05:17:54,532 | server.py:125 | fit progress: (2, 1.9686373472213745, {'accuracy': 0.4925, 'data_size': 10000}, 50.26102140999865)
INFO flwr 2024-04-18 05:17:54,532 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 05:17:54,532 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:18:11,124 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 05:18:15,330 | server.py:125 | fit progress: (3, 2.270343065261841, {'accuracy': 0.1908, 'data_size': 10000}, 71.05897413397906)
INFO flwr 2024-04-18 05:18:15,330 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 05:18:15,330 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:18:31,524 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 05:18:35,675 | server.py:125 | fit progress: (4, 2.3281421661376953, {'accuracy': 0.133, 'data_size': 10000}, 91.40378247198532)
INFO flwr 2024-04-18 05:18:35,675 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 05:18:35,675 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:18:51,634 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 05:18:55,586 | server.py:125 | fit progress: (5, 2.3554422855377197, {'accuracy': 0.1057, 'data_size': 10000}, 111.31503189899377)
INFO flwr 2024-04-18 05:18:55,586 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 05:18:55,587 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:19:13,571 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 05:19:17,738 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 133.4667198949901)
INFO flwr 2024-04-18 05:19:17,738 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 05:19:17,738 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:19:35,684 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 05:19:39,638 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 155.36674848099938)
INFO flwr 2024-04-18 05:19:39,638 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 05:19:39,639 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:19:55,639 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 05:19:59,821 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 175.54986284999177)
INFO flwr 2024-04-18 05:19:59,821 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 05:19:59,821 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:20:15,494 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 05:20:19,741 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 195.4704335029819)
INFO flwr 2024-04-18 05:20:19,742 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 05:20:19,742 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:20:37,707 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 05:20:41,885 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 217.61408554197988)
INFO flwr 2024-04-18 05:20:41,885 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 05:20:41,885 | server.py:153 | FL finished in 217.6146308069874
INFO flwr 2024-04-18 05:20:41,886 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 05:20:41,886 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 05:20:41,886 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 05:20:41,886 | app.py:229 | app_fit: losses_centralized [(0, 2.3028411865234375), (1, 1.6607794761657715), (2, 1.9686373472213745), (3, 2.270343065261841), (4, 2.3281421661376953), (5, 2.3554422855377197), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-18 05:20:41,886 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0599), (1, 0.8004), (2, 0.4925), (3, 0.1908), (4, 0.133), (5, 0.1057), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_051636-x7ihzxyb
wandb: Find logs at: ./wandb/offline-run-20240418_051636-x7ihzxyb/logs
INFO flwr 2024-04-18 05:20:45,487 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 05:28:08,944 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1061904)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1061904)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 05:28:13,746	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 05:28:14,681	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 05:28:15,133	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 05:28:15,277	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (23.32MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 05:28:15,582	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_384e350a47e9ce69.zip' (61.93MiB) to Ray cluster...
2024-04-18 05:28:15,790	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_384e350a47e9ce69.zip'.
INFO flwr 2024-04-18 05:28:26,939 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169518001152.0, 'CPU': 64.0, 'object_store_memory': 76936286208.0}
INFO flwr 2024-04-18 05:28:26,940 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 05:28:26,940 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 05:28:26,965 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 05:28:26,966 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 05:28:26,967 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 05:28:26,967 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1066307)[0m 2024-04-18 05:28:32.921627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1066307)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1066307)[0m 2024-04-18 05:28:35.285356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 05:28:36,769 | server.py:94 | initial parameters (loss, other metrics): 2.3027243614196777, {'accuracy': 0.0749, 'data_size': 10000}
INFO flwr 2024-04-18 05:28:36,769 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 05:28:36,769 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1066307)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1066307)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1066304)[0m 2024-04-18 05:28:33.329480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1066304)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1066304)[0m 2024-04-18 05:28:35.503327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1066303)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1066303)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 05:29:01,536 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 05:29:05,785 | server.py:125 | fit progress: (1, 2.0298516750335693, {'accuracy': 0.8759, 'data_size': 10000}, 29.01531272698776)
INFO flwr 2024-04-18 05:29:05,785 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 05:29:05,785 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:29:21,557 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 05:29:25,503 | server.py:125 | fit progress: (2, 1.5282509326934814, {'accuracy': 0.9483, 'data_size': 10000}, 48.73339918698184)
INFO flwr 2024-04-18 05:29:25,503 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 05:29:25,503 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:29:42,647 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 05:29:46,794 | server.py:125 | fit progress: (3, 1.4982750415802002, {'accuracy': 0.9634, 'data_size': 10000}, 70.02462892598123)
INFO flwr 2024-04-18 05:29:46,794 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 05:29:46,794 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:30:04,396 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 05:30:08,546 | server.py:125 | fit progress: (4, 1.5002309083938599, {'accuracy': 0.9603, 'data_size': 10000}, 91.77669281398994)
INFO flwr 2024-04-18 05:30:08,546 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 05:30:08,546 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:30:24,637 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 05:30:28,551 | server.py:125 | fit progress: (5, 1.4989081621170044, {'accuracy': 0.9624, 'data_size': 10000}, 111.78197210497456)
INFO flwr 2024-04-18 05:30:28,552 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 05:30:28,552 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:30:44,699 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 05:30:48,852 | server.py:125 | fit progress: (6, 1.4928977489471436, {'accuracy': 0.9682, 'data_size': 10000}, 132.08256849797908)
INFO flwr 2024-04-18 05:30:48,852 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 05:30:48,852 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:31:07,496 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 05:31:11,434 | server.py:125 | fit progress: (7, 1.492182970046997, {'accuracy': 0.969, 'data_size': 10000}, 154.66424115098198)
INFO flwr 2024-04-18 05:31:11,434 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 05:31:11,434 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:31:25,775 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 05:31:29,951 | server.py:125 | fit progress: (8, 1.4945573806762695, {'accuracy': 0.9665, 'data_size': 10000}, 173.18188158099656)
INFO flwr 2024-04-18 05:31:29,952 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 05:31:29,952 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:31:46,079 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 05:31:50,237 | server.py:125 | fit progress: (9, 1.4949545860290527, {'accuracy': 0.9662, 'data_size': 10000}, 193.46820238098735)
INFO flwr 2024-04-18 05:31:50,238 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 05:31:50,238 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:32:05,223 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 05:32:09,433 | server.py:125 | fit progress: (10, 1.4948629140853882, {'accuracy': 0.9663, 'data_size': 10000}, 212.664183957997)
INFO flwr 2024-04-18 05:32:09,434 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 05:32:09,434 | server.py:153 | FL finished in 212.66470554898842
INFO flwr 2024-04-18 05:32:09,434 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 05:32:09,434 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 05:32:09,434 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 05:32:09,435 | app.py:229 | app_fit: losses_centralized [(0, 2.3027243614196777), (1, 2.0298516750335693), (2, 1.5282509326934814), (3, 1.4982750415802002), (4, 1.5002309083938599), (5, 1.4989081621170044), (6, 1.4928977489471436), (7, 1.492182970046997), (8, 1.4945573806762695), (9, 1.4949545860290527), (10, 1.4948629140853882)]
INFO flwr 2024-04-18 05:32:09,435 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0749), (1, 0.8759), (2, 0.9483), (3, 0.9634), (4, 0.9603), (5, 0.9624), (6, 0.9682), (7, 0.969), (8, 0.9665), (9, 0.9662), (10, 0.9663)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9663
wandb:     loss 1.49486
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_052808-4m18zcqj
wandb: Find logs at: ./wandb/offline-run-20240418_052808-4m18zcqj/logs
INFO flwr 2024-04-18 05:32:12,956 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 05:39:35,901 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1066296)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1066296)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 05:39:40,329	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 05:39:41,327	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 05:39:41,812	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 05:39:41,966	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (23.81MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 05:39:42,271	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cddb4ad3ce24b810.zip' (62.43MiB) to Ray cluster...
2024-04-18 05:39:42,486	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cddb4ad3ce24b810.zip'.
INFO flwr 2024-04-18 05:39:53,638 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169401475277.0, 'object_store_memory': 76886346547.0}
INFO flwr 2024-04-18 05:39:53,639 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 05:39:53,639 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 05:39:53,659 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 05:39:53,660 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 05:39:53,661 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 05:39:53,661 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1070668)[0m 2024-04-18 05:39:59.818951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1070668)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1070668)[0m 2024-04-18 05:40:02.156808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 05:40:02,843 | server.py:94 | initial parameters (loss, other metrics): 2.3027853965759277, {'accuracy': 0.1018, 'data_size': 10000}
INFO flwr 2024-04-18 05:40:02,843 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 05:40:02,843 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1070680)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1070680)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1070677)[0m 2024-04-18 05:40:00.025312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1070677)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1070680)[0m 2024-04-18 05:40:02.207389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1070673)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1070673)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 05:40:28,164 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 05:40:32,399 | server.py:125 | fit progress: (1, 2.294590711593628, {'accuracy': 0.4046, 'data_size': 10000}, 29.555953275994398)
INFO flwr 2024-04-18 05:40:32,399 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 05:40:32,400 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:40:50,105 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 05:40:54,061 | server.py:125 | fit progress: (2, 2.2773666381835938, {'accuracy': 0.6797, 'data_size': 10000}, 51.21806287599611)
INFO flwr 2024-04-18 05:40:54,062 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 05:40:54,062 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:41:10,150 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 05:41:14,345 | server.py:125 | fit progress: (3, 2.2460544109344482, {'accuracy': 0.8175, 'data_size': 10000}, 71.5020427670097)
INFO flwr 2024-04-18 05:41:14,345 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 05:41:14,346 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:41:29,818 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 05:41:33,985 | server.py:125 | fit progress: (4, 2.187748908996582, {'accuracy': 0.8813, 'data_size': 10000}, 91.14230453499476)
INFO flwr 2024-04-18 05:41:33,986 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 05:41:33,986 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:41:51,780 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 05:41:55,741 | server.py:125 | fit progress: (5, 2.0869016647338867, {'accuracy': 0.9092, 'data_size': 10000}, 112.89802903099917)
INFO flwr 2024-04-18 05:41:55,742 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 05:41:55,742 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:42:11,633 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 05:42:15,795 | server.py:125 | fit progress: (6, 1.9415528774261475, {'accuracy': 0.9278, 'data_size': 10000}, 132.95224537700415)
INFO flwr 2024-04-18 05:42:15,796 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 05:42:15,796 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:42:31,650 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 05:42:35,672 | server.py:125 | fit progress: (7, 1.7823067903518677, {'accuracy': 0.9394, 'data_size': 10000}, 152.82839030399919)
INFO flwr 2024-04-18 05:42:35,672 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 05:42:35,672 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:42:52,392 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 05:42:56,524 | server.py:125 | fit progress: (8, 1.6553293466567993, {'accuracy': 0.9461, 'data_size': 10000}, 173.68099121999694)
INFO flwr 2024-04-18 05:42:56,525 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 05:42:56,525 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:43:11,381 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 05:43:15,529 | server.py:125 | fit progress: (9, 1.5763717889785767, {'accuracy': 0.9511, 'data_size': 10000}, 192.68593783400138)
INFO flwr 2024-04-18 05:43:15,530 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 05:43:15,530 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:43:31,779 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 05:43:36,002 | server.py:125 | fit progress: (10, 1.5366674661636353, {'accuracy': 0.9561, 'data_size': 10000}, 213.1584312680061)
INFO flwr 2024-04-18 05:43:36,002 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 05:43:36,002 | server.py:153 | FL finished in 213.1590367470053
INFO flwr 2024-04-18 05:43:36,009 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 05:43:36,009 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 05:43:36,010 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 05:43:36,010 | app.py:229 | app_fit: losses_centralized [(0, 2.3027853965759277), (1, 2.294590711593628), (2, 2.2773666381835938), (3, 2.2460544109344482), (4, 2.187748908996582), (5, 2.0869016647338867), (6, 1.9415528774261475), (7, 1.7823067903518677), (8, 1.6553293466567993), (9, 1.5763717889785767), (10, 1.5366674661636353)]
INFO flwr 2024-04-18 05:43:36,010 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1018), (1, 0.4046), (2, 0.6797), (3, 0.8175), (4, 0.8813), (5, 0.9092), (6, 0.9278), (7, 0.9394), (8, 0.9461), (9, 0.9511), (10, 0.9561)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9561
wandb:     loss 1.53667
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_053935-gfwffcvz
wandb: Find logs at: ./wandb/offline-run-20240418_053935-gfwffcvz/logs
INFO flwr 2024-04-18 05:43:39,537 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 05:51:02,567 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1070668)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1070668)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 05:51:07,633	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 05:51:08,595	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 05:51:09,071	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 05:51:09,236	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (24.29MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 05:51:09,550	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3c13623c16ed7eb9.zip' (62.93MiB) to Ray cluster...
2024-04-18 05:51:09,762	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3c13623c16ed7eb9.zip'.
INFO flwr 2024-04-18 05:51:20,843 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 76902346752.0, 'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169438809088.0}
INFO flwr 2024-04-18 05:51:20,843 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 05:51:20,843 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 05:51:20,861 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 05:51:20,862 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 05:51:20,862 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 05:51:20,863 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1076137)[0m 2024-04-18 05:51:26.963479: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1076137)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1076137)[0m 2024-04-18 05:51:29.298001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 05:51:30,480 | server.py:94 | initial parameters (loss, other metrics): 2.302762031555176, {'accuracy': 0.0875, 'data_size': 10000}
INFO flwr 2024-04-18 05:51:30,480 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 05:51:30,481 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1076144)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1076144)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1076135)[0m 2024-04-18 05:51:27.269646: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1076135)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1076135)[0m 2024-04-18 05:51:29.492728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1076136)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1076136)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 05:51:54,939 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 05:51:59,156 | server.py:125 | fit progress: (1, 2.3022868633270264, {'accuracy': 0.1495, 'data_size': 10000}, 28.675881100993138)
INFO flwr 2024-04-18 05:51:59,157 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 05:51:59,157 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:52:15,928 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 05:52:19,890 | server.py:125 | fit progress: (2, 2.3016300201416016, {'accuracy': 0.29, 'data_size': 10000}, 49.40975615900243)
INFO flwr 2024-04-18 05:52:19,891 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 05:52:19,891 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:52:37,180 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 05:52:41,357 | server.py:125 | fit progress: (3, 2.3007822036743164, {'accuracy': 0.3689, 'data_size': 10000}, 70.87626821899903)
INFO flwr 2024-04-18 05:52:41,357 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 05:52:41,357 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:52:57,158 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 05:53:01,327 | server.py:125 | fit progress: (4, 2.29975962638855, {'accuracy': 0.4197, 'data_size': 10000}, 90.84654095998849)
INFO flwr 2024-04-18 05:53:01,328 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 05:53:01,328 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:53:16,620 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 05:53:20,575 | server.py:125 | fit progress: (5, 2.298558235168457, {'accuracy': 0.4604, 'data_size': 10000}, 110.09454088998609)
INFO flwr 2024-04-18 05:53:20,575 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 05:53:20,576 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:53:36,192 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 05:53:40,347 | server.py:125 | fit progress: (6, 2.297173023223877, {'accuracy': 0.5112, 'data_size': 10000}, 129.86609528199187)
INFO flwr 2024-04-18 05:53:40,347 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 05:53:40,347 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:53:56,762 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 05:54:00,699 | server.py:125 | fit progress: (7, 2.2955715656280518, {'accuracy': 0.5629, 'data_size': 10000}, 150.2185036459996)
INFO flwr 2024-04-18 05:54:00,699 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 05:54:00,700 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:54:17,522 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 05:54:21,698 | server.py:125 | fit progress: (8, 2.2937541007995605, {'accuracy': 0.6163, 'data_size': 10000}, 171.2178745289857)
INFO flwr 2024-04-18 05:54:21,699 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 05:54:21,699 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:54:39,190 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 05:54:43,356 | server.py:125 | fit progress: (9, 2.291691541671753, {'accuracy': 0.6659, 'data_size': 10000}, 192.87581172698992)
INFO flwr 2024-04-18 05:54:43,357 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 05:54:43,357 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 05:54:58,869 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 05:55:03,072 | server.py:125 | fit progress: (10, 2.28936767578125, {'accuracy': 0.7071, 'data_size': 10000}, 212.5915463809797)
INFO flwr 2024-04-18 05:55:03,072 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 05:55:03,072 | server.py:153 | FL finished in 212.5920209659962
INFO flwr 2024-04-18 05:55:03,077 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 05:55:03,077 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 05:55:03,077 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 05:55:03,077 | app.py:229 | app_fit: losses_centralized [(0, 2.302762031555176), (1, 2.3022868633270264), (2, 2.3016300201416016), (3, 2.3007822036743164), (4, 2.29975962638855), (5, 2.298558235168457), (6, 2.297173023223877), (7, 2.2955715656280518), (8, 2.2937541007995605), (9, 2.291691541671753), (10, 2.28936767578125)]
INFO flwr 2024-04-18 05:55:03,078 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0875), (1, 0.1495), (2, 0.29), (3, 0.3689), (4, 0.4197), (5, 0.4604), (6, 0.5112), (7, 0.5629), (8, 0.6163), (9, 0.6659), (10, 0.7071)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7071
wandb:     loss 2.28937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_055102-st4aew8x
wandb: Find logs at: ./wandb/offline-run-20240418_055102-st4aew8x/logs
INFO flwr 2024-04-18 05:55:06,634 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 06:02:29,682 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1076133)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1076133)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 06:02:34,339	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 06:02:35,476	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 06:02:35,954	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 06:02:36,110	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (24.78MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 06:02:36,425	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_797885905a277912.zip' (63.42MiB) to Ray cluster...
2024-04-18 06:02:36,636	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_797885905a277912.zip'.
INFO flwr 2024-04-18 06:02:47,749 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 76919772364.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 169479468852.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 06:02:47,749 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 06:02:47,749 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 06:02:47,767 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 06:02:47,768 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 06:02:47,768 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 06:02:47,768 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1080536)[0m 2024-04-18 06:02:53.796824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1080536)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1080531)[0m 2024-04-18 06:02:56.137761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 06:02:57,151 | server.py:94 | initial parameters (loss, other metrics): 2.302736759185791, {'accuracy': 0.0609, 'data_size': 10000}
INFO flwr 2024-04-18 06:02:57,152 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 06:02:57,152 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1080541)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1080541)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1080529)[0m 2024-04-18 06:02:54.118605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1080529)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1080529)[0m 2024-04-18 06:02:56.369726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1080533)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1080533)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 06:03:21,443 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 06:03:25,733 | server.py:125 | fit progress: (1, 2.3026845455169678, {'accuracy': 0.0633, 'data_size': 10000}, 28.5810208870098)
INFO flwr 2024-04-18 06:03:25,733 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 06:03:25,733 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:03:42,289 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 06:03:46,253 | server.py:125 | fit progress: (2, 2.302614688873291, {'accuracy': 0.0684, 'data_size': 10000}, 49.1011118190072)
INFO flwr 2024-04-18 06:03:46,253 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 06:03:46,254 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:04:02,408 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 06:04:06,607 | server.py:125 | fit progress: (3, 2.3025341033935547, {'accuracy': 0.0735, 'data_size': 10000}, 69.45508992200484)
INFO flwr 2024-04-18 06:04:06,607 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 06:04:06,608 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:04:22,074 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 06:04:26,218 | server.py:125 | fit progress: (4, 2.302445411682129, {'accuracy': 0.083, 'data_size': 10000}, 89.06611510500079)
INFO flwr 2024-04-18 06:04:26,218 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 06:04:26,219 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:04:43,150 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 06:04:47,105 | server.py:125 | fit progress: (5, 2.3023500442504883, {'accuracy': 0.0941, 'data_size': 10000}, 109.95302702099434)
INFO flwr 2024-04-18 06:04:47,105 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 06:04:47,120 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:05:04,579 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 06:05:08,770 | server.py:125 | fit progress: (6, 2.302250862121582, {'accuracy': 0.1048, 'data_size': 10000}, 131.61785442498513)
INFO flwr 2024-04-18 06:05:08,770 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 06:05:08,770 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:05:26,770 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 06:05:30,728 | server.py:125 | fit progress: (7, 2.30214786529541, {'accuracy': 0.1197, 'data_size': 10000}, 153.57622997599537)
INFO flwr 2024-04-18 06:05:30,729 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 06:05:30,729 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:05:46,824 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 06:05:51,049 | server.py:125 | fit progress: (8, 2.302039861679077, {'accuracy': 0.1322, 'data_size': 10000}, 173.89723293599673)
INFO flwr 2024-04-18 06:05:51,049 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 06:05:51,050 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:06:06,036 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 06:06:10,203 | server.py:125 | fit progress: (9, 2.3019301891326904, {'accuracy': 0.1401, 'data_size': 10000}, 193.05140582099557)
INFO flwr 2024-04-18 06:06:10,204 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 06:06:10,204 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:06:27,109 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 06:06:31,316 | server.py:125 | fit progress: (10, 2.301818370819092, {'accuracy': 0.148, 'data_size': 10000}, 214.16367364701)
INFO flwr 2024-04-18 06:06:31,316 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 06:06:31,316 | server.py:153 | FL finished in 214.16416581399972
INFO flwr 2024-04-18 06:06:31,323 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 06:06:31,323 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 06:06:31,323 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 06:06:31,323 | app.py:229 | app_fit: losses_centralized [(0, 2.302736759185791), (1, 2.3026845455169678), (2, 2.302614688873291), (3, 2.3025341033935547), (4, 2.302445411682129), (5, 2.3023500442504883), (6, 2.302250862121582), (7, 2.30214786529541), (8, 2.302039861679077), (9, 2.3019301891326904), (10, 2.301818370819092)]
INFO flwr 2024-04-18 06:06:31,324 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0609), (1, 0.0633), (2, 0.0684), (3, 0.0735), (4, 0.083), (5, 0.0941), (6, 0.1048), (7, 0.1197), (8, 0.1322), (9, 0.1401), (10, 0.148)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.148
wandb:     loss 2.30182
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_060229-hlj5hugi
wandb: Find logs at: ./wandb/offline-run-20240418_060229-hlj5hugi/logs
INFO flwr 2024-04-18 06:06:34,845 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 06:13:57,831 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1080529)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1080529)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 06:14:02,544	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 06:14:03,524	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 06:14:04,042	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 06:14:04,199	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (25.26MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 06:14:04,511	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8c8fcad5184c941c.zip' (63.92MiB) to Ray cluster...
2024-04-18 06:14:04,719	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8c8fcad5184c941c.zip'.
INFO flwr 2024-04-18 06:14:15,812 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169478754919.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 76919466393.0}
INFO flwr 2024-04-18 06:14:15,812 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 06:14:15,813 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 06:14:15,828 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 06:14:15,829 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 06:14:15,830 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 06:14:15,830 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1085333)[0m 2024-04-18 06:14:21.877604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1085333)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1085333)[0m 2024-04-18 06:14:24.195872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 06:14:25,020 | server.py:94 | initial parameters (loss, other metrics): 2.302586555480957, {'accuracy': 0.1748, 'data_size': 10000}
INFO flwr 2024-04-18 06:14:25,020 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 06:14:25,021 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1085337)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1085337)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1085330)[0m 2024-04-18 06:14:22.113020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1085330)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1085329)[0m 2024-04-18 06:14:24.376935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1085329)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1085329)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 06:14:50,999 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 06:14:55,291 | server.py:125 | fit progress: (1, 1.6429435014724731, {'accuracy': 0.8178, 'data_size': 10000}, 30.26994993700646)
INFO flwr 2024-04-18 06:14:55,291 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 06:14:55,291 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:15:11,866 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 06:15:15,841 | server.py:125 | fit progress: (2, 1.9606517553329468, {'accuracy': 0.5004, 'data_size': 10000}, 50.82059631199809)
INFO flwr 2024-04-18 06:15:15,842 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 06:15:15,842 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:15:31,696 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 06:15:35,853 | server.py:125 | fit progress: (3, 2.006883382797241, {'accuracy': 0.4542, 'data_size': 10000}, 70.8322444249934)
INFO flwr 2024-04-18 06:15:35,853 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 06:15:35,853 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:15:51,118 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 06:15:55,316 | server.py:125 | fit progress: (4, 2.1010427474975586, {'accuracy': 0.3601, 'data_size': 10000}, 90.29560165898874)
INFO flwr 2024-04-18 06:15:55,317 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 06:15:55,317 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:16:11,304 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 06:16:15,250 | server.py:125 | fit progress: (5, 2.1270439624786377, {'accuracy': 0.3341, 'data_size': 10000}, 110.22913029399933)
INFO flwr 2024-04-18 06:16:15,250 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 06:16:15,250 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:16:32,147 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 06:16:36,305 | server.py:125 | fit progress: (6, 2.138643741607666, {'accuracy': 0.3225, 'data_size': 10000}, 131.28479493799387)
INFO flwr 2024-04-18 06:16:36,306 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 06:16:36,306 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:16:50,880 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 06:16:54,788 | server.py:125 | fit progress: (7, 2.1450438499450684, {'accuracy': 0.3161, 'data_size': 10000}, 149.76774792998913)
INFO flwr 2024-04-18 06:16:54,789 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 06:16:54,789 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:17:10,998 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 06:17:15,155 | server.py:125 | fit progress: (8, 2.148043632507324, {'accuracy': 0.3131, 'data_size': 10000}, 170.1348763369897)
INFO flwr 2024-04-18 06:17:15,156 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 06:17:15,156 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:17:31,963 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 06:17:36,145 | server.py:125 | fit progress: (9, 2.1905436515808105, {'accuracy': 0.2706, 'data_size': 10000}, 191.12449348601513)
INFO flwr 2024-04-18 06:17:36,146 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 06:17:36,146 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:17:53,093 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 06:17:57,264 | server.py:125 | fit progress: (10, 2.2512435913085938, {'accuracy': 0.2099, 'data_size': 10000}, 212.2435511299991)
INFO flwr 2024-04-18 06:17:57,264 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 06:17:57,265 | server.py:153 | FL finished in 212.24400006400538
INFO flwr 2024-04-18 06:17:57,270 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 06:17:57,270 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 06:17:57,270 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 06:17:57,270 | app.py:229 | app_fit: losses_centralized [(0, 2.302586555480957), (1, 1.6429435014724731), (2, 1.9606517553329468), (3, 2.006883382797241), (4, 2.1010427474975586), (5, 2.1270439624786377), (6, 2.138643741607666), (7, 2.1450438499450684), (8, 2.148043632507324), (9, 2.1905436515808105), (10, 2.2512435913085938)]
INFO flwr 2024-04-18 06:17:57,270 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1748), (1, 0.8178), (2, 0.5004), (3, 0.4542), (4, 0.3601), (5, 0.3341), (6, 0.3225), (7, 0.3161), (8, 0.3131), (9, 0.2706), (10, 0.2099)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2099
wandb:     loss 2.25124
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_061357-49kkqjf7
wandb: Find logs at: ./wandb/offline-run-20240418_061357-49kkqjf7/logs
INFO flwr 2024-04-18 06:18:00,810 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 06:25:23,767 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1085324)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1085324)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 06:25:29,505	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 06:25:30,499	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 06:25:30,966	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 06:25:31,122	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (25.75MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 06:25:31,434	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8f5abc3cf77fcf7c.zip' (64.42MiB) to Ray cluster...
2024-04-18 06:25:31,646	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8f5abc3cf77fcf7c.zip'.
INFO flwr 2024-04-18 06:25:42,792 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169289849447.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'object_store_memory': 76838506905.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-18 06:25:42,792 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 06:25:42,793 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 06:25:42,811 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 06:25:42,812 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 06:25:42,812 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 06:25:42,813 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1089782)[0m 2024-04-18 06:25:48.981307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1089782)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1089782)[0m 2024-04-18 06:25:51.297652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 06:25:51,627 | server.py:94 | initial parameters (loss, other metrics): 2.3028831481933594, {'accuracy': 0.0765, 'data_size': 10000}
INFO flwr 2024-04-18 06:25:51,628 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 06:25:51,628 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1089784)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1089784)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1089777)[0m 2024-04-18 06:25:49.212452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1089777)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1089777)[0m 2024-04-18 06:25:51.462925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1089778)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1089778)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 06:26:17,066 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 06:26:21,292 | server.py:125 | fit progress: (1, 2.107886552810669, {'accuracy': 0.8245, 'data_size': 10000}, 29.66456410297542)
INFO flwr 2024-04-18 06:26:21,293 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 06:26:21,293 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:26:38,145 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 06:26:42,102 | server.py:125 | fit progress: (2, 1.5213243961334229, {'accuracy': 0.9554, 'data_size': 10000}, 50.47394546598662)
INFO flwr 2024-04-18 06:26:42,102 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 06:26:42,102 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:26:57,812 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 06:27:01,971 | server.py:125 | fit progress: (3, 1.4979453086853027, {'accuracy': 0.9635, 'data_size': 10000}, 70.34297873798641)
INFO flwr 2024-04-18 06:27:01,971 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 06:27:01,971 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:27:16,106 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 06:27:20,279 | server.py:125 | fit progress: (4, 1.4924745559692383, {'accuracy': 0.9685, 'data_size': 10000}, 88.65087028199923)
INFO flwr 2024-04-18 06:27:20,279 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 06:27:20,279 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:27:36,202 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 06:27:40,165 | server.py:125 | fit progress: (5, 1.497893214225769, {'accuracy': 0.9629, 'data_size': 10000}, 108.53742797198356)
INFO flwr 2024-04-18 06:27:40,166 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 06:27:40,166 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:27:56,077 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 06:28:00,272 | server.py:125 | fit progress: (6, 1.500905156135559, {'accuracy': 0.9602, 'data_size': 10000}, 128.64392153997323)
INFO flwr 2024-04-18 06:28:00,272 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 06:28:00,272 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:28:17,471 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 06:28:21,386 | server.py:125 | fit progress: (7, 1.4977418184280396, {'accuracy': 0.9634, 'data_size': 10000}, 149.75816819199827)
INFO flwr 2024-04-18 06:28:21,386 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 06:28:21,387 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:28:37,654 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 06:28:41,847 | server.py:125 | fit progress: (8, 1.4991556406021118, {'accuracy': 0.962, 'data_size': 10000}, 170.21895567697356)
INFO flwr 2024-04-18 06:28:41,847 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 06:28:41,847 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:28:58,156 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 06:29:02,352 | server.py:125 | fit progress: (9, 1.5021076202392578, {'accuracy': 0.9591, 'data_size': 10000}, 190.72393964199)
INFO flwr 2024-04-18 06:29:02,352 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 06:29:02,352 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:29:18,233 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 06:29:22,422 | server.py:125 | fit progress: (10, 1.5041495561599731, {'accuracy': 0.957, 'data_size': 10000}, 210.7946203579777)
INFO flwr 2024-04-18 06:29:22,423 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 06:29:22,423 | server.py:153 | FL finished in 210.79518399797962
INFO flwr 2024-04-18 06:29:22,430 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 06:29:22,430 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 06:29:22,430 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 06:29:22,430 | app.py:229 | app_fit: losses_centralized [(0, 2.3028831481933594), (1, 2.107886552810669), (2, 1.5213243961334229), (3, 1.4979453086853027), (4, 1.4924745559692383), (5, 1.497893214225769), (6, 1.500905156135559), (7, 1.4977418184280396), (8, 1.4991556406021118), (9, 1.5021076202392578), (10, 1.5041495561599731)]
INFO flwr 2024-04-18 06:29:22,430 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0765), (1, 0.8245), (2, 0.9554), (3, 0.9635), (4, 0.9685), (5, 0.9629), (6, 0.9602), (7, 0.9634), (8, 0.962), (9, 0.9591), (10, 0.957)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.957
wandb:     loss 1.50415
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_062523-3lphf3eh
wandb: Find logs at: ./wandb/offline-run-20240418_062523-3lphf3eh/logs
INFO flwr 2024-04-18 06:29:26,001 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 06:36:49,008 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1089775)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1089775)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 06:36:53,685	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 06:36:54,722	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 06:36:55,183	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 06:36:55,339	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (26.23MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 06:36:55,652	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ac30e4ccb06d6978.zip' (64.92MiB) to Ray cluster...
2024-04-18 06:36:55,873	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ac30e4ccb06d6978.zip'.
INFO flwr 2024-04-18 06:37:06,923 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169374285620.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 76874693836.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 06:37:06,924 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 06:37:06,924 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 06:37:06,942 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 06:37:06,943 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 06:37:06,944 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 06:37:06,944 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1094753)[0m 2024-04-18 06:37:13.049216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1094753)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1094753)[0m 2024-04-18 06:37:15.325957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 06:37:16,151 | server.py:94 | initial parameters (loss, other metrics): 2.3028085231781006, {'accuracy': 0.0883, 'data_size': 10000}
INFO flwr 2024-04-18 06:37:16,151 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 06:37:16,152 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1094756)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1094756)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1094754)[0m 2024-04-18 06:37:13.228038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1094754)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1094754)[0m 2024-04-18 06:37:15.517996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1094747)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1094747)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 06:37:40,752 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 06:37:45,002 | server.py:125 | fit progress: (1, 2.296780586242676, {'accuracy': 0.1781, 'data_size': 10000}, 28.850639488984598)
INFO flwr 2024-04-18 06:37:45,003 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 06:37:45,003 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:38:03,016 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 06:38:06,999 | server.py:125 | fit progress: (2, 2.2840068340301514, {'accuracy': 0.512, 'data_size': 10000}, 50.847831656981725)
INFO flwr 2024-04-18 06:38:07,000 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 06:38:07,000 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:38:23,569 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 06:38:27,801 | server.py:125 | fit progress: (3, 2.2609565258026123, {'accuracy': 0.8104, 'data_size': 10000}, 71.64918601198588)
INFO flwr 2024-04-18 06:38:27,801 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 06:38:27,801 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:38:44,859 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 06:38:49,094 | server.py:125 | fit progress: (4, 2.2227513790130615, {'accuracy': 0.9036, 'data_size': 10000}, 92.9423658719752)
INFO flwr 2024-04-18 06:38:49,094 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 06:38:49,094 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:39:05,077 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 06:39:09,043 | server.py:125 | fit progress: (5, 2.1586577892303467, {'accuracy': 0.9267, 'data_size': 10000}, 112.89105188197573)
INFO flwr 2024-04-18 06:39:09,043 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 06:39:09,044 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:39:26,258 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 06:39:30,473 | server.py:125 | fit progress: (6, 2.058250904083252, {'accuracy': 0.936, 'data_size': 10000}, 134.3219160619774)
INFO flwr 2024-04-18 06:39:30,474 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 06:39:30,474 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:39:46,538 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 06:39:50,474 | server.py:125 | fit progress: (7, 1.921586275100708, {'accuracy': 0.9423, 'data_size': 10000}, 154.32275745098013)
INFO flwr 2024-04-18 06:39:50,475 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 06:39:50,475 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:40:07,438 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 06:40:11,511 | server.py:125 | fit progress: (8, 1.7728124856948853, {'accuracy': 0.9479, 'data_size': 10000}, 175.3597828009806)
INFO flwr 2024-04-18 06:40:11,512 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 06:40:11,512 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:40:27,968 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 06:40:32,206 | server.py:125 | fit progress: (9, 1.6539570093154907, {'accuracy': 0.9527, 'data_size': 10000}, 196.05434279399924)
INFO flwr 2024-04-18 06:40:32,206 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 06:40:32,206 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:40:48,723 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 06:40:52,683 | server.py:125 | fit progress: (10, 1.5791168212890625, {'accuracy': 0.9565, 'data_size': 10000}, 216.5311404489912)
INFO flwr 2024-04-18 06:40:52,683 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 06:40:52,683 | server.py:153 | FL finished in 216.53168253100011
INFO flwr 2024-04-18 06:40:52,688 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 06:40:52,688 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 06:40:52,688 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 06:40:52,689 | app.py:229 | app_fit: losses_centralized [(0, 2.3028085231781006), (1, 2.296780586242676), (2, 2.2840068340301514), (3, 2.2609565258026123), (4, 2.2227513790130615), (5, 2.1586577892303467), (6, 2.058250904083252), (7, 1.921586275100708), (8, 1.7728124856948853), (9, 1.6539570093154907), (10, 1.5791168212890625)]
INFO flwr 2024-04-18 06:40:52,689 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0883), (1, 0.1781), (2, 0.512), (3, 0.8104), (4, 0.9036), (5, 0.9267), (6, 0.936), (7, 0.9423), (8, 0.9479), (9, 0.9527), (10, 0.9565)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9565
wandb:     loss 1.57912
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_063648-fqqres84
wandb: Find logs at: ./wandb/offline-run-20240418_063648-fqqres84/logs
INFO flwr 2024-04-18 06:40:56,207 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 06:48:19,444 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1094742)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1094742)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 06:48:24,145	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 06:48:25,159	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 06:48:25,641	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 06:48:25,795	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (26.72MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 06:48:26,116	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_43db0d4d3a7ba56e.zip' (65.42MiB) to Ray cluster...
2024-04-18 06:48:26,342	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_43db0d4d3a7ba56e.zip'.
INFO flwr 2024-04-18 06:48:37,409 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 76866181939.0, 'accelerator_type:TITAN': 1.0, 'memory': 169354424525.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 06:48:37,409 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 06:48:37,409 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 06:48:37,428 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 06:48:37,429 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 06:48:37,429 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 06:48:37,429 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1099513)[0m 2024-04-18 06:48:43.496070: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1099513)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1099512)[0m 2024-04-18 06:48:45.822021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 06:48:46,909 | server.py:94 | initial parameters (loss, other metrics): 2.302983283996582, {'accuracy': 0.0688, 'data_size': 10000}
INFO flwr 2024-04-18 06:48:46,910 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 06:48:46,910 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1099520)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1099520)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1099519)[0m 2024-04-18 06:48:43.842602: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1099519)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1099519)[0m 2024-04-18 06:48:46.044609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1099513)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1099513)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 06:49:12,506 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 06:49:16,764 | server.py:125 | fit progress: (1, 2.3023416996002197, {'accuracy': 0.1693, 'data_size': 10000}, 29.85375660401769)
INFO flwr 2024-04-18 06:49:16,764 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 06:49:16,764 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:49:34,138 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 06:49:38,072 | server.py:125 | fit progress: (2, 2.3014824390411377, {'accuracy': 0.2714, 'data_size': 10000}, 51.16223302501021)
INFO flwr 2024-04-18 06:49:38,072 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 06:49:38,073 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:49:54,351 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 06:49:58,557 | server.py:125 | fit progress: (3, 2.300466775894165, {'accuracy': 0.3403, 'data_size': 10000}, 71.64717542199651)
INFO flwr 2024-04-18 06:49:58,557 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 06:49:58,558 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:50:14,350 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 06:50:18,512 | server.py:125 | fit progress: (4, 2.299290895462036, {'accuracy': 0.3905, 'data_size': 10000}, 91.60198728000978)
INFO flwr 2024-04-18 06:50:18,512 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 06:50:18,512 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:50:33,923 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 06:50:37,835 | server.py:125 | fit progress: (5, 2.2979695796966553, {'accuracy': 0.4443, 'data_size': 10000}, 110.92543129401747)
INFO flwr 2024-04-18 06:50:37,836 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 06:50:37,836 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:50:54,937 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 06:50:59,189 | server.py:125 | fit progress: (6, 2.296504259109497, {'accuracy': 0.5237, 'data_size': 10000}, 132.2791384299926)
INFO flwr 2024-04-18 06:50:59,189 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 06:50:59,190 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:51:14,917 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 06:51:18,856 | server.py:125 | fit progress: (7, 2.294895648956299, {'accuracy': 0.6134, 'data_size': 10000}, 151.94625393199385)
INFO flwr 2024-04-18 06:51:18,856 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 06:51:18,857 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:51:34,460 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 06:51:38,621 | server.py:125 | fit progress: (8, 2.2931389808654785, {'accuracy': 0.6832, 'data_size': 10000}, 171.71108324901434)
INFO flwr 2024-04-18 06:51:38,621 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 06:51:38,622 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:51:55,604 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 06:51:59,755 | server.py:125 | fit progress: (9, 2.2912425994873047, {'accuracy': 0.7287, 'data_size': 10000}, 192.84519063599873)
INFO flwr 2024-04-18 06:51:59,755 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 06:51:59,756 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 06:52:16,741 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 06:52:20,934 | server.py:125 | fit progress: (10, 2.2892186641693115, {'accuracy': 0.7608, 'data_size': 10000}, 214.02422253400437)
INFO flwr 2024-04-18 06:52:20,935 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 06:52:20,935 | server.py:153 | FL finished in 214.02499016199727
INFO flwr 2024-04-18 06:52:20,937 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 06:52:20,937 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 06:52:20,938 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 06:52:20,938 | app.py:229 | app_fit: losses_centralized [(0, 2.302983283996582), (1, 2.3023416996002197), (2, 2.3014824390411377), (3, 2.300466775894165), (4, 2.299290895462036), (5, 2.2979695796966553), (6, 2.296504259109497), (7, 2.294895648956299), (8, 2.2931389808654785), (9, 2.2912425994873047), (10, 2.2892186641693115)]
INFO flwr 2024-04-18 06:52:20,938 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0688), (1, 0.1693), (2, 0.2714), (3, 0.3403), (4, 0.3905), (5, 0.4443), (6, 0.5237), (7, 0.6134), (8, 0.6832), (9, 0.7287), (10, 0.7608)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7608
wandb:     loss 2.28922
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_064819-84y6yb47
wandb: Find logs at: ./wandb/offline-run-20240418_064819-84y6yb47/logs
INFO flwr 2024-04-18 06:52:24,507 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 06:59:47,788 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1099509)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1099509)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 06:59:52,599	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 06:59:53,715	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 06:59:54,199	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 06:59:54,355	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (27.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 06:59:54,678	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d06ea54568af4a30.zip' (65.91MiB) to Ray cluster...
2024-04-18 06:59:54,900	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d06ea54568af4a30.zip'.
INFO flwr 2024-04-18 07:00:05,955 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 169326721639.0, 'object_store_memory': 76854309273.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 07:00:05,956 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 07:00:05,956 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 07:00:05,978 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 07:00:05,979 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 07:00:05,979 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 07:00:05,979 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1103919)[0m 2024-04-18 07:00:12.125084: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1103919)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1103919)[0m 2024-04-18 07:00:14.387262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 07:00:15,256 | server.py:94 | initial parameters (loss, other metrics): 2.3026819229125977, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-18 07:00:15,257 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 07:00:15,258 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1103919)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1103919)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1103918)[0m 2024-04-18 07:00:12.358773: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1103918)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1103918)[0m 2024-04-18 07:00:14.929764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1103907)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1103907)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 07:00:39,165 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 07:00:43,379 | server.py:125 | fit progress: (1, 2.3026130199432373, {'accuracy': 0.1032, 'data_size': 10000}, 28.1218487599981)
INFO flwr 2024-04-18 07:00:43,380 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 07:00:43,380 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:01:01,267 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 07:01:05,206 | server.py:125 | fit progress: (2, 2.302525043487549, {'accuracy': 0.1032, 'data_size': 10000}, 49.94860808699741)
INFO flwr 2024-04-18 07:01:05,206 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 07:01:05,207 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:01:22,175 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 07:01:26,373 | server.py:125 | fit progress: (3, 2.3024213314056396, {'accuracy': 0.1032, 'data_size': 10000}, 71.1152939730091)
INFO flwr 2024-04-18 07:01:26,373 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 07:01:26,373 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:01:41,537 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 07:01:45,787 | server.py:125 | fit progress: (4, 2.3023061752319336, {'accuracy': 0.1032, 'data_size': 10000}, 90.52921233800589)
INFO flwr 2024-04-18 07:01:45,787 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 07:01:45,787 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:02:01,877 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 07:02:05,834 | server.py:125 | fit progress: (5, 2.302182912826538, {'accuracy': 0.1032, 'data_size': 10000}, 110.57698388301651)
INFO flwr 2024-04-18 07:02:05,835 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 07:02:05,835 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:02:22,242 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 07:02:26,435 | server.py:125 | fit progress: (6, 2.302058458328247, {'accuracy': 0.1032, 'data_size': 10000}, 131.17717104099574)
INFO flwr 2024-04-18 07:02:26,435 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 07:02:26,435 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:02:42,518 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 07:02:46,453 | server.py:125 | fit progress: (7, 2.3019237518310547, {'accuracy': 0.1032, 'data_size': 10000}, 151.19590837100986)
INFO flwr 2024-04-18 07:02:46,454 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 07:02:46,454 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:03:04,806 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 07:03:09,033 | server.py:125 | fit progress: (8, 2.301783800125122, {'accuracy': 0.1033, 'data_size': 10000}, 173.77520218701102)
INFO flwr 2024-04-18 07:03:09,033 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 07:03:09,033 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:03:24,376 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 07:03:28,554 | server.py:125 | fit progress: (9, 2.3016395568847656, {'accuracy': 0.1036, 'data_size': 10000}, 193.29637165600434)
INFO flwr 2024-04-18 07:03:28,554 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 07:03:28,555 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:03:44,686 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 07:03:48,881 | server.py:125 | fit progress: (10, 2.301490306854248, {'accuracy': 0.1042, 'data_size': 10000}, 213.62350276400684)
INFO flwr 2024-04-18 07:03:48,881 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 07:03:48,881 | server.py:153 | FL finished in 213.62396713902126
INFO flwr 2024-04-18 07:03:48,886 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 07:03:48,887 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 07:03:48,887 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 07:03:48,887 | app.py:229 | app_fit: losses_centralized [(0, 2.3026819229125977), (1, 2.3026130199432373), (2, 2.302525043487549), (3, 2.3024213314056396), (4, 2.3023061752319336), (5, 2.302182912826538), (6, 2.302058458328247), (7, 2.3019237518310547), (8, 2.301783800125122), (9, 2.3016395568847656), (10, 2.301490306854248)]
INFO flwr 2024-04-18 07:03:48,887 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.1032), (2, 0.1032), (3, 0.1032), (4, 0.1032), (5, 0.1032), (6, 0.1032), (7, 0.1032), (8, 0.1033), (9, 0.1036), (10, 0.1042)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1042
wandb:     loss 2.30149
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_065947-jeuae2m8
wandb: Find logs at: ./wandb/offline-run-20240418_065947-jeuae2m8/logs
INFO flwr 2024-04-18 07:03:52,420 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 07:11:15,692 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1103903)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1103903)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 07:11:20,466	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 07:11:21,473	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 07:11:21,948	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 07:11:22,102	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (27.69MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 07:11:22,424	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f1c8503cfd302950.zip' (66.42MiB) to Ray cluster...
2024-04-18 07:11:22,654	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f1c8503cfd302950.zip'.
INFO flwr 2024-04-18 07:11:33,714 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 76761398476.0, 'accelerator_type:TITAN': 1.0, 'memory': 169109929780.0}
INFO flwr 2024-04-18 07:11:33,714 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 07:11:33,714 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 07:11:33,731 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 07:11:33,733 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 07:11:33,733 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 07:11:33,734 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1108892)[0m 2024-04-18 07:11:39.817252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1108892)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1108892)[0m 2024-04-18 07:11:42.091399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 07:11:43,465 | server.py:94 | initial parameters (loss, other metrics): 2.3024885654449463, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-18 07:11:43,465 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 07:11:43,466 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1108900)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1108900)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1108888)[0m 2024-04-18 07:11:40.111568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1108888)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1108888)[0m 2024-04-18 07:11:42.335522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1108892)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1108892)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 07:12:31,834 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 07:12:36,025 | server.py:125 | fit progress: (1, 1.5951638221740723, {'accuracy': 0.8661, 'data_size': 10000}, 52.55946973100072)
INFO flwr 2024-04-18 07:12:36,025 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 07:12:36,026 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:13:11,472 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 07:13:15,436 | server.py:125 | fit progress: (2, 1.7805017232894897, {'accuracy': 0.6807, 'data_size': 10000}, 91.97070836299099)
INFO flwr 2024-04-18 07:13:15,437 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 07:13:15,437 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:13:51,545 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 07:13:55,718 | server.py:125 | fit progress: (3, 2.0716633796691895, {'accuracy': 0.3895, 'data_size': 10000}, 132.25276256600046)
INFO flwr 2024-04-18 07:13:55,719 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 07:13:55,719 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:14:32,847 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 07:14:36,990 | server.py:125 | fit progress: (4, 2.1154439449310303, {'accuracy': 0.3457, 'data_size': 10000}, 173.52415412600385)
INFO flwr 2024-04-18 07:14:36,990 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 07:14:36,990 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:15:11,566 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 07:15:15,483 | server.py:125 | fit progress: (5, 2.153543710708618, {'accuracy': 0.3076, 'data_size': 10000}, 212.01721136199194)
INFO flwr 2024-04-18 07:15:15,483 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 07:15:15,483 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:15:51,868 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 07:15:56,135 | server.py:125 | fit progress: (6, 2.1898436546325684, {'accuracy': 0.2713, 'data_size': 10000}, 252.6696934189822)
INFO flwr 2024-04-18 07:15:56,135 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 07:15:56,136 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:16:32,936 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 07:16:36,897 | server.py:125 | fit progress: (7, 2.2192435264587402, {'accuracy': 0.2419, 'data_size': 10000}, 293.43195791199105)
INFO flwr 2024-04-18 07:16:36,898 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 07:16:36,898 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:17:10,605 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 07:17:14,573 | server.py:125 | fit progress: (8, 2.2380433082580566, {'accuracy': 0.2231, 'data_size': 10000}, 331.1072545089992)
INFO flwr 2024-04-18 07:17:14,573 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 07:17:14,573 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:17:52,629 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 07:17:56,824 | server.py:125 | fit progress: (9, 2.2544431686401367, {'accuracy': 0.2067, 'data_size': 10000}, 373.35851955399266)
INFO flwr 2024-04-18 07:17:56,825 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 07:17:56,825 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:18:31,074 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 07:18:35,026 | server.py:125 | fit progress: (10, 2.267742872238159, {'accuracy': 0.1934, 'data_size': 10000}, 411.56069262398523)
INFO flwr 2024-04-18 07:18:35,041 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 07:18:35,041 | server.py:153 | FL finished in 411.5757279879763
INFO flwr 2024-04-18 07:18:35,046 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 07:18:35,046 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 07:18:35,046 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 07:18:35,046 | app.py:229 | app_fit: losses_centralized [(0, 2.3024885654449463), (1, 1.5951638221740723), (2, 1.7805017232894897), (3, 2.0716633796691895), (4, 2.1154439449310303), (5, 2.153543710708618), (6, 2.1898436546325684), (7, 2.2192435264587402), (8, 2.2380433082580566), (9, 2.2544431686401367), (10, 2.267742872238159)]
INFO flwr 2024-04-18 07:18:35,046 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.8661), (2, 0.6807), (3, 0.3895), (4, 0.3457), (5, 0.3076), (6, 0.2713), (7, 0.2419), (8, 0.2231), (9, 0.2067), (10, 0.1934)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1934
wandb:     loss 2.26774
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_071115-e9i1s8t1
wandb: Find logs at: ./wandb/offline-run-20240418_071115-e9i1s8t1/logs
INFO flwr 2024-04-18 07:18:38,579 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 07:26:01,584 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1108886)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1108886)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 07:26:07,630	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 07:26:08,580	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 07:26:09,025	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 07:26:09,177	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (28.65MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 07:26:09,502	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_99d33df73555ec16.zip' (67.39MiB) to Ray cluster...
2024-04-18 07:26:09,724	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_99d33df73555ec16.zip'.
INFO flwr 2024-04-18 07:26:20,848 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169362085684.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 76869465292.0, 'CPU': 64.0}
INFO flwr 2024-04-18 07:26:20,849 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 07:26:20,849 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 07:26:20,867 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 07:26:20,868 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 07:26:20,868 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 07:26:20,869 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1113926)[0m 2024-04-18 07:26:27.084293: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1113926)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1113918)[0m 2024-04-18 07:26:29.371050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 07:26:29,708 | server.py:94 | initial parameters (loss, other metrics): 2.302541494369507, {'accuracy': 0.0926, 'data_size': 10000}
INFO flwr 2024-04-18 07:26:29,708 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 07:26:29,709 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1113927)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1113927)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1113925)[0m 2024-04-18 07:26:27.297331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1113925)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1113925)[0m 2024-04-18 07:26:29.853383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1113921)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1113921)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 07:27:19,658 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 07:27:23,876 | server.py:125 | fit progress: (1, 1.9212560653686523, {'accuracy': 0.78, 'data_size': 10000}, 54.16719638698851)
INFO flwr 2024-04-18 07:27:23,876 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 07:27:23,876 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:28:03,332 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 07:28:07,339 | server.py:125 | fit progress: (2, 1.5162123441696167, {'accuracy': 0.9513, 'data_size': 10000}, 97.63058521799394)
INFO flwr 2024-04-18 07:28:07,339 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 07:28:07,340 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:28:40,612 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 07:28:44,798 | server.py:125 | fit progress: (3, 1.4935113191604614, {'accuracy': 0.9673, 'data_size': 10000}, 135.0891880239942)
INFO flwr 2024-04-18 07:28:44,798 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 07:28:44,798 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:29:21,348 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 07:29:25,565 | server.py:125 | fit progress: (4, 1.4909521341323853, {'accuracy': 0.9704, 'data_size': 10000}, 175.85659362000297)
INFO flwr 2024-04-18 07:29:25,565 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 07:29:25,566 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:29:58,386 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 07:30:02,349 | server.py:125 | fit progress: (5, 1.486889123916626, {'accuracy': 0.9744, 'data_size': 10000}, 212.6404038350156)
INFO flwr 2024-04-18 07:30:02,349 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 07:30:02,349 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:30:41,444 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 07:30:45,631 | server.py:125 | fit progress: (6, 1.4878361225128174, {'accuracy': 0.9732, 'data_size': 10000}, 255.9225952400011)
INFO flwr 2024-04-18 07:30:45,632 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 07:30:45,632 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:31:30,605 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 07:31:34,593 | server.py:125 | fit progress: (7, 1.491422414779663, {'accuracy': 0.9698, 'data_size': 10000}, 304.88415235001594)
INFO flwr 2024-04-18 07:31:34,593 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 07:31:34,593 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:32:12,343 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 07:32:16,506 | server.py:125 | fit progress: (8, 1.4933737516403198, {'accuracy': 0.9679, 'data_size': 10000}, 346.79759238898987)
INFO flwr 2024-04-18 07:32:16,506 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 07:32:16,507 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:32:53,451 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 07:32:57,652 | server.py:125 | fit progress: (9, 1.4927566051483154, {'accuracy': 0.9684, 'data_size': 10000}, 387.94410643700394)
INFO flwr 2024-04-18 07:32:57,653 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 07:32:57,653 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:33:36,862 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 07:33:41,041 | server.py:125 | fit progress: (10, 1.4929779767990112, {'accuracy': 0.968, 'data_size': 10000}, 431.33309429400833)
INFO flwr 2024-04-18 07:33:41,042 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 07:33:41,042 | server.py:153 | FL finished in 431.33359134799684
INFO flwr 2024-04-18 07:33:41,047 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 07:33:41,047 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 07:33:41,047 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 07:33:41,047 | app.py:229 | app_fit: losses_centralized [(0, 2.302541494369507), (1, 1.9212560653686523), (2, 1.5162123441696167), (3, 1.4935113191604614), (4, 1.4909521341323853), (5, 1.486889123916626), (6, 1.4878361225128174), (7, 1.491422414779663), (8, 1.4933737516403198), (9, 1.4927566051483154), (10, 1.4929779767990112)]
INFO flwr 2024-04-18 07:33:41,047 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0926), (1, 0.78), (2, 0.9513), (3, 0.9673), (4, 0.9704), (5, 0.9744), (6, 0.9732), (7, 0.9698), (8, 0.9679), (9, 0.9684), (10, 0.968)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.968
wandb:     loss 1.49298
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_072601-0ai9m96c
wandb: Find logs at: ./wandb/offline-run-20240418_072601-0ai9m96c/logs
INFO flwr 2024-04-18 07:33:44,582 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 07:41:07,640 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1113918)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1113918)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 07:41:13,309	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 07:41:14,296	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 07:41:14,758	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 07:41:14,915	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (29.14MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 07:41:15,279	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7601cc2002484d54.zip' (67.89MiB) to Ray cluster...
2024-04-18 07:41:15,507	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7601cc2002484d54.zip'.
INFO flwr 2024-04-18 07:41:26,698 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 76877194444.0, 'memory': 169380120372.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 07:41:26,698 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 07:41:26,698 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 07:41:26,720 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 07:41:26,722 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 07:41:26,722 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 07:41:26,722 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1118720)[0m 2024-04-18 07:41:32.813598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1118720)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1118719)[0m 2024-04-18 07:41:35.155047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 07:41:36,169 | server.py:94 | initial parameters (loss, other metrics): 2.3026304244995117, {'accuracy': 0.1184, 'data_size': 10000}
INFO flwr 2024-04-18 07:41:36,170 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 07:41:36,170 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1118722)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1118722)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1118721)[0m 2024-04-18 07:41:33.119016: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1118721)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1118721)[0m 2024-04-18 07:41:35.341738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1118716)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1118716)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 07:42:19,190 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 07:42:23,433 | server.py:125 | fit progress: (1, 2.292689800262451, {'accuracy': 0.3526, 'data_size': 10000}, 47.26290722299018)
INFO flwr 2024-04-18 07:42:23,433 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 07:42:23,434 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:43:01,605 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 07:43:05,574 | server.py:125 | fit progress: (2, 2.270230770111084, {'accuracy': 0.7235, 'data_size': 10000}, 89.40349090500968)
INFO flwr 2024-04-18 07:43:05,574 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 07:43:05,574 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:43:39,744 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 07:43:43,998 | server.py:125 | fit progress: (3, 2.2237861156463623, {'accuracy': 0.8524, 'data_size': 10000}, 127.82808420198853)
INFO flwr 2024-04-18 07:43:43,998 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 07:43:43,999 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:44:18,545 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 07:44:22,739 | server.py:125 | fit progress: (4, 2.1335246562957764, {'accuracy': 0.8976, 'data_size': 10000}, 166.5689958060102)
INFO flwr 2024-04-18 07:44:22,739 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 07:44:22,740 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:44:59,740 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 07:45:03,721 | server.py:125 | fit progress: (5, 1.9813451766967773, {'accuracy': 0.9217, 'data_size': 10000}, 207.55139400900225)
INFO flwr 2024-04-18 07:45:03,722 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 07:45:03,722 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:45:39,432 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 07:45:43,695 | server.py:125 | fit progress: (6, 1.798723816871643, {'accuracy': 0.9354, 'data_size': 10000}, 247.52536749499268)
INFO flwr 2024-04-18 07:45:43,696 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 07:45:43,696 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:46:20,169 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 07:46:24,182 | server.py:125 | fit progress: (7, 1.655761957168579, {'accuracy': 0.9466, 'data_size': 10000}, 288.01225650298875)
INFO flwr 2024-04-18 07:46:24,183 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 07:46:24,183 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:47:02,476 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 07:47:06,454 | server.py:125 | fit progress: (8, 1.573777675628662, {'accuracy': 0.953, 'data_size': 10000}, 330.2835260719876)
INFO flwr 2024-04-18 07:47:06,454 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 07:47:06,454 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:47:45,264 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 07:47:49,512 | server.py:125 | fit progress: (9, 1.533385992050171, {'accuracy': 0.9574, 'data_size': 10000}, 373.341884346999)
INFO flwr 2024-04-18 07:47:49,512 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 07:47:49,512 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:48:26,013 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 07:48:29,975 | server.py:125 | fit progress: (10, 1.5144305229187012, {'accuracy': 0.9603, 'data_size': 10000}, 413.8047665129998)
INFO flwr 2024-04-18 07:48:29,975 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 07:48:29,975 | server.py:153 | FL finished in 413.80531139700906
INFO flwr 2024-04-18 07:48:29,980 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 07:48:29,980 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 07:48:29,980 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 07:48:29,980 | app.py:229 | app_fit: losses_centralized [(0, 2.3026304244995117), (1, 2.292689800262451), (2, 2.270230770111084), (3, 2.2237861156463623), (4, 2.1335246562957764), (5, 1.9813451766967773), (6, 1.798723816871643), (7, 1.655761957168579), (8, 1.573777675628662), (9, 1.533385992050171), (10, 1.5144305229187012)]
INFO flwr 2024-04-18 07:48:29,980 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1184), (1, 0.3526), (2, 0.7235), (3, 0.8524), (4, 0.8976), (5, 0.9217), (6, 0.9354), (7, 0.9466), (8, 0.953), (9, 0.9574), (10, 0.9603)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9603
wandb:     loss 1.51443
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_074107-xudldyaz
wandb: Find logs at: ./wandb/offline-run-20240418_074107-xudldyaz/logs
INFO flwr 2024-04-18 07:48:33,538 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 07:55:56,475 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1118711)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1118711)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 07:56:01,262	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 07:56:02,300	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 07:56:02,789	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 07:56:02,949	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (30.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 07:56:03,282	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_13457f8a9c85a173.zip' (68.87MiB) to Ray cluster...
2024-04-18 07:56:03,512	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_13457f8a9c85a173.zip'.
INFO flwr 2024-04-18 07:56:14,600 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 76940083200.0, 'memory': 169526860800.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 07:56:14,601 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 07:56:14,601 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 07:56:14,621 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 07:56:14,622 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 07:56:14,622 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 07:56:14,623 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1124110)[0m 2024-04-18 07:56:20.684513: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1124110)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1124103)[0m 2024-04-18 07:56:22.969451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 07:56:24,140 | server.py:94 | initial parameters (loss, other metrics): 2.3029956817626953, {'accuracy': 0.0723, 'data_size': 10000}
INFO flwr 2024-04-18 07:56:24,140 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 07:56:24,140 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1124115)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1124115)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1124111)[0m 2024-04-18 07:56:20.968473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1124111)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1124111)[0m 2024-04-18 07:56:23.283183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1124104)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1124104)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 07:57:07,150 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 07:57:11,344 | server.py:125 | fit progress: (1, 2.302236795425415, {'accuracy': 0.1376, 'data_size': 10000}, 47.20406278898008)
INFO flwr 2024-04-18 07:57:11,345 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 07:57:11,345 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:57:47,149 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 07:57:51,071 | server.py:125 | fit progress: (2, 2.3011763095855713, {'accuracy': 0.1934, 'data_size': 10000}, 86.93068814298022)
INFO flwr 2024-04-18 07:57:51,071 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 07:57:51,072 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:58:26,568 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 07:58:30,797 | server.py:125 | fit progress: (3, 2.299832344055176, {'accuracy': 0.2256, 'data_size': 10000}, 126.65648977700039)
INFO flwr 2024-04-18 07:58:30,797 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 07:58:30,797 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:59:08,037 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 07:59:12,194 | server.py:125 | fit progress: (4, 2.298187017440796, {'accuracy': 0.33, 'data_size': 10000}, 168.05323937299545)
INFO flwr 2024-04-18 07:59:12,194 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 07:59:12,194 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 07:59:49,713 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 07:59:53,700 | server.py:125 | fit progress: (5, 2.296254873275757, {'accuracy': 0.4854, 'data_size': 10000}, 209.55958318000194)
INFO flwr 2024-04-18 07:59:53,700 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 07:59:53,700 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:00:28,168 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 08:00:32,332 | server.py:125 | fit progress: (6, 2.294003963470459, {'accuracy': 0.6055, 'data_size': 10000}, 248.19175194599666)
INFO flwr 2024-04-18 08:00:32,332 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 08:00:32,333 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:01:02,615 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 08:01:06,609 | server.py:125 | fit progress: (7, 2.2914071083068848, {'accuracy': 0.6841, 'data_size': 10000}, 282.46832009698846)
INFO flwr 2024-04-18 08:01:06,609 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 08:01:06,609 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:01:47,201 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 08:01:51,512 | server.py:125 | fit progress: (8, 2.2884721755981445, {'accuracy': 0.7267, 'data_size': 10000}, 327.3713150009862)
INFO flwr 2024-04-18 08:01:51,512 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 08:01:51,512 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:02:31,741 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 08:02:35,931 | server.py:125 | fit progress: (9, 2.2851967811584473, {'accuracy': 0.7553, 'data_size': 10000}, 371.79080506798346)
INFO flwr 2024-04-18 08:02:35,931 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 08:02:35,932 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:03:13,591 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 08:03:17,762 | server.py:125 | fit progress: (10, 2.281522750854492, {'accuracy': 0.7758, 'data_size': 10000}, 413.62130330500077)
INFO flwr 2024-04-18 08:03:17,762 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 08:03:17,762 | server.py:153 | FL finished in 413.6217561189842
INFO flwr 2024-04-18 08:03:17,766 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 08:03:17,767 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 08:03:17,767 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 08:03:17,767 | app.py:229 | app_fit: losses_centralized [(0, 2.3029956817626953), (1, 2.302236795425415), (2, 2.3011763095855713), (3, 2.299832344055176), (4, 2.298187017440796), (5, 2.296254873275757), (6, 2.294003963470459), (7, 2.2914071083068848), (8, 2.2884721755981445), (9, 2.2851967811584473), (10, 2.281522750854492)]
INFO flwr 2024-04-18 08:03:17,767 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0723), (1, 0.1376), (2, 0.1934), (3, 0.2256), (4, 0.33), (5, 0.4854), (6, 0.6055), (7, 0.6841), (8, 0.7267), (9, 0.7553), (10, 0.7758)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7758
wandb:     loss 2.28152
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_075556-zulk1jqk
wandb: Find logs at: ./wandb/offline-run-20240418_075556-zulk1jqk/logs
INFO flwr 2024-04-18 08:03:21,334 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 08:10:44,705 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1124100)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1124100)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 08:10:49,967	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 08:10:50,956	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 08:10:51,420	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 08:10:51,573	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (30.59MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 08:10:51,904	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_97041ce5e61e0dcc.zip' (69.37MiB) to Ray cluster...
2024-04-18 08:10:52,132	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_97041ce5e61e0dcc.zip'.
INFO flwr 2024-04-18 08:11:03,245 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 76912309862.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169462056346.0}
INFO flwr 2024-04-18 08:11:03,245 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 08:11:03,245 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 08:11:03,263 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 08:11:03,264 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 08:11:03,264 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 08:11:03,264 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1129135)[0m 2024-04-18 08:11:09.419294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1129135)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1129135)[0m 2024-04-18 08:11:11.727165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 08:11:12,249 | server.py:94 | initial parameters (loss, other metrics): 2.3025906085968018, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-18 08:11:12,249 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 08:11:12,250 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1129135)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1129135)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1129133)[0m 2024-04-18 08:11:09.680759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1129133)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1129129)[0m 2024-04-18 08:11:11.911376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1129125)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1129125)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 08:11:58,302 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 08:12:02,523 | server.py:125 | fit progress: (1, 2.3025362491607666, {'accuracy': 0.0963, 'data_size': 10000}, 50.27372965801624)
INFO flwr 2024-04-18 08:12:02,523 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 08:12:02,524 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:12:37,603 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 08:12:41,554 | server.py:125 | fit progress: (2, 2.3024613857269287, {'accuracy': 0.099, 'data_size': 10000}, 89.30481046601199)
INFO flwr 2024-04-18 08:12:41,555 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 08:12:41,555 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:13:20,005 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 08:13:24,145 | server.py:125 | fit progress: (3, 2.302372932434082, {'accuracy': 0.1032, 'data_size': 10000}, 131.8956804490008)
INFO flwr 2024-04-18 08:13:24,145 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 08:13:24,146 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:13:58,161 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 08:14:02,337 | server.py:125 | fit progress: (4, 2.302276372909546, {'accuracy': 0.1077, 'data_size': 10000}, 170.08761791701545)
INFO flwr 2024-04-18 08:14:02,337 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 08:14:02,338 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:14:41,245 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 08:14:45,133 | server.py:125 | fit progress: (5, 2.3021721839904785, {'accuracy': 0.1158, 'data_size': 10000}, 212.88318566500675)
INFO flwr 2024-04-18 08:14:45,133 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 08:14:45,133 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:15:22,655 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 08:15:26,850 | server.py:125 | fit progress: (6, 2.3020641803741455, {'accuracy': 0.1268, 'data_size': 10000}, 254.60008602001471)
INFO flwr 2024-04-18 08:15:26,850 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 08:15:26,850 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:16:03,189 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 08:16:07,105 | server.py:125 | fit progress: (7, 2.3019487857818604, {'accuracy': 0.1419, 'data_size': 10000}, 294.85517339099897)
INFO flwr 2024-04-18 08:16:07,105 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 08:16:07,105 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:16:44,135 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 08:16:48,330 | server.py:125 | fit progress: (8, 2.301828384399414, {'accuracy': 0.1599, 'data_size': 10000}, 336.0804528169974)
INFO flwr 2024-04-18 08:16:48,330 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 08:16:48,331 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:17:28,666 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 08:17:32,807 | server.py:125 | fit progress: (9, 2.3017027378082275, {'accuracy': 0.1781, 'data_size': 10000}, 380.55787213600706)
INFO flwr 2024-04-18 08:17:32,808 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 08:17:32,808 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:18:10,099 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 08:18:14,296 | server.py:125 | fit progress: (10, 2.301574230194092, {'accuracy': 0.1949, 'data_size': 10000}, 422.0462921640137)
INFO flwr 2024-04-18 08:18:14,296 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 08:18:14,296 | server.py:153 | FL finished in 422.0467792300042
INFO flwr 2024-04-18 08:18:14,303 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 08:18:14,303 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 08:18:14,303 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 08:18:14,303 | app.py:229 | app_fit: losses_centralized [(0, 2.3025906085968018), (1, 2.3025362491607666), (2, 2.3024613857269287), (3, 2.302372932434082), (4, 2.302276372909546), (5, 2.3021721839904785), (6, 2.3020641803741455), (7, 2.3019487857818604), (8, 2.301828384399414), (9, 2.3017027378082275), (10, 2.301574230194092)]
INFO flwr 2024-04-18 08:18:14,304 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.0963), (2, 0.099), (3, 0.1032), (4, 0.1077), (5, 0.1158), (6, 0.1268), (7, 0.1419), (8, 0.1599), (9, 0.1781), (10, 0.1949)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1949
wandb:     loss 2.30157
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_081044-h8b6fs0n
wandb: Find logs at: ./wandb/offline-run-20240418_081044-h8b6fs0n/logs
INFO flwr 2024-04-18 08:18:17,839 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 08:25:41,271 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1129123)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1129123)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 08:25:47,030	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 08:25:48,003	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 08:25:48,470	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 08:25:48,625	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (31.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 08:25:48,956	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_bcb447265afcb316.zip' (70.35MiB) to Ray cluster...
2024-04-18 08:25:49,192	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_bcb447265afcb316.zip'.
INFO flwr 2024-04-18 08:26:00,319 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169415908762.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 76892532326.0, 'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 08:26:00,319 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 08:26:00,319 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 08:26:00,337 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 08:26:00,339 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 08:26:00,339 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 08:26:00,339 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1134530)[0m 2024-04-18 08:26:06.486440: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1134530)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1134530)[0m 2024-04-18 08:26:08.766754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 08:26:09,482 | server.py:94 | initial parameters (loss, other metrics): 2.3024842739105225, {'accuracy': 0.049, 'data_size': 10000}
INFO flwr 2024-04-18 08:26:09,482 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 08:26:09,483 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1134539)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1134539)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1134536)[0m 2024-04-18 08:26:06.933786: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1134536)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1134536)[0m 2024-04-18 08:26:09.250200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1134526)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1134526)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 08:26:52,301 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 08:26:56,599 | server.py:125 | fit progress: (1, 1.5759127140045166, {'accuracy': 0.8847, 'data_size': 10000}, 47.11667381200823)
INFO flwr 2024-04-18 08:26:56,600 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 08:26:56,600 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:27:31,830 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 08:27:35,768 | server.py:125 | fit progress: (2, 2.1106512546539307, {'accuracy': 0.3505, 'data_size': 10000}, 86.28509693799424)
INFO flwr 2024-04-18 08:27:35,768 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 08:27:35,768 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:28:12,647 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 08:28:16,835 | server.py:125 | fit progress: (3, 2.16144061088562, {'accuracy': 0.2997, 'data_size': 10000}, 127.35207154299133)
INFO flwr 2024-04-18 08:28:16,835 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 08:28:16,835 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:28:53,447 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 08:28:57,601 | server.py:125 | fit progress: (4, 2.2515389919281006, {'accuracy': 0.2096, 'data_size': 10000}, 168.11897052900167)
INFO flwr 2024-04-18 08:28:57,602 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 08:28:57,602 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:29:33,258 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 08:29:37,146 | server.py:125 | fit progress: (5, 2.267843246459961, {'accuracy': 0.1933, 'data_size': 10000}, 207.6633246440033)
INFO flwr 2024-04-18 08:29:37,146 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 08:29:37,146 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:30:16,028 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 08:30:20,179 | server.py:125 | fit progress: (6, 2.273543119430542, {'accuracy': 0.1876, 'data_size': 10000}, 250.6961881989846)
INFO flwr 2024-04-18 08:30:20,179 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 08:30:20,179 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:31:00,854 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 08:31:04,836 | server.py:125 | fit progress: (7, 2.277238368988037, {'accuracy': 0.1839, 'data_size': 10000}, 295.3532188810059)
INFO flwr 2024-04-18 08:31:04,836 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 08:31:04,836 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:31:41,978 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 08:31:45,914 | server.py:125 | fit progress: (8, 2.2783429622650146, {'accuracy': 0.1828, 'data_size': 10000}, 336.43176049698377)
INFO flwr 2024-04-18 08:31:45,915 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 08:31:45,915 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:32:23,267 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 08:32:27,446 | server.py:125 | fit progress: (9, 2.279242992401123, {'accuracy': 0.1819, 'data_size': 10000}, 377.9632327349973)
INFO flwr 2024-04-18 08:32:27,446 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 08:32:27,446 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:33:01,335 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 08:33:05,291 | server.py:125 | fit progress: (10, 2.280042886734009, {'accuracy': 0.1811, 'data_size': 10000}, 415.8080477519834)
INFO flwr 2024-04-18 08:33:05,291 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 08:33:05,291 | server.py:153 | FL finished in 415.8086355859996
INFO flwr 2024-04-18 08:33:05,298 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 08:33:05,298 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 08:33:05,298 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 08:33:05,298 | app.py:229 | app_fit: losses_centralized [(0, 2.3024842739105225), (1, 1.5759127140045166), (2, 2.1106512546539307), (3, 2.16144061088562), (4, 2.2515389919281006), (5, 2.267843246459961), (6, 2.273543119430542), (7, 2.277238368988037), (8, 2.2783429622650146), (9, 2.279242992401123), (10, 2.280042886734009)]
INFO flwr 2024-04-18 08:33:05,298 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.049), (1, 0.8847), (2, 0.3505), (3, 0.2997), (4, 0.2096), (5, 0.1933), (6, 0.1876), (7, 0.1839), (8, 0.1828), (9, 0.1819), (10, 0.1811)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1811
wandb:     loss 2.28004
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_082540-12n5fhc1
wandb: Find logs at: ./wandb/offline-run-20240418_082540-12n5fhc1/logs
INFO flwr 2024-04-18 08:33:08,833 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 08:40:31,615 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1134523)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1134523)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 08:40:37,658	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 08:40:38,598	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 08:40:39,061	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 08:40:39,215	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (32.05MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 08:40:39,551	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_17ecdfd0ca8f607b.zip' (70.85MiB) to Ray cluster...
2024-04-18 08:40:39,786	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_17ecdfd0ca8f607b.zip'.
INFO flwr 2024-04-18 08:40:50,965 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 76932304896.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 169508711424.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 08:40:50,965 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 08:40:50,965 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 08:40:50,982 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 08:40:50,984 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 08:40:50,984 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 08:40:50,984 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1139561)[0m 2024-04-18 08:40:56.981041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1139561)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1139561)[0m 2024-04-18 08:40:59.367914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 08:41:00,543 | server.py:94 | initial parameters (loss, other metrics): 2.3024861812591553, {'accuracy': 0.1101, 'data_size': 10000}
INFO flwr 2024-04-18 08:41:00,543 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 08:41:00,544 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1139569)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1139569)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1139568)[0m 2024-04-18 08:40:57.457904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1139568)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1139568)[0m 2024-04-18 08:40:59.585942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1139559)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1139559)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 08:41:46,419 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 08:41:50,623 | server.py:125 | fit progress: (1, 2.0559680461883545, {'accuracy': 0.9089, 'data_size': 10000}, 50.078864095994504)
INFO flwr 2024-04-18 08:41:50,623 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 08:41:50,623 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:42:28,710 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 08:42:32,718 | server.py:125 | fit progress: (2, 1.5195493698120117, {'accuracy': 0.9516, 'data_size': 10000}, 92.17439759301487)
INFO flwr 2024-04-18 08:42:32,718 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 08:42:32,719 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:43:06,883 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 08:43:11,071 | server.py:125 | fit progress: (3, 1.497691035270691, {'accuracy': 0.9638, 'data_size': 10000}, 130.5273656710051)
INFO flwr 2024-04-18 08:43:11,071 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 08:43:11,072 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:43:46,807 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 08:43:50,986 | server.py:125 | fit progress: (4, 1.4901055097579956, {'accuracy': 0.971, 'data_size': 10000}, 170.44276346400147)
INFO flwr 2024-04-18 08:43:50,987 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 08:43:50,987 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:44:26,528 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 08:44:30,468 | server.py:125 | fit progress: (5, 1.4876636266708374, {'accuracy': 0.9732, 'data_size': 10000}, 209.924454041)
INFO flwr 2024-04-18 08:44:30,469 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 08:44:30,469 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:45:07,315 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 08:45:11,611 | server.py:125 | fit progress: (6, 1.4922271966934204, {'accuracy': 0.9692, 'data_size': 10000}, 251.06773240098846)
INFO flwr 2024-04-18 08:45:11,612 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 08:45:11,612 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:45:54,167 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 08:45:58,120 | server.py:125 | fit progress: (7, 1.492736577987671, {'accuracy': 0.9683, 'data_size': 10000}, 297.5762675930164)
INFO flwr 2024-04-18 08:45:58,120 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 08:45:58,121 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:46:34,953 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 08:46:39,169 | server.py:125 | fit progress: (8, 1.4915541410446167, {'accuracy': 0.9696, 'data_size': 10000}, 338.624932945997)
INFO flwr 2024-04-18 08:46:39,169 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 08:46:39,169 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:47:14,983 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 08:47:19,163 | server.py:125 | fit progress: (9, 1.491988182067871, {'accuracy': 0.9691, 'data_size': 10000}, 378.6195629980066)
INFO flwr 2024-04-18 08:47:19,164 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 08:47:19,164 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:47:57,236 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 08:48:01,436 | server.py:125 | fit progress: (10, 1.4942235946655273, {'accuracy': 0.9669, 'data_size': 10000}, 420.89274631801527)
INFO flwr 2024-04-18 08:48:01,437 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 08:48:01,437 | server.py:153 | FL finished in 420.893305487989
INFO flwr 2024-04-18 08:48:01,441 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 08:48:01,441 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 08:48:01,442 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 08:48:01,442 | app.py:229 | app_fit: losses_centralized [(0, 2.3024861812591553), (1, 2.0559680461883545), (2, 1.5195493698120117), (3, 1.497691035270691), (4, 1.4901055097579956), (5, 1.4876636266708374), (6, 1.4922271966934204), (7, 1.492736577987671), (8, 1.4915541410446167), (9, 1.491988182067871), (10, 1.4942235946655273)]
INFO flwr 2024-04-18 08:48:01,442 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1101), (1, 0.9089), (2, 0.9516), (3, 0.9638), (4, 0.971), (5, 0.9732), (6, 0.9692), (7, 0.9683), (8, 0.9696), (9, 0.9691), (10, 0.9669)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9669
wandb:     loss 1.49422
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_084031-owa2xj4z
wandb: Find logs at: ./wandb/offline-run-20240418_084031-owa2xj4z/logs
INFO flwr 2024-04-18 08:48:05,019 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 08:55:28,160 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1139556)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1139556)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 08:55:33,535	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 08:55:34,543	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 08:55:35,011	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 08:55:35,168	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (32.54MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 08:55:35,502	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a53f9ff8e21e7ce2.zip' (71.36MiB) to Ray cluster...
2024-04-18 08:55:35,753	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a53f9ff8e21e7ce2.zip'.
INFO flwr 2024-04-18 08:55:46,926 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 76980648345.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169621512807.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 08:55:46,926 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 08:55:46,926 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 08:55:46,943 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 08:55:46,944 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 08:55:46,944 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 08:55:46,944 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1144941)[0m 2024-04-18 08:55:52.955042: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1144941)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1144935)[0m 2024-04-18 08:55:55.313009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 08:55:56,391 | server.py:94 | initial parameters (loss, other metrics): 2.3026442527770996, {'accuracy': 0.0931, 'data_size': 10000}
INFO flwr 2024-04-18 08:55:56,392 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 08:55:56,392 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1144943)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1144943)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1144940)[0m 2024-04-18 08:55:53.218285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1144940)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1144940)[0m 2024-04-18 08:55:55.463147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1144937)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1144937)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 08:56:40,430 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 08:56:44,642 | server.py:125 | fit progress: (1, 2.295897960662842, {'accuracy': 0.6636, 'data_size': 10000}, 48.24989609900513)
INFO flwr 2024-04-18 08:56:44,642 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 08:56:44,643 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:57:19,529 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 08:57:23,617 | server.py:125 | fit progress: (2, 2.281446695327759, {'accuracy': 0.8555, 'data_size': 10000}, 87.22474065099959)
INFO flwr 2024-04-18 08:57:23,617 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 08:57:23,617 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:58:02,620 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 08:58:06,940 | server.py:125 | fit progress: (3, 2.2509782314300537, {'accuracy': 0.9127, 'data_size': 10000}, 130.548082866997)
INFO flwr 2024-04-18 08:58:06,940 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 08:58:06,941 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:58:45,377 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 08:58:49,620 | server.py:125 | fit progress: (4, 2.189939022064209, {'accuracy': 0.9313, 'data_size': 10000}, 173.2278541260166)
INFO flwr 2024-04-18 08:58:49,620 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 08:58:49,621 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 08:59:28,396 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 08:59:32,404 | server.py:125 | fit progress: (5, 2.080918788909912, {'accuracy': 0.9412, 'data_size': 10000}, 216.01208272000076)
INFO flwr 2024-04-18 08:59:32,404 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 08:59:32,405 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:00:08,764 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 09:00:12,918 | server.py:125 | fit progress: (6, 1.918238878250122, {'accuracy': 0.9459, 'data_size': 10000}, 256.52574202101096)
INFO flwr 2024-04-18 09:00:12,918 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 09:00:12,918 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:00:49,517 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 09:00:53,432 | server.py:125 | fit progress: (7, 1.7461402416229248, {'accuracy': 0.9496, 'data_size': 10000}, 297.04021184600424)
INFO flwr 2024-04-18 09:00:53,433 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 09:00:53,433 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:01:33,741 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 09:01:37,708 | server.py:125 | fit progress: (8, 1.6203091144561768, {'accuracy': 0.9519, 'data_size': 10000}, 341.31560188301955)
INFO flwr 2024-04-18 09:01:37,708 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 09:01:37,708 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:02:15,595 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 09:02:19,916 | server.py:125 | fit progress: (9, 1.5535532236099243, {'accuracy': 0.9554, 'data_size': 10000}, 383.5240026440006)
INFO flwr 2024-04-18 09:02:19,917 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 09:02:19,917 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:03:01,331 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 09:03:05,312 | server.py:125 | fit progress: (10, 1.5238903760910034, {'accuracy': 0.9576, 'data_size': 10000}, 428.92001091499696)
INFO flwr 2024-04-18 09:03:05,312 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 09:03:05,313 | server.py:153 | FL finished in 428.92048923601396
INFO flwr 2024-04-18 09:03:05,317 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 09:03:05,317 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 09:03:05,318 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 09:03:05,318 | app.py:229 | app_fit: losses_centralized [(0, 2.3026442527770996), (1, 2.295897960662842), (2, 2.281446695327759), (3, 2.2509782314300537), (4, 2.189939022064209), (5, 2.080918788909912), (6, 1.918238878250122), (7, 1.7461402416229248), (8, 1.6203091144561768), (9, 1.5535532236099243), (10, 1.5238903760910034)]
INFO flwr 2024-04-18 09:03:05,318 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0931), (1, 0.6636), (2, 0.8555), (3, 0.9127), (4, 0.9313), (5, 0.9412), (6, 0.9459), (7, 0.9496), (8, 0.9519), (9, 0.9554), (10, 0.9576)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9576
wandb:     loss 1.52389
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_085527-ehnfmiwg
wandb: Find logs at: ./wandb/offline-run-20240418_085527-ehnfmiwg/logs
INFO flwr 2024-04-18 09:03:09,410 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 09:10:32,444 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1144934)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1144934)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 09:10:38,219	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 09:10:39,258	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 09:10:39,738	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 09:10:39,896	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (33.50MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 09:10:40,237	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7a722b2c9eadf1c6.zip' (72.33MiB) to Ray cluster...
2024-04-18 09:10:40,478	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7a722b2c9eadf1c6.zip'.
INFO flwr 2024-04-18 09:10:51,521 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'memory': 169382672180.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 76878288076.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 09:10:51,521 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 09:10:51,522 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 09:10:51,542 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 09:10:51,543 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 09:10:51,544 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 09:10:51,544 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1150027)[0m 2024-04-18 09:10:57.677193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1150027)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1150027)[0m 2024-04-18 09:10:59.979345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 09:11:00,846 | server.py:94 | initial parameters (loss, other metrics): 2.302600145339966, {'accuracy': 0.0953, 'data_size': 10000}
INFO flwr 2024-04-18 09:11:00,846 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 09:11:00,847 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1150027)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1150027)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1150026)[0m 2024-04-18 09:10:58.002829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1150026)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1150026)[0m 2024-04-18 09:11:00.293541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1150019)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1150019)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 09:11:46,387 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 09:11:50,628 | server.py:125 | fit progress: (1, 2.3019607067108154, {'accuracy': 0.0979, 'data_size': 10000}, 49.78122217801865)
INFO flwr 2024-04-18 09:11:50,628 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 09:11:50,629 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:12:24,158 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 09:12:28,142 | server.py:125 | fit progress: (2, 2.301048994064331, {'accuracy': 0.1116, 'data_size': 10000}, 87.2954483750218)
INFO flwr 2024-04-18 09:12:28,143 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 09:12:28,143 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:13:06,621 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 09:13:10,747 | server.py:125 | fit progress: (3, 2.2999205589294434, {'accuracy': 0.2117, 'data_size': 10000}, 129.9000122230209)
INFO flwr 2024-04-18 09:13:10,747 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 09:13:10,747 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:13:52,724 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 09:13:56,898 | server.py:125 | fit progress: (4, 2.298614740371704, {'accuracy': 0.3549, 'data_size': 10000}, 176.05076247500256)
INFO flwr 2024-04-18 09:13:56,898 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 09:13:56,898 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:14:36,100 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 09:14:40,007 | server.py:125 | fit progress: (5, 2.297109365463257, {'accuracy': 0.4491, 'data_size': 10000}, 219.16027478899923)
INFO flwr 2024-04-18 09:14:40,008 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 09:14:40,008 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:15:16,320 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 09:15:20,493 | server.py:125 | fit progress: (6, 2.2954087257385254, {'accuracy': 0.5582, 'data_size': 10000}, 259.6465187270078)
INFO flwr 2024-04-18 09:15:20,494 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 09:15:20,494 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:16:01,063 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 09:16:05,000 | server.py:125 | fit progress: (7, 2.293499708175659, {'accuracy': 0.6327, 'data_size': 10000}, 304.1534880970139)
INFO flwr 2024-04-18 09:16:05,001 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 09:16:05,001 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:16:41,764 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 09:16:45,712 | server.py:125 | fit progress: (8, 2.2913460731506348, {'accuracy': 0.684, 'data_size': 10000}, 344.8653462990187)
INFO flwr 2024-04-18 09:16:45,712 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 09:16:45,713 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:17:24,085 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 09:17:28,297 | server.py:125 | fit progress: (9, 2.2889583110809326, {'accuracy': 0.7203, 'data_size': 10000}, 387.450198053004)
INFO flwr 2024-04-18 09:17:28,297 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 09:17:28,298 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:18:06,706 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 09:18:10,628 | server.py:125 | fit progress: (10, 2.2863001823425293, {'accuracy': 0.747, 'data_size': 10000}, 429.7814933329937)
INFO flwr 2024-04-18 09:18:10,629 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 09:18:10,629 | server.py:153 | FL finished in 429.7820428800187
INFO flwr 2024-04-18 09:18:10,635 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 09:18:10,635 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 09:18:10,636 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 09:18:10,636 | app.py:229 | app_fit: losses_centralized [(0, 2.302600145339966), (1, 2.3019607067108154), (2, 2.301048994064331), (3, 2.2999205589294434), (4, 2.298614740371704), (5, 2.297109365463257), (6, 2.2954087257385254), (7, 2.293499708175659), (8, 2.2913460731506348), (9, 2.2889583110809326), (10, 2.2863001823425293)]
INFO flwr 2024-04-18 09:18:10,636 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0953), (1, 0.0979), (2, 0.1116), (3, 0.2117), (4, 0.3549), (5, 0.4491), (6, 0.5582), (7, 0.6327), (8, 0.684), (9, 0.7203), (10, 0.747)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.747
wandb:     loss 2.2863
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_091031-1bmfk64r
wandb: Find logs at: ./wandb/offline-run-20240418_091031-1bmfk64r/logs
INFO flwr 2024-04-18 09:18:14,215 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 09:25:37,191 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1150018)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1150018)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 09:25:41,684	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 09:25:42,658	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 09:25:43,118	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 09:25:43,272	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (33.99MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 09:25:43,611	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ae917ed53b909622.zip' (72.83MiB) to Ray cluster...
2024-04-18 09:25:43,852	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ae917ed53b909622.zip'.
INFO flwr 2024-04-18 09:25:54,990 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 169422916199.0, 'object_store_memory': 76895535513.0}
INFO flwr 2024-04-18 09:25:54,990 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 09:25:54,990 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 09:25:55,005 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 09:25:55,007 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 09:25:55,008 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 09:25:55,008 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1155421)[0m 2024-04-18 09:26:01.000183: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1155421)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1155421)[0m 2024-04-18 09:26:03.284581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 09:26:04,765 | server.py:94 | initial parameters (loss, other metrics): 2.3026926517486572, {'accuracy': 0.1021, 'data_size': 10000}
INFO flwr 2024-04-18 09:26:04,766 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 09:26:04,766 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1155429)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1155429)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1155427)[0m 2024-04-18 09:26:01.300251: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1155427)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1155427)[0m 2024-04-18 09:26:03.735824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1155424)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1155424)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 09:26:51,093 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 09:26:55,323 | server.py:125 | fit progress: (1, 2.302628993988037, {'accuracy': 0.1096, 'data_size': 10000}, 50.55707814800553)
INFO flwr 2024-04-18 09:26:55,323 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 09:26:55,323 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:27:31,594 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 09:27:35,561 | server.py:125 | fit progress: (2, 2.302549362182617, {'accuracy': 0.1204, 'data_size': 10000}, 90.79489040299086)
INFO flwr 2024-04-18 09:27:35,561 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 09:27:35,561 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:28:17,632 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 09:28:21,786 | server.py:125 | fit progress: (3, 2.302457571029663, {'accuracy': 0.1328, 'data_size': 10000}, 137.02053149300627)
INFO flwr 2024-04-18 09:28:21,787 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 09:28:21,787 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:28:58,968 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 09:29:03,152 | server.py:125 | fit progress: (4, 2.3023579120635986, {'accuracy': 0.1457, 'data_size': 10000}, 178.38584214798175)
INFO flwr 2024-04-18 09:29:03,152 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 09:29:03,152 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:29:39,471 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 09:29:43,379 | server.py:125 | fit progress: (5, 2.3022499084472656, {'accuracy': 0.1579, 'data_size': 10000}, 218.61271070298972)
INFO flwr 2024-04-18 09:29:43,379 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 09:29:43,380 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:30:19,041 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 09:30:23,264 | server.py:125 | fit progress: (6, 2.3021366596221924, {'accuracy': 0.1754, 'data_size': 10000}, 258.4980866200058)
INFO flwr 2024-04-18 09:30:23,264 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 09:30:23,265 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:31:01,084 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 09:31:05,027 | server.py:125 | fit progress: (7, 2.3020191192626953, {'accuracy': 0.1902, 'data_size': 10000}, 300.2610874660022)
INFO flwr 2024-04-18 09:31:05,028 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 09:31:05,028 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:31:40,028 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 09:31:44,403 | server.py:125 | fit progress: (8, 2.3018975257873535, {'accuracy': 0.2035, 'data_size': 10000}, 339.6366960220039)
INFO flwr 2024-04-18 09:31:44,403 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 09:31:44,403 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:32:19,811 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 09:32:23,965 | server.py:125 | fit progress: (9, 2.301774501800537, {'accuracy': 0.2141, 'data_size': 10000}, 379.199577508989)
INFO flwr 2024-04-18 09:32:23,966 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 09:32:23,966 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:33:00,787 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 09:33:05,038 | server.py:125 | fit progress: (10, 2.3016469478607178, {'accuracy': 0.2249, 'data_size': 10000}, 420.2725244750036)
INFO flwr 2024-04-18 09:33:05,039 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 09:33:05,039 | server.py:153 | FL finished in 420.27299371300614
INFO flwr 2024-04-18 09:33:05,046 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 09:33:05,046 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 09:33:05,046 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 09:33:05,046 | app.py:229 | app_fit: losses_centralized [(0, 2.3026926517486572), (1, 2.302628993988037), (2, 2.302549362182617), (3, 2.302457571029663), (4, 2.3023579120635986), (5, 2.3022499084472656), (6, 2.3021366596221924), (7, 2.3020191192626953), (8, 2.3018975257873535), (9, 2.301774501800537), (10, 2.3016469478607178)]
INFO flwr 2024-04-18 09:33:05,046 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1021), (1, 0.1096), (2, 0.1204), (3, 0.1328), (4, 0.1457), (5, 0.1579), (6, 0.1754), (7, 0.1902), (8, 0.2035), (9, 0.2141), (10, 0.2249)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2249
wandb:     loss 2.30165
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_092536-b6kpxhcx
wandb: Find logs at: ./wandb/offline-run-20240418_092536-b6kpxhcx/logs
INFO flwr 2024-04-18 09:33:08,662 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 09:40:36,459 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1155419)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1155419)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 09:41:03,028	ERROR services.py:1207 -- Failed to start the dashboard 
2024-04-18 09:41:03,030	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-04-18 09:41:03,031	ERROR services.py:1276 -- 
The last 20 lines of /local/ray/session_2024-04-18_09-40-38_844825_960917/logs/dashboard.log (it contains the error message from the dashboard): 
2024-04-18 09:41:03,234	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 09:41:17,994	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 09:41:18,511	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 09:41:18,678	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (34.96MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 09:41:19,025	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_79606f93491576b8.zip' (73.81MiB) to Ray cluster...
2024-04-18 09:41:19,253	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_79606f93491576b8.zip'.
INFO flwr 2024-04-18 09:41:30,563 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 75799050240.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 166864450560.0}
INFO flwr 2024-04-18 09:41:30,563 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 09:41:30,563 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 09:41:30,586 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 09:41:30,587 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 09:41:30,587 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 09:41:30,587 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 09:41:37,717 | server.py:94 | initial parameters (loss, other metrics): 2.302731990814209, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-18 09:41:37,717 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 09:41:37,717 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1160531)[0m 2024-04-18 09:41:41.552336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1160531)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1160531)[0m 2024-04-18 09:42:02.311533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1160519)[0m 2024-04-18 09:41:41.596633: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1160519)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1160531)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1160531)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1160518)[0m 2024-04-18 09:42:02.311541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 09:43:57,241 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 09:44:01,564 | server.py:125 | fit progress: (1, 1.603808045387268, {'accuracy': 0.8572, 'data_size': 10000}, 143.84662940099952)
INFO flwr 2024-04-18 09:44:01,564 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 09:44:01,565 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:44:42,789 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 09:44:46,881 | server.py:125 | fit progress: (2, 1.7487688064575195, {'accuracy': 0.7123, 'data_size': 10000}, 189.16390047600726)
INFO flwr 2024-04-18 09:44:46,881 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 09:44:46,882 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:45:23,385 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 09:45:27,729 | server.py:125 | fit progress: (3, 1.8140318393707275, {'accuracy': 0.647, 'data_size': 10000}, 230.0121832450095)
INFO flwr 2024-04-18 09:45:27,730 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 09:45:27,730 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:46:08,200 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 09:46:12,505 | server.py:125 | fit progress: (4, 1.8102478981018066, {'accuracy': 0.6509, 'data_size': 10000}, 274.7881170999899)
INFO flwr 2024-04-18 09:46:12,506 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 09:46:12,506 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:46:50,885 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 09:46:54,965 | server.py:125 | fit progress: (5, 1.913846731185913, {'accuracy': 0.5473, 'data_size': 10000}, 317.2477665670158)
INFO flwr 2024-04-18 09:46:54,965 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 09:46:54,965 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:47:32,726 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 09:47:37,039 | server.py:125 | fit progress: (6, 2.164940595626831, {'accuracy': 0.2962, 'data_size': 10000}, 359.32151400501607)
INFO flwr 2024-04-18 09:47:37,039 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 09:47:37,040 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:48:15,549 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 09:48:19,642 | server.py:125 | fit progress: (7, 2.347642421722412, {'accuracy': 0.1135, 'data_size': 10000}, 401.92450030701)
INFO flwr 2024-04-18 09:48:19,642 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 09:48:19,642 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:48:56,167 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 09:49:00,477 | server.py:125 | fit progress: (8, 2.359842300415039, {'accuracy': 0.1013, 'data_size': 10000}, 442.75947643100517)
INFO flwr 2024-04-18 09:49:00,477 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 09:49:00,477 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:49:30,198 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 09:49:34,493 | server.py:125 | fit progress: (9, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 476.77546030501253)
INFO flwr 2024-04-18 09:49:34,493 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 09:49:34,493 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:50:13,294 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 09:50:17,588 | server.py:125 | fit progress: (10, 2.3602421283721924, {'accuracy': 0.1009, 'data_size': 10000}, 519.8703191000095)
INFO flwr 2024-04-18 09:50:17,588 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 09:50:17,588 | server.py:153 | FL finished in 519.8708300419967
INFO flwr 2024-04-18 09:50:17,595 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 09:50:17,595 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 09:50:17,595 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 09:50:17,595 | app.py:229 | app_fit: losses_centralized [(0, 2.302731990814209), (1, 1.603808045387268), (2, 1.7487688064575195), (3, 1.8140318393707275), (4, 1.8102478981018066), (5, 1.913846731185913), (6, 2.164940595626831), (7, 2.347642421722412), (8, 2.359842300415039), (9, 2.3602421283721924), (10, 2.3602421283721924)]
INFO flwr 2024-04-18 09:50:17,596 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.8572), (2, 0.7123), (3, 0.647), (4, 0.6509), (5, 0.5473), (6, 0.2962), (7, 0.1135), (8, 0.1013), (9, 0.1009), (10, 0.1009)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1009
wandb:     loss 2.36024
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_094033-r069n1ya
wandb: Find logs at: ./wandb/offline-run-20240418_094033-r069n1ya/logs
INFO flwr 2024-04-18 09:50:21,198 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 09:57:45,644 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1160518)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1160518)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 09:57:50,223	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 09:57:51,189	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 09:57:51,658	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 09:57:51,816	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (35.44MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 09:57:52,166	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2815e6d7db8f31f3.zip' (74.31MiB) to Ray cluster...
2024-04-18 09:57:52,398	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2815e6d7db8f31f3.zip'.
INFO flwr 2024-04-18 09:58:03,567 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75556690329.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 166298944103.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 09:58:03,567 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 09:58:03,567 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 09:58:03,583 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 09:58:03,584 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 09:58:03,584 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 09:58:03,584 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1166517)[0m 2024-04-18 09:58:09.600653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1166517)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1166517)[0m 2024-04-18 09:58:11.858792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 09:58:13,066 | server.py:94 | initial parameters (loss, other metrics): 2.3023605346679688, {'accuracy': 0.1717, 'data_size': 10000}
INFO flwr 2024-04-18 09:58:13,066 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 09:58:13,066 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1166521)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1166521)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1166521)[0m 2024-04-18 09:58:09.931185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1166521)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1166521)[0m 2024-04-18 09:58:12.396269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1166513)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1166513)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 09:58:56,627 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 09:59:01,010 | server.py:125 | fit progress: (1, 2.2076377868652344, {'accuracy': 0.775, 'data_size': 10000}, 47.94373896700563)
INFO flwr 2024-04-18 09:59:01,010 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 09:59:01,011 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 09:59:37,390 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 09:59:41,507 | server.py:125 | fit progress: (2, 1.5401910543441772, {'accuracy': 0.9432, 'data_size': 10000}, 88.44110838000779)
INFO flwr 2024-04-18 09:59:41,508 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 09:59:41,508 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:00:16,123 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 10:00:20,447 | server.py:125 | fit progress: (3, 1.4978750944137573, {'accuracy': 0.9632, 'data_size': 10000}, 127.38081421799143)
INFO flwr 2024-04-18 10:00:20,447 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 10:00:20,448 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:00:59,041 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 10:01:03,377 | server.py:125 | fit progress: (4, 1.500241756439209, {'accuracy': 0.9612, 'data_size': 10000}, 170.31115594899165)
INFO flwr 2024-04-18 10:01:03,378 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 10:01:03,378 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:01:43,340 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 10:01:47,406 | server.py:125 | fit progress: (5, 1.507790446281433, {'accuracy': 0.9531, 'data_size': 10000}, 214.33957049099263)
INFO flwr 2024-04-18 10:01:47,406 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 10:01:47,406 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:02:25,380 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 10:02:29,687 | server.py:125 | fit progress: (6, 1.507965087890625, {'accuracy': 0.9531, 'data_size': 10000}, 256.62038361799205)
INFO flwr 2024-04-18 10:02:29,687 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 10:02:29,687 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:03:09,993 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 10:03:14,098 | server.py:125 | fit progress: (7, 1.5106923580169678, {'accuracy': 0.9506, 'data_size': 10000}, 301.03169546998106)
INFO flwr 2024-04-18 10:03:14,098 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 10:03:14,099 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:03:50,179 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 10:03:54,528 | server.py:125 | fit progress: (8, 1.5113526582717896, {'accuracy': 0.9498, 'data_size': 10000}, 341.46162795700366)
INFO flwr 2024-04-18 10:03:54,528 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 10:03:54,528 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:04:31,597 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 10:04:35,978 | server.py:125 | fit progress: (9, 1.5144715309143066, {'accuracy': 0.9467, 'data_size': 10000}, 382.91129497799557)
INFO flwr 2024-04-18 10:04:35,978 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 10:04:35,978 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:05:12,591 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 10:05:16,938 | server.py:125 | fit progress: (10, 1.5209839344024658, {'accuracy': 0.9401, 'data_size': 10000}, 423.87137288600206)
INFO flwr 2024-04-18 10:05:16,938 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 10:05:16,938 | server.py:153 | FL finished in 423.87185655199573
INFO flwr 2024-04-18 10:05:16,943 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 10:05:16,943 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 10:05:16,943 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 10:05:16,943 | app.py:229 | app_fit: losses_centralized [(0, 2.3023605346679688), (1, 2.2076377868652344), (2, 1.5401910543441772), (3, 1.4978750944137573), (4, 1.500241756439209), (5, 1.507790446281433), (6, 1.507965087890625), (7, 1.5106923580169678), (8, 1.5113526582717896), (9, 1.5144715309143066), (10, 1.5209839344024658)]
INFO flwr 2024-04-18 10:05:16,944 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1717), (1, 0.775), (2, 0.9432), (3, 0.9632), (4, 0.9612), (5, 0.9531), (6, 0.9531), (7, 0.9506), (8, 0.9498), (9, 0.9467), (10, 0.9401)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9401
wandb:     loss 1.52098
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_095745-0ktfnk6v
wandb: Find logs at: ./wandb/offline-run-20240418_095745-0ktfnk6v/logs
INFO flwr 2024-04-18 10:05:20,499 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 10:12:44,984 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1166507)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1166507)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 10:12:50,604	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 10:12:51,605	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 10:12:52,053	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 10:12:52,205	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (36.41MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 10:12:52,552	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a27189a24af98132.zip' (75.29MiB) to Ray cluster...
2024-04-18 10:12:52,791	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a27189a24af98132.zip'.
INFO flwr 2024-04-18 10:13:05,660 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75566824243.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 166322589901.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-18 10:13:05,660 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 10:13:05,660 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 10:13:05,684 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 10:13:05,685 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 10:13:05,686 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 10:13:05,686 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1171909)[0m 2024-04-18 10:13:11.693671: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1171909)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1171909)[0m 2024-04-18 10:13:14.022556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 10:13:14,230 | server.py:94 | initial parameters (loss, other metrics): 2.3025834560394287, {'accuracy': 0.099, 'data_size': 10000}
INFO flwr 2024-04-18 10:13:14,231 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 10:13:14,231 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1171913)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1171913)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1171900)[0m 2024-04-18 10:13:11.981179: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1171900)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1171900)[0m 2024-04-18 10:13:14.210674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1171902)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1171902)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1171898)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1171898)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
DEBUG flwr 2024-04-18 10:13:58,040 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 10:14:02,374 | server.py:125 | fit progress: (1, 2.2953221797943115, {'accuracy': 0.5606, 'data_size': 10000}, 48.14324036400649)
INFO flwr 2024-04-18 10:14:02,375 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 10:14:02,375 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:14:41,640 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 10:14:45,696 | server.py:125 | fit progress: (2, 2.2817468643188477, {'accuracy': 0.7149, 'data_size': 10000}, 91.46531729801791)
INFO flwr 2024-04-18 10:14:45,697 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 10:14:45,697 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:15:25,161 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 10:15:29,453 | server.py:125 | fit progress: (3, 2.2591562271118164, {'accuracy': 0.7981, 'data_size': 10000}, 135.22147952401428)
INFO flwr 2024-04-18 10:15:29,453 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 10:15:29,453 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:16:05,416 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 10:16:09,699 | server.py:125 | fit progress: (4, 2.2201225757598877, {'accuracy': 0.8591, 'data_size': 10000}, 175.46765157501795)
INFO flwr 2024-04-18 10:16:09,699 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 10:16:09,699 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:16:43,619 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 10:16:47,681 | server.py:125 | fit progress: (5, 2.1526713371276855, {'accuracy': 0.9004, 'data_size': 10000}, 213.4499437630002)
INFO flwr 2024-04-18 10:16:47,681 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 10:16:47,682 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:17:27,195 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 10:17:31,551 | server.py:125 | fit progress: (6, 2.0449910163879395, {'accuracy': 0.9245, 'data_size': 10000}, 257.3194438579958)
INFO flwr 2024-04-18 10:17:31,551 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 10:17:31,551 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:18:06,869 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 10:18:10,989 | server.py:125 | fit progress: (7, 1.8958321809768677, {'accuracy': 0.937, 'data_size': 10000}, 296.75828909300617)
INFO flwr 2024-04-18 10:18:10,990 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 10:18:10,990 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:18:49,170 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 10:18:53,343 | server.py:125 | fit progress: (8, 1.746070146560669, {'accuracy': 0.9456, 'data_size': 10000}, 339.11242602200946)
INFO flwr 2024-04-18 10:18:53,344 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 10:18:53,344 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:19:31,337 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 10:19:35,675 | server.py:125 | fit progress: (9, 1.6312075853347778, {'accuracy': 0.9515, 'data_size': 10000}, 381.4443877930171)
INFO flwr 2024-04-18 10:19:35,676 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 10:19:35,676 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:20:10,767 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 10:20:14,827 | server.py:125 | fit progress: (10, 1.5628044605255127, {'accuracy': 0.9552, 'data_size': 10000}, 420.5957251500222)
INFO flwr 2024-04-18 10:20:14,827 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 10:20:14,827 | server.py:153 | FL finished in 420.59617167600663
INFO flwr 2024-04-18 10:20:14,833 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 10:20:14,833 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 10:20:14,833 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 10:20:14,833 | app.py:229 | app_fit: losses_centralized [(0, 2.3025834560394287), (1, 2.2953221797943115), (2, 2.2817468643188477), (3, 2.2591562271118164), (4, 2.2201225757598877), (5, 2.1526713371276855), (6, 2.0449910163879395), (7, 1.8958321809768677), (8, 1.746070146560669), (9, 1.6312075853347778), (10, 1.5628044605255127)]
INFO flwr 2024-04-18 10:20:14,833 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.099), (1, 0.5606), (2, 0.7149), (3, 0.7981), (4, 0.8591), (5, 0.9004), (6, 0.9245), (7, 0.937), (8, 0.9456), (9, 0.9515), (10, 0.9552)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9552
wandb:     loss 1.5628
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_101244-m0yghrxl
wandb: Find logs at: ./wandb/offline-run-20240418_101244-m0yghrxl/logs
INFO flwr 2024-04-18 10:20:18,393 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 10:27:42,777 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 10:27:47,637	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 10:27:48,670	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 10:27:49,144	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 10:27:49,302	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (36.89MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 10:27:49,665	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b254c04b85b83221.zip' (75.79MiB) to Ray cluster...
2024-04-18 10:27:49,934	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b254c04b85b83221.zip'.
INFO flwr 2024-04-18 10:28:01,003 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 75567040512.0, 'CPU': 64.0, 'memory': 166323094528.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 10:28:01,004 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 10:28:01,004 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 10:28:01,022 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 10:28:01,024 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 10:28:01,024 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 10:28:01,024 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1176330)[0m 2024-04-18 10:28:07.091629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1176330)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1176330)[0m 2024-04-18 10:28:09.400554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 10:28:10,127 | server.py:94 | initial parameters (loss, other metrics): 2.302597761154175, {'accuracy': 0.1026, 'data_size': 10000}
INFO flwr 2024-04-18 10:28:10,127 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 10:28:10,127 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1176345)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1176345)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1176344)[0m 2024-04-18 10:28:07.314014: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1176344)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1176344)[0m 2024-04-18 10:28:09.543774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1176335)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1176335)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 10:29:00,631 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 10:29:05,015 | server.py:125 | fit progress: (1, 2.3021163940429688, {'accuracy': 0.1143, 'data_size': 10000}, 54.88741352799116)
INFO flwr 2024-04-18 10:29:05,015 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 10:29:05,015 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:29:41,339 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 10:29:45,463 | server.py:125 | fit progress: (2, 2.3014495372772217, {'accuracy': 0.126, 'data_size': 10000}, 95.33553415597999)
INFO flwr 2024-04-18 10:29:45,463 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 10:29:45,463 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:30:23,974 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 10:30:28,355 | server.py:125 | fit progress: (3, 2.3006651401519775, {'accuracy': 0.1974, 'data_size': 10000}, 138.22817731898976)
INFO flwr 2024-04-18 10:30:28,356 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 10:30:28,356 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:31:04,392 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 10:31:08,792 | server.py:125 | fit progress: (4, 2.299755811691284, {'accuracy': 0.3377, 'data_size': 10000}, 178.66492877400015)
INFO flwr 2024-04-18 10:31:08,793 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 10:31:08,793 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:31:51,374 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 10:31:55,452 | server.py:125 | fit progress: (5, 2.298724889755249, {'accuracy': 0.4737, 'data_size': 10000}, 225.32506159698823)
INFO flwr 2024-04-18 10:31:55,453 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 10:31:55,453 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:32:35,755 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 10:32:40,093 | server.py:125 | fit progress: (6, 2.297593116760254, {'accuracy': 0.5568, 'data_size': 10000}, 269.9656063769944)
INFO flwr 2024-04-18 10:32:40,093 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 10:32:40,094 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:33:18,158 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 10:33:22,367 | server.py:125 | fit progress: (7, 2.296337127685547, {'accuracy': 0.6115, 'data_size': 10000}, 312.23923676199047)
INFO flwr 2024-04-18 10:33:22,367 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 10:33:22,367 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:33:57,346 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 10:34:01,451 | server.py:125 | fit progress: (8, 2.2949657440185547, {'accuracy': 0.6555, 'data_size': 10000}, 351.3236056379974)
INFO flwr 2024-04-18 10:34:01,451 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 10:34:01,451 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:34:41,349 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 10:34:45,700 | server.py:125 | fit progress: (9, 2.2934389114379883, {'accuracy': 0.7054, 'data_size': 10000}, 395.5729124339996)
INFO flwr 2024-04-18 10:34:45,701 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 10:34:45,701 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:35:20,275 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 10:35:24,390 | server.py:125 | fit progress: (10, 2.2917428016662598, {'accuracy': 0.7548, 'data_size': 10000}, 434.2626530729758)
INFO flwr 2024-04-18 10:35:24,390 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 10:35:24,390 | server.py:153 | FL finished in 434.2631798129878
INFO flwr 2024-04-18 10:35:24,395 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 10:35:24,395 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 10:35:24,395 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 10:35:24,395 | app.py:229 | app_fit: losses_centralized [(0, 2.302597761154175), (1, 2.3021163940429688), (2, 2.3014495372772217), (3, 2.3006651401519775), (4, 2.299755811691284), (5, 2.298724889755249), (6, 2.297593116760254), (7, 2.296337127685547), (8, 2.2949657440185547), (9, 2.2934389114379883), (10, 2.2917428016662598)]
INFO flwr 2024-04-18 10:35:24,396 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1026), (1, 0.1143), (2, 0.126), (3, 0.1974), (4, 0.3377), (5, 0.4737), (6, 0.5568), (7, 0.6115), (8, 0.6555), (9, 0.7054), (10, 0.7548)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7548
wandb:     loss 2.29174
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_102742-cuj4998l
wandb: Find logs at: ./wandb/offline-run-20240418_102742-cuj4998l/logs
INFO flwr 2024-04-18 10:35:27,991 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 10:42:52,252 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1176330)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1176330)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 10:42:57,902	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 10:42:58,938	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 10:42:59,410	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 10:42:59,569	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (37.39MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 10:42:59,972	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2a3e25896b1d1e46.zip' (76.30MiB) to Ray cluster...
2024-04-18 10:43:00,237	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2a3e25896b1d1e46.zip'.
INFO flwr 2024-04-18 10:43:11,515 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75418662912.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 165976880128.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 10:43:11,516 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 10:43:11,516 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 10:43:11,537 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 10:43:11,538 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 10:43:11,538 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 10:43:11,538 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1181741)[0m 2024-04-18 10:43:17.533064: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1181741)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1181744)[0m 2024-04-18 10:43:19.856409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 10:43:21,283 | server.py:94 | initial parameters (loss, other metrics): 2.3024702072143555, {'accuracy': 0.0862, 'data_size': 10000}
INFO flwr 2024-04-18 10:43:21,284 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 10:43:21,284 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1181744)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1181744)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1181735)[0m 2024-04-18 10:43:17.912907: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1181735)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1181735)[0m 2024-04-18 10:43:20.340510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1181738)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1181738)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 10:44:03,971 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 10:44:08,353 | server.py:125 | fit progress: (1, 2.3024091720581055, {'accuracy': 0.0885, 'data_size': 10000}, 47.06942393502686)
INFO flwr 2024-04-18 10:44:08,354 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 10:44:08,354 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:44:46,157 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 10:44:50,266 | server.py:125 | fit progress: (2, 2.3023293018341064, {'accuracy': 0.093, 'data_size': 10000}, 88.98164161900058)
INFO flwr 2024-04-18 10:44:50,266 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 10:44:50,266 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:45:31,683 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 10:45:36,037 | server.py:125 | fit progress: (3, 2.3022360801696777, {'accuracy': 0.0989, 'data_size': 10000}, 134.75296203402104)
INFO flwr 2024-04-18 10:45:36,037 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 10:45:36,038 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:46:14,256 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 10:46:18,549 | server.py:125 | fit progress: (4, 2.3021342754364014, {'accuracy': 0.1062, 'data_size': 10000}, 177.2648956150224)
INFO flwr 2024-04-18 10:46:18,549 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 10:46:18,550 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:46:59,531 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 10:47:03,673 | server.py:125 | fit progress: (5, 2.3020267486572266, {'accuracy': 0.1141, 'data_size': 10000}, 222.3887467890163)
INFO flwr 2024-04-18 10:47:03,673 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 10:47:03,673 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:47:44,794 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 10:47:49,185 | server.py:125 | fit progress: (6, 2.3019137382507324, {'accuracy': 0.1234, 'data_size': 10000}, 267.90144714701455)
INFO flwr 2024-04-18 10:47:49,186 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 10:47:49,186 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:48:28,110 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 10:48:32,289 | server.py:125 | fit progress: (7, 2.301797389984131, {'accuracy': 0.1311, 'data_size': 10000}, 311.00500571302837)
INFO flwr 2024-04-18 10:48:32,289 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 10:48:32,290 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:49:10,055 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 10:49:14,181 | server.py:125 | fit progress: (8, 2.30167818069458, {'accuracy': 0.1417, 'data_size': 10000}, 352.8966580570268)
INFO flwr 2024-04-18 10:49:14,181 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 10:49:14,181 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:50:00,255 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 10:50:04,676 | server.py:125 | fit progress: (9, 2.3015599250793457, {'accuracy': 0.1492, 'data_size': 10000}, 403.3921896270185)
INFO flwr 2024-04-18 10:50:04,677 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 10:50:04,677 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:50:44,364 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 10:50:48,530 | server.py:125 | fit progress: (10, 2.301438570022583, {'accuracy': 0.1577, 'data_size': 10000}, 447.24606986902654)
INFO flwr 2024-04-18 10:50:48,530 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 10:50:48,531 | server.py:153 | FL finished in 447.24656001801486
INFO flwr 2024-04-18 10:50:48,533 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 10:50:48,534 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 10:50:48,534 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 10:50:48,534 | app.py:229 | app_fit: losses_centralized [(0, 2.3024702072143555), (1, 2.3024091720581055), (2, 2.3023293018341064), (3, 2.3022360801696777), (4, 2.3021342754364014), (5, 2.3020267486572266), (6, 2.3019137382507324), (7, 2.301797389984131), (8, 2.30167818069458), (9, 2.3015599250793457), (10, 2.301438570022583)]
INFO flwr 2024-04-18 10:50:48,534 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0862), (1, 0.0885), (2, 0.093), (3, 0.0989), (4, 0.1062), (5, 0.1141), (6, 0.1234), (7, 0.1311), (8, 0.1417), (9, 0.1492), (10, 0.1577)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1577
wandb:     loss 2.30144
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_104251-k8nswasg
wandb: Find logs at: ./wandb/offline-run-20240418_104251-k8nswasg/logs
INFO flwr 2024-04-18 10:50:52,087 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 10:58:16,397 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1181734)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1181734)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 10:58:21,136	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 10:58:22,193	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 10:58:22,639	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 10:58:22,791	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (38.35MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 10:58:23,158	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_070d1b01bb6ace80.zip' (77.27MiB) to Ray cluster...
2024-04-18 10:58:23,429	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_070d1b01bb6ace80.zip'.
INFO flwr 2024-04-18 10:58:34,668 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 75486728601.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 166135700071.0}
INFO flwr 2024-04-18 10:58:34,669 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 10:58:34,669 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 10:58:34,690 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 10:58:34,690 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 10:58:34,691 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 10:58:34,691 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1187362)[0m 2024-04-18 10:58:40.727563: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1187362)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1187362)[0m 2024-04-18 10:58:43.046239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 10:58:43,419 | server.py:94 | initial parameters (loss, other metrics): 2.3024840354919434, {'accuracy': 0.0965, 'data_size': 10000}
INFO flwr 2024-04-18 10:58:43,420 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 10:58:43,420 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1187366)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1187366)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1187363)[0m 2024-04-18 10:58:40.972282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1187363)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1187363)[0m 2024-04-18 10:58:43.273937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1187356)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1187356)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 10:59:01,075 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 10:59:05,481 | server.py:125 | fit progress: (1, 2.0154242515563965, {'accuracy': 0.4457, 'data_size': 10000}, 22.060874374990817)
INFO flwr 2024-04-18 10:59:05,481 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 10:59:05,482 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:59:16,059 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 10:59:20,162 | server.py:125 | fit progress: (2, 2.186842918395996, {'accuracy': 0.2743, 'data_size': 10000}, 36.74205463498947)
INFO flwr 2024-04-18 10:59:20,162 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 10:59:20,163 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:59:29,670 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 10:59:33,985 | server.py:125 | fit progress: (3, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 50.56483522799681)
INFO flwr 2024-04-18 10:59:33,985 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 10:59:33,985 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:59:43,241 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 10:59:47,539 | server.py:125 | fit progress: (4, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 64.1191722569929)
INFO flwr 2024-04-18 10:59:47,540 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 10:59:47,540 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 10:59:56,959 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 11:00:01,109 | server.py:125 | fit progress: (5, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 77.6892873949837)
INFO flwr 2024-04-18 11:00:01,110 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 11:00:01,110 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:00:11,995 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 11:00:16,323 | server.py:125 | fit progress: (6, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 92.9032628099958)
INFO flwr 2024-04-18 11:00:16,324 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 11:00:16,324 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:00:26,196 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 11:00:30,323 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 106.90310189998127)
INFO flwr 2024-04-18 11:00:30,323 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 11:00:30,324 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:00:40,227 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 11:00:44,312 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 120.89239855099004)
INFO flwr 2024-04-18 11:00:44,313 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 11:00:44,313 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:00:54,021 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 11:00:58,424 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 135.00378330698004)
INFO flwr 2024-04-18 11:00:58,424 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 11:00:58,424 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:01:08,333 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 11:01:12,475 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 149.0544695419958)
INFO flwr 2024-04-18 11:01:12,475 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 11:01:12,475 | server.py:153 | FL finished in 149.05495693997364
INFO flwr 2024-04-18 11:01:12,478 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 11:01:12,478 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 11:01:12,478 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 11:01:12,478 | app.py:229 | app_fit: losses_centralized [(0, 2.3024840354919434), (1, 2.0154242515563965), (2, 2.186842918395996), (3, 2.358342170715332), (4, 2.358342170715332), (5, 2.358342170715332), (6, 2.358342170715332), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-18 11:01:12,478 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0965), (1, 0.4457), (2, 0.2743), (3, 0.1028), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_105815-sbt83w0u
wandb: Find logs at: ./wandb/offline-run-20240418_105815-sbt83w0u/logs
INFO flwr 2024-04-18 11:01:16,059 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 11:08:41,761 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1187353)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1187353)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 11:08:46,582	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 11:08:47,713	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 11:08:48,190	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 11:08:48,350	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (38.83MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 11:08:48,709	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_35980ededff7316a.zip' (77.77MiB) to Ray cluster...
2024-04-18 11:08:48,980	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_35980ededff7316a.zip'.
INFO flwr 2024-04-18 11:09:00,238 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 75455796019.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 166063524045.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 11:09:00,239 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 11:09:00,239 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 11:09:00,259 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 11:09:00,261 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 11:09:00,261 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 11:09:00,261 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1191747)[0m 2024-04-18 11:09:06.312118: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1191747)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1191747)[0m 2024-04-18 11:09:08.740487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 11:09:09,256 | server.py:94 | initial parameters (loss, other metrics): 2.3025095462799072, {'accuracy': 0.0929, 'data_size': 10000}
INFO flwr 2024-04-18 11:09:09,256 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 11:09:09,257 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1191751)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1191751)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1191749)[0m 2024-04-18 11:09:06.726814: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1191749)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1191749)[0m 2024-04-18 11:09:08.947979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1191744)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1191744)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 11:09:26,982 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 11:09:31,356 | server.py:125 | fit progress: (1, 1.811749815940857, {'accuracy': 0.6803, 'data_size': 10000}, 22.09952832601266)
INFO flwr 2024-04-18 11:09:31,357 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 11:09:31,357 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:09:41,992 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 11:09:46,143 | server.py:125 | fit progress: (2, 1.5432597398757935, {'accuracy': 0.9221, 'data_size': 10000}, 36.88671252000495)
INFO flwr 2024-04-18 11:09:46,144 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 11:09:46,144 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:09:55,676 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 11:09:59,769 | server.py:125 | fit progress: (3, 1.5106098651885986, {'accuracy': 0.9503, 'data_size': 10000}, 50.51273568099714)
INFO flwr 2024-04-18 11:09:59,770 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 11:09:59,770 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:10:09,470 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 11:10:13,544 | server.py:125 | fit progress: (4, 1.5021265745162964, {'accuracy': 0.9589, 'data_size': 10000}, 64.28770771299605)
INFO flwr 2024-04-18 11:10:13,545 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 11:10:13,545 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:10:23,633 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 11:10:27,957 | server.py:125 | fit progress: (5, 1.4909969568252563, {'accuracy': 0.9699, 'data_size': 10000}, 78.70070343499538)
INFO flwr 2024-04-18 11:10:27,958 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 11:10:27,958 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:10:38,332 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 11:10:42,414 | server.py:125 | fit progress: (6, 1.4878326654434204, {'accuracy': 0.9732, 'data_size': 10000}, 93.1572715670045)
INFO flwr 2024-04-18 11:10:42,414 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 11:10:42,414 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:10:51,790 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 11:10:56,146 | server.py:125 | fit progress: (7, 1.4852545261383057, {'accuracy': 0.9758, 'data_size': 10000}, 106.88913126001717)
INFO flwr 2024-04-18 11:10:56,146 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 11:10:56,146 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:11:05,539 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 11:11:09,678 | server.py:125 | fit progress: (8, 1.4862390756607056, {'accuracy': 0.9748, 'data_size': 10000}, 120.42123573599383)
INFO flwr 2024-04-18 11:11:09,678 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 11:11:09,678 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:11:19,659 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 11:11:23,970 | server.py:125 | fit progress: (9, 1.483039140701294, {'accuracy': 0.9781, 'data_size': 10000}, 134.71355874300934)
INFO flwr 2024-04-18 11:11:23,971 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 11:11:23,971 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:11:33,801 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 11:11:38,116 | server.py:125 | fit progress: (10, 1.4816339015960693, {'accuracy': 0.9796, 'data_size': 10000}, 148.85906000301475)
INFO flwr 2024-04-18 11:11:38,116 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 11:11:38,116 | server.py:153 | FL finished in 148.8598379759933
INFO flwr 2024-04-18 11:11:38,127 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 11:11:38,127 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 11:11:38,127 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 11:11:38,127 | app.py:229 | app_fit: losses_centralized [(0, 2.3025095462799072), (1, 1.811749815940857), (2, 1.5432597398757935), (3, 1.5106098651885986), (4, 1.5021265745162964), (5, 1.4909969568252563), (6, 1.4878326654434204), (7, 1.4852545261383057), (8, 1.4862390756607056), (9, 1.483039140701294), (10, 1.4816339015960693)]
INFO flwr 2024-04-18 11:11:38,127 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0929), (1, 0.6803), (2, 0.9221), (3, 0.9503), (4, 0.9589), (5, 0.9699), (6, 0.9732), (7, 0.9758), (8, 0.9748), (9, 0.9781), (10, 0.9796)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9796
wandb:     loss 1.48163
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_110841-30hdbc52
wandb: Find logs at: ./wandb/offline-run-20240418_110841-30hdbc52/logs
INFO flwr 2024-04-18 11:11:41,723 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 11:19:07,806 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 11:19:20,067	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 11:19:24,242	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 11:19:24,712	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 11:19:24,872	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (39.32MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 11:19:25,234	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b149601f5d4294ea.zip' (78.27MiB) to Ray cluster...
2024-04-18 11:19:25,521	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b149601f5d4294ea.zip'.
INFO flwr 2024-04-18 11:19:36,714 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 166106798695.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 75474342297.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 11:19:36,714 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 11:19:36,714 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 11:19:36,734 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 11:19:36,735 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 11:19:36,736 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 11:19:36,736 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 11:19:44,217 | server.py:94 | initial parameters (loss, other metrics): 2.3026280403137207, {'accuracy': 0.095, 'data_size': 10000}
INFO flwr 2024-04-18 11:19:44,217 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 11:19:44,217 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1196474)[0m 2024-04-18 11:19:46.689680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1196474)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1196477)[0m 2024-04-18 11:19:52.188698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1196473)[0m 2024-04-18 11:19:46.792546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1196473)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1196477)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1196477)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1196473)[0m 2024-04-18 11:19:52.182009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 11:20:24,247 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 11:20:28,647 | server.py:125 | fit progress: (1, 2.291929006576538, {'accuracy': 0.518, 'data_size': 10000}, 44.429307104001055)
INFO flwr 2024-04-18 11:20:28,647 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 11:20:28,647 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:20:39,057 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 11:20:43,176 | server.py:125 | fit progress: (2, 2.26005482673645, {'accuracy': 0.7642, 'data_size': 10000}, 58.95838984500733)
INFO flwr 2024-04-18 11:20:43,176 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 11:20:43,176 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:20:53,188 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 11:20:57,316 | server.py:125 | fit progress: (3, 2.1827354431152344, {'accuracy': 0.8509, 'data_size': 10000}, 73.0984806890192)
INFO flwr 2024-04-18 11:20:57,316 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 11:20:57,316 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:21:07,045 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 11:21:11,106 | server.py:125 | fit progress: (4, 2.0305559635162354, {'accuracy': 0.8974, 'data_size': 10000}, 86.88850456100772)
INFO flwr 2024-04-18 11:21:11,106 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 11:21:11,106 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:21:20,728 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 11:21:25,093 | server.py:125 | fit progress: (5, 1.8341480493545532, {'accuracy': 0.9187, 'data_size': 10000}, 100.87515437300317)
INFO flwr 2024-04-18 11:21:25,093 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 11:21:25,093 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:21:35,021 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 11:21:39,089 | server.py:125 | fit progress: (6, 1.679018497467041, {'accuracy': 0.9301, 'data_size': 10000}, 114.87192414401216)
INFO flwr 2024-04-18 11:21:39,090 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 11:21:39,090 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:21:49,126 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 11:21:53,470 | server.py:125 | fit progress: (7, 1.5924556255340576, {'accuracy': 0.9378, 'data_size': 10000}, 129.25304506902467)
INFO flwr 2024-04-18 11:21:53,471 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 11:21:53,471 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:22:03,072 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 11:22:07,361 | server.py:125 | fit progress: (8, 1.5490748882293701, {'accuracy': 0.9456, 'data_size': 10000}, 143.14384962900658)
INFO flwr 2024-04-18 11:22:07,362 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 11:22:07,362 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:22:17,012 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 11:22:21,103 | server.py:125 | fit progress: (9, 1.5274934768676758, {'accuracy': 0.9503, 'data_size': 10000}, 156.8859495310171)
INFO flwr 2024-04-18 11:22:21,104 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 11:22:21,104 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:22:31,108 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 11:22:35,471 | server.py:125 | fit progress: (10, 1.5169721841812134, {'accuracy': 0.9513, 'data_size': 10000}, 171.25364130802336)
INFO flwr 2024-04-18 11:22:35,472 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 11:22:35,472 | server.py:153 | FL finished in 171.25431718101026
INFO flwr 2024-04-18 11:22:35,472 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 11:22:35,472 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 11:22:35,472 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 11:22:35,472 | app.py:229 | app_fit: losses_centralized [(0, 2.3026280403137207), (1, 2.291929006576538), (2, 2.26005482673645), (3, 2.1827354431152344), (4, 2.0305559635162354), (5, 1.8341480493545532), (6, 1.679018497467041), (7, 1.5924556255340576), (8, 1.5490748882293701), (9, 1.5274934768676758), (10, 1.5169721841812134)]
INFO flwr 2024-04-18 11:22:35,473 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.095), (1, 0.518), (2, 0.7642), (3, 0.8509), (4, 0.8974), (5, 0.9187), (6, 0.9301), (7, 0.9378), (8, 0.9456), (9, 0.9503), (10, 0.9513)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9513
wandb:     loss 1.51697
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_111907-xwqypfoj
wandb: Find logs at: ./wandb/offline-run-20240418_111907-xwqypfoj/logs
INFO flwr 2024-04-18 11:22:39,115 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 11:30:04,294 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1196473)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1196473)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 11:30:09,873	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 11:30:11,408	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 11:30:11,878	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 11:30:12,027	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (39.80MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 11:30:12,388	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3879858eaab30ad1.zip' (78.76MiB) to Ray cluster...
2024-04-18 11:30:12,643	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3879858eaab30ad1.zip'.
INFO flwr 2024-04-18 11:30:23,784 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75442012569.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 166031362663.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 11:30:23,784 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 11:30:23,784 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 11:30:23,805 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 11:30:23,808 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 11:30:23,808 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 11:30:23,808 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1200881)[0m 2024-04-18 11:30:30.302338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1200881)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-18 11:30:32,081 | server.py:94 | initial parameters (loss, other metrics): 2.303145408630371, {'accuracy': 0.0575, 'data_size': 10000}
INFO flwr 2024-04-18 11:30:32,081 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 11:30:32,081 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1200881)[0m 2024-04-18 11:30:33.665905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1200886)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1200886)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1200883)[0m 2024-04-18 11:30:30.400418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1200883)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1200884)[0m 2024-04-18 11:30:33.665653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 11:30:54,949 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 11:30:59,441 | server.py:125 | fit progress: (1, 2.3022263050079346, {'accuracy': 0.1125, 'data_size': 10000}, 27.35942572497879)
INFO flwr 2024-04-18 11:30:59,441 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 11:30:59,441 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:31:10,000 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 11:31:14,104 | server.py:125 | fit progress: (2, 2.3007826805114746, {'accuracy': 0.193, 'data_size': 10000}, 42.02279822298442)
INFO flwr 2024-04-18 11:31:14,104 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 11:31:14,105 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:31:23,551 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 11:31:27,601 | server.py:125 | fit progress: (3, 2.298732280731201, {'accuracy': 0.3403, 'data_size': 10000}, 55.52003873497597)
INFO flwr 2024-04-18 11:31:27,602 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 11:31:27,602 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:31:37,106 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 11:31:41,462 | server.py:125 | fit progress: (4, 2.2959885597229004, {'accuracy': 0.4749, 'data_size': 10000}, 69.38092526397668)
INFO flwr 2024-04-18 11:31:41,463 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 11:31:41,463 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:31:51,086 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 11:31:55,414 | server.py:125 | fit progress: (5, 2.2925150394439697, {'accuracy': 0.5144, 'data_size': 10000}, 83.33261164400028)
INFO flwr 2024-04-18 11:31:55,414 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 11:31:55,414 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:32:04,771 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 11:32:08,917 | server.py:125 | fit progress: (6, 2.288234233856201, {'accuracy': 0.5549, 'data_size': 10000}, 96.83621665797546)
INFO flwr 2024-04-18 11:32:08,918 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 11:32:08,918 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:32:18,445 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 11:32:22,734 | server.py:125 | fit progress: (7, 2.283142328262329, {'accuracy': 0.6077, 'data_size': 10000}, 110.6530751909886)
INFO flwr 2024-04-18 11:32:22,735 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 11:32:22,735 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:32:32,829 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 11:32:36,936 | server.py:125 | fit progress: (8, 2.277200937271118, {'accuracy': 0.6556, 'data_size': 10000}, 124.85507447097916)
INFO flwr 2024-04-18 11:32:36,937 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 11:32:36,937 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:32:46,591 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 11:32:50,930 | server.py:125 | fit progress: (9, 2.27036452293396, {'accuracy': 0.7003, 'data_size': 10000}, 138.84889088297496)
INFO flwr 2024-04-18 11:32:50,931 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 11:32:50,931 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:33:00,686 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 11:33:04,765 | server.py:125 | fit progress: (10, 2.262646198272705, {'accuracy': 0.7342, 'data_size': 10000}, 152.68348541099112)
INFO flwr 2024-04-18 11:33:04,765 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 11:33:04,765 | server.py:153 | FL finished in 152.68404698299128
INFO flwr 2024-04-18 11:33:04,773 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 11:33:04,773 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 11:33:04,774 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 11:33:04,774 | app.py:229 | app_fit: losses_centralized [(0, 2.303145408630371), (1, 2.3022263050079346), (2, 2.3007826805114746), (3, 2.298732280731201), (4, 2.2959885597229004), (5, 2.2925150394439697), (6, 2.288234233856201), (7, 2.283142328262329), (8, 2.277200937271118), (9, 2.27036452293396), (10, 2.262646198272705)]
INFO flwr 2024-04-18 11:33:04,774 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0575), (1, 0.1125), (2, 0.193), (3, 0.3403), (4, 0.4749), (5, 0.5144), (6, 0.5549), (7, 0.6077), (8, 0.6556), (9, 0.7003), (10, 0.7342)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7342
wandb:     loss 2.26265
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_113003-7cp4omsc
wandb: Find logs at: ./wandb/offline-run-20240418_113003-7cp4omsc/logs
INFO flwr 2024-04-18 11:33:08,694 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 11:40:38,858 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1200878)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1200878)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 11:40:52,651	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 11:41:05,395	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 11:41:05,887	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 11:41:06,056	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (40.29MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 11:41:06,429	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_02f9bd62fdea13bc.zip' (79.26MiB) to Ray cluster...
2024-04-18 11:41:06,695	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_02f9bd62fdea13bc.zip'.
INFO flwr 2024-04-18 11:41:18,033 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'memory': 166096539853.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 75469945651.0}
INFO flwr 2024-04-18 11:41:18,034 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 11:41:18,034 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 11:41:18,055 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 11:41:18,056 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 11:41:18,056 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 11:41:18,057 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1205275)[0m 2024-04-18 11:41:25.318589: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1205275)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-18 11:41:26,222 | server.py:94 | initial parameters (loss, other metrics): 2.30243182182312, {'accuracy': 0.0774, 'data_size': 10000}
INFO flwr 2024-04-18 11:41:26,223 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 11:41:26,223 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1205275)[0m 2024-04-18 11:41:29.052670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1205275)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1205275)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1205270)[0m 2024-04-18 11:41:25.464984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1205270)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1205269)[0m 2024-04-18 11:41:29.054162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 11:41:57,089 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 11:42:01,546 | server.py:125 | fit progress: (1, 2.3023362159729004, {'accuracy': 0.086, 'data_size': 10000}, 35.322981380013516)
INFO flwr 2024-04-18 11:42:01,546 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 11:42:01,547 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:42:12,193 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 11:42:16,275 | server.py:125 | fit progress: (2, 2.302210569381714, {'accuracy': 0.0937, 'data_size': 10000}, 50.05239980400074)
INFO flwr 2024-04-18 11:42:16,276 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 11:42:16,276 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:42:26,299 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 11:42:30,431 | server.py:125 | fit progress: (3, 2.302060604095459, {'accuracy': 0.1043, 'data_size': 10000}, 64.20820696701412)
INFO flwr 2024-04-18 11:42:30,431 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 11:42:30,432 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:42:39,812 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 11:42:43,873 | server.py:125 | fit progress: (4, 2.301893711090088, {'accuracy': 0.1159, 'data_size': 10000}, 77.65024027100299)
INFO flwr 2024-04-18 11:42:43,873 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 11:42:43,874 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:42:53,861 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 11:42:58,210 | server.py:125 | fit progress: (5, 2.3017098903656006, {'accuracy': 0.1287, 'data_size': 10000}, 91.98726150701987)
INFO flwr 2024-04-18 11:42:58,211 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 11:42:58,211 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:43:08,027 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 11:43:12,097 | server.py:125 | fit progress: (6, 2.3015177249908447, {'accuracy': 0.1409, 'data_size': 10000}, 105.87366933800513)
INFO flwr 2024-04-18 11:43:12,097 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 11:43:12,097 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:43:21,629 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 11:43:26,005 | server.py:125 | fit progress: (7, 2.3013110160827637, {'accuracy': 0.1546, 'data_size': 10000}, 119.78198883100413)
INFO flwr 2024-04-18 11:43:26,005 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 11:43:26,005 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:43:35,857 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 11:43:39,954 | server.py:125 | fit progress: (8, 2.3010919094085693, {'accuracy': 0.1717, 'data_size': 10000}, 133.73059991700575)
INFO flwr 2024-04-18 11:43:39,954 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 11:43:39,954 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:43:49,589 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 11:43:53,946 | server.py:125 | fit progress: (9, 2.300863027572632, {'accuracy': 0.1908, 'data_size': 10000}, 147.723035980016)
INFO flwr 2024-04-18 11:43:53,946 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 11:43:53,947 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:44:03,864 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 11:44:08,204 | server.py:125 | fit progress: (10, 2.3006255626678467, {'accuracy': 0.2097, 'data_size': 10000}, 161.98079833801603)
INFO flwr 2024-04-18 11:44:08,204 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 11:44:08,205 | server.py:153 | FL finished in 161.98160415500752
INFO flwr 2024-04-18 11:44:08,209 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 11:44:08,209 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 11:44:08,210 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 11:44:08,210 | app.py:229 | app_fit: losses_centralized [(0, 2.30243182182312), (1, 2.3023362159729004), (2, 2.302210569381714), (3, 2.302060604095459), (4, 2.301893711090088), (5, 2.3017098903656006), (6, 2.3015177249908447), (7, 2.3013110160827637), (8, 2.3010919094085693), (9, 2.300863027572632), (10, 2.3006255626678467)]
INFO flwr 2024-04-18 11:44:08,210 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0774), (1, 0.086), (2, 0.0937), (3, 0.1043), (4, 0.1159), (5, 0.1287), (6, 0.1409), (7, 0.1546), (8, 0.1717), (9, 0.1908), (10, 0.2097)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2097
wandb:     loss 2.30063
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_114034-9iiow497
wandb: Find logs at: ./wandb/offline-run-20240418_114034-9iiow497/logs
INFO flwr 2024-04-18 11:44:11,774 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 11:51:37,736 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1205269)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1205269)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 11:51:45,900	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 11:52:01,238	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 11:52:01,695	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 11:52:01,846	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (40.77MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 11:52:02,214	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b356a9038b5beacf.zip' (79.75MiB) to Ray cluster...
2024-04-18 11:52:02,469	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b356a9038b5beacf.zip'.
INFO flwr 2024-04-18 11:52:23,184 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 166117025997.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 75478725427.0}
INFO flwr 2024-04-18 11:52:23,184 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 11:52:23,185 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 11:52:23,204 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 11:52:23,206 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 11:52:23,206 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 11:52:23,206 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1210015)[0m 2024-04-18 11:52:30.009375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1210015)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-18 11:52:31,189 | server.py:94 | initial parameters (loss, other metrics): 2.302694797515869, {'accuracy': 0.1001, 'data_size': 10000}
INFO flwr 2024-04-18 11:52:31,189 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 11:52:31,190 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1210015)[0m 2024-04-18 11:52:33.211492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1210015)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1210015)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1209998)[0m 2024-04-18 11:52:30.177345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1209998)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1210011)[0m 2024-04-18 11:52:33.209714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 11:52:53,610 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 11:52:57,951 | server.py:125 | fit progress: (1, 2.0259907245635986, {'accuracy': 0.4351, 'data_size': 10000}, 26.761431159015046)
INFO flwr 2024-04-18 11:52:57,951 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 11:52:57,952 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:53:08,450 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 11:53:12,630 | server.py:125 | fit progress: (2, 2.1979522705078125, {'accuracy': 0.2632, 'data_size': 10000}, 41.4405713280139)
INFO flwr 2024-04-18 11:53:12,631 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 11:53:12,631 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:53:22,342 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 11:53:26,396 | server.py:125 | fit progress: (3, 2.1830437183380127, {'accuracy': 0.2781, 'data_size': 10000}, 55.20644173299661)
INFO flwr 2024-04-18 11:53:26,397 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 11:53:26,397 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:53:36,275 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 11:53:40,605 | server.py:125 | fit progress: (4, 2.184243679046631, {'accuracy': 0.2769, 'data_size': 10000}, 69.41497179900762)
INFO flwr 2024-04-18 11:53:40,605 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 11:53:40,605 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:53:49,940 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 11:53:54,288 | server.py:125 | fit progress: (5, 2.1928436756134033, {'accuracy': 0.2683, 'data_size': 10000}, 83.09822701700614)
INFO flwr 2024-04-18 11:53:54,288 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 11:53:54,288 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:54:03,753 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 11:54:07,874 | server.py:125 | fit progress: (6, 2.2019436359405518, {'accuracy': 0.2592, 'data_size': 10000}, 96.68444039201131)
INFO flwr 2024-04-18 11:54:07,874 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 11:54:07,875 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:54:18,289 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 11:54:22,559 | server.py:125 | fit progress: (7, 2.2117435932159424, {'accuracy': 0.2494, 'data_size': 10000}, 111.36986421002075)
INFO flwr 2024-04-18 11:54:22,560 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 11:54:22,560 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:54:32,257 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 11:54:36,339 | server.py:125 | fit progress: (8, 2.2219436168670654, {'accuracy': 0.2392, 'data_size': 10000}, 125.14901692100102)
INFO flwr 2024-04-18 11:54:36,339 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 11:54:36,339 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:54:46,071 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 11:54:50,372 | server.py:125 | fit progress: (9, 2.2311434745788574, {'accuracy': 0.23, 'data_size': 10000}, 139.18251443302142)
INFO flwr 2024-04-18 11:54:50,373 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 11:54:50,373 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 11:55:00,332 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 11:55:04,431 | server.py:125 | fit progress: (10, 2.240943670272827, {'accuracy': 0.2202, 'data_size': 10000}, 153.24123212500126)
INFO flwr 2024-04-18 11:55:04,431 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 11:55:04,431 | server.py:153 | FL finished in 153.24171224201564
INFO flwr 2024-04-18 11:55:04,434 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 11:55:04,434 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 11:55:04,434 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 11:55:04,434 | app.py:229 | app_fit: losses_centralized [(0, 2.302694797515869), (1, 2.0259907245635986), (2, 2.1979522705078125), (3, 2.1830437183380127), (4, 2.184243679046631), (5, 2.1928436756134033), (6, 2.2019436359405518), (7, 2.2117435932159424), (8, 2.2219436168670654), (9, 2.2311434745788574), (10, 2.240943670272827)]
INFO flwr 2024-04-18 11:55:04,434 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1001), (1, 0.4351), (2, 0.2632), (3, 0.2781), (4, 0.2769), (5, 0.2683), (6, 0.2592), (7, 0.2494), (8, 0.2392), (9, 0.23), (10, 0.2202)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2202
wandb:     loss 2.24094
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_115136-asdmaylu
wandb: Find logs at: ./wandb/offline-run-20240418_115136-asdmaylu/logs
INFO flwr 2024-04-18 11:55:08,039 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 12:02:32,775 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1209998)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1209998)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 12:02:37,458	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 12:02:38,489	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 12:02:38,955	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 12:02:39,105	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (41.25MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 12:02:39,469	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1292fbdba225a124.zip' (80.25MiB) to Ray cluster...
2024-04-18 12:02:39,729	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1292fbdba225a124.zip'.
INFO flwr 2024-04-18 12:02:50,902 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75463665254.0, 'CPU': 64.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 166081885594.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-18 12:02:50,902 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 12:02:50,903 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 12:02:50,930 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 12:02:50,932 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 12:02:50,932 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 12:02:50,932 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1214393)[0m 2024-04-18 12:02:56.924980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1214393)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1214393)[0m 2024-04-18 12:02:59.224795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 12:03:00,258 | server.py:94 | initial parameters (loss, other metrics): 2.3026509284973145, {'accuracy': 0.1136, 'data_size': 10000}
INFO flwr 2024-04-18 12:03:00,258 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 12:03:00,259 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1214398)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1214398)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1214398)[0m 2024-04-18 12:02:57.241624: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1214398)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1214398)[0m 2024-04-18 12:02:59.486613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1214393)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1214393)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 12:03:17,541 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 12:03:21,976 | server.py:125 | fit progress: (1, 2.024665117263794, {'accuracy': 0.4384, 'data_size': 10000}, 21.717467479989864)
INFO flwr 2024-04-18 12:03:21,977 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 12:03:21,977 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:03:32,631 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 12:03:36,780 | server.py:125 | fit progress: (2, 1.6532487869262695, {'accuracy': 0.8081, 'data_size': 10000}, 36.52122683697962)
INFO flwr 2024-04-18 12:03:36,780 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 12:03:36,781 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:03:47,094 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 12:03:51,469 | server.py:125 | fit progress: (3, 1.6013126373291016, {'accuracy': 0.8592, 'data_size': 10000}, 51.20995083198068)
INFO flwr 2024-04-18 12:03:51,469 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 12:03:51,469 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:04:00,935 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 12:04:05,323 | server.py:125 | fit progress: (4, 1.5970606803894043, {'accuracy': 0.864, 'data_size': 10000}, 65.0643851890054)
INFO flwr 2024-04-18 12:04:05,324 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 12:04:05,324 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:04:15,144 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 12:04:19,264 | server.py:125 | fit progress: (5, 1.5870342254638672, {'accuracy': 0.8728, 'data_size': 10000}, 79.00537076898036)
INFO flwr 2024-04-18 12:04:19,265 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 12:04:19,265 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:04:28,539 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 12:04:32,936 | server.py:125 | fit progress: (6, 1.5821362733840942, {'accuracy': 0.8761, 'data_size': 10000}, 92.67743857699679)
INFO flwr 2024-04-18 12:04:32,937 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 12:04:32,937 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:04:43,149 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 12:04:47,282 | server.py:125 | fit progress: (7, 1.5457783937454224, {'accuracy': 0.9153, 'data_size': 10000}, 107.02329239700339)
INFO flwr 2024-04-18 12:04:47,282 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 12:04:47,283 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:04:57,110 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 12:05:01,196 | server.py:125 | fit progress: (8, 1.5036851167678833, {'accuracy': 0.9577, 'data_size': 10000}, 120.93669940999825)
INFO flwr 2024-04-18 12:05:01,196 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 12:05:01,196 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:05:11,247 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 12:05:15,574 | server.py:125 | fit progress: (9, 1.4942748546600342, {'accuracy': 0.9669, 'data_size': 10000}, 135.31504136600415)
INFO flwr 2024-04-18 12:05:15,574 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 12:05:15,575 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:05:24,622 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 12:05:28,738 | server.py:125 | fit progress: (10, 1.501315712928772, {'accuracy': 0.9597, 'data_size': 10000}, 148.47937734398874)
INFO flwr 2024-04-18 12:05:28,739 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 12:05:28,739 | server.py:153 | FL finished in 148.47994069298147
INFO flwr 2024-04-18 12:05:28,746 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 12:05:28,747 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 12:05:28,747 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 12:05:28,747 | app.py:229 | app_fit: losses_centralized [(0, 2.3026509284973145), (1, 2.024665117263794), (2, 1.6532487869262695), (3, 1.6013126373291016), (4, 1.5970606803894043), (5, 1.5870342254638672), (6, 1.5821362733840942), (7, 1.5457783937454224), (8, 1.5036851167678833), (9, 1.4942748546600342), (10, 1.501315712928772)]
INFO flwr 2024-04-18 12:05:28,747 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1136), (1, 0.4384), (2, 0.8081), (3, 0.8592), (4, 0.864), (5, 0.8728), (6, 0.8761), (7, 0.9153), (8, 0.9577), (9, 0.9669), (10, 0.9597)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9597
wandb:     loss 1.50132
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_120232-hyoudd8z
wandb: Find logs at: ./wandb/offline-run-20240418_120232-hyoudd8z/logs
INFO flwr 2024-04-18 12:05:32,366 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 12:12:58,355 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1214391)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1214391)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 12:13:03,244	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 12:13:04,257	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 12:13:04,763	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 12:13:04,921	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (41.74MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 12:13:05,296	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_639d0d8ac0dc4aaa.zip' (80.75MiB) to Ray cluster...
2024-04-18 12:13:05,566	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_639d0d8ac0dc4aaa.zip'.
INFO flwr 2024-04-18 12:13:16,667 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'memory': 166084893287.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 75464954265.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 12:13:16,667 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 12:13:16,667 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 12:13:16,687 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 12:13:16,688 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 12:13:16,688 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 12:13:16,689 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1219107)[0m 2024-04-18 12:13:22.626473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1219107)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1219107)[0m 2024-04-18 12:13:24.922773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 12:13:26,149 | server.py:94 | initial parameters (loss, other metrics): 2.3026530742645264, {'accuracy': 0.1123, 'data_size': 10000}
INFO flwr 2024-04-18 12:13:26,150 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 12:13:26,150 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1219121)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1219121)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1219120)[0m 2024-04-18 12:13:22.985396: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1219120)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1219120)[0m 2024-04-18 12:13:25.335898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1219107)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1219107)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 12:13:43,452 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 12:13:47,848 | server.py:125 | fit progress: (1, 2.2891845703125, {'accuracy': 0.5119, 'data_size': 10000}, 21.698007883009268)
INFO flwr 2024-04-18 12:13:47,849 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 12:13:47,849 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:13:58,433 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 12:14:02,516 | server.py:125 | fit progress: (2, 2.2528254985809326, {'accuracy': 0.6756, 'data_size': 10000}, 36.366118845005985)
INFO flwr 2024-04-18 12:14:02,517 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 12:14:02,517 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:14:11,984 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 12:14:16,097 | server.py:125 | fit progress: (3, 2.1772499084472656, {'accuracy': 0.7432, 'data_size': 10000}, 49.9469423959963)
INFO flwr 2024-04-18 12:14:16,097 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 12:14:16,097 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:14:25,941 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 12:14:30,265 | server.py:125 | fit progress: (4, 2.051445484161377, {'accuracy': 0.7724, 'data_size': 10000}, 64.1146748859901)
INFO flwr 2024-04-18 12:14:30,265 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 12:14:30,265 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:14:39,452 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 12:14:43,769 | server.py:125 | fit progress: (5, 1.8868705034255981, {'accuracy': 0.8272, 'data_size': 10000}, 77.61881635399186)
INFO flwr 2024-04-18 12:14:43,769 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 12:14:43,769 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:14:52,748 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 12:14:56,823 | server.py:125 | fit progress: (6, 1.7350863218307495, {'accuracy': 0.8822, 'data_size': 10000}, 90.67287261600723)
INFO flwr 2024-04-18 12:14:56,824 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 12:14:56,824 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:15:06,394 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 12:15:10,738 | server.py:125 | fit progress: (7, 1.6281108856201172, {'accuracy': 0.9214, 'data_size': 10000}, 104.58807088300819)
INFO flwr 2024-04-18 12:15:10,738 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 12:15:10,739 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:15:20,438 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 12:15:24,540 | server.py:125 | fit progress: (8, 1.5652066469192505, {'accuracy': 0.9397, 'data_size': 10000}, 118.39027381598135)
INFO flwr 2024-04-18 12:15:24,541 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 12:15:24,541 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:15:34,356 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 12:15:38,487 | server.py:125 | fit progress: (9, 1.5305441617965698, {'accuracy': 0.9526, 'data_size': 10000}, 132.33675767100067)
INFO flwr 2024-04-18 12:15:38,487 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 12:15:38,487 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:15:47,877 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 12:15:52,193 | server.py:125 | fit progress: (10, 1.514062523841858, {'accuracy': 0.9575, 'data_size': 10000}, 146.0432466439961)
INFO flwr 2024-04-18 12:15:52,194 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 12:15:52,194 | server.py:153 | FL finished in 146.043741528003
INFO flwr 2024-04-18 12:15:52,199 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 12:15:52,199 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 12:15:52,199 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 12:15:52,199 | app.py:229 | app_fit: losses_centralized [(0, 2.3026530742645264), (1, 2.2891845703125), (2, 2.2528254985809326), (3, 2.1772499084472656), (4, 2.051445484161377), (5, 1.8868705034255981), (6, 1.7350863218307495), (7, 1.6281108856201172), (8, 1.5652066469192505), (9, 1.5305441617965698), (10, 1.514062523841858)]
INFO flwr 2024-04-18 12:15:52,199 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1123), (1, 0.5119), (2, 0.6756), (3, 0.7432), (4, 0.7724), (5, 0.8272), (6, 0.8822), (7, 0.9214), (8, 0.9397), (9, 0.9526), (10, 0.9575)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9575
wandb:     loss 1.51406
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_121257-u6yncw8p
wandb: Find logs at: ./wandb/offline-run-20240418_121257-u6yncw8p/logs
INFO flwr 2024-04-18 12:15:55,773 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 12:23:22,194 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1219105)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1219105)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 12:23:27,035	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 12:23:28,151	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 12:23:28,625	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 12:23:28,785	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (42.22MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 12:23:29,161	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cf909e7448074008.zip' (81.24MiB) to Ray cluster...
2024-04-18 12:23:29,433	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cf909e7448074008.zip'.
INFO flwr 2024-04-18 12:23:40,552 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 75465795993.0, 'CPU': 64.0, 'memory': 166086857319.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-18 12:23:40,552 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 12:23:40,552 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 12:23:40,571 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 12:23:40,572 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 12:23:40,572 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 12:23:40,572 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1223505)[0m 2024-04-18 12:23:46.588302: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1223505)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1223505)[0m 2024-04-18 12:23:48.857838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 12:23:50,068 | server.py:94 | initial parameters (loss, other metrics): 2.30277156829834, {'accuracy': 0.0985, 'data_size': 10000}
INFO flwr 2024-04-18 12:23:50,068 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 12:23:50,069 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1223510)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1223510)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1223503)[0m 2024-04-18 12:23:47.007540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1223503)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1223503)[0m 2024-04-18 12:23:49.331928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1223506)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1223506)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-18 12:24:07,559 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 12:24:11,959 | server.py:125 | fit progress: (1, 2.30184268951416, {'accuracy': 0.1749, 'data_size': 10000}, 21.89000911999028)
INFO flwr 2024-04-18 12:24:11,959 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 12:24:11,959 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:24:22,232 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 12:24:26,306 | server.py:125 | fit progress: (2, 2.300398588180542, {'accuracy': 0.2224, 'data_size': 10000}, 36.23774295300245)
INFO flwr 2024-04-18 12:24:26,307 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 12:24:26,307 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:24:35,665 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 12:24:39,724 | server.py:125 | fit progress: (3, 2.2984085083007812, {'accuracy': 0.2664, 'data_size': 10000}, 49.655772752012126)
INFO flwr 2024-04-18 12:24:39,725 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 12:24:39,725 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:24:49,676 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 12:24:53,772 | server.py:125 | fit progress: (4, 2.2958216667175293, {'accuracy': 0.3992, 'data_size': 10000}, 63.70334834200912)
INFO flwr 2024-04-18 12:24:53,772 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 12:24:53,773 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:25:03,606 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 12:25:07,977 | server.py:125 | fit progress: (5, 2.292527198791504, {'accuracy': 0.5323, 'data_size': 10000}, 77.90843862801557)
INFO flwr 2024-04-18 12:25:07,977 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 12:25:07,978 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:25:17,687 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 12:25:21,758 | server.py:125 | fit progress: (6, 2.2885046005249023, {'accuracy': 0.6395, 'data_size': 10000}, 91.68967868300388)
INFO flwr 2024-04-18 12:25:21,759 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 12:25:21,759 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:25:31,634 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 12:25:35,977 | server.py:125 | fit progress: (7, 2.28369402885437, {'accuracy': 0.7041, 'data_size': 10000}, 105.90818458198919)
INFO flwr 2024-04-18 12:25:35,977 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 12:25:35,977 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:25:45,564 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 12:25:49,658 | server.py:125 | fit progress: (8, 2.2780401706695557, {'accuracy': 0.7473, 'data_size': 10000}, 119.58908624001197)
INFO flwr 2024-04-18 12:25:49,658 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 12:25:49,659 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:25:59,768 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 12:26:04,149 | server.py:125 | fit progress: (9, 2.271576166152954, {'accuracy': 0.7735, 'data_size': 10000}, 134.08018117101165)
INFO flwr 2024-04-18 12:26:04,149 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 12:26:04,149 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:26:13,681 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 12:26:17,965 | server.py:125 | fit progress: (10, 2.26428484916687, {'accuracy': 0.7924, 'data_size': 10000}, 147.89645860699238)
INFO flwr 2024-04-18 12:26:17,965 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 12:26:17,966 | server.py:153 | FL finished in 147.89691740900162
INFO flwr 2024-04-18 12:26:17,968 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 12:26:17,968 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 12:26:17,969 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 12:26:17,969 | app.py:229 | app_fit: losses_centralized [(0, 2.30277156829834), (1, 2.30184268951416), (2, 2.300398588180542), (3, 2.2984085083007812), (4, 2.2958216667175293), (5, 2.292527198791504), (6, 2.2885046005249023), (7, 2.28369402885437), (8, 2.2780401706695557), (9, 2.271576166152954), (10, 2.26428484916687)]
INFO flwr 2024-04-18 12:26:17,969 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0985), (1, 0.1749), (2, 0.2224), (3, 0.2664), (4, 0.3992), (5, 0.5323), (6, 0.6395), (7, 0.7041), (8, 0.7473), (9, 0.7735), (10, 0.7924)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7924
wandb:     loss 2.26428
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_122321-xhxfpigh
wandb: Find logs at: ./wandb/offline-run-20240418_122321-xhxfpigh/logs
INFO flwr 2024-04-18 12:26:21,543 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 12:33:48,011 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1223497)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1223497)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-18 12:33:58,606	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 12:34:00,342	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 12:34:00,837	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 12:34:01,014	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (42.71MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 12:34:01,407	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8fc4a2af4c5a35dd.zip' (81.74MiB) to Ray cluster...
2024-04-18 12:34:01,690	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8fc4a2af4c5a35dd.zip'.
INFO flwr 2024-04-18 12:34:13,846 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 75390906777.0, 'accelerator_type:TITAN': 1.0, 'memory': 165912115815.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 12:34:13,846 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 12:34:13,846 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 12:34:13,869 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 12:34:13,870 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 12:34:13,870 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 12:34:13,870 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 12:34:21,504 | server.py:94 | initial parameters (loss, other metrics): 2.3025646209716797, {'accuracy': 0.0842, 'data_size': 10000}
INFO flwr 2024-04-18 12:34:21,505 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 12:34:21,505 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1228475)[0m 2024-04-18 12:34:22.818907: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1228475)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1228475)[0m 2024-04-18 12:34:35.340106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1228468)[0m 2024-04-18 12:34:22.773991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1228468)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1228475)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1228475)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1228468)[0m 2024-04-18 12:34:35.338114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 12:35:22,687 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 12:35:27,025 | server.py:125 | fit progress: (1, 2.302515745162964, {'accuracy': 0.089, 'data_size': 10000}, 65.52025058300933)
INFO flwr 2024-04-18 12:35:27,026 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 12:35:27,026 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:35:37,552 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 12:35:41,618 | server.py:125 | fit progress: (2, 2.3024489879608154, {'accuracy': 0.0952, 'data_size': 10000}, 80.1131279580004)
INFO flwr 2024-04-18 12:35:41,618 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 12:35:41,619 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:35:51,506 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 12:35:55,619 | server.py:125 | fit progress: (3, 2.3023664951324463, {'accuracy': 0.1027, 'data_size': 10000}, 94.11400013999082)
INFO flwr 2024-04-18 12:35:55,619 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 12:35:55,619 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:36:05,106 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 12:36:09,242 | server.py:125 | fit progress: (4, 2.3022711277008057, {'accuracy': 0.1122, 'data_size': 10000}, 107.73754524299875)
INFO flwr 2024-04-18 12:36:09,243 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 12:36:09,243 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:36:19,093 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 12:36:23,422 | server.py:125 | fit progress: (5, 2.30216908454895, {'accuracy': 0.1226, 'data_size': 10000}, 121.91695091698784)
INFO flwr 2024-04-18 12:36:23,422 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 12:36:23,422 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:36:33,372 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 12:36:37,448 | server.py:125 | fit progress: (6, 2.302060604095459, {'accuracy': 0.1345, 'data_size': 10000}, 135.9427370140038)
INFO flwr 2024-04-18 12:36:37,448 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 12:36:37,448 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:36:47,036 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 12:36:51,333 | server.py:125 | fit progress: (7, 2.301945209503174, {'accuracy': 0.1484, 'data_size': 10000}, 149.82826419099)
INFO flwr 2024-04-18 12:36:51,334 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 12:36:51,334 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:37:01,101 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 12:37:05,192 | server.py:125 | fit progress: (8, 2.301823139190674, {'accuracy': 0.1644, 'data_size': 10000}, 163.68724289198872)
INFO flwr 2024-04-18 12:37:05,192 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 12:37:05,193 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:37:14,530 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 12:37:18,807 | server.py:125 | fit progress: (9, 2.301696300506592, {'accuracy': 0.1834, 'data_size': 10000}, 177.3016818589822)
INFO flwr 2024-04-18 12:37:18,807 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 12:37:18,807 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:37:28,168 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 12:37:32,470 | server.py:125 | fit progress: (10, 2.3015623092651367, {'accuracy': 0.1975, 'data_size': 10000}, 190.96510785300052)
INFO flwr 2024-04-18 12:37:32,470 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 12:37:32,471 | server.py:153 | FL finished in 190.9656029889884
INFO flwr 2024-04-18 12:37:32,479 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 12:37:32,479 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 12:37:32,479 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 12:37:32,479 | app.py:229 | app_fit: losses_centralized [(0, 2.3025646209716797), (1, 2.302515745162964), (2, 2.3024489879608154), (3, 2.3023664951324463), (4, 2.3022711277008057), (5, 2.30216908454895), (6, 2.302060604095459), (7, 2.301945209503174), (8, 2.301823139190674), (9, 2.301696300506592), (10, 2.3015623092651367)]
INFO flwr 2024-04-18 12:37:32,479 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0842), (1, 0.089), (2, 0.0952), (3, 0.1027), (4, 0.1122), (5, 0.1226), (6, 0.1345), (7, 0.1484), (8, 0.1644), (9, 0.1834), (10, 0.1975)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1975
wandb:     loss 2.30156
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_123347-kvepw801
wandb: Find logs at: ./wandb/offline-run-20240418_123347-kvepw801/logs
INFO flwr 2024-04-18 12:37:36,048 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 12:45:02,815 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1228468)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1228468)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 12:45:09,271	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 12:45:21,561	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 12:45:22,050	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 12:45:22,211	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (43.19MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 12:45:22,750	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7e71375e435a716a.zip' (82.24MiB) to Ray cluster...
2024-04-18 12:45:23,026	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7e71375e435a716a.zip'.
INFO flwr 2024-04-18 12:45:34,312 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 75423365529.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 165987852903.0}
INFO flwr 2024-04-18 12:45:34,313 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 12:45:34,313 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 12:45:34,330 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 12:45:34,331 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 12:45:34,331 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 12:45:34,332 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1233237)[0m 2024-04-18 12:45:40.839634: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1233237)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-18 12:45:42,783 | server.py:94 | initial parameters (loss, other metrics): 2.302441120147705, {'accuracy': 0.1121, 'data_size': 10000}
INFO flwr 2024-04-18 12:45:42,784 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 12:45:42,784 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1233237)[0m 2024-04-18 12:45:43.287631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1233237)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1233237)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1233238)[0m 2024-04-18 12:45:41.036455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1233238)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1233232)[0m 2024-04-18 12:45:43.285515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 12:46:01,758 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 12:46:06,189 | server.py:125 | fit progress: (1, 1.776858925819397, {'accuracy': 0.6844, 'data_size': 10000}, 23.40519789402606)
INFO flwr 2024-04-18 12:46:06,190 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 12:46:06,190 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:46:16,501 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 12:46:20,611 | server.py:125 | fit progress: (2, 2.1469428539276123, {'accuracy': 0.3142, 'data_size': 10000}, 37.827269622008316)
INFO flwr 2024-04-18 12:46:20,612 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 12:46:20,612 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:46:30,903 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 12:46:35,237 | server.py:125 | fit progress: (3, 2.239640712738037, {'accuracy': 0.2215, 'data_size': 10000}, 52.452606268023374)
INFO flwr 2024-04-18 12:46:35,237 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 12:46:35,237 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:46:44,588 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 12:46:48,892 | server.py:125 | fit progress: (4, 2.310142993927002, {'accuracy': 0.151, 'data_size': 10000}, 66.10756985502667)
INFO flwr 2024-04-18 12:46:48,892 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 12:46:48,893 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:46:58,459 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 12:47:02,579 | server.py:125 | fit progress: (5, 2.350642442703247, {'accuracy': 0.1105, 'data_size': 10000}, 79.79455652702018)
INFO flwr 2024-04-18 12:47:02,579 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 12:47:02,579 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:47:12,077 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 12:47:16,428 | server.py:125 | fit progress: (6, 2.3341426849365234, {'accuracy': 0.127, 'data_size': 10000}, 93.64379011301207)
INFO flwr 2024-04-18 12:47:16,428 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 12:47:16,428 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:47:26,650 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 12:47:30,824 | server.py:125 | fit progress: (7, 2.321342706680298, {'accuracy': 0.1398, 'data_size': 10000}, 108.0397264350031)
INFO flwr 2024-04-18 12:47:30,824 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 12:47:30,824 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:47:40,365 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 12:47:44,497 | server.py:125 | fit progress: (8, 2.309453010559082, {'accuracy': 0.1517, 'data_size': 10000}, 121.71276130902697)
INFO flwr 2024-04-18 12:47:44,497 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 12:47:44,497 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:47:54,678 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 12:47:59,009 | server.py:125 | fit progress: (9, 2.3007428646087646, {'accuracy': 0.1604, 'data_size': 10000}, 136.22485958301695)
INFO flwr 2024-04-18 12:47:59,009 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 12:47:59,010 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:48:08,759 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 12:48:12,839 | server.py:125 | fit progress: (10, 2.293743133544922, {'accuracy': 0.1674, 'data_size': 10000}, 150.05495422001695)
INFO flwr 2024-04-18 12:48:12,839 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 12:48:12,840 | server.py:153 | FL finished in 150.05562113202177
INFO flwr 2024-04-18 12:48:12,843 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 12:48:12,844 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 12:48:12,844 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 12:48:12,844 | app.py:229 | app_fit: losses_centralized [(0, 2.302441120147705), (1, 1.776858925819397), (2, 2.1469428539276123), (3, 2.239640712738037), (4, 2.310142993927002), (5, 2.350642442703247), (6, 2.3341426849365234), (7, 2.321342706680298), (8, 2.309453010559082), (9, 2.3007428646087646), (10, 2.293743133544922)]
INFO flwr 2024-04-18 12:48:12,844 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1121), (1, 0.6844), (2, 0.3142), (3, 0.2215), (4, 0.151), (5, 0.1105), (6, 0.127), (7, 0.1398), (8, 0.1517), (9, 0.1604), (10, 0.1674)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1674
wandb:     loss 2.29374
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_124500-zcgzlsxj
wandb: Find logs at: ./wandb/offline-run-20240418_124500-zcgzlsxj/logs
INFO flwr 2024-04-18 12:48:16,409 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 12:55:42,743 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1233229)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1233229)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 12:55:49,174	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 12:55:51,516	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 12:55:51,981	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 12:55:52,141	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (43.67MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 12:55:52,522	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_673dbaf8bdf42d7b.zip' (82.74MiB) to Ray cluster...
2024-04-18 12:55:52,785	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_673dbaf8bdf42d7b.zip'.
INFO flwr 2024-04-18 12:56:05,719 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75501931315.0, 'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 166171173069.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-18 12:56:05,719 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 12:56:05,719 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 12:56:05,737 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 12:56:05,740 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 12:56:05,741 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 12:56:05,741 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1237595)[0m 2024-04-18 12:56:13.042257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1237595)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-18 12:56:13,688 | server.py:94 | initial parameters (loss, other metrics): 2.30255389213562, {'accuracy': 0.1328, 'data_size': 10000}
INFO flwr 2024-04-18 12:56:13,689 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 12:56:13,689 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1237593)[0m 2024-04-18 12:56:17.703729: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1237593)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1237593)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1237603)[0m 2024-04-18 12:56:13.196729: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1237603)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1237603)[0m 2024-04-18 12:56:17.700098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 12:56:42,143 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 12:56:46,532 | server.py:125 | fit progress: (1, 1.8152371644973755, {'accuracy': 0.8186, 'data_size': 10000}, 32.84314212301979)
INFO flwr 2024-04-18 12:56:46,533 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 12:56:46,533 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:56:57,060 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 12:57:01,167 | server.py:125 | fit progress: (2, 1.5248831510543823, {'accuracy': 0.9408, 'data_size': 10000}, 47.478021649003495)
INFO flwr 2024-04-18 12:57:01,168 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 12:57:01,168 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:57:10,789 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 12:57:14,909 | server.py:125 | fit progress: (3, 1.5012339353561401, {'accuracy': 0.96, 'data_size': 10000}, 61.219524906016886)
INFO flwr 2024-04-18 12:57:14,909 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 12:57:14,909 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:57:24,864 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 12:57:29,156 | server.py:125 | fit progress: (4, 1.4943703413009644, {'accuracy': 0.9672, 'data_size': 10000}, 75.46630375602399)
INFO flwr 2024-04-18 12:57:29,156 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 12:57:29,156 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:57:38,556 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 12:57:42,858 | server.py:125 | fit progress: (5, 1.4899531602859497, {'accuracy': 0.9712, 'data_size': 10000}, 89.16874958801782)
INFO flwr 2024-04-18 12:57:42,858 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 12:57:42,859 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:57:53,123 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 12:57:57,240 | server.py:125 | fit progress: (6, 1.4950753450393677, {'accuracy': 0.9658, 'data_size': 10000}, 103.55084806101513)
INFO flwr 2024-04-18 12:57:57,241 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 12:57:57,241 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:58:07,106 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 12:58:11,465 | server.py:125 | fit progress: (7, 1.4882959127426147, {'accuracy': 0.9728, 'data_size': 10000}, 117.77556119501241)
INFO flwr 2024-04-18 12:58:11,465 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 12:58:11,465 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:58:21,064 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 12:58:25,163 | server.py:125 | fit progress: (8, 1.4864962100982666, {'accuracy': 0.9745, 'data_size': 10000}, 131.47370712301927)
INFO flwr 2024-04-18 12:58:25,163 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 12:58:25,164 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:58:34,523 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 12:58:38,825 | server.py:125 | fit progress: (9, 1.4865219593048096, {'accuracy': 0.9745, 'data_size': 10000}, 145.13605236401781)
INFO flwr 2024-04-18 12:58:38,826 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 12:58:38,826 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 12:58:48,655 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 12:58:52,721 | server.py:125 | fit progress: (10, 1.4880139827728271, {'accuracy': 0.973, 'data_size': 10000}, 159.03157375502633)
INFO flwr 2024-04-18 12:58:52,721 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 12:58:52,721 | server.py:153 | FL finished in 159.03205173602328
INFO flwr 2024-04-18 12:58:52,722 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 12:58:52,722 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 12:58:52,722 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 12:58:52,722 | app.py:229 | app_fit: losses_centralized [(0, 2.30255389213562), (1, 1.8152371644973755), (2, 1.5248831510543823), (3, 1.5012339353561401), (4, 1.4943703413009644), (5, 1.4899531602859497), (6, 1.4950753450393677), (7, 1.4882959127426147), (8, 1.4864962100982666), (9, 1.4865219593048096), (10, 1.4880139827728271)]
INFO flwr 2024-04-18 12:58:52,722 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1328), (1, 0.8186), (2, 0.9408), (3, 0.96), (4, 0.9672), (5, 0.9712), (6, 0.9658), (7, 0.9728), (8, 0.9745), (9, 0.9745), (10, 0.973)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.973
wandb:     loss 1.48801
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_125541-vvyt0vap
wandb: Find logs at: ./wandb/offline-run-20240418_125541-vvyt0vap/logs
INFO flwr 2024-04-18 12:58:56,286 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 13:06:21,967 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1237603)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1237603)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 13:06:26,758	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 13:06:40,741	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 13:06:41,233	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 13:06:41,404	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (44.16MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 13:06:41,794	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_037226c65f6b1945.zip' (83.24MiB) to Ray cluster...
2024-04-18 13:06:42,071	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_037226c65f6b1945.zip'.
INFO flwr 2024-04-18 13:06:53,250 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'memory': 165949931316.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'object_store_memory': 75407113420.0}
INFO flwr 2024-04-18 13:06:53,251 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 13:06:53,251 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 13:06:53,268 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 13:06:53,270 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 13:06:53,270 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 13:06:53,270 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1241976)[0m 2024-04-18 13:06:59.430053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1241976)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1241976)[0m 2024-04-18 13:07:01.741441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 13:07:02,058 | server.py:94 | initial parameters (loss, other metrics): 2.3025243282318115, {'accuracy': 0.1038, 'data_size': 10000}
INFO flwr 2024-04-18 13:07:02,059 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 13:07:02,059 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1241988)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1241988)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1241982)[0m 2024-04-18 13:06:59.640456: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1241982)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1241982)[0m 2024-04-18 13:07:01.907825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1241979)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1241979)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-18 13:07:19,862 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 13:07:24,270 | server.py:125 | fit progress: (1, 2.2929270267486572, {'accuracy': 0.5124, 'data_size': 10000}, 22.210514914011583)
INFO flwr 2024-04-18 13:07:24,270 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 13:07:24,270 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:07:34,893 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 13:07:38,962 | server.py:125 | fit progress: (2, 2.267662286758423, {'accuracy': 0.7268, 'data_size': 10000}, 36.902566214004764)
INFO flwr 2024-04-18 13:07:38,962 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 13:07:38,962 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:07:48,205 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 13:07:52,268 | server.py:125 | fit progress: (3, 2.217458963394165, {'accuracy': 0.8572, 'data_size': 10000}, 50.20884829800343)
INFO flwr 2024-04-18 13:07:52,269 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 13:07:52,269 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:08:01,853 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 13:08:06,226 | server.py:125 | fit progress: (4, 2.1226346492767334, {'accuracy': 0.9064, 'data_size': 10000}, 64.16670770398923)
INFO flwr 2024-04-18 13:08:06,226 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 13:08:06,226 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:08:15,336 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 13:08:19,618 | server.py:125 | fit progress: (5, 1.9699244499206543, {'accuracy': 0.926, 'data_size': 10000}, 77.55867972999113)
INFO flwr 2024-04-18 13:08:19,618 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 13:08:19,618 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:08:29,132 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 13:08:33,177 | server.py:125 | fit progress: (6, 1.7932875156402588, {'accuracy': 0.939, 'data_size': 10000}, 91.1178287040093)
INFO flwr 2024-04-18 13:08:33,177 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 13:08:33,178 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:08:42,670 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 13:08:46,955 | server.py:125 | fit progress: (7, 1.6524208784103394, {'accuracy': 0.9468, 'data_size': 10000}, 104.8959584539989)
INFO flwr 2024-04-18 13:08:46,956 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 13:08:46,956 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:08:56,479 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 13:09:00,602 | server.py:125 | fit progress: (8, 1.5719627141952515, {'accuracy': 0.9519, 'data_size': 10000}, 118.54296049900586)
INFO flwr 2024-04-18 13:09:00,602 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 13:09:00,603 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:09:09,728 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 13:09:14,070 | server.py:125 | fit progress: (9, 1.5332796573638916, {'accuracy': 0.9553, 'data_size': 10000}, 132.01082282999414)
INFO flwr 2024-04-18 13:09:14,070 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 13:09:14,071 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:09:24,292 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 13:09:28,376 | server.py:125 | fit progress: (10, 1.5160237550735474, {'accuracy': 0.9587, 'data_size': 10000}, 146.31649498001207)
INFO flwr 2024-04-18 13:09:28,376 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 13:09:28,376 | server.py:153 | FL finished in 146.3170530780044
INFO flwr 2024-04-18 13:09:28,390 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 13:09:28,390 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 13:09:28,390 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 13:09:28,390 | app.py:229 | app_fit: losses_centralized [(0, 2.3025243282318115), (1, 2.2929270267486572), (2, 2.267662286758423), (3, 2.217458963394165), (4, 2.1226346492767334), (5, 1.9699244499206543), (6, 1.7932875156402588), (7, 1.6524208784103394), (8, 1.5719627141952515), (9, 1.5332796573638916), (10, 1.5160237550735474)]
INFO flwr 2024-04-18 13:09:28,390 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1038), (1, 0.5124), (2, 0.7268), (3, 0.8572), (4, 0.9064), (5, 0.926), (6, 0.939), (7, 0.9468), (8, 0.9519), (9, 0.9553), (10, 0.9587)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9587
wandb:     loss 1.51602
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_130621-dcg3on50
wandb: Find logs at: ./wandb/offline-run-20240418_130621-dcg3on50/logs
INFO flwr 2024-04-18 13:09:31,775 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 13:16:57,530 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1241976)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1241976)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-18 13:17:05,022	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 13:17:16,973	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 13:17:17,476	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 13:17:17,639	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (44.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 13:17:18,018	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_073c8b10229bc903.zip' (83.73MiB) to Ray cluster...
2024-04-18 13:17:18,305	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_073c8b10229bc903.zip'.
INFO flwr 2024-04-18 13:17:29,552 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 75507860275.0, 'accelerator_type:TITAN': 1.0, 'memory': 166185007309.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-18 13:17:29,552 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 13:17:29,552 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 13:17:29,572 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 13:17:29,574 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 13:17:29,575 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 13:17:29,575 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-18 13:17:37,049 | server.py:94 | initial parameters (loss, other metrics): 2.302327871322632, {'accuracy': 0.095, 'data_size': 10000}
INFO flwr 2024-04-18 13:17:37,050 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 13:17:37,050 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1246709)[0m 2024-04-18 13:17:37.891368: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1246709)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1246709)[0m 2024-04-18 13:17:42.247505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1246709)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1246709)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1246708)[0m 2024-04-18 13:17:38.011744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1246708)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1246713)[0m 2024-04-18 13:17:42.248597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-18 13:18:07,693 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 13:18:12,105 | server.py:125 | fit progress: (1, 2.30159068107605, {'accuracy': 0.2206, 'data_size': 10000}, 35.05460080198827)
INFO flwr 2024-04-18 13:18:12,105 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 13:18:12,105 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:18:22,727 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 13:18:26,909 | server.py:125 | fit progress: (2, 2.300452470779419, {'accuracy': 0.3625, 'data_size': 10000}, 49.85924021998653)
INFO flwr 2024-04-18 13:18:26,910 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 13:18:26,910 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:18:36,524 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 13:18:40,633 | server.py:125 | fit progress: (3, 2.298922300338745, {'accuracy': 0.4576, 'data_size': 10000}, 63.58324518799782)
INFO flwr 2024-04-18 13:18:40,634 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 13:18:40,634 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:18:50,340 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 13:18:54,654 | server.py:125 | fit progress: (4, 2.297006607055664, {'accuracy': 0.4955, 'data_size': 10000}, 77.6035755869816)
INFO flwr 2024-04-18 13:18:54,654 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 13:18:54,655 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:19:04,120 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 13:19:08,628 | server.py:125 | fit progress: (5, 2.294635772705078, {'accuracy': 0.5896, 'data_size': 10000}, 91.57751163499779)
INFO flwr 2024-04-18 13:19:08,628 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 13:19:08,628 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:19:18,464 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 13:19:22,573 | server.py:125 | fit progress: (6, 2.2917633056640625, {'accuracy': 0.6793, 'data_size': 10000}, 105.52343061298598)
INFO flwr 2024-04-18 13:19:22,574 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 13:19:22,574 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:19:31,880 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 13:19:36,205 | server.py:125 | fit progress: (7, 2.2883479595184326, {'accuracy': 0.7262, 'data_size': 10000}, 119.15527936798753)
INFO flwr 2024-04-18 13:19:36,206 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 13:19:36,206 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:19:45,732 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 13:19:49,896 | server.py:125 | fit progress: (8, 2.284388303756714, {'accuracy': 0.764, 'data_size': 10000}, 132.84577970398823)
INFO flwr 2024-04-18 13:19:49,896 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 13:19:49,897 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:19:59,388 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 13:20:03,737 | server.py:125 | fit progress: (9, 2.2798209190368652, {'accuracy': 0.7956, 'data_size': 10000}, 146.68685557798017)
INFO flwr 2024-04-18 13:20:03,737 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 13:20:03,737 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:20:13,526 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 13:20:17,621 | server.py:125 | fit progress: (10, 2.2746188640594482, {'accuracy': 0.814, 'data_size': 10000}, 160.57110701399506)
INFO flwr 2024-04-18 13:20:17,621 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 13:20:17,622 | server.py:153 | FL finished in 160.5716418819793
INFO flwr 2024-04-18 13:20:17,624 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 13:20:17,624 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 13:20:17,625 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 13:20:17,625 | app.py:229 | app_fit: losses_centralized [(0, 2.302327871322632), (1, 2.30159068107605), (2, 2.300452470779419), (3, 2.298922300338745), (4, 2.297006607055664), (5, 2.294635772705078), (6, 2.2917633056640625), (7, 2.2883479595184326), (8, 2.284388303756714), (9, 2.2798209190368652), (10, 2.2746188640594482)]
INFO flwr 2024-04-18 13:20:17,625 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.095), (1, 0.2206), (2, 0.3625), (3, 0.4576), (4, 0.4955), (5, 0.5896), (6, 0.6793), (7, 0.7262), (8, 0.764), (9, 0.7956), (10, 0.814)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.814
wandb:     loss 2.27462
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_131657-uf0ygcjj
wandb: Find logs at: ./wandb/offline-run-20240418_131657-uf0ygcjj/logs
INFO flwr 2024-04-18 13:20:21,186 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 13:27:46,910 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1246713)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1246713)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-18 13:27:52,752	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 13:27:53,933	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 13:27:54,444	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 13:27:54,610	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.13MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 13:27:54,993	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8d6e591411e1985b.zip' (84.23MiB) to Ray cluster...
2024-04-18 13:27:55,275	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8d6e591411e1985b.zip'.
INFO flwr 2024-04-18 13:28:06,390 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 166105127117.0, 'object_store_memory': 75473625907.0}
INFO flwr 2024-04-18 13:28:06,391 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-18 13:28:06,391 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-18 13:28:06,412 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-18 13:28:06,414 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-18 13:28:06,415 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-18 13:28:06,415 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1251658)[0m 2024-04-18 13:28:12.378430: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1251658)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1251658)[0m 2024-04-18 13:28:14.730009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-18 13:28:15,528 | server.py:94 | initial parameters (loss, other metrics): 2.3026506900787354, {'accuracy': 0.0735, 'data_size': 10000}
INFO flwr 2024-04-18 13:28:15,529 | server.py:104 | FL starting
DEBUG flwr 2024-04-18 13:28:15,529 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1251660)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1251660)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1251657)[0m 2024-04-18 13:28:12.736618: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1251657)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1251657)[0m 2024-04-18 13:28:14.942793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1251656)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1251656)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-18 13:28:33,751 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-18 13:28:38,126 | server.py:125 | fit progress: (1, 2.3025777339935303, {'accuracy': 0.0738, 'data_size': 10000}, 22.596685899974545)
INFO flwr 2024-04-18 13:28:38,126 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-18 13:28:38,126 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:28:48,638 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-18 13:28:52,751 | server.py:125 | fit progress: (2, 2.302483081817627, {'accuracy': 0.0763, 'data_size': 10000}, 37.22211021799012)
INFO flwr 2024-04-18 13:28:52,751 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-18 13:28:52,752 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:29:02,223 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-18 13:29:06,352 | server.py:125 | fit progress: (3, 2.302372932434082, {'accuracy': 0.0821, 'data_size': 10000}, 50.82298018998699)
INFO flwr 2024-04-18 13:29:06,352 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-18 13:29:06,353 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:29:16,317 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-18 13:29:20,646 | server.py:125 | fit progress: (4, 2.302248954772949, {'accuracy': 0.09, 'data_size': 10000}, 65.11699712497648)
INFO flwr 2024-04-18 13:29:20,646 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-18 13:29:20,647 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:29:30,068 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-18 13:29:34,408 | server.py:125 | fit progress: (5, 2.302114725112915, {'accuracy': 0.0989, 'data_size': 10000}, 78.8793289239984)
INFO flwr 2024-04-18 13:29:34,409 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-18 13:29:34,409 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:29:44,116 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-18 13:29:48,230 | server.py:125 | fit progress: (6, 2.301971435546875, {'accuracy': 0.1091, 'data_size': 10000}, 92.701095625991)
INFO flwr 2024-04-18 13:29:48,230 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-18 13:29:48,231 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:29:57,930 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-18 13:30:02,208 | server.py:125 | fit progress: (7, 2.3018202781677246, {'accuracy': 0.1187, 'data_size': 10000}, 106.67909066498396)
INFO flwr 2024-04-18 13:30:02,208 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-18 13:30:02,209 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:30:12,031 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-18 13:30:16,119 | server.py:125 | fit progress: (8, 2.3016610145568848, {'accuracy': 0.128, 'data_size': 10000}, 120.59005140297813)
INFO flwr 2024-04-18 13:30:16,119 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-18 13:30:16,120 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:30:25,046 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-18 13:30:29,417 | server.py:125 | fit progress: (9, 2.3014957904815674, {'accuracy': 0.1379, 'data_size': 10000}, 133.88828836698667)
INFO flwr 2024-04-18 13:30:29,418 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-18 13:30:29,418 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-18 13:30:39,481 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-18 13:30:43,661 | server.py:125 | fit progress: (10, 2.301327705383301, {'accuracy': 0.1459, 'data_size': 10000}, 148.13172770297388)
INFO flwr 2024-04-18 13:30:43,661 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-18 13:30:43,661 | server.py:153 | FL finished in 148.13234964598087
INFO flwr 2024-04-18 13:30:43,666 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-18 13:30:43,667 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-18 13:30:43,667 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-18 13:30:43,667 | app.py:229 | app_fit: losses_centralized [(0, 2.3026506900787354), (1, 2.3025777339935303), (2, 2.302483081817627), (3, 2.302372932434082), (4, 2.302248954772949), (5, 2.302114725112915), (6, 2.301971435546875), (7, 2.3018202781677246), (8, 2.3016610145568848), (9, 2.3014957904815674), (10, 2.301327705383301)]
INFO flwr 2024-04-18 13:30:43,667 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0735), (1, 0.0738), (2, 0.0763), (3, 0.0821), (4, 0.09), (5, 0.0989), (6, 0.1091), (7, 0.1187), (8, 0.128), (9, 0.1379), (10, 0.1459)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1459
wandb:     loss 2.30133
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_132746-mh484gii
wandb: Find logs at: ./wandb/offline-run-20240418_132746-mh484gii/logs
INFO flwr 2024-04-18 13:30:47,282 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 13:38:13,254 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1251652)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1251652)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-18 13:38:18,257	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 13:39:11,188	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 13:39:11,211	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 13:39:12,326	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_0yS0R7 is very large (202.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_0yS0R7']})`
2024-04-18 13:39:13,427	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 13:39:13,635	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 13:39:13,795	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.61MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 13:39:14,100	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz is very large (126.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz']})`
2024-04-18 13:39:14,396	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz is very large (12690.51MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_133812-2ticjoxl
wandb: Find logs at: ./wandb/offline-run-20240418_133812-2ticjoxl/logs
INFO flwr 2024-04-18 13:39:55,434 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 13:47:19,807 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 13:47:24,410	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 13:47:54,595	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 13:47:54,626	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 13:47:55,673	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 13:47:57,577	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 13:47:57,775	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 13:47:57,940	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.62MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 13:47:58,221	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz is very large (126.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz']})`
2024-04-18 13:47:58,489	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz is very large (12690.51MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_134719-vpfq2i4b
wandb: Find logs at: ./wandb/offline-run-20240418_134719-vpfq2i4b/logs
INFO flwr 2024-04-18 13:48:39,646 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 13:56:03,835 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 13:56:08,447	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 13:56:37,470	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 13:56:37,492	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 13:56:38,517	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 13:56:40,389	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 13:56:40,583	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 13:56:40,744	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.62MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 13:56:41,021	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz is very large (126.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz']})`
2024-04-18 13:56:41,282	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz is very large (12690.51MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_135603-er2xp0dv
wandb: Find logs at: ./wandb/offline-run-20240418_135603-er2xp0dv/logs
INFO flwr 2024-04-18 13:57:20,671 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 14:04:45,143 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 14:04:52,538	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 14:05:28,568	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 14:05:28,595	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 14:05:29,593	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 14:05:31,467	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 14:05:31,668	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 14:05:31,833	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.62MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 14:05:32,113	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz is very large (126.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz']})`
2024-04-18 14:05:32,375	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz is very large (12690.51MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_140444-gj2zwhzn
wandb: Find logs at: ./wandb/offline-run-20240418_140444-gj2zwhzn/logs
INFO flwr 2024-04-18 14:06:13,135 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 14:13:37,107 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 14:13:41,729	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 14:14:10,780	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 14:14:10,803	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 14:14:11,816	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 14:14:13,651	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 14:14:13,844	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 14:14:14,001	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.63MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 14:14:14,281	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz is very large (126.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz']})`
2024-04-18 14:14:14,561	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz is very large (12690.51MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_141336-edct6n5q
wandb: Find logs at: ./wandb/offline-run-20240418_141336-edct6n5q/logs
INFO flwr 2024-04-18 14:14:54,537 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 14:22:18,758 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 14:22:23,656	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 14:22:52,714	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 14:22:52,737	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 14:22:53,737	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 14:22:55,587	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 14:22:55,789	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 14:22:55,954	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.63MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 14:22:56,233	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz is very large (126.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz']})`
2024-04-18 14:22:56,496	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz is very large (12690.51MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_142218-z5uf7vck
wandb: Find logs at: ./wandb/offline-run-20240418_142218-z5uf7vck/logs
INFO flwr 2024-04-18 14:23:37,353 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 14:31:01,195 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 14:31:07,339	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 14:31:36,386	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 14:31:36,408	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 14:31:37,435	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 14:31:39,294	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 14:31:39,489	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 14:31:39,654	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
2024-04-18 14:31:39,932	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz is very large (126.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/aggregates/parameters.npz']})`
2024-04-18 14:31:40,198	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz is very large (12690.51MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/CNN/FedAvg/2024-04-18_12-48-3D7/parameters.npz']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_143100-t76g3n14
wandb: Find logs at: ./wandb/offline-run-20240418_143100-t76g3n14/logs
INFO flwr 2024-04-18 14:32:20,346 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 14:39:44,438 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 14:39:50,009	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 14:39:53,728	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 14:39:53,748	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 14:39:54,767	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 14:39:56,618	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 14:39:56,815	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 14:39:56,979	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_143944-dks8y3xm
wandb: Find logs at: ./wandb/offline-run-20240418_143944-dks8y3xm/logs
INFO flwr 2024-04-18 14:40:01,703 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 14:47:25,640 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 14:47:30,098	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 14:47:33,884	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 14:47:33,903	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 14:47:34,916	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 14:47:36,762	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 14:47:36,956	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 14:47:37,119	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.65MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_144725-ovadftsa
wandb: Find logs at: ./wandb/offline-run-20240418_144725-ovadftsa/logs
INFO flwr 2024-04-18 14:47:41,824 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 14:55:05,989 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 14:55:10,409	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 14:55:14,051	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 14:55:14,072	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 14:55:15,070	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 14:55:16,934	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 14:55:17,128	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 14:55:17,290	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.65MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_145505-gytg237q
wandb: Find logs at: ./wandb/offline-run-20240418_145505-gytg237q/logs
INFO flwr 2024-04-18 14:55:22,079 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 15:02:46,233 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 15:02:51,920	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 15:02:55,575	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 15:02:55,596	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 15:02:56,633	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 15:02:58,567	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 15:02:58,771	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 15:02:58,937	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.65MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_150245-2rs68i1f
wandb: Find logs at: ./wandb/offline-run-20240418_150245-2rs68i1f/logs
INFO flwr 2024-04-18 15:03:03,813 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 15:10:28,335 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 15:10:32,811	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 15:10:36,536	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 15:10:36,557	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 15:10:37,616	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 15:10:39,579	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 15:10:39,784	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 15:10:39,949	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.66MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_151027-ifzbu1s5
wandb: Find logs at: ./wandb/offline-run-20240418_151027-ifzbu1s5/logs
INFO flwr 2024-04-18 15:10:44,922 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 15:18:19,167 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 15:18:29,076	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 15:19:01,550	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 15:19:01,576	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 15:19:02,623	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 15:19:04,579	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 15:19:04,788	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 15:19:04,950	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.66MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_151818-fiknkkhl
wandb: Find logs at: ./wandb/offline-run-20240418_151818-fiknkkhl/logs
INFO flwr 2024-04-18 15:19:09,712 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 15:26:33,928 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 15:26:38,645	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 15:26:42,736	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 15:26:42,757	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 15:26:43,770	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack is very large (694.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/pack-3f4c80f60be41f377318c93dc1220defaca0c029.pack']})`
2024-04-18 15:26:45,692	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 15:26:45,889	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 15:26:46,055	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.66MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_152633-2cysjewn
wandb: Find logs at: ./wandb/offline-run-20240418_152633-2cysjewn/logs
INFO flwr 2024-04-18 15:26:50,837 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 15:34:15,824 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 15:34:21,645	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 15:34:25,048	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 15:34:25,078	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 15:34:26,524	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 15:34:26,721	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 15:34:26,876	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.67MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_153415-pi4rpiru
wandb: Find logs at: ./wandb/offline-run-20240418_153415-pi4rpiru/logs
INFO flwr 2024-04-18 15:34:31,185 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 15:41:55,200 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 15:41:59,862	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 15:42:02,271	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 15:42:02,294	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 15:42:03,766	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 15:42:03,966	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 15:42:04,139	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.67MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_154154-7v33fxn6
wandb: Find logs at: ./wandb/offline-run-20240418_154154-7v33fxn6/logs
INFO flwr 2024-04-18 15:42:08,501 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 15:49:32,305 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 15:49:36,922	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 15:49:39,320	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 15:49:39,346	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 15:49:40,816	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 15:49:41,014	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 15:49:41,184	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.67MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_154931-09uw12ah
wandb: Find logs at: ./wandb/offline-run-20240418_154931-09uw12ah/logs
INFO flwr 2024-04-18 15:49:45,481 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 15:57:09,430 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 15:57:15,579	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 15:57:17,928	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 15:57:17,954	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 15:57:19,410	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 15:57:19,607	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 15:57:19,776	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.68MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_155708-ah8wupa4
wandb: Find logs at: ./wandb/offline-run-20240418_155708-ah8wupa4/logs
INFO flwr 2024-04-18 15:57:24,088 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 16:04:48,106 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 16:04:54,827	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 16:04:57,148	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 16:04:57,168	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 16:04:58,590	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 16:04:58,794	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 16:04:58,952	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.68MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_160447-sjjetg35
wandb: Find logs at: ./wandb/offline-run-20240418_160447-sjjetg35/logs
INFO flwr 2024-04-18 16:05:03,246 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 16:12:27,224 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 16:12:31,687	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 16:12:34,085	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 16:12:34,107	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 16:12:35,570	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 16:12:35,768	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 16:12:35,939	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.68MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_161226-hstnuxzc
wandb: Find logs at: ./wandb/offline-run-20240418_161226-hstnuxzc/logs
INFO flwr 2024-04-18 16:12:40,253 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 16:20:04,141 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 16:20:09,020	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 16:20:11,374	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 16:20:11,401	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 16:20:12,865	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 16:20:13,063	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 16:20:13,232	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.69MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_162003-cql0uivo
wandb: Find logs at: ./wandb/offline-run-20240418_162003-cql0uivo/logs
INFO flwr 2024-04-18 16:20:17,553 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 16:27:41,704 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 16:27:46,207	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 16:27:48,592	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 16:27:48,618	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 16:27:50,534	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 16:27:50,743	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 16:27:50,915	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.69MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_162741-9fw29p1c
wandb: Find logs at: ./wandb/offline-run-20240418_162741-9fw29p1c/logs
INFO flwr 2024-04-18 16:27:55,261 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 16:35:19,144 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 16:35:24,718	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 16:35:27,103	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 16:35:27,128	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 16:35:28,563	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 16:35:28,759	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 16:35:28,922	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.69MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_163518-33q9zxm0
wandb: Find logs at: ./wandb/offline-run-20240418_163518-33q9zxm0/logs
INFO flwr 2024-04-18 16:35:33,274 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 16:42:57,753 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 16:43:02,707	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 16:43:05,046	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 16:43:05,069	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 16:43:06,592	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 16:43:06,789	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 16:43:06,960	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.70MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_164257-045bcl9s
wandb: Find logs at: ./wandb/offline-run-20240418_164257-045bcl9s/logs
INFO flwr 2024-04-18 16:43:11,270 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 16:50:35,466 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 16:50:40,041	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 16:50:42,418	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 16:50:42,440	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 16:50:43,924	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 16:50:44,121	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 16:50:44,296	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.70MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_165035-5v9pm7l4
wandb: Find logs at: ./wandb/offline-run-20240418_165035-5v9pm7l4/logs
INFO flwr 2024-04-18 16:50:48,637 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 16:58:12,733 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 16:58:17,187	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 16:58:19,524	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 16:58:19,545	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 16:58:20,999	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 16:58:21,195	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 16:58:21,368	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.70MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_165812-svwfj3ah
wandb: Find logs at: ./wandb/offline-run-20240418_165812-svwfj3ah/logs
INFO flwr 2024-04-18 16:58:25,647 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 17:05:49,650 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 17:05:54,172	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 17:05:56,552	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 17:05:56,573	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 17:05:58,033	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 17:05:58,232	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 17:05:58,402	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.71MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_170549-zcxpp9zz
wandb: Find logs at: ./wandb/offline-run-20240418_170549-zcxpp9zz/logs
INFO flwr 2024-04-18 17:06:02,744 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 17:13:27,013 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 17:13:33,514	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 17:13:35,875	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 17:13:35,897	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 17:13:37,369	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 17:13:37,570	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 17:13:37,742	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.71MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_171326-g7qfzptf
wandb: Find logs at: ./wandb/offline-run-20240418_171326-g7qfzptf/logs
INFO flwr 2024-04-18 17:13:42,077 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 17:21:06,501 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 17:21:11,125	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 17:21:13,533	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 17:21:13,559	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 17:21:15,028	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 17:21:15,225	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 17:21:15,387	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.71MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_172106-5xv1m5jo
wandb: Find logs at: ./wandb/offline-run-20240418_172106-5xv1m5jo/logs
INFO flwr 2024-04-18 17:21:19,715 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 17:28:44,532 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 17:28:49,057	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 17:28:51,473	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 17:28:51,495	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 17:28:52,970	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 17:28:53,169	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 17:28:53,347	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.72MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_172844-pdccp0fe
wandb: Find logs at: ./wandb/offline-run-20240418_172844-pdccp0fe/logs
INFO flwr 2024-04-18 17:28:57,636 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 17:36:22,152 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 17:36:26,572	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 17:36:28,975	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 17:36:29,001	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 17:36:30,478	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 17:36:30,679	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 17:36:30,865	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.72MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_173621-c5to2zxr
wandb: Find logs at: ./wandb/offline-run-20240418_173621-c5to2zxr/logs
INFO flwr 2024-04-18 17:36:35,255 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 17:43:59,702 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 17:44:04,382	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 17:44:06,760	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 17:44:06,781	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 17:44:08,254	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 17:44:08,454	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 17:44:08,625	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.72MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_174359-gfhbhmtf
wandb: Find logs at: ./wandb/offline-run-20240418_174359-gfhbhmtf/logs
INFO flwr 2024-04-18 17:44:12,983 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 17:51:37,262 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 17:51:41,748	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 17:51:44,150	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 17:51:44,170	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 17:51:45,640	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 17:51:45,839	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 17:51:46,010	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.73MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_175136-ad9lhlw7
wandb: Find logs at: ./wandb/offline-run-20240418_175136-ad9lhlw7/logs
INFO flwr 2024-04-18 17:51:50,360 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 17:59:14,742 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 17:59:19,112	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 17:59:21,504	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 17:59:21,525	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 17:59:22,991	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 17:59:23,193	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 17:59:23,362	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.73MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_175914-fzf6ga46
wandb: Find logs at: ./wandb/offline-run-20240418_175914-fzf6ga46/logs
INFO flwr 2024-04-18 17:59:27,700 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 18:06:51,824 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 18:06:56,326	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 18:06:58,703	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 18:06:58,725	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 18:07:00,212	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 18:07:00,413	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 18:07:00,584	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.73MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_180651-jtxsuy89
wandb: Find logs at: ./wandb/offline-run-20240418_180651-jtxsuy89/logs
INFO flwr 2024-04-18 18:07:04,860 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 18:14:29,117 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 18:14:33,513	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 18:14:35,887	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 18:14:35,909	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 18:14:37,369	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 18:14:37,567	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 18:14:37,739	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.73MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_181428-7av7tzi9
wandb: Find logs at: ./wandb/offline-run-20240418_181428-7av7tzi9/logs
INFO flwr 2024-04-18 18:14:42,059 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 18:22:06,450 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 18:22:10,975	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 18:22:13,362	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 18:22:13,388	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 18:22:14,853	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 18:22:15,051	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 18:22:15,224	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.74MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_182206-q5z4c4ug
wandb: Find logs at: ./wandb/offline-run-20240418_182206-q5z4c4ug/logs
INFO flwr 2024-04-18 18:22:19,561 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 18:29:43,677 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 18:29:48,053	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 18:29:50,405	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 18:29:50,429	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 18:29:51,879	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 18:29:52,077	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 18:29:52,246	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.74MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_182943-hoi7yg76
wandb: Find logs at: ./wandb/offline-run-20240418_182943-hoi7yg76/logs
INFO flwr 2024-04-18 18:29:56,516 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 18:37:20,733 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 18:37:26,146	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 18:37:28,538	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 18:37:28,563	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 18:37:30,003	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 18:37:30,200	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 18:37:30,363	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.74MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_183720-3pqdmrqj
wandb: Find logs at: ./wandb/offline-run-20240418_183720-3pqdmrqj/logs
INFO flwr 2024-04-18 18:37:34,677 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 18:44:59,506 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 18:45:04,017	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 18:45:06,402	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 18:45:06,429	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 18:45:07,907	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 18:45:08,106	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 18:45:08,282	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.75MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_184459-vlw90ilu
wandb: Find logs at: ./wandb/offline-run-20240418_184459-vlw90ilu/logs
INFO flwr 2024-04-18 18:45:12,643 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 18:52:37,104 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 18:52:41,631	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 18:52:44,064	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 18:52:44,090	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 18:52:45,562	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 18:52:45,762	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 18:52:45,936	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.75MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_185236-g9v0yuu6
wandb: Find logs at: ./wandb/offline-run-20240418_185236-g9v0yuu6/logs
INFO flwr 2024-04-18 18:52:50,224 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 19:00:14,550 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 19:00:19,142	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 19:00:21,564	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 19:00:21,592	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 19:00:23,060	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 19:00:23,259	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 19:00:23,432	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.75MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_190014-eictgdmy
wandb: Find logs at: ./wandb/offline-run-20240418_190014-eictgdmy/logs
INFO flwr 2024-04-18 19:00:27,732 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 19:07:52,409 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 19:07:57,833	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 19:08:00,241	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 19:08:00,262	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 19:08:01,726	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 19:08:01,927	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 19:08:02,097	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.76MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_190752-09sb10za
wandb: Find logs at: ./wandb/offline-run-20240418_190752-09sb10za/logs
INFO flwr 2024-04-18 19:08:06,388 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 19:15:30,650 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 19:15:35,141	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 19:15:37,494	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 19:15:37,520	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 19:15:38,975	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 19:15:39,175	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 19:15:39,348	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.76MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_191530-veyru5jp
wandb: Find logs at: ./wandb/offline-run-20240418_191530-veyru5jp/logs
INFO flwr 2024-04-18 19:15:43,650 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 19:23:07,990 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 19:23:12,401	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 19:23:14,772	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 19:23:14,793	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 19:23:16,252	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 19:23:16,449	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 19:23:16,618	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.76MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_192307-dwuk7d8k
wandb: Find logs at: ./wandb/offline-run-20240418_192307-dwuk7d8k/logs
INFO flwr 2024-04-18 19:23:20,949 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 19:30:45,354 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 19:30:49,791	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 19:30:52,159	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 19:30:52,180	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 19:30:53,646	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 19:30:53,845	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 19:30:54,025	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.77MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_193044-y7nb59gz
wandb: Find logs at: ./wandb/offline-run-20240418_193044-y7nb59gz/logs
INFO flwr 2024-04-18 19:30:58,317 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 19:38:22,916 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 19:38:27,472	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 19:38:29,870	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 19:38:29,895	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 19:38:31,321	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 19:38:31,528	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 19:38:31,701	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.77MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_193822-b7c6grk9
wandb: Find logs at: ./wandb/offline-run-20240418_193822-b7c6grk9/logs
INFO flwr 2024-04-18 19:38:36,043 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 19:46:00,465 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 19:46:05,050	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 19:46:07,582	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 19:46:07,608	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 19:46:09,029	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 19:46:09,226	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 19:46:09,386	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.77MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_194600-2qjkq2a6
wandb: Find logs at: ./wandb/offline-run-20240418_194600-2qjkq2a6/logs
INFO flwr 2024-04-18 19:46:13,723 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 19:53:38,255 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 19:53:43,210	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 19:53:45,662	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 19:53:45,689	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 19:53:47,154	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 19:53:47,354	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 19:53:47,526	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.78MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_195337-ms84vlqv
wandb: Find logs at: ./wandb/offline-run-20240418_195337-ms84vlqv/logs
INFO flwr 2024-04-18 19:53:51,819 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 20:01:16,253 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 20:01:23,296	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 20:01:25,728	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 20:01:25,750	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 20:01:27,210	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 20:01:27,409	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 20:01:27,582	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.78MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_200115-r5f4ip3l
wandb: Find logs at: ./wandb/offline-run-20240418_200115-r5f4ip3l/logs
INFO flwr 2024-04-18 20:01:31,919 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 20:08:56,811 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 20:09:02,736	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 20:09:07,141	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 20:09:07,167	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 20:09:08,630	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 20:09:08,827	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 20:09:08,993	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.78MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_200855-qrtjbtw4
wandb: Find logs at: ./wandb/offline-run-20240418_200855-qrtjbtw4/logs
INFO flwr 2024-04-18 20:09:13,280 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 20:16:37,661 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 20:16:43,124	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 20:16:45,516	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 20:16:45,538	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 20:16:46,989	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 20:16:47,187	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 20:16:47,359	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.79MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_201637-cnlqmxal
wandb: Find logs at: ./wandb/offline-run-20240418_201637-cnlqmxal/logs
INFO flwr 2024-04-18 20:16:51,719 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 20:24:18,927 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 20:24:37,735	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 20:24:44,688	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 20:24:44,713	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 20:24:46,244	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 20:24:46,454	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 20:24:46,647	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.79MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_202417-cyvwwegw
wandb: Find logs at: ./wandb/offline-run-20240418_202417-cyvwwegw/logs
INFO flwr 2024-04-18 20:24:51,077 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 20:32:15,870 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 20:32:20,545	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 20:32:22,906	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 20:32:22,933	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 20:32:24,405	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 20:32:24,605	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 20:32:24,768	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.79MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_203215-u1f8kxn6
wandb: Find logs at: ./wandb/offline-run-20240418_203215-u1f8kxn6/logs
INFO flwr 2024-04-18 20:32:29,105 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 20:39:53,922 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 20:39:58,601	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 20:40:01,005	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 20:40:01,027	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 20:40:02,500	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 20:40:02,703	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 20:40:02,874	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.80MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_203953-pwvpi4co
wandb: Find logs at: ./wandb/offline-run-20240418_203953-pwvpi4co/logs
INFO flwr 2024-04-18 20:40:07,212 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 20:47:32,338 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 20:47:36,844	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 20:47:39,240	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 20:47:39,262	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 20:47:40,729	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 20:47:40,929	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 20:47:41,100	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.80MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_204731-aqugrlq4
wandb: Find logs at: ./wandb/offline-run-20240418_204731-aqugrlq4/logs
INFO flwr 2024-04-18 20:47:45,461 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 20:55:11,116 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 20:55:15,593	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 20:55:18,034	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 20:55:18,056	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 20:55:19,565	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 20:55:19,765	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 20:55:19,944	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.80MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_205510-bggpu3kk
wandb: Find logs at: ./wandb/offline-run-20240418_205510-bggpu3kk/logs
INFO flwr 2024-04-18 20:55:24,302 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 21:02:49,873 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 21:02:54,608	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 21:02:57,055	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 21:02:57,075	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 21:02:58,581	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 21:02:58,782	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 21:02:58,962	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.81MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_210249-v55w4lg3
wandb: Find logs at: ./wandb/offline-run-20240418_210249-v55w4lg3/logs
INFO flwr 2024-04-18 21:03:03,315 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 21:10:27,566 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 21:10:31,999	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 21:10:34,364	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 21:10:34,384	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 21:10:35,821	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 21:10:36,019	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 21:10:36,181	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.81MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_211027-bzrjyr9s
wandb: Find logs at: ./wandb/offline-run-20240418_211027-bzrjyr9s/logs
INFO flwr 2024-04-18 21:10:40,507 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 21:18:04,967 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 21:18:09,596	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 21:18:11,972	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 21:18:11,993	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 21:18:13,465	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 21:18:13,662	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 21:18:13,832	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.81MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_211804-kt7ibd4a
wandb: Find logs at: ./wandb/offline-run-20240418_211804-kt7ibd4a/logs
INFO flwr 2024-04-18 21:18:18,200 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 21:25:42,583 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 21:25:47,052	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 21:25:49,433	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 21:25:49,455	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 21:25:50,932	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 21:25:51,133	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 21:25:51,309	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.82MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_212542-74i26wxw
wandb: Find logs at: ./wandb/offline-run-20240418_212542-74i26wxw/logs
INFO flwr 2024-04-18 21:25:55,606 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 21:33:20,256 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 21:33:24,726	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 21:33:27,123	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 21:33:27,149	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 21:33:28,615	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 21:33:28,812	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 21:33:28,979	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.82MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_213319-1zpqw44t
wandb: Find logs at: ./wandb/offline-run-20240418_213319-1zpqw44t/logs
INFO flwr 2024-04-18 21:33:33,318 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 21:40:57,655 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 21:41:02,146	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 21:41:04,522	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 21:41:04,544	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 21:41:06,017	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 21:41:06,216	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 21:41:06,388	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.82MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_214057-q1vyy5v3
wandb: Find logs at: ./wandb/offline-run-20240418_214057-q1vyy5v3/logs
INFO flwr 2024-04-18 21:41:10,692 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 21:48:35,301 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 21:48:40,761	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 21:48:43,136	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 21:48:43,157	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 21:48:44,618	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 21:48:44,815	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 21:48:44,974	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.83MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_214834-1veg46eg
wandb: Find logs at: ./wandb/offline-run-20240418_214834-1veg46eg/logs
INFO flwr 2024-04-18 21:48:49,364 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 21:56:14,031 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 21:56:18,625	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 21:56:20,999	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 21:56:21,022	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 21:56:22,459	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 21:56:22,699	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 21:56:22,863	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.83MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_215613-gprz8i0f
wandb: Find logs at: ./wandb/offline-run-20240418_215613-gprz8i0f/logs
INFO flwr 2024-04-18 21:56:27,200 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 22:03:51,939 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 22:03:56,542	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 22:03:58,931	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 22:03:58,953	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 22:04:00,418	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 22:04:00,618	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 22:04:00,788	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.83MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_220351-1o7jbrc6
wandb: Find logs at: ./wandb/offline-run-20240418_220351-1o7jbrc6/logs
INFO flwr 2024-04-18 22:04:05,104 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 22:11:30,128 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 22:11:34,715	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 22:11:37,137	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 22:11:37,167	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 22:11:38,598	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 22:11:38,796	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 22:11:38,961	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.84MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_221129-0ey4kw60
wandb: Find logs at: ./wandb/offline-run-20240418_221129-0ey4kw60/logs
INFO flwr 2024-04-18 22:11:43,267 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 22:19:07,637 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 22:19:12,174	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 22:19:14,587	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 22:19:14,609	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 22:19:16,086	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 22:19:16,287	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 22:19:16,460	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.84MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_221907-hu9yc3e4
wandb: Find logs at: ./wandb/offline-run-20240418_221907-hu9yc3e4/logs
INFO flwr 2024-04-18 22:19:20,830 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 22:26:45,386 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 22:26:50,109	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 22:26:52,482	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 22:26:52,503	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 22:26:53,981	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 22:26:54,182	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 22:26:54,357	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.84MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_222645-bml4ygju
wandb: Find logs at: ./wandb/offline-run-20240418_222645-bml4ygju/logs
INFO flwr 2024-04-18 22:26:58,724 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 22:34:23,627 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 22:34:28,176	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 22:34:30,538	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 22:34:30,562	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 22:34:32,045	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 22:34:32,245	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 22:34:32,415	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.85MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_223423-2qey3fzl
wandb: Find logs at: ./wandb/offline-run-20240418_223423-2qey3fzl/logs
INFO flwr 2024-04-18 22:34:36,731 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 22:42:01,369 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 22:42:05,752	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 22:42:08,151	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 22:42:08,172	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 22:42:09,660	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 22:42:09,860	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 22:42:10,033	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.85MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_224200-fk5v6mva
wandb: Find logs at: ./wandb/offline-run-20240418_224200-fk5v6mva/logs
INFO flwr 2024-04-18 22:42:14,403 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 22:49:38,970 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 22:49:43,430	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 22:49:45,808	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 22:49:45,829	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 22:49:47,290	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 22:49:47,489	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 22:49:47,660	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.85MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_224938-98lqh0y6
wandb: Find logs at: ./wandb/offline-run-20240418_224938-98lqh0y6/logs
INFO flwr 2024-04-18 22:49:52,023 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 22:57:16,426 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 22:57:20,874	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 22:57:23,324	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 22:57:23,345	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 22:57:24,792	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 22:57:24,994	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 22:57:25,165	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.85MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_225716-w8denyxx
wandb: Find logs at: ./wandb/offline-run-20240418_225716-w8denyxx/logs
INFO flwr 2024-04-18 22:57:29,460 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 23:04:54,585 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 23:04:59,059	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 23:05:01,449	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 23:05:01,476	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 23:05:02,949	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 23:05:03,149	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 23:05:03,320	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.86MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_230454-eu2643la
wandb: Find logs at: ./wandb/offline-run-20240418_230454-eu2643la/logs
INFO flwr 2024-04-18 23:05:07,695 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 23:12:30,822 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 23:12:35,383	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 23:12:37,789	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 23:12:37,815	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 23:12:39,256	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 23:12:39,450	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 23:12:39,620	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.86MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_231230-zud4iq5p
wandb: Find logs at: ./wandb/offline-run-20240418_231230-zud4iq5p/logs
INFO flwr 2024-04-18 23:12:43,980 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 23:20:07,316 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 23:20:11,751	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 23:20:14,135	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 23:20:14,166	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 23:20:15,604	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 23:20:15,800	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 23:20:15,969	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.86MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_232006-mfgqe7eh
wandb: Find logs at: ./wandb/offline-run-20240418_232006-mfgqe7eh/logs
INFO flwr 2024-04-18 23:20:20,319 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 23:27:43,162 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 23:27:47,685	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 23:27:50,075	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 23:27:50,100	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 23:27:51,529	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 23:27:51,725	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 23:27:51,891	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.87MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_232742-yipw0f6x
wandb: Find logs at: ./wandb/offline-run-20240418_232742-yipw0f6x/logs
INFO flwr 2024-04-18 23:27:56,185 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 23:35:19,528 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 23:35:23,990	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 23:35:26,415	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 23:35:26,436	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 23:35:27,884	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 23:35:28,080	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 23:35:28,247	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.87MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_233519-px8e9z7p
wandb: Find logs at: ./wandb/offline-run-20240418_233519-px8e9z7p/logs
INFO flwr 2024-04-18 23:35:32,536 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 23:42:55,907 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 23:43:00,364	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 23:43:02,839	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 23:43:02,862	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 23:43:04,294	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 23:43:04,489	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 23:43:04,659	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.87MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_234255-zusyfk0m
wandb: Find logs at: ./wandb/offline-run-20240418_234255-zusyfk0m/logs
INFO flwr 2024-04-18 23:43:08,947 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 23:50:32,250 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 23:50:36,752	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 23:50:39,121	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 23:50:39,142	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 23:50:40,577	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 23:50:40,774	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 23:50:40,943	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.88MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_235031-4pngpjel
wandb: Find logs at: ./wandb/offline-run-20240418_235031-4pngpjel/logs
INFO flwr 2024-04-18 23:50:45,262 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-18 23:58:08,213 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-18 23:58:12,757	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-18 23:58:15,209	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-18 23:58:15,230	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-18 23:58:16,653	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-18 23:58:16,846	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-18 23:58:17,006	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.88MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240418_235807-68td6ild
wandb: Find logs at: ./wandb/offline-run-20240418_235807-68td6ild/logs
INFO flwr 2024-04-18 23:58:21,364 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 00:05:44,313 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 00:05:48,733	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 00:05:51,142	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 00:05:51,163	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 00:05:52,617	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 00:05:52,816	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 00:05:52,993	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.88MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_000543-cy3ktzmc
wandb: Find logs at: ./wandb/offline-run-20240419_000543-cy3ktzmc/logs
INFO flwr 2024-04-19 00:05:57,333 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 00:13:20,711 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 00:13:25,161	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 00:13:27,589	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 00:13:27,609	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 00:13:29,052	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 00:13:29,249	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 00:13:29,418	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.89MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_001320-m1d9clof
wandb: Find logs at: ./wandb/offline-run-20240419_001320-m1d9clof/logs
INFO flwr 2024-04-19 00:13:33,700 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 00:20:57,089 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 00:21:01,495	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 00:21:03,941	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 00:21:03,962	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 00:21:05,437	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 00:21:05,645	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 00:21:05,822	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.89MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_002056-sgdwe3ir
wandb: Find logs at: ./wandb/offline-run-20240419_002056-sgdwe3ir/logs
INFO flwr 2024-04-19 00:21:10,180 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 00:28:33,537 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 00:28:37,998	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 00:28:40,391	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 00:28:40,411	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 00:28:41,865	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 00:28:42,062	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 00:28:42,231	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.89MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_002833-0smgxrmn
wandb: Find logs at: ./wandb/offline-run-20240419_002833-0smgxrmn/logs
INFO flwr 2024-04-19 00:28:46,579 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 00:36:09,892 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 00:36:14,382	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 00:36:16,811	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 00:36:16,834	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 00:36:18,277	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 00:36:18,474	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 00:36:18,641	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_003609-7lrf1zgv
wandb: Find logs at: ./wandb/offline-run-20240419_003609-7lrf1zgv/logs
INFO flwr 2024-04-19 00:36:22,987 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 00:43:45,836 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 00:43:51,359	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 00:43:53,818	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 00:43:53,844	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 00:43:55,278	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 00:43:55,475	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 00:43:55,649	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_004345-k3bytxvi
wandb: Find logs at: ./wandb/offline-run-20240419_004345-k3bytxvi/logs
INFO flwr 2024-04-19 00:43:59,933 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 00:51:23,326 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 00:51:27,935	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 00:51:30,376	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 00:51:30,396	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 00:51:31,860	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 00:51:32,057	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 00:51:32,231	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.90MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_005122-24td7dvd
wandb: Find logs at: ./wandb/offline-run-20240419_005122-24td7dvd/logs
INFO flwr 2024-04-19 00:51:36,616 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 00:59:00,154 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 00:59:04,694	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 00:59:07,139	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 00:59:07,159	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 00:59:08,616	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 00:59:08,811	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 00:59:08,997	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_005859-y6gpa79p
wandb: Find logs at: ./wandb/offline-run-20240419_005859-y6gpa79p/logs
INFO flwr 2024-04-19 00:59:13,331 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 01:06:37,215 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 01:06:41,783	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 01:06:44,191	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 01:06:44,215	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 01:06:45,656	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 01:06:45,851	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 01:06:46,021	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_010636-yycp44zz
wandb: Find logs at: ./wandb/offline-run-20240419_010636-yycp44zz/logs
INFO flwr 2024-04-19 01:06:50,314 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 01:14:14,270 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 01:14:18,679	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 01:14:21,112	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 01:14:21,139	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 01:14:22,591	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 01:14:22,785	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 01:14:22,950	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_011413-bhv8djjr
wandb: Find logs at: ./wandb/offline-run-20240419_011413-bhv8djjr/logs
INFO flwr 2024-04-19 01:14:27,286 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 01:21:50,530 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 01:21:54,961	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 01:21:57,387	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 01:21:57,408	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 01:21:58,838	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 01:21:59,034	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 01:21:59,204	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_012150-youwyh98
wandb: Find logs at: ./wandb/offline-run-20240419_012150-youwyh98/logs
INFO flwr 2024-04-19 01:22:03,514 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 01:29:26,777 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 01:29:31,836	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 01:29:34,239	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 01:29:34,266	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 01:29:35,718	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 01:29:35,913	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 01:29:36,082	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_012926-809uy42w
wandb: Find logs at: ./wandb/offline-run-20240419_012926-809uy42w/logs
INFO flwr 2024-04-19 01:29:40,403 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 01:37:03,499 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 01:37:07,916	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 01:37:10,321	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 01:37:10,346	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 01:37:11,781	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 01:37:11,978	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 01:37:12,150	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_013703-vbk7r231
wandb: Find logs at: ./wandb/offline-run-20240419_013703-vbk7r231/logs
INFO flwr 2024-04-19 01:37:16,479 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 01:44:39,684 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 01:44:44,305	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 01:44:46,691	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 01:44:46,721	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 01:44:48,157	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 01:44:48,350	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 01:44:48,521	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_014439-flqq4j11
wandb: Find logs at: ./wandb/offline-run-20240419_014439-flqq4j11/logs
INFO flwr 2024-04-19 01:44:52,788 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 01:52:16,210 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 01:52:20,658	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 01:52:23,025	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 01:52:23,046	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 01:52:24,465	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 01:52:24,658	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 01:52:24,813	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_015215-v1fnudxm
wandb: Find logs at: ./wandb/offline-run-20240419_015215-v1fnudxm/logs
INFO flwr 2024-04-19 01:52:29,123 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 01:59:52,142 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 01:59:56,565	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 01:59:58,996	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 01:59:59,022	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 02:00:00,468	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 02:00:00,665	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 02:00:00,833	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_015951-s1y9te3s
wandb: Find logs at: ./wandb/offline-run-20240419_015951-s1y9te3s/logs
INFO flwr 2024-04-19 02:00:05,108 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 02:07:28,217 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 02:07:32,651	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 02:07:35,079	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 02:07:35,100	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 02:07:36,543	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 02:07:36,739	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 02:07:36,908	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_020727-2qj3hy4f
wandb: Find logs at: ./wandb/offline-run-20240419_020727-2qj3hy4f/logs
INFO flwr 2024-04-19 02:07:41,181 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 02:15:04,452 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 02:15:09,037	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 02:15:11,420	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 02:15:11,447	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 02:15:12,875	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 02:15:13,076	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 02:15:13,263	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_021504-md7x68n1
wandb: Find logs at: ./wandb/offline-run-20240419_021504-md7x68n1/logs
INFO flwr 2024-04-19 02:15:17,567 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 02:22:40,531 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 02:22:45,011	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 02:22:47,418	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 02:22:47,443	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 02:22:48,875	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 02:22:49,069	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 02:22:49,237	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_022240-60rzp2pp
wandb: Find logs at: ./wandb/offline-run-20240419_022240-60rzp2pp/logs
INFO flwr 2024-04-19 02:22:53,512 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 02:30:16,910 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 02:30:21,506	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 02:30:23,866	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 02:30:23,892	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 02:30:25,327	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 02:30:25,523	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 02:30:25,693	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_023016-zp5p5wck
wandb: Find logs at: ./wandb/offline-run-20240419_023016-zp5p5wck/logs
INFO flwr 2024-04-19 02:30:29,999 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 02:37:53,200 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 02:37:57,697	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 02:38:00,066	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 02:38:00,087	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 02:38:01,533	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 02:38:01,729	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 02:38:01,899	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_023752-dbsryew9
wandb: Find logs at: ./wandb/offline-run-20240419_023752-dbsryew9/logs
INFO flwr 2024-04-19 02:38:06,172 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 02:45:29,364 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 02:45:33,750	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 02:45:36,114	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 02:45:36,138	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 02:45:37,575	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 02:45:37,771	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 02:45:37,939	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_024529-k1q6le7y
wandb: Find logs at: ./wandb/offline-run-20240419_024529-k1q6le7y/logs
INFO flwr 2024-04-19 02:45:42,251 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 02:53:05,925 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 02:53:10,342	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 02:53:12,758	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 02:53:12,780	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 02:53:14,233	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 02:53:14,428	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 02:53:14,596	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_025305-780czggw
wandb: Find logs at: ./wandb/offline-run-20240419_025305-780czggw/logs
INFO flwr 2024-04-19 02:53:18,950 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 03:00:42,437 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 03:00:46,893	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 03:00:49,270	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 03:00:49,291	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 03:00:50,740	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 03:00:50,939	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 03:00:51,109	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_030042-1iyfxfiw
wandb: Find logs at: ./wandb/offline-run-20240419_030042-1iyfxfiw/logs
INFO flwr 2024-04-19 03:00:55,423 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 03:08:18,861 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 03:08:23,269	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 03:08:25,653	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 03:08:25,674	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 03:08:27,121	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 03:08:27,325	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 03:08:27,493	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_030818-rztjukg0
wandb: Find logs at: ./wandb/offline-run-20240419_030818-rztjukg0/logs
INFO flwr 2024-04-19 03:08:31,815 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 03:15:55,111 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 03:15:59,573	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 03:16:01,998	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 03:16:02,019	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 03:16:03,471	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 03:16:03,670	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 03:16:03,837	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_031554-kbwfzz8e
wandb: Find logs at: ./wandb/offline-run-20240419_031554-kbwfzz8e/logs
INFO flwr 2024-04-19 03:16:08,143 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 03:23:31,304 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 03:23:36,820	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 03:23:39,164	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 03:23:39,184	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 03:23:40,637	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 03:23:40,834	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 03:23:41,007	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_032330-rhc8u1v7
wandb: Find logs at: ./wandb/offline-run-20240419_032330-rhc8u1v7/logs
INFO flwr 2024-04-19 03:23:45,283 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 03:31:08,602 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 03:31:13,009	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 03:31:15,450	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 03:31:15,472	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 03:31:16,941	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 03:31:17,134	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 03:31:17,319	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_033108-tzee8qm4
wandb: Find logs at: ./wandb/offline-run-20240419_033108-tzee8qm4/logs
INFO flwr 2024-04-19 03:31:21,599 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 03:38:45,143 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 03:38:50,611	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 03:38:52,999	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 03:38:53,025	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 03:38:54,450	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 03:38:54,645	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 03:38:54,816	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_033844-dygawndt
wandb: Find logs at: ./wandb/offline-run-20240419_033844-dygawndt/logs
INFO flwr 2024-04-19 03:38:59,127 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 03:46:22,774 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 03:46:27,277	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 03:46:29,873	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 03:46:29,897	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 03:46:31,345	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 03:46:31,540	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 03:46:31,711	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_034622-2gcvtrdw
wandb: Find logs at: ./wandb/offline-run-20240419_034622-2gcvtrdw/logs
INFO flwr 2024-04-19 03:46:36,014 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 03:53:59,646 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 03:54:04,568	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 03:54:07,230	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 03:54:07,251	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 03:54:08,704	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 03:54:08,898	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 03:54:09,068	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_035359-hbud4bj0
wandb: Find logs at: ./wandb/offline-run-20240419_035359-hbud4bj0/logs
INFO flwr 2024-04-19 03:54:13,387 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 04:01:38,894 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 04:01:44,739	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 04:01:57,100	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 04:01:57,125	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 04:01:58,581	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 04:01:58,780	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 04:01:58,950	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_040136-64pdtafm
wandb: Find logs at: ./wandb/offline-run-20240419_040136-64pdtafm/logs
INFO flwr 2024-04-19 04:02:03,296 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 04:09:26,949 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 04:09:33,732	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 04:09:36,512	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 04:09:36,533	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 04:09:37,977	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 04:09:38,174	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 04:09:38,344	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_040926-9frzhyti
wandb: Find logs at: ./wandb/offline-run-20240419_040926-9frzhyti/logs
INFO flwr 2024-04-19 04:09:42,663 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.25}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 04:17:06,349 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 04:17:10,906	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 04:17:13,379	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 04:17:13,400	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 04:17:14,839	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 04:17:15,036	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 04:17:15,207	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_041705-gs5t4s2z
wandb: Find logs at: ./wandb/offline-run-20240419_041705-gs5t4s2z/logs
INFO flwr 2024-04-19 04:17:19,487 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 04:24:43,057 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 04:24:48,102	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 04:24:51,154	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 04:24:51,173	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 04:24:52,562	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 04:24:52,755	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 04:24:52,913	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_042442-5a4p6m5m
wandb: Find logs at: ./wandb/offline-run-20240419_042442-5a4p6m5m/logs
INFO flwr 2024-04-19 04:24:57,207 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 04:32:20,520 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 04:32:24,989	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 04:32:27,341	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 04:32:27,370	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 04:32:28,813	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 04:32:29,011	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 04:32:29,179	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_043220-8dic25lv
wandb: Find logs at: ./wandb/offline-run-20240419_043220-8dic25lv/logs
INFO flwr 2024-04-19 04:32:33,441 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 04:39:57,278 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 04:40:02,643	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 04:40:05,053	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 04:40:05,074	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 04:40:06,517	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 04:40:06,716	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 04:40:06,891	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_043956-om269fay
wandb: Find logs at: ./wandb/offline-run-20240419_043956-om269fay/logs
INFO flwr 2024-04-19 04:40:11,231 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 04:47:35,228 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 04:47:39,762	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 04:47:42,184	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 04:47:42,206	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 04:47:43,657	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 04:47:43,855	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 04:47:44,029	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_044734-nb49gbpg
wandb: Find logs at: ./wandb/offline-run-20240419_044734-nb49gbpg/logs
INFO flwr 2024-04-19 04:47:48,355 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 64
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 50
		Optimizer: FedAdam
			local: {'lr': 0.3}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 04:55:11,842 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 04:55:16,426	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 04:55:18,834	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 04:55:18,861	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc is very large (483.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/pack/tmp_pack_C6t8Cc']})`
2024-04-19 04:55:20,297	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80 is very large (75.64MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/bf/8639baef4d84289fa71f49ac46bc1c12b22f80']})`
2024-04-19 04:55:20,493	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7 is very large (14.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.git/objects/2a/3119490a84d3a285f37aae2b4854ab15843cb7']})`
2024-04-19 04:55:20,670	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out is very large (45.91MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/performance_simulation/mnist/learning_method/fed_nag/slurm-281485.out']})`
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_045511-h4jvvajt
wandb: Find logs at: ./wandb/offline-run-20240419_045511-h4jvvajt/logs
2024-04-19 04:56:59.962840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-19 04:57:23.708010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-19 04:58:49,781 | batch_run_simulation.py:80 | Loaded 240 configs with name MINST-LOGISTICREGRESSION-FEDADAM, running...
INFO flwr 2024-04-19 04:58:49,781 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 05:06:33,025 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 05:06:48,280	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 05:06:57,268	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 05:06:58,802	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5c6dacbcd42ee2f8.zip' (0.20MiB) to Ray cluster...
2024-04-19 05:06:58,804	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5c6dacbcd42ee2f8.zip'.
