ctit088
2024-04-19 11:37:34.050282: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-19 11:37:39.675378: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-19 11:38:03.078733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-19 11:40:10,954 | batch_run_simulation.py:80 | Loaded 240 configs with name MINST-LOGISTICREGRESSION-FEDADAM, running...
INFO flwr 2024-04-19 11:40:10,954 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 11:47:25,274 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-19 11:47:36,348	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 11:47:38,234	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 11:47:38,502	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 11:47:38,504	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 11:47:48,357 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 145142569165.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66489672499.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-19 11:47:48,357 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 11:47:48,358 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 11:47:48,380 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 11:47:48,381 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 11:47:48,381 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 11:47:48,381 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1030771)[0m 2024-04-19 11:47:53.713581: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1030771)[0m 2024-04-19 11:47:53.777095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1030771)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1030771)[0m 2024-04-19 11:47:55.306840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-19 11:47:56,905 | server.py:94 | initial parameters (loss, other metrics): 2.3044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-04-19 11:47:56,905 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 11:47:56,905 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1030771)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1030771)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1030773)[0m 2024-04-19 11:47:54.130924: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1030773)[0m 2024-04-19 11:47:54.221198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1030773)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1030773)[0m 2024-04-19 11:47:56.093998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 11:48:18,471 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 11:48:19,504 | server.py:125 | fit progress: (1, 1.9040095806121826, {'accuracy': 0.5857, 'data_size': 10000}, 22.598282430088148)
INFO flwr 2024-04-19 11:48:19,504 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 11:48:19,504 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:48:28,598 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 11:48:29,942 | server.py:125 | fit progress: (2, 1.6897104978561401, {'accuracy': 0.7889, 'data_size': 10000}, 33.036354643059894)
INFO flwr 2024-04-19 11:48:29,942 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 11:48:29,942 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:48:38,735 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 11:48:40,097 | server.py:125 | fit progress: (3, 1.6346701383590698, {'accuracy': 0.8376, 'data_size': 10000}, 43.19225773308426)
INFO flwr 2024-04-19 11:48:40,098 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 11:48:40,098 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:48:48,885 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 11:48:50,161 | server.py:125 | fit progress: (4, 1.6046377420425415, {'accuracy': 0.8611, 'data_size': 10000}, 53.25528243998997)
INFO flwr 2024-04-19 11:48:50,161 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 11:48:50,161 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:48:58,716 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 11:48:59,758 | server.py:125 | fit progress: (5, 1.615552544593811, {'accuracy': 0.8466, 'data_size': 10000}, 62.85291403206065)
INFO flwr 2024-04-19 11:48:59,758 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 11:48:59,759 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:49:08,328 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 11:49:09,593 | server.py:125 | fit progress: (6, 1.5814604759216309, {'accuracy': 0.8817, 'data_size': 10000}, 72.68756275204942)
INFO flwr 2024-04-19 11:49:09,593 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 11:49:09,593 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:49:18,501 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 11:49:19,531 | server.py:125 | fit progress: (7, 1.5712499618530273, {'accuracy': 0.8908, 'data_size': 10000}, 82.62540709599853)
INFO flwr 2024-04-19 11:49:19,531 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 11:49:19,531 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:49:28,404 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 11:49:29,419 | server.py:125 | fit progress: (8, 1.5861256122589111, {'accuracy': 0.8761, 'data_size': 10000}, 92.5138192831073)
INFO flwr 2024-04-19 11:49:29,419 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 11:49:29,420 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:49:38,233 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 11:49:39,500 | server.py:125 | fit progress: (9, 1.592566967010498, {'accuracy': 0.8695, 'data_size': 10000}, 102.59450935409404)
INFO flwr 2024-04-19 11:49:39,500 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 11:49:39,500 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:49:48,358 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 11:49:49,357 | server.py:125 | fit progress: (10, 1.5705530643463135, {'accuracy': 0.8917, 'data_size': 10000}, 112.45129680610262)
INFO flwr 2024-04-19 11:49:49,357 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 11:49:49,357 | server.py:153 | FL finished in 112.45172816305421
INFO flwr 2024-04-19 11:49:49,357 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 11:49:49,357 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 11:49:49,358 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 11:49:49,358 | app.py:229 | app_fit: losses_centralized [(0, 2.3044753074645996), (1, 1.9040095806121826), (2, 1.6897104978561401), (3, 1.6346701383590698), (4, 1.6046377420425415), (5, 1.615552544593811), (6, 1.5814604759216309), (7, 1.5712499618530273), (8, 1.5861256122589111), (9, 1.592566967010498), (10, 1.5705530643463135)]
INFO flwr 2024-04-19 11:49:49,358 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0654), (1, 0.5857), (2, 0.7889), (3, 0.8376), (4, 0.8611), (5, 0.8466), (6, 0.8817), (7, 0.8908), (8, 0.8761), (9, 0.8695), (10, 0.8917)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8917
wandb:     loss 1.57055
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_114721-edrwtcyd
wandb: Find logs at: ./wandb/offline-run-20240419_114721-edrwtcyd/logs
INFO flwr 2024-04-19 11:49:52,768 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 11:56:57,732 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1030774)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1030774)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 11:57:03,036	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 11:57:03,939	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 11:57:04,205	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 11:57:04,208	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 11:57:14,154 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 141744033997.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65033157427.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-19 11:57:14,155 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 11:57:14,155 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 11:57:14,171 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 11:57:14,172 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 11:57:14,172 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 11:57:14,172 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 11:57:16,695 | server.py:94 | initial parameters (loss, other metrics): 2.3022866249084473, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-19 11:57:16,695 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 11:57:16,696 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1035294)[0m 2024-04-19 11:57:20.000306: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1035294)[0m 2024-04-19 11:57:20.103866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1035294)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1035294)[0m 2024-04-19 11:57:22.292270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1035294)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1035294)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1035284)[0m 2024-04-19 11:57:20.111702: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1035284)[0m 2024-04-19 11:57:20.201098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1035284)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1035284)[0m 2024-04-19 11:57:22.314243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 11:57:37,078 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 11:57:38,113 | server.py:125 | fit progress: (1, 2.2390072345733643, {'accuracy': 0.5572, 'data_size': 10000}, 21.416855201823637)
INFO flwr 2024-04-19 11:57:38,113 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 11:57:38,113 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:57:47,290 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 11:57:48,249 | server.py:125 | fit progress: (2, 2.1355299949645996, {'accuracy': 0.6415, 'data_size': 10000}, 31.553091841982678)
INFO flwr 2024-04-19 11:57:48,249 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 11:57:48,249 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:57:56,245 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 11:57:57,498 | server.py:125 | fit progress: (3, 2.026954412460327, {'accuracy': 0.6469, 'data_size': 10000}, 40.8020065149758)
INFO flwr 2024-04-19 11:57:57,498 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 11:57:57,498 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:58:05,770 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 11:58:07,136 | server.py:125 | fit progress: (4, 1.9292405843734741, {'accuracy': 0.6749, 'data_size': 10000}, 50.440046874806285)
INFO flwr 2024-04-19 11:58:07,136 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 11:58:07,136 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:58:16,150 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 11:58:17,176 | server.py:125 | fit progress: (5, 1.8517922163009644, {'accuracy': 0.7085, 'data_size': 10000}, 60.48056984785944)
INFO flwr 2024-04-19 11:58:17,177 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 11:58:17,177 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:58:26,023 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 11:58:27,320 | server.py:125 | fit progress: (6, 1.7884562015533447, {'accuracy': 0.7483, 'data_size': 10000}, 70.62445053784177)
INFO flwr 2024-04-19 11:58:27,320 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 11:58:27,321 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:58:36,849 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 11:58:38,240 | server.py:125 | fit progress: (7, 1.736694574356079, {'accuracy': 0.7916, 'data_size': 10000}, 81.54400001186877)
INFO flwr 2024-04-19 11:58:38,240 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 11:58:38,240 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:58:47,491 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 11:58:48,547 | server.py:125 | fit progress: (8, 1.6993508338928223, {'accuracy': 0.8219, 'data_size': 10000}, 91.85116467485204)
INFO flwr 2024-04-19 11:58:48,547 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 11:58:48,547 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:58:58,119 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 11:58:59,447 | server.py:125 | fit progress: (9, 1.6685686111450195, {'accuracy': 0.8519, 'data_size': 10000}, 102.7509312198963)
INFO flwr 2024-04-19 11:58:59,447 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 11:58:59,447 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 11:59:09,308 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 11:59:10,597 | server.py:125 | fit progress: (10, 1.6451574563980103, {'accuracy': 0.871, 'data_size': 10000}, 113.90089984494261)
INFO flwr 2024-04-19 11:59:10,597 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 11:59:10,597 | server.py:153 | FL finished in 113.90128764277324
INFO flwr 2024-04-19 11:59:10,597 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 11:59:10,597 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 11:59:10,597 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 11:59:10,597 | app.py:229 | app_fit: losses_centralized [(0, 2.3022866249084473), (1, 2.2390072345733643), (2, 2.1355299949645996), (3, 2.026954412460327), (4, 1.9292405843734741), (5, 1.8517922163009644), (6, 1.7884562015533447), (7, 1.736694574356079), (8, 1.6993508338928223), (9, 1.6685686111450195), (10, 1.6451574563980103)]
INFO flwr 2024-04-19 11:59:10,597 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.5572), (2, 0.6415), (3, 0.6469), (4, 0.6749), (5, 0.7085), (6, 0.7483), (7, 0.7916), (8, 0.8219), (9, 0.8519), (10, 0.871)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.871
wandb:     loss 1.64516
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_115657-sut4dswl
wandb: Find logs at: ./wandb/offline-run-20240419_115657-sut4dswl/logs
INFO flwr 2024-04-19 11:59:14,086 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 12:06:19,353 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1035291)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1035291)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 12:06:33,373	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 12:06:36,840	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 12:06:37,988	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 12:06:37,990	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 12:06:49,756 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60499321651.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 131165083853.0}
INFO flwr 2024-04-19 12:06:49,757 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 12:06:49,757 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 12:06:49,776 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 12:06:49,778 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 12:06:49,778 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 12:06:49,778 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 12:06:52,364 | server.py:94 | initial parameters (loss, other metrics): 2.302816867828369, {'accuracy': 0.0734, 'data_size': 10000}
INFO flwr 2024-04-19 12:06:52,365 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 12:06:52,365 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1039946)[0m 2024-04-19 12:06:56.058694: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1039953)[0m 2024-04-19 12:06:56.175809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1039953)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1039953)[0m 2024-04-19 12:06:58.304232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1039953)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1039953)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1039961)[0m 2024-04-19 12:06:56.467232: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1039961)[0m 2024-04-19 12:06:56.561092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1039961)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1039961)[0m 2024-04-19 12:06:58.652502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 12:07:11,607 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 12:07:12,928 | server.py:125 | fit progress: (1, 2.2978553771972656, {'accuracy': 0.1281, 'data_size': 10000}, 20.563207577914)
INFO flwr 2024-04-19 12:07:12,929 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 12:07:12,929 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:07:22,968 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 12:07:24,364 | server.py:125 | fit progress: (2, 2.2908177375793457, {'accuracy': 0.2237, 'data_size': 10000}, 31.999401583103463)
INFO flwr 2024-04-19 12:07:24,365 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 12:07:24,365 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:07:33,688 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 12:07:35,050 | server.py:125 | fit progress: (3, 2.2819557189941406, {'accuracy': 0.339, 'data_size': 10000}, 42.68467311793938)
INFO flwr 2024-04-19 12:07:35,050 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 12:07:35,050 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:07:44,262 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 12:07:45,332 | server.py:125 | fit progress: (4, 2.2716925144195557, {'accuracy': 0.4514, 'data_size': 10000}, 52.96695598401129)
INFO flwr 2024-04-19 12:07:45,332 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 12:07:45,333 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:07:54,845 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 12:07:56,211 | server.py:125 | fit progress: (5, 2.2599189281463623, {'accuracy': 0.5462, 'data_size': 10000}, 63.84550349297933)
INFO flwr 2024-04-19 12:07:56,211 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 12:07:56,211 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:08:05,292 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 12:08:06,692 | server.py:125 | fit progress: (6, 2.24685001373291, {'accuracy': 0.6098, 'data_size': 10000}, 74.32708159391768)
INFO flwr 2024-04-19 12:08:06,692 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 12:08:06,693 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:08:15,844 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 12:08:16,908 | server.py:125 | fit progress: (7, 2.232402801513672, {'accuracy': 0.6486, 'data_size': 10000}, 84.54303790791892)
INFO flwr 2024-04-19 12:08:16,909 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 12:08:16,909 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:08:25,072 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 12:08:26,398 | server.py:125 | fit progress: (8, 2.216825246810913, {'accuracy': 0.6723, 'data_size': 10000}, 94.03286199993454)
INFO flwr 2024-04-19 12:08:26,398 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 12:08:26,398 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:08:35,899 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 12:08:36,965 | server.py:125 | fit progress: (9, 2.2001991271972656, {'accuracy': 0.6878, 'data_size': 10000}, 104.60034779598936)
INFO flwr 2024-04-19 12:08:36,966 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 12:08:36,966 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:08:45,925 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 12:08:46,970 | server.py:125 | fit progress: (10, 2.1828503608703613, {'accuracy': 0.6944, 'data_size': 10000}, 114.60536053404212)
INFO flwr 2024-04-19 12:08:46,971 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 12:08:46,971 | server.py:153 | FL finished in 114.60580899892375
INFO flwr 2024-04-19 12:08:46,971 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 12:08:46,971 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 12:08:46,971 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 12:08:46,971 | app.py:229 | app_fit: losses_centralized [(0, 2.302816867828369), (1, 2.2978553771972656), (2, 2.2908177375793457), (3, 2.2819557189941406), (4, 2.2716925144195557), (5, 2.2599189281463623), (6, 2.24685001373291), (7, 2.232402801513672), (8, 2.216825246810913), (9, 2.2001991271972656), (10, 2.1828503608703613)]
INFO flwr 2024-04-19 12:08:46,971 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0734), (1, 0.1281), (2, 0.2237), (3, 0.339), (4, 0.4514), (5, 0.5462), (6, 0.6098), (7, 0.6486), (8, 0.6723), (9, 0.6878), (10, 0.6944)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6944
wandb:     loss 2.18285
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_120619-dn26wm7g
wandb: Find logs at: ./wandb/offline-run-20240419_120619-dn26wm7g/logs
INFO flwr 2024-04-19 12:08:50,468 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 12:15:55,755 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1039946)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1039946)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 12:16:00,758	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 12:16:01,309	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 12:16:01,571	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 12:16:01,573	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 12:16:12,704 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 59184227942.0, 'memory': 128096531866.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-19 12:16:12,704 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 12:16:12,704 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 12:16:12,725 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 12:16:12,728 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 12:16:12,728 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 12:16:12,728 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 12:16:15,571 | server.py:94 | initial parameters (loss, other metrics): 2.304152250289917, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-19 12:16:15,572 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 12:16:15,572 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1044798)[0m 2024-04-19 12:16:18.945645: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1044798)[0m 2024-04-19 12:16:19.039701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1044798)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1044798)[0m 2024-04-19 12:16:20.989674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1044798)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1044798)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1044803)[0m 2024-04-19 12:16:19.239114: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1044803)[0m 2024-04-19 12:16:19.333972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1044803)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1044803)[0m 2024-04-19 12:16:21.513457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 12:16:34,270 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 12:16:35,601 | server.py:125 | fit progress: (1, 2.303678512573242, {'accuracy': 0.098, 'data_size': 10000}, 20.02908891905099)
INFO flwr 2024-04-19 12:16:35,601 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 12:16:35,601 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:16:45,307 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 12:16:46,379 | server.py:125 | fit progress: (2, 2.303058385848999, {'accuracy': 0.1026, 'data_size': 10000}, 30.80698759900406)
INFO flwr 2024-04-19 12:16:46,379 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 12:16:46,379 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:16:55,461 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 12:16:56,531 | server.py:125 | fit progress: (3, 2.3023462295532227, {'accuracy': 0.1096, 'data_size': 10000}, 40.95940377213992)
INFO flwr 2024-04-19 12:16:56,532 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 12:16:56,532 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:17:06,557 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 12:17:07,643 | server.py:125 | fit progress: (4, 2.301570177078247, {'accuracy': 0.1167, 'data_size': 10000}, 52.07108592707664)
INFO flwr 2024-04-19 12:17:07,643 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 12:17:07,643 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:17:17,098 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 12:17:18,436 | server.py:125 | fit progress: (5, 2.300727605819702, {'accuracy': 0.1246, 'data_size': 10000}, 62.86377448611893)
INFO flwr 2024-04-19 12:17:18,436 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 12:17:18,436 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:17:27,726 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 12:17:28,777 | server.py:125 | fit progress: (6, 2.299844264984131, {'accuracy': 0.1324, 'data_size': 10000}, 73.20468510012142)
INFO flwr 2024-04-19 12:17:28,777 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 12:17:28,777 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:17:37,830 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 12:17:38,906 | server.py:125 | fit progress: (7, 2.2989211082458496, {'accuracy': 0.1418, 'data_size': 10000}, 83.33404292119667)
INFO flwr 2024-04-19 12:17:38,906 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 12:17:38,906 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:17:48,254 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 12:17:49,331 | server.py:125 | fit progress: (8, 2.2979636192321777, {'accuracy': 0.1528, 'data_size': 10000}, 93.75873716012575)
INFO flwr 2024-04-19 12:17:49,331 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 12:17:49,331 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:17:58,725 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 12:18:00,059 | server.py:125 | fit progress: (9, 2.2969706058502197, {'accuracy': 0.1657, 'data_size': 10000}, 104.48741702409461)
INFO flwr 2024-04-19 12:18:00,059 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 12:18:00,060 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:18:09,251 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 12:18:10,274 | server.py:125 | fit progress: (10, 2.295964002609253, {'accuracy': 0.1775, 'data_size': 10000}, 114.70210320409387)
INFO flwr 2024-04-19 12:18:10,274 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 12:18:10,274 | server.py:153 | FL finished in 114.70266784797423
INFO flwr 2024-04-19 12:18:10,275 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 12:18:10,275 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 12:18:10,275 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 12:18:10,275 | app.py:229 | app_fit: losses_centralized [(0, 2.304152250289917), (1, 2.303678512573242), (2, 2.303058385848999), (3, 2.3023462295532227), (4, 2.301570177078247), (5, 2.300727605819702), (6, 2.299844264984131), (7, 2.2989211082458496), (8, 2.2979636192321777), (9, 2.2969706058502197), (10, 2.295964002609253)]
INFO flwr 2024-04-19 12:18:10,275 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.098), (2, 0.1026), (3, 0.1096), (4, 0.1167), (5, 0.1246), (6, 0.1324), (7, 0.1418), (8, 0.1528), (9, 0.1657), (10, 0.1775)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1775
wandb:     loss 2.29596
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_121555-y3ihvup4
wandb: Find logs at: ./wandb/offline-run-20240419_121555-y3ihvup4/logs
INFO flwr 2024-04-19 12:18:13,805 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 12:25:20,222 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1044793)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1044793)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 12:25:26,122	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 12:25:26,496	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 12:25:26,758	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 12:25:26,761	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 12:25:37,821 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 125231089460.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 57956181196.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-19 12:25:37,822 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 12:25:37,822 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 12:25:37,843 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 12:25:37,844 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 12:25:37,844 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 12:25:37,844 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 12:25:40,563 | server.py:94 | initial parameters (loss, other metrics): 2.3035645484924316, {'accuracy': 0.0768, 'data_size': 10000}
INFO flwr 2024-04-19 12:25:40,564 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 12:25:40,565 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1049189)[0m 2024-04-19 12:25:44.064297: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1049189)[0m 2024-04-19 12:25:44.163837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1049189)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1049186)[0m 2024-04-19 12:25:46.218342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1049191)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1049191)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1049182)[0m 2024-04-19 12:25:44.327076: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1049182)[0m 2024-04-19 12:25:44.421630: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1049182)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1049192)[0m 2024-04-19 12:25:46.707835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 12:25:59,530 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 12:26:00,866 | server.py:125 | fit progress: (1, 2.3035144805908203, {'accuracy': 0.0774, 'data_size': 10000}, 20.30197972501628)
INFO flwr 2024-04-19 12:26:00,867 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 12:26:00,867 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:26:10,644 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 12:26:11,699 | server.py:125 | fit progress: (2, 2.3034493923187256, {'accuracy': 0.0779, 'data_size': 10000}, 31.13430976588279)
INFO flwr 2024-04-19 12:26:11,699 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 12:26:11,699 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:26:20,931 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 12:26:22,265 | server.py:125 | fit progress: (3, 2.303375482559204, {'accuracy': 0.0783, 'data_size': 10000}, 41.70043140696362)
INFO flwr 2024-04-19 12:26:22,265 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 12:26:22,265 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:26:31,646 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 12:26:33,041 | server.py:125 | fit progress: (4, 2.303292751312256, {'accuracy': 0.079, 'data_size': 10000}, 52.47624303493649)
INFO flwr 2024-04-19 12:26:33,041 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 12:26:33,041 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:26:42,171 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 12:26:43,243 | server.py:125 | fit progress: (5, 2.303205966949463, {'accuracy': 0.0799, 'data_size': 10000}, 62.67814593296498)
INFO flwr 2024-04-19 12:26:43,243 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 12:26:43,243 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:26:52,933 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 12:26:54,343 | server.py:125 | fit progress: (6, 2.303115129470825, {'accuracy': 0.081, 'data_size': 10000}, 73.77830371796153)
INFO flwr 2024-04-19 12:26:54,343 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 12:26:54,343 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:27:03,909 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 12:27:05,259 | server.py:125 | fit progress: (7, 2.3030219078063965, {'accuracy': 0.0815, 'data_size': 10000}, 84.69499420607463)
INFO flwr 2024-04-19 12:27:05,260 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 12:27:05,260 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:27:14,418 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 12:27:15,474 | server.py:125 | fit progress: (8, 2.3029253482818604, {'accuracy': 0.0826, 'data_size': 10000}, 94.90941501990892)
INFO flwr 2024-04-19 12:27:15,474 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 12:27:15,474 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:27:25,504 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 12:27:26,825 | server.py:125 | fit progress: (9, 2.3028273582458496, {'accuracy': 0.0844, 'data_size': 10000}, 106.26081167394295)
INFO flwr 2024-04-19 12:27:26,825 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 12:27:26,826 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:27:36,219 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 12:27:37,562 | server.py:125 | fit progress: (10, 2.3027286529541016, {'accuracy': 0.0861, 'data_size': 10000}, 116.9978302239906)
INFO flwr 2024-04-19 12:27:37,563 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 12:27:37,563 | server.py:153 | FL finished in 116.9983304538764
INFO flwr 2024-04-19 12:27:37,563 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 12:27:37,563 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 12:27:37,563 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 12:27:37,563 | app.py:229 | app_fit: losses_centralized [(0, 2.3035645484924316), (1, 2.3035144805908203), (2, 2.3034493923187256), (3, 2.303375482559204), (4, 2.303292751312256), (5, 2.303205966949463), (6, 2.303115129470825), (7, 2.3030219078063965), (8, 2.3029253482818604), (9, 2.3028273582458496), (10, 2.3027286529541016)]
INFO flwr 2024-04-19 12:27:37,563 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0768), (1, 0.0774), (2, 0.0779), (3, 0.0783), (4, 0.079), (5, 0.0799), (6, 0.081), (7, 0.0815), (8, 0.0826), (9, 0.0844), (10, 0.0861)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0861
wandb:     loss 2.30273
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_122519-tsnh0xyr
wandb: Find logs at: ./wandb/offline-run-20240419_122519-tsnh0xyr/logs
INFO flwr 2024-04-19 12:27:41,053 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 12:34:46,571 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1049192)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1049192)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 12:34:51,526	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 12:34:52,173	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 12:34:52,441	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 12:34:52,442	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 12:35:03,486 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 122350161306.0, 'object_store_memory': 56721497702.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-19 12:35:03,486 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 12:35:03,486 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 12:35:03,519 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 12:35:03,520 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 12:35:03,521 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 12:35:03,521 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 12:35:05,783 | server.py:94 | initial parameters (loss, other metrics): 2.304069757461548, {'accuracy': 0.0882, 'data_size': 10000}
INFO flwr 2024-04-19 12:35:05,784 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 12:35:05,785 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1053369)[0m 2024-04-19 12:35:09.610432: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1053369)[0m 2024-04-19 12:35:09.699590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1053369)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1053377)[0m 2024-04-19 12:35:11.840788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1053377)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1053377)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1053338)[0m 2024-04-19 12:35:10.211747: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1053338)[0m 2024-04-19 12:35:10.303669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1053338)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1053338)[0m 2024-04-19 12:35:12.581112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 12:35:26,148 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 12:35:27,474 | server.py:125 | fit progress: (1, 1.8610420227050781, {'accuracy': 0.6341, 'data_size': 10000}, 21.689650462009013)
INFO flwr 2024-04-19 12:35:27,475 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 12:35:27,475 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:35:37,815 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 12:35:39,190 | server.py:125 | fit progress: (2, 1.7024853229522705, {'accuracy': 0.7716, 'data_size': 10000}, 33.40561191504821)
INFO flwr 2024-04-19 12:35:39,191 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 12:35:39,191 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:35:48,714 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 12:35:50,084 | server.py:125 | fit progress: (3, 1.6842423677444458, {'accuracy': 0.7818, 'data_size': 10000}, 44.29923918796703)
INFO flwr 2024-04-19 12:35:50,084 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 12:35:50,085 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:35:59,390 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 12:36:00,494 | server.py:125 | fit progress: (4, 1.652207374572754, {'accuracy': 0.8102, 'data_size': 10000}, 54.709693694952875)
INFO flwr 2024-04-19 12:36:00,495 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 12:36:00,495 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:36:09,615 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 12:36:10,947 | server.py:125 | fit progress: (5, 1.6509562730789185, {'accuracy': 0.8116, 'data_size': 10000}, 65.16189605300315)
INFO flwr 2024-04-19 12:36:10,947 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 12:36:10,947 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:36:20,588 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 12:36:21,900 | server.py:125 | fit progress: (6, 1.6447445154190063, {'accuracy': 0.8174, 'data_size': 10000}, 76.11519895587116)
INFO flwr 2024-04-19 12:36:21,900 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 12:36:21,900 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:36:31,053 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 12:36:32,114 | server.py:125 | fit progress: (7, 1.6406675577163696, {'accuracy': 0.82, 'data_size': 10000}, 86.32972438307479)
INFO flwr 2024-04-19 12:36:32,115 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 12:36:32,115 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:36:41,325 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 12:36:42,702 | server.py:125 | fit progress: (8, 1.6340916156768799, {'accuracy': 0.8267, 'data_size': 10000}, 96.91765953600407)
INFO flwr 2024-04-19 12:36:42,703 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 12:36:42,703 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:36:52,457 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 12:36:53,530 | server.py:125 | fit progress: (9, 1.641599416732788, {'accuracy': 0.82, 'data_size': 10000}, 107.74487701198086)
INFO flwr 2024-04-19 12:36:53,530 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 12:36:53,530 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:37:03,076 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 12:37:04,155 | server.py:125 | fit progress: (10, 1.633580207824707, {'accuracy': 0.8283, 'data_size': 10000}, 118.37073574285023)
INFO flwr 2024-04-19 12:37:04,156 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 12:37:04,156 | server.py:153 | FL finished in 118.37124325404875
INFO flwr 2024-04-19 12:37:04,156 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 12:37:04,156 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 12:37:04,156 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 12:37:04,156 | app.py:229 | app_fit: losses_centralized [(0, 2.304069757461548), (1, 1.8610420227050781), (2, 1.7024853229522705), (3, 1.6842423677444458), (4, 1.652207374572754), (5, 1.6509562730789185), (6, 1.6447445154190063), (7, 1.6406675577163696), (8, 1.6340916156768799), (9, 1.641599416732788), (10, 1.633580207824707)]
INFO flwr 2024-04-19 12:37:04,156 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0882), (1, 0.6341), (2, 0.7716), (3, 0.7818), (4, 0.8102), (5, 0.8116), (6, 0.8174), (7, 0.82), (8, 0.8267), (9, 0.82), (10, 0.8283)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8283
wandb:     loss 1.63358
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_123446-x1wqtb1t
wandb: Find logs at: ./wandb/offline-run-20240419_123446-x1wqtb1t/logs
INFO flwr 2024-04-19 12:37:07,686 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 12:44:12,819 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1053338)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1053338)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 12:44:17,662	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 12:44:18,054	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 12:44:18,307	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 12:44:18,309	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 12:44:29,380 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 119680270541.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 55577258803.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-19 12:44:29,380 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 12:44:29,380 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 12:44:29,399 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 12:44:29,403 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 12:44:29,403 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 12:44:29,404 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 12:44:32,759 | server.py:94 | initial parameters (loss, other metrics): 2.299149513244629, {'accuracy': 0.1618, 'data_size': 10000}
INFO flwr 2024-04-19 12:44:32,759 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 12:44:32,760 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1057580)[0m 2024-04-19 12:44:35.499041: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1057580)[0m 2024-04-19 12:44:35.593373: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1057580)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1057580)[0m 2024-04-19 12:44:37.616050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1057584)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1057584)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1057584)[0m 2024-04-19 12:44:35.849600: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1057584)[0m 2024-04-19 12:44:35.944219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1057584)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1057592)[0m 2024-04-19 12:44:38.078661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 12:44:51,051 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 12:44:52,367 | server.py:125 | fit progress: (1, 2.2410948276519775, {'accuracy': 0.5921, 'data_size': 10000}, 19.607092972146347)
INFO flwr 2024-04-19 12:44:52,367 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 12:44:52,367 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:45:01,823 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 12:45:02,879 | server.py:125 | fit progress: (2, 2.1390602588653564, {'accuracy': 0.7071, 'data_size': 10000}, 30.11954257101752)
INFO flwr 2024-04-19 12:45:02,880 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 12:45:02,880 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:45:11,558 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 12:45:12,647 | server.py:125 | fit progress: (3, 2.022127628326416, {'accuracy': 0.7296, 'data_size': 10000}, 39.88740112516098)
INFO flwr 2024-04-19 12:45:12,647 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 12:45:12,647 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:45:21,640 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 12:45:22,702 | server.py:125 | fit progress: (4, 1.9206851720809937, {'accuracy': 0.7525, 'data_size': 10000}, 49.94295977894217)
INFO flwr 2024-04-19 12:45:22,703 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 12:45:22,703 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:45:31,231 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 12:45:32,559 | server.py:125 | fit progress: (5, 1.8427658081054688, {'accuracy': 0.7722, 'data_size': 10000}, 59.79930730396882)
INFO flwr 2024-04-19 12:45:32,559 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 12:45:32,559 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:45:41,581 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 12:45:42,659 | server.py:125 | fit progress: (6, 1.7847346067428589, {'accuracy': 0.7884, 'data_size': 10000}, 69.89958771900274)
INFO flwr 2024-04-19 12:45:42,659 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 12:45:42,660 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:45:51,916 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 12:45:52,999 | server.py:125 | fit progress: (7, 1.7420119047164917, {'accuracy': 0.8023, 'data_size': 10000}, 80.23895376897417)
INFO flwr 2024-04-19 12:45:52,999 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 12:45:52,999 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:46:02,618 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 12:46:03,682 | server.py:125 | fit progress: (8, 1.7116906642913818, {'accuracy': 0.8132, 'data_size': 10000}, 90.92224168800749)
INFO flwr 2024-04-19 12:46:03,682 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 12:46:03,682 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:46:12,881 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 12:46:13,947 | server.py:125 | fit progress: (9, 1.6844576597213745, {'accuracy': 0.8293, 'data_size': 10000}, 101.1879428550601)
INFO flwr 2024-04-19 12:46:13,948 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 12:46:13,948 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:46:23,299 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 12:46:24,382 | server.py:125 | fit progress: (10, 1.6622326374053955, {'accuracy': 0.8446, 'data_size': 10000}, 111.62285861396231)
INFO flwr 2024-04-19 12:46:24,383 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 12:46:24,383 | server.py:153 | FL finished in 111.62346159294248
INFO flwr 2024-04-19 12:46:24,383 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 12:46:24,383 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 12:46:24,383 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 12:46:24,383 | app.py:229 | app_fit: losses_centralized [(0, 2.299149513244629), (1, 2.2410948276519775), (2, 2.1390602588653564), (3, 2.022127628326416), (4, 1.9206851720809937), (5, 1.8427658081054688), (6, 1.7847346067428589), (7, 1.7420119047164917), (8, 1.7116906642913818), (9, 1.6844576597213745), (10, 1.6622326374053955)]
INFO flwr 2024-04-19 12:46:24,384 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1618), (1, 0.5921), (2, 0.7071), (3, 0.7296), (4, 0.7525), (5, 0.7722), (6, 0.7884), (7, 0.8023), (8, 0.8132), (9, 0.8293), (10, 0.8446)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8446
wandb:     loss 1.66223
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_124412-we9py2xt
wandb: Find logs at: ./wandb/offline-run-20240419_124412-we9py2xt/logs
INFO flwr 2024-04-19 12:46:27,919 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 12:53:33,127 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1057579)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1057579)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 12:53:38,110	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 12:53:38,684	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 12:53:38,951	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 12:53:38,954	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 12:53:50,188 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54432442368.0, 'CPU': 64.0, 'memory': 117009032192.0}
INFO flwr 2024-04-19 12:53:50,188 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 12:53:50,188 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 12:53:50,204 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 12:53:50,205 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 12:53:50,205 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 12:53:50,205 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 12:53:53,229 | server.py:94 | initial parameters (loss, other metrics): 2.3048603534698486, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-19 12:53:53,230 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 12:53:53,230 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1061932)[0m 2024-04-19 12:53:56.331798: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1061932)[0m 2024-04-19 12:53:56.441125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1061932)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1061932)[0m 2024-04-19 12:53:58.582078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1061932)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1061932)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1061925)[0m 2024-04-19 12:53:56.705202: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1061925)[0m 2024-04-19 12:53:56.796498: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1061925)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1061925)[0m 2024-04-19 12:53:58.735569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 12:54:12,053 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 12:54:13,440 | server.py:125 | fit progress: (1, 2.300015687942505, {'accuracy': 0.1469, 'data_size': 10000}, 20.209766001906246)
INFO flwr 2024-04-19 12:54:13,440 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 12:54:13,441 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:54:23,334 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 12:54:24,406 | server.py:125 | fit progress: (2, 2.2933242321014404, {'accuracy': 0.2264, 'data_size': 10000}, 31.17595468694344)
INFO flwr 2024-04-19 12:54:24,406 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 12:54:24,407 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:54:33,967 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 12:54:35,024 | server.py:125 | fit progress: (3, 2.2850561141967773, {'accuracy': 0.3425, 'data_size': 10000}, 41.79413231788203)
INFO flwr 2024-04-19 12:54:35,025 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 12:54:35,025 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:54:43,895 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 12:54:44,968 | server.py:125 | fit progress: (4, 2.275426149368286, {'accuracy': 0.4485, 'data_size': 10000}, 51.73815305996686)
INFO flwr 2024-04-19 12:54:44,969 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 12:54:44,969 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:54:54,060 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 12:54:55,466 | server.py:125 | fit progress: (5, 2.2643773555755615, {'accuracy': 0.5409, 'data_size': 10000}, 62.2358785928227)
INFO flwr 2024-04-19 12:54:55,466 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 12:54:55,467 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:55:04,652 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 12:55:05,743 | server.py:125 | fit progress: (6, 2.252227783203125, {'accuracy': 0.6059, 'data_size': 10000}, 72.51277951081283)
INFO flwr 2024-04-19 12:55:05,743 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 12:55:05,744 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:55:14,364 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 12:55:15,647 | server.py:125 | fit progress: (7, 2.2389283180236816, {'accuracy': 0.6519, 'data_size': 10000}, 82.41682551987469)
INFO flwr 2024-04-19 12:55:15,647 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 12:55:15,648 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:55:24,687 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 12:55:26,003 | server.py:125 | fit progress: (8, 2.2244040966033936, {'accuracy': 0.6805, 'data_size': 10000}, 92.77239335584454)
INFO flwr 2024-04-19 12:55:26,003 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 12:55:26,003 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:55:35,952 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 12:55:37,020 | server.py:125 | fit progress: (9, 2.2089030742645264, {'accuracy': 0.6961, 'data_size': 10000}, 103.78969247196801)
INFO flwr 2024-04-19 12:55:37,020 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 12:55:37,020 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 12:55:45,928 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 12:55:47,287 | server.py:125 | fit progress: (10, 2.1926801204681396, {'accuracy': 0.7086, 'data_size': 10000}, 114.05663339677267)
INFO flwr 2024-04-19 12:55:47,287 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 12:55:47,287 | server.py:153 | FL finished in 114.05700173485093
INFO flwr 2024-04-19 12:55:47,287 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 12:55:47,287 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 12:55:47,288 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 12:55:47,288 | app.py:229 | app_fit: losses_centralized [(0, 2.3048603534698486), (1, 2.300015687942505), (2, 2.2933242321014404), (3, 2.2850561141967773), (4, 2.275426149368286), (5, 2.2643773555755615), (6, 2.252227783203125), (7, 2.2389283180236816), (8, 2.2244040966033936), (9, 2.2089030742645264), (10, 2.1926801204681396)]
INFO flwr 2024-04-19 12:55:47,288 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.1469), (2, 0.2264), (3, 0.3425), (4, 0.4485), (5, 0.5409), (6, 0.6059), (7, 0.6519), (8, 0.6805), (9, 0.6961), (10, 0.7086)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7086
wandb:     loss 2.19268
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_125332-p13amo34
wandb: Find logs at: ./wandb/offline-run-20240419_125332-p13amo34/logs
INFO flwr 2024-04-19 12:55:50,805 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 13:02:55,773 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1061925)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1061925)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 13:03:00,589	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 13:03:01,138	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 13:03:01,507	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 13:03:01,509	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 13:03:12,589 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 119212704768.0, 'object_store_memory': 55376873472.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-19 13:03:12,589 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 13:03:12,589 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 13:03:12,604 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 13:03:12,605 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 13:03:12,605 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 13:03:12,605 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 13:03:15,401 | server.py:94 | initial parameters (loss, other metrics): 2.3016185760498047, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-19 13:03:15,401 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 13:03:15,401 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1066565)[0m 2024-04-19 13:03:18.859602: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1066565)[0m 2024-04-19 13:03:18.956645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1066565)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1066561)[0m 2024-04-19 13:03:20.950062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1066572)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1066572)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1066570)[0m 2024-04-19 13:03:19.032390: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1066570)[0m 2024-04-19 13:03:19.121923: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1066570)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1066566)[0m 2024-04-19 13:03:21.372699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 13:03:34,529 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 13:03:35,590 | server.py:125 | fit progress: (1, 2.301130771636963, {'accuracy': 0.1282, 'data_size': 10000}, 20.18917856621556)
INFO flwr 2024-04-19 13:03:35,591 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 13:03:35,591 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:03:45,316 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 13:03:46,610 | server.py:125 | fit progress: (2, 2.3005058765411377, {'accuracy': 0.1316, 'data_size': 10000}, 31.208922433201224)
INFO flwr 2024-04-19 13:03:46,610 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 13:03:46,610 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:03:56,064 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 13:03:57,389 | server.py:125 | fit progress: (3, 2.2997841835021973, {'accuracy': 0.1376, 'data_size': 10000}, 41.98802692000754)
INFO flwr 2024-04-19 13:03:57,389 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 13:03:57,390 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:04:06,120 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 13:04:07,477 | server.py:125 | fit progress: (4, 2.2989845275878906, {'accuracy': 0.1437, 'data_size': 10000}, 52.075976527063176)
INFO flwr 2024-04-19 13:04:07,477 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 13:04:07,478 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:04:16,491 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 13:04:17,557 | server.py:125 | fit progress: (5, 2.2981150150299072, {'accuracy': 0.1512, 'data_size': 10000}, 62.15547006111592)
INFO flwr 2024-04-19 13:04:17,557 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 13:04:17,557 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:04:26,672 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 13:04:27,990 | server.py:125 | fit progress: (6, 2.297210693359375, {'accuracy': 0.1593, 'data_size': 10000}, 72.58885430800728)
INFO flwr 2024-04-19 13:04:27,990 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 13:04:27,990 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:04:37,837 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 13:04:38,899 | server.py:125 | fit progress: (7, 2.2962584495544434, {'accuracy': 0.1679, 'data_size': 10000}, 83.49756069108844)
INFO flwr 2024-04-19 13:04:38,899 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 13:04:38,899 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:04:48,541 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 13:04:49,603 | server.py:125 | fit progress: (8, 2.2952799797058105, {'accuracy': 0.1761, 'data_size': 10000}, 94.20228098402731)
INFO flwr 2024-04-19 13:04:49,604 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 13:04:49,604 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:04:59,012 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 13:05:00,384 | server.py:125 | fit progress: (9, 2.294281482696533, {'accuracy': 0.1844, 'data_size': 10000}, 104.9827759040054)
INFO flwr 2024-04-19 13:05:00,384 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 13:05:00,384 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:05:09,233 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 13:05:10,362 | server.py:125 | fit progress: (10, 2.293269634246826, {'accuracy': 0.1943, 'data_size': 10000}, 114.96083527710289)
INFO flwr 2024-04-19 13:05:10,362 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 13:05:10,362 | server.py:153 | FL finished in 114.96121042221785
INFO flwr 2024-04-19 13:05:10,362 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 13:05:10,363 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 13:05:10,363 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 13:05:10,363 | app.py:229 | app_fit: losses_centralized [(0, 2.3016185760498047), (1, 2.301130771636963), (2, 2.3005058765411377), (3, 2.2997841835021973), (4, 2.2989845275878906), (5, 2.2981150150299072), (6, 2.297210693359375), (7, 2.2962584495544434), (8, 2.2952799797058105), (9, 2.294281482696533), (10, 2.293269634246826)]
INFO flwr 2024-04-19 13:05:10,363 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.1282), (2, 0.1316), (3, 0.1376), (4, 0.1437), (5, 0.1512), (6, 0.1593), (7, 0.1679), (8, 0.1761), (9, 0.1844), (10, 0.1943)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1943
wandb:     loss 2.29327
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_130255-vrxhmcmn
wandb: Find logs at: ./wandb/offline-run-20240419_130255-vrxhmcmn/logs
INFO flwr 2024-04-19 13:05:13,957 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 13:12:19,147 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1066565)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1066565)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 13:12:25,008	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 13:12:25,411	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 13:12:25,668	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 13:12:25,669	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 13:12:36,711 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 119051946599.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 55307977113.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-19 13:12:36,711 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 13:12:36,711 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 13:12:36,731 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 13:12:36,733 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 13:12:36,733 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 13:12:36,733 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 13:12:40,004 | server.py:94 | initial parameters (loss, other metrics): 2.3034725189208984, {'accuracy': 0.077, 'data_size': 10000}
INFO flwr 2024-04-19 13:12:40,005 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 13:12:40,005 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1070910)[0m 2024-04-19 13:12:42.972846: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1070916)[0m 2024-04-19 13:12:43.029587: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1070916)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1070910)[0m 2024-04-19 13:12:45.166409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1070915)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1070915)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1070909)[0m 2024-04-19 13:12:43.236957: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1070909)[0m 2024-04-19 13:12:43.339187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1070909)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1070914)[0m 2024-04-19 13:12:45.484552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 13:12:58,163 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 13:12:59,580 | server.py:125 | fit progress: (1, 2.3034250736236572, {'accuracy': 0.0772, 'data_size': 10000}, 19.574496211018413)
INFO flwr 2024-04-19 13:12:59,580 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 13:12:59,580 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:13:09,313 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 13:13:10,408 | server.py:125 | fit progress: (2, 2.3033640384674072, {'accuracy': 0.0774, 'data_size': 10000}, 30.40246409806423)
INFO flwr 2024-04-19 13:13:10,408 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 13:13:10,408 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:13:19,722 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 13:13:21,118 | server.py:125 | fit progress: (3, 2.3032944202423096, {'accuracy': 0.078, 'data_size': 10000}, 41.11298779188655)
INFO flwr 2024-04-19 13:13:21,118 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 13:13:21,119 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:13:29,526 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 13:13:30,890 | server.py:125 | fit progress: (4, 2.303218126296997, {'accuracy': 0.0785, 'data_size': 10000}, 50.88491284195334)
INFO flwr 2024-04-19 13:13:30,890 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 13:13:30,890 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:13:40,226 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 13:13:41,311 | server.py:125 | fit progress: (5, 2.303135633468628, {'accuracy': 0.0791, 'data_size': 10000}, 61.305956376949325)
INFO flwr 2024-04-19 13:13:41,311 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 13:13:41,312 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:13:49,952 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 13:13:51,255 | server.py:125 | fit progress: (6, 2.3030498027801514, {'accuracy': 0.08, 'data_size': 10000}, 71.25016933307052)
INFO flwr 2024-04-19 13:13:51,256 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 13:13:51,256 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:13:59,928 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 13:14:01,250 | server.py:125 | fit progress: (7, 2.3029603958129883, {'accuracy': 0.081, 'data_size': 10000}, 81.24505722802132)
INFO flwr 2024-04-19 13:14:01,250 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 13:14:01,251 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:14:10,309 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 13:14:11,352 | server.py:125 | fit progress: (8, 2.3028693199157715, {'accuracy': 0.0819, 'data_size': 10000}, 91.34659814788029)
INFO flwr 2024-04-19 13:14:11,352 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 13:14:11,352 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:14:20,151 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 13:14:21,494 | server.py:125 | fit progress: (9, 2.3027760982513428, {'accuracy': 0.0824, 'data_size': 10000}, 101.48905052989721)
INFO flwr 2024-04-19 13:14:21,494 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 13:14:21,495 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:14:30,405 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 13:14:31,713 | server.py:125 | fit progress: (10, 2.302682399749756, {'accuracy': 0.083, 'data_size': 10000}, 111.70817480492406)
INFO flwr 2024-04-19 13:14:31,714 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 13:14:31,714 | server.py:153 | FL finished in 111.70854813396
INFO flwr 2024-04-19 13:14:31,714 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 13:14:31,714 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 13:14:31,714 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 13:14:31,714 | app.py:229 | app_fit: losses_centralized [(0, 2.3034725189208984), (1, 2.3034250736236572), (2, 2.3033640384674072), (3, 2.3032944202423096), (4, 2.303218126296997), (5, 2.303135633468628), (6, 2.3030498027801514), (7, 2.3029603958129883), (8, 2.3028693199157715), (9, 2.3027760982513428), (10, 2.302682399749756)]
INFO flwr 2024-04-19 13:14:31,714 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.077), (1, 0.0772), (2, 0.0774), (3, 0.078), (4, 0.0785), (5, 0.0791), (6, 0.08), (7, 0.081), (8, 0.0819), (9, 0.0824), (10, 0.083)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.083
wandb:     loss 2.30268
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_131218-9vv49djd
wandb: Find logs at: ./wandb/offline-run-20240419_131218-9vv49djd/logs
INFO flwr 2024-04-19 13:14:35,236 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 13:21:40,479 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1070910)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1070910)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 13:21:45,545	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 13:21:46,098	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 13:21:46,524	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 13:21:46,526	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 13:21:57,594 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55228887859.0, 'memory': 118867405005.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-19 13:21:57,594 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 13:21:57,594 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 13:21:57,610 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 13:21:57,612 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 13:21:57,612 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 13:21:57,612 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 13:21:59,911 | server.py:94 | initial parameters (loss, other metrics): 2.30568265914917, {'accuracy': 0.0528, 'data_size': 10000}
INFO flwr 2024-04-19 13:21:59,912 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 13:21:59,912 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1075279)[0m 2024-04-19 13:22:03.745521: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1075279)[0m 2024-04-19 13:22:03.838756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1075279)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1075285)[0m 2024-04-19 13:22:05.894882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1075285)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1075285)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1075286)[0m 2024-04-19 13:22:03.997593: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1075286)[0m 2024-04-19 13:22:04.093264: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1075286)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1075286)[0m 2024-04-19 13:22:06.251966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 13:22:19,193 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 13:22:20,511 | server.py:125 | fit progress: (1, 1.863115668296814, {'accuracy': 0.6631, 'data_size': 10000}, 20.598779986146837)
INFO flwr 2024-04-19 13:22:20,511 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 13:22:20,511 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:22:30,496 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 13:22:31,788 | server.py:125 | fit progress: (2, 1.7315040826797485, {'accuracy': 0.7396, 'data_size': 10000}, 31.87564903101884)
INFO flwr 2024-04-19 13:22:31,788 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 13:22:31,788 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:22:40,662 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 13:22:41,975 | server.py:125 | fit progress: (3, 1.7078169584274292, {'accuracy': 0.7557, 'data_size': 10000}, 42.062685589073226)
INFO flwr 2024-04-19 13:22:41,975 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 13:22:41,975 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:22:51,917 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 13:22:52,986 | server.py:125 | fit progress: (4, 1.6583521366119385, {'accuracy': 0.8042, 'data_size': 10000}, 53.07458772114478)
INFO flwr 2024-04-19 13:22:52,987 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 13:22:52,987 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:23:02,033 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 13:23:03,396 | server.py:125 | fit progress: (5, 1.5864747762680054, {'accuracy': 0.8759, 'data_size': 10000}, 63.4838962550275)
INFO flwr 2024-04-19 13:23:03,396 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 13:23:03,396 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:23:12,841 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 13:23:14,126 | server.py:125 | fit progress: (6, 1.6051212549209595, {'accuracy': 0.8575, 'data_size': 10000}, 74.21408810606226)
INFO flwr 2024-04-19 13:23:14,126 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 13:23:14,126 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:23:23,153 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 13:23:24,240 | server.py:125 | fit progress: (7, 1.5940977334976196, {'accuracy': 0.8686, 'data_size': 10000}, 84.32849133713171)
INFO flwr 2024-04-19 13:23:24,241 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 13:23:24,241 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:23:32,973 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 13:23:34,267 | server.py:125 | fit progress: (8, 1.5903106927871704, {'accuracy': 0.8685, 'data_size': 10000}, 94.35517530422658)
INFO flwr 2024-04-19 13:23:34,267 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 13:23:34,268 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:23:43,250 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 13:23:44,304 | server.py:125 | fit progress: (9, 1.569475769996643, {'accuracy': 0.8926, 'data_size': 10000}, 104.39247103314847)
INFO flwr 2024-04-19 13:23:44,305 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 13:23:44,305 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:23:53,695 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 13:23:54,755 | server.py:125 | fit progress: (10, 1.561643362045288, {'accuracy': 0.9002, 'data_size': 10000}, 114.84269030718133)
INFO flwr 2024-04-19 13:23:54,755 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 13:23:54,755 | server.py:153 | FL finished in 114.84304194105789
INFO flwr 2024-04-19 13:23:54,755 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 13:23:54,755 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 13:23:54,755 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 13:23:54,755 | app.py:229 | app_fit: losses_centralized [(0, 2.30568265914917), (1, 1.863115668296814), (2, 1.7315040826797485), (3, 1.7078169584274292), (4, 1.6583521366119385), (5, 1.5864747762680054), (6, 1.6051212549209595), (7, 1.5940977334976196), (8, 1.5903106927871704), (9, 1.569475769996643), (10, 1.561643362045288)]
INFO flwr 2024-04-19 13:23:54,755 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0528), (1, 0.6631), (2, 0.7396), (3, 0.7557), (4, 0.8042), (5, 0.8759), (6, 0.8575), (7, 0.8686), (8, 0.8685), (9, 0.8926), (10, 0.9002)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9002
wandb:     loss 1.56164
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_132140-em5ijcwp
wandb: Find logs at: ./wandb/offline-run-20240419_132140-em5ijcwp/logs
INFO flwr 2024-04-19 13:23:58,233 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 13:31:03,036 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1075276)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1075276)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 13:31:18,201	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 13:31:19,321	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 13:31:19,577	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 13:31:19,579	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 13:31:29,608 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60665590579.0, 'memory': 131553044685.0}
INFO flwr 2024-04-19 13:31:29,608 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 13:31:29,608 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 13:31:29,625 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 13:31:29,626 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 13:31:29,626 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 13:31:29,626 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 13:31:32,478 | server.py:94 | initial parameters (loss, other metrics): 2.3044581413269043, {'accuracy': 0.0625, 'data_size': 10000}
INFO flwr 2024-04-19 13:31:32,479 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 13:31:32,479 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1080830)[0m 2024-04-19 13:31:37.039278: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1080830)[0m 2024-04-19 13:31:37.179872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1080830)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1080830)[0m 2024-04-19 13:31:40.679353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1080830)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1080830)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1080821)[0m 2024-04-19 13:31:37.128574: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1080821)[0m 2024-04-19 13:31:37.224018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1080821)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1080821)[0m 2024-04-19 13:31:40.679352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 13:32:05,023 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 13:32:06,313 | server.py:125 | fit progress: (1, 2.2494287490844727, {'accuracy': 0.5277, 'data_size': 10000}, 33.83457735693082)
INFO flwr 2024-04-19 13:32:06,314 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 13:32:06,314 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:32:16,101 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 13:32:17,440 | server.py:125 | fit progress: (2, 2.1512036323547363, {'accuracy': 0.6234, 'data_size': 10000}, 44.960876399884)
INFO flwr 2024-04-19 13:32:17,440 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 13:32:17,440 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:32:26,289 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 13:32:27,280 | server.py:125 | fit progress: (3, 2.031764507293701, {'accuracy': 0.7103, 'data_size': 10000}, 54.80133665702306)
INFO flwr 2024-04-19 13:32:27,280 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 13:32:27,281 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:32:36,135 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 13:32:37,401 | server.py:125 | fit progress: (4, 1.9292806386947632, {'accuracy': 0.7467, 'data_size': 10000}, 64.92238670494407)
INFO flwr 2024-04-19 13:32:37,401 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 13:32:37,402 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:32:46,738 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 13:32:48,102 | server.py:125 | fit progress: (5, 1.8516263961791992, {'accuracy': 0.7718, 'data_size': 10000}, 75.62284911703318)
INFO flwr 2024-04-19 13:32:48,102 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 13:32:48,102 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:32:57,017 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 13:32:58,049 | server.py:125 | fit progress: (6, 1.793852686882019, {'accuracy': 0.7911, 'data_size': 10000}, 85.57002026494592)
INFO flwr 2024-04-19 13:32:58,049 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 13:32:58,049 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:33:06,724 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 13:33:07,955 | server.py:125 | fit progress: (7, 1.750620722770691, {'accuracy': 0.8057, 'data_size': 10000}, 95.47610216704197)
INFO flwr 2024-04-19 13:33:07,955 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 13:33:07,955 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:33:17,141 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 13:33:18,438 | server.py:125 | fit progress: (8, 1.718315839767456, {'accuracy': 0.8157, 'data_size': 10000}, 105.95880049304105)
INFO flwr 2024-04-19 13:33:18,438 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 13:33:18,438 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:33:26,990 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 13:33:27,962 | server.py:125 | fit progress: (9, 1.6927738189697266, {'accuracy': 0.8274, 'data_size': 10000}, 115.48341208091006)
INFO flwr 2024-04-19 13:33:27,962 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 13:33:27,963 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:33:36,885 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 13:33:38,156 | server.py:125 | fit progress: (10, 1.673120141029358, {'accuracy': 0.8357, 'data_size': 10000}, 125.67710323003121)
INFO flwr 2024-04-19 13:33:38,156 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 13:33:38,156 | server.py:153 | FL finished in 125.67750841006637
INFO flwr 2024-04-19 13:33:38,156 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 13:33:38,157 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 13:33:38,157 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 13:33:38,157 | app.py:229 | app_fit: losses_centralized [(0, 2.3044581413269043), (1, 2.2494287490844727), (2, 2.1512036323547363), (3, 2.031764507293701), (4, 1.9292806386947632), (5, 1.8516263961791992), (6, 1.793852686882019), (7, 1.750620722770691), (8, 1.718315839767456), (9, 1.6927738189697266), (10, 1.673120141029358)]
INFO flwr 2024-04-19 13:33:38,157 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0625), (1, 0.5277), (2, 0.6234), (3, 0.7103), (4, 0.7467), (5, 0.7718), (6, 0.7911), (7, 0.8057), (8, 0.8157), (9, 0.8274), (10, 0.8357)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8357
wandb:     loss 1.67312
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_133102-6leyqzk8
wandb: Find logs at: ./wandb/offline-run-20240419_133102-6leyqzk8/logs
INFO flwr 2024-04-19 13:33:41,615 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 13:40:47,655 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1080821)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1080821)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 13:40:52,641	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 13:40:53,181	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 13:40:53,545	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 13:40:53,546	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 13:41:04,660 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 132726044877.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61168304947.0}
INFO flwr 2024-04-19 13:41:04,660 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 13:41:04,660 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 13:41:04,675 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 13:41:04,676 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 13:41:04,676 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 13:41:04,676 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 13:41:07,759 | server.py:94 | initial parameters (loss, other metrics): 2.3048858642578125, {'accuracy': 0.0696, 'data_size': 10000}
INFO flwr 2024-04-19 13:41:07,760 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 13:41:07,760 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1086400)[0m 2024-04-19 13:41:10.946166: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1086400)[0m 2024-04-19 13:41:11.040793: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1086400)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1086400)[0m 2024-04-19 13:41:13.095574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1086412)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1086412)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1086406)[0m 2024-04-19 13:41:11.455305: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1086406)[0m 2024-04-19 13:41:11.552909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1086406)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1086406)[0m 2024-04-19 13:41:13.651222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 13:41:25,938 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 13:41:26,994 | server.py:125 | fit progress: (1, 2.300009250640869, {'accuracy': 0.1268, 'data_size': 10000}, 19.233370939968154)
INFO flwr 2024-04-19 13:41:26,994 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 13:41:26,994 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:41:36,729 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 13:41:38,062 | server.py:125 | fit progress: (2, 2.293426275253296, {'accuracy': 0.2278, 'data_size': 10000}, 30.302144845947623)
INFO flwr 2024-04-19 13:41:38,063 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 13:41:38,063 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:41:47,315 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 13:41:48,613 | server.py:125 | fit progress: (3, 2.285223960876465, {'accuracy': 0.3562, 'data_size': 10000}, 40.85264714900404)
INFO flwr 2024-04-19 13:41:48,613 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 13:41:48,613 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:41:58,077 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 13:41:59,164 | server.py:125 | fit progress: (4, 2.2756166458129883, {'accuracy': 0.4757, 'data_size': 10000}, 51.40339017682709)
INFO flwr 2024-04-19 13:41:59,164 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 13:41:59,164 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:42:08,148 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 13:42:09,474 | server.py:125 | fit progress: (5, 2.264786720275879, {'accuracy': 0.5653, 'data_size': 10000}, 61.713768081972376)
INFO flwr 2024-04-19 13:42:09,475 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 13:42:09,475 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:42:18,420 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 13:42:19,758 | server.py:125 | fit progress: (6, 2.252586841583252, {'accuracy': 0.6283, 'data_size': 10000}, 71.99733449378982)
INFO flwr 2024-04-19 13:42:19,758 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 13:42:19,758 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:42:28,722 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 13:42:29,790 | server.py:125 | fit progress: (7, 2.239198923110962, {'accuracy': 0.6743, 'data_size': 10000}, 82.0297369309701)
INFO flwr 2024-04-19 13:42:29,790 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 13:42:29,790 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:42:39,322 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 13:42:40,634 | server.py:125 | fit progress: (8, 2.224668264389038, {'accuracy': 0.6965, 'data_size': 10000}, 92.87347841798328)
INFO flwr 2024-04-19 13:42:40,634 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 13:42:40,634 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:42:49,792 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 13:42:51,131 | server.py:125 | fit progress: (9, 2.209451675415039, {'accuracy': 0.7145, 'data_size': 10000}, 103.37048839800991)
INFO flwr 2024-04-19 13:42:51,131 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 13:42:51,131 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:43:00,237 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 13:43:01,320 | server.py:125 | fit progress: (10, 2.193756103515625, {'accuracy': 0.7274, 'data_size': 10000}, 113.55930423992686)
INFO flwr 2024-04-19 13:43:01,320 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 13:43:01,320 | server.py:153 | FL finished in 113.5596712778788
INFO flwr 2024-04-19 13:43:01,320 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 13:43:01,320 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 13:43:01,320 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 13:43:01,320 | app.py:229 | app_fit: losses_centralized [(0, 2.3048858642578125), (1, 2.300009250640869), (2, 2.293426275253296), (3, 2.285223960876465), (4, 2.2756166458129883), (5, 2.264786720275879), (6, 2.252586841583252), (7, 2.239198923110962), (8, 2.224668264389038), (9, 2.209451675415039), (10, 2.193756103515625)]
INFO flwr 2024-04-19 13:43:01,320 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0696), (1, 0.1268), (2, 0.2278), (3, 0.3562), (4, 0.4757), (5, 0.5653), (6, 0.6283), (7, 0.6743), (8, 0.6965), (9, 0.7145), (10, 0.7274)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7274
wandb:     loss 2.19376
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_134047-am9h8xlz
wandb: Find logs at: ./wandb/offline-run-20240419_134047-am9h8xlz/logs
INFO flwr 2024-04-19 13:43:04,777 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 13:50:14,262 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1086399)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1086399)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 13:50:22,388	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 13:50:24,284	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 13:50:24,768	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 13:50:24,770	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 13:50:44,018 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 60979614105.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 132285766247.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-19 13:50:44,019 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 13:50:44,019 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 13:50:44,050 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 13:50:44,050 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 13:50:44,051 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 13:50:44,051 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 13:50:46,419 | server.py:94 | initial parameters (loss, other metrics): 2.3002536296844482, {'accuracy': 0.1356, 'data_size': 10000}
INFO flwr 2024-04-19 13:50:46,419 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 13:50:46,421 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1092776)[0m 2024-04-19 13:50:54.216928: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1092776)[0m 2024-04-19 13:50:54.315710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1092776)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1092776)[0m 2024-04-19 13:50:58.708546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1092781)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1092781)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1092781)[0m 2024-04-19 13:50:54.435999: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1092781)[0m 2024-04-19 13:50:54.529964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1092781)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1092777)[0m 2024-04-19 13:50:58.764772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 13:51:24,720 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 13:51:26,090 | server.py:125 | fit progress: (1, 2.2997875213623047, {'accuracy': 0.1389, 'data_size': 10000}, 39.66909123212099)
INFO flwr 2024-04-19 13:51:26,090 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 13:51:26,090 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:51:35,699 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 13:51:36,765 | server.py:125 | fit progress: (2, 2.2991647720336914, {'accuracy': 0.1439, 'data_size': 10000}, 50.34467324009165)
INFO flwr 2024-04-19 13:51:36,766 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 13:51:36,766 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:51:45,566 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 13:51:46,648 | server.py:125 | fit progress: (3, 2.2984354496002197, {'accuracy': 0.149, 'data_size': 10000}, 60.22734707710333)
INFO flwr 2024-04-19 13:51:46,648 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 13:51:46,648 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:51:55,343 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 13:51:56,409 | server.py:125 | fit progress: (4, 2.2976317405700684, {'accuracy': 0.154, 'data_size': 10000}, 69.98886792198755)
INFO flwr 2024-04-19 13:51:56,410 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 13:51:56,410 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:52:05,298 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 13:52:06,615 | server.py:125 | fit progress: (5, 2.296778440475464, {'accuracy': 0.1594, 'data_size': 10000}, 80.19481834489852)
INFO flwr 2024-04-19 13:52:06,616 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 13:52:06,616 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:52:15,630 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 13:52:16,703 | server.py:125 | fit progress: (6, 2.2958719730377197, {'accuracy': 0.1662, 'data_size': 10000}, 90.28221017401665)
INFO flwr 2024-04-19 13:52:16,703 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 13:52:16,703 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:52:25,367 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 13:52:26,447 | server.py:125 | fit progress: (7, 2.2949185371398926, {'accuracy': 0.1738, 'data_size': 10000}, 100.02682935190387)
INFO flwr 2024-04-19 13:52:26,448 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 13:52:26,448 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:52:35,170 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 13:52:36,230 | server.py:125 | fit progress: (8, 2.2939391136169434, {'accuracy': 0.1801, 'data_size': 10000}, 109.80984233412892)
INFO flwr 2024-04-19 13:52:36,231 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 13:52:36,231 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:52:45,329 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 13:52:46,673 | server.py:125 | fit progress: (9, 2.292928457260132, {'accuracy': 0.1875, 'data_size': 10000}, 120.25222237501293)
INFO flwr 2024-04-19 13:52:46,673 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 13:52:46,673 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 13:52:55,532 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 13:52:56,599 | server.py:125 | fit progress: (10, 2.2918899059295654, {'accuracy': 0.1953, 'data_size': 10000}, 130.17821694607846)
INFO flwr 2024-04-19 13:52:56,599 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 13:52:56,599 | server.py:153 | FL finished in 130.17877506604418
INFO flwr 2024-04-19 13:52:56,599 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 13:52:56,600 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 13:52:56,600 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 13:52:56,600 | app.py:229 | app_fit: losses_centralized [(0, 2.3002536296844482), (1, 2.2997875213623047), (2, 2.2991647720336914), (3, 2.2984354496002197), (4, 2.2976317405700684), (5, 2.296778440475464), (6, 2.2958719730377197), (7, 2.2949185371398926), (8, 2.2939391136169434), (9, 2.292928457260132), (10, 2.2918899059295654)]
INFO flwr 2024-04-19 13:52:56,600 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1356), (1, 0.1389), (2, 0.1439), (3, 0.149), (4, 0.154), (5, 0.1594), (6, 0.1662), (7, 0.1738), (8, 0.1801), (9, 0.1875), (10, 0.1953)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1953
wandb:     loss 2.29189
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_135010-0j26do49
wandb: Find logs at: ./wandb/offline-run-20240419_135010-0j26do49/logs
INFO flwr 2024-04-19 13:53:00,117 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 14:00:05,703 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1092776)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1092776)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 14:00:11,581	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 14:00:12,002	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 14:00:12,287	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 14:00:12,288	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 14:00:23,576 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 132050245837.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60878676787.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-19 14:00:23,576 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 14:00:23,576 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 14:00:23,591 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 14:00:23,591 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 14:00:23,592 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 14:00:23,592 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 14:00:25,849 | server.py:94 | initial parameters (loss, other metrics): 2.3023762702941895, {'accuracy': 0.0925, 'data_size': 10000}
INFO flwr 2024-04-19 14:00:25,851 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 14:00:25,851 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1097397)[0m 2024-04-19 14:00:29.872905: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1097397)[0m 2024-04-19 14:00:29.966905: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1097397)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1097391)[0m 2024-04-19 14:00:32.100624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1097393)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1097393)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1097396)[0m 2024-04-19 14:00:30.089137: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1097390)[0m 2024-04-19 14:00:30.132214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1097390)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1097394)[0m 2024-04-19 14:00:32.225739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 14:00:45,581 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 14:00:46,971 | server.py:125 | fit progress: (1, 2.3023288249969482, {'accuracy': 0.0927, 'data_size': 10000}, 21.12019112915732)
INFO flwr 2024-04-19 14:00:46,971 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 14:00:46,972 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:00:56,349 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 14:00:57,402 | server.py:125 | fit progress: (2, 2.30226731300354, {'accuracy': 0.0933, 'data_size': 10000}, 31.55067992117256)
INFO flwr 2024-04-19 14:00:57,402 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 14:00:57,402 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:01:06,753 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 14:01:08,073 | server.py:125 | fit progress: (3, 2.3021957874298096, {'accuracy': 0.0936, 'data_size': 10000}, 42.22251769108698)
INFO flwr 2024-04-19 14:01:08,074 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 14:01:08,074 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:01:17,005 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 14:01:18,341 | server.py:125 | fit progress: (4, 2.3021178245544434, {'accuracy': 0.0941, 'data_size': 10000}, 52.49023116007447)
INFO flwr 2024-04-19 14:01:18,341 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 14:01:18,342 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:01:27,518 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 14:01:28,622 | server.py:125 | fit progress: (5, 2.302034378051758, {'accuracy': 0.0945, 'data_size': 10000}, 62.7714932220988)
INFO flwr 2024-04-19 14:01:28,623 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 14:01:28,623 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:01:37,821 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 14:01:39,147 | server.py:125 | fit progress: (6, 2.3019485473632812, {'accuracy': 0.0947, 'data_size': 10000}, 73.29601822420955)
INFO flwr 2024-04-19 14:01:39,147 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 14:01:39,147 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:01:48,384 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 14:01:49,737 | server.py:125 | fit progress: (7, 2.3018579483032227, {'accuracy': 0.0956, 'data_size': 10000}, 83.88640567217954)
INFO flwr 2024-04-19 14:01:49,738 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 14:01:49,738 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:01:58,706 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 14:01:59,769 | server.py:125 | fit progress: (8, 2.301767110824585, {'accuracy': 0.0962, 'data_size': 10000}, 93.91786328912713)
INFO flwr 2024-04-19 14:01:59,769 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 14:01:59,769 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:02:08,676 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 14:02:10,037 | server.py:125 | fit progress: (9, 2.3016741275787354, {'accuracy': 0.0964, 'data_size': 10000}, 104.1858101550024)
INFO flwr 2024-04-19 14:02:10,037 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 14:02:10,037 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:02:19,562 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 14:02:20,908 | server.py:125 | fit progress: (10, 2.3015811443328857, {'accuracy': 0.0969, 'data_size': 10000}, 115.05748375412077)
INFO flwr 2024-04-19 14:02:20,909 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 14:02:20,909 | server.py:153 | FL finished in 115.05786360614002
INFO flwr 2024-04-19 14:02:20,909 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 14:02:20,909 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 14:02:20,909 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 14:02:20,909 | app.py:229 | app_fit: losses_centralized [(0, 2.3023762702941895), (1, 2.3023288249969482), (2, 2.30226731300354), (3, 2.3021957874298096), (4, 2.3021178245544434), (5, 2.302034378051758), (6, 2.3019485473632812), (7, 2.3018579483032227), (8, 2.301767110824585), (9, 2.3016741275787354), (10, 2.3015811443328857)]
INFO flwr 2024-04-19 14:02:20,909 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0925), (1, 0.0927), (2, 0.0933), (3, 0.0936), (4, 0.0941), (5, 0.0945), (6, 0.0947), (7, 0.0956), (8, 0.0962), (9, 0.0964), (10, 0.0969)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0969
wandb:     loss 2.30158
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_140005-iuas7cj0
wandb: Find logs at: ./wandb/offline-run-20240419_140005-iuas7cj0/logs
INFO flwr 2024-04-19 14:02:24,393 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 14:09:29,628 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1097397)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1097397)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 14:09:34,630	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 14:09:35,002	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 14:09:35,428	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 14:09:35,429	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 14:09:46,529 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 131842413978.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60789605990.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-19 14:09:46,529 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 14:09:46,530 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 14:09:46,548 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 14:09:46,549 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 14:09:46,549 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 14:09:46,549 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 14:09:48,533 | server.py:94 | initial parameters (loss, other metrics): 2.3067264556884766, {'accuracy': 0.0637, 'data_size': 10000}
INFO flwr 2024-04-19 14:09:48,534 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 14:09:48,535 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1101177)[0m 2024-04-19 14:09:52.744824: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1101177)[0m 2024-04-19 14:09:52.836393: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1101177)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1101180)[0m 2024-04-19 14:09:54.793158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1101180)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1101180)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1101187)[0m 2024-04-19 14:09:53.272356: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1101187)[0m 2024-04-19 14:09:53.366599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1101187)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1101187)[0m 2024-04-19 14:09:55.303561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 14:10:14,912 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 14:10:16,224 | server.py:125 | fit progress: (1, 1.9120043516159058, {'accuracy': 0.6046, 'data_size': 10000}, 27.689309110864997)
INFO flwr 2024-04-19 14:10:16,224 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 14:10:16,224 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:10:31,092 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 14:10:32,394 | server.py:125 | fit progress: (2, 1.6790460348129272, {'accuracy': 0.7915, 'data_size': 10000}, 43.85953143797815)
INFO flwr 2024-04-19 14:10:32,394 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 14:10:32,394 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:10:48,800 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 14:10:50,127 | server.py:125 | fit progress: (3, 1.6475419998168945, {'accuracy': 0.8198, 'data_size': 10000}, 61.59252261999063)
INFO flwr 2024-04-19 14:10:50,127 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 14:10:50,128 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:11:04,299 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 14:11:05,380 | server.py:125 | fit progress: (4, 1.5744649171829224, {'accuracy': 0.8927, 'data_size': 10000}, 76.84554661996663)
INFO flwr 2024-04-19 14:11:05,380 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 14:11:05,381 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:11:19,764 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 14:11:21,085 | server.py:125 | fit progress: (5, 1.5792896747589111, {'accuracy': 0.8843, 'data_size': 10000}, 92.55040341382846)
INFO flwr 2024-04-19 14:11:21,085 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 14:11:21,085 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:11:34,914 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 14:11:36,317 | server.py:125 | fit progress: (6, 1.5624632835388184, {'accuracy': 0.9013, 'data_size': 10000}, 107.782167943893)
INFO flwr 2024-04-19 14:11:36,317 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 14:11:36,317 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:11:49,468 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 14:11:50,527 | server.py:125 | fit progress: (7, 1.5645397901535034, {'accuracy': 0.9001, 'data_size': 10000}, 121.99275403702632)
INFO flwr 2024-04-19 14:11:50,528 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 14:11:50,528 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:12:04,198 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 14:12:05,507 | server.py:125 | fit progress: (8, 1.5924686193466187, {'accuracy': 0.8707, 'data_size': 10000}, 136.97264696704224)
INFO flwr 2024-04-19 14:12:05,507 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 14:12:05,508 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:12:19,632 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 14:12:20,732 | server.py:125 | fit progress: (9, 1.5815763473510742, {'accuracy': 0.8809, 'data_size': 10000}, 152.19719234202057)
INFO flwr 2024-04-19 14:12:20,732 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 14:12:20,732 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:12:34,899 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 14:12:35,966 | server.py:125 | fit progress: (10, 1.5503020286560059, {'accuracy': 0.914, 'data_size': 10000}, 167.43171202996746)
INFO flwr 2024-04-19 14:12:35,967 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 14:12:35,967 | server.py:153 | FL finished in 167.43213749094866
INFO flwr 2024-04-19 14:12:35,967 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 14:12:35,967 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 14:12:35,967 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 14:12:35,967 | app.py:229 | app_fit: losses_centralized [(0, 2.3067264556884766), (1, 1.9120043516159058), (2, 1.6790460348129272), (3, 1.6475419998168945), (4, 1.5744649171829224), (5, 1.5792896747589111), (6, 1.5624632835388184), (7, 1.5645397901535034), (8, 1.5924686193466187), (9, 1.5815763473510742), (10, 1.5503020286560059)]
INFO flwr 2024-04-19 14:12:35,967 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0637), (1, 0.6046), (2, 0.7915), (3, 0.8198), (4, 0.8927), (5, 0.8843), (6, 0.9013), (7, 0.9001), (8, 0.8707), (9, 0.8809), (10, 0.914)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.914
wandb:     loss 1.5503
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_140929-kjm6fh11
wandb: Find logs at: ./wandb/offline-run-20240419_140929-kjm6fh11/logs
INFO flwr 2024-04-19 14:12:39,474 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 14:19:45,427 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1101175)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1101175)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 14:19:51,294	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 14:19:51,776	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 14:19:52,033	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 14:19:52,035	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 14:20:03,150 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 131736373453.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 60744160051.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-19 14:20:03,150 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 14:20:03,151 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 14:20:03,174 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 14:20:03,175 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 14:20:03,175 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 14:20:03,176 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 14:20:05,825 | server.py:94 | initial parameters (loss, other metrics): 2.3047380447387695, {'accuracy': 0.0849, 'data_size': 10000}
INFO flwr 2024-04-19 14:20:05,826 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 14:20:05,827 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1105584)[0m 2024-04-19 14:20:09.237967: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1105584)[0m 2024-04-19 14:20:09.332198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1105584)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1105587)[0m 2024-04-19 14:20:11.386093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1105582)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1105582)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1105577)[0m 2024-04-19 14:20:09.556065: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1105577)[0m 2024-04-19 14:20:09.649332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1105577)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1105590)[0m 2024-04-19 14:20:11.875160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 14:20:30,853 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 14:20:32,239 | server.py:125 | fit progress: (1, 2.2478415966033936, {'accuracy': 0.5399, 'data_size': 10000}, 26.413078903919086)
INFO flwr 2024-04-19 14:20:32,240 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 14:20:32,240 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:20:47,304 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 14:20:48,615 | server.py:125 | fit progress: (2, 2.151106834411621, {'accuracy': 0.6284, 'data_size': 10000}, 42.78842039895244)
INFO flwr 2024-04-19 14:20:48,615 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 14:20:48,615 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:21:02,406 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 14:21:03,490 | server.py:125 | fit progress: (3, 2.0513806343078613, {'accuracy': 0.6562, 'data_size': 10000}, 57.663432280067354)
INFO flwr 2024-04-19 14:21:03,490 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 14:21:03,490 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:21:17,787 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 14:21:19,100 | server.py:125 | fit progress: (4, 1.9666974544525146, {'accuracy': 0.6752, 'data_size': 10000}, 73.27358318609186)
INFO flwr 2024-04-19 14:21:19,100 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 14:21:19,100 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:21:33,515 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 14:21:34,801 | server.py:125 | fit progress: (5, 1.8984599113464355, {'accuracy': 0.6952, 'data_size': 10000}, 88.97497730888426)
INFO flwr 2024-04-19 14:21:34,802 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 14:21:34,802 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:21:49,106 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 14:21:50,195 | server.py:125 | fit progress: (6, 1.8403843641281128, {'accuracy': 0.7205, 'data_size': 10000}, 104.36870462191291)
INFO flwr 2024-04-19 14:21:50,195 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 14:21:50,195 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:22:06,104 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 14:22:07,182 | server.py:125 | fit progress: (7, 1.7913964986801147, {'accuracy': 0.7552, 'data_size': 10000}, 121.35604026191868)
INFO flwr 2024-04-19 14:22:07,183 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 14:22:07,183 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:22:22,044 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 14:22:23,103 | server.py:125 | fit progress: (8, 1.7477431297302246, {'accuracy': 0.7863, 'data_size': 10000}, 137.27628140803427)
INFO flwr 2024-04-19 14:22:23,103 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 14:22:23,103 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:22:36,388 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 14:22:37,700 | server.py:125 | fit progress: (9, 1.7142223119735718, {'accuracy': 0.8061, 'data_size': 10000}, 151.8737022210844)
INFO flwr 2024-04-19 14:22:37,700 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 14:22:37,701 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:22:52,638 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 14:22:54,030 | server.py:125 | fit progress: (10, 1.686077356338501, {'accuracy': 0.8238, 'data_size': 10000}, 168.20351879508235)
INFO flwr 2024-04-19 14:22:54,030 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 14:22:54,030 | server.py:153 | FL finished in 168.20393974310718
INFO flwr 2024-04-19 14:22:54,030 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 14:22:54,031 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 14:22:54,031 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 14:22:54,031 | app.py:229 | app_fit: losses_centralized [(0, 2.3047380447387695), (1, 2.2478415966033936), (2, 2.151106834411621), (3, 2.0513806343078613), (4, 1.9666974544525146), (5, 1.8984599113464355), (6, 1.8403843641281128), (7, 1.7913964986801147), (8, 1.7477431297302246), (9, 1.7142223119735718), (10, 1.686077356338501)]
INFO flwr 2024-04-19 14:22:54,031 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0849), (1, 0.5399), (2, 0.6284), (3, 0.6562), (4, 0.6752), (5, 0.6952), (6, 0.7205), (7, 0.7552), (8, 0.7863), (9, 0.8061), (10, 0.8238)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8238
wandb:     loss 1.68608
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_141945-qm5hrylm
wandb: Find logs at: ./wandb/offline-run-20240419_141945-qm5hrylm/logs
INFO flwr 2024-04-19 14:22:57,556 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 14:30:03,827 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1105577)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1105577)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 14:30:09,489	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 14:30:09,969	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 14:30:10,295	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 14:30:10,297	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 14:30:21,340 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 60065616691.0, 'node:10.20.240.18': 1.0, 'memory': 130153105613.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-19 14:30:21,340 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 14:30:21,340 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 14:30:21,357 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 14:30:21,358 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 14:30:21,358 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 14:30:21,358 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 14:30:23,850 | server.py:94 | initial parameters (loss, other metrics): 2.3052616119384766, {'accuracy': 0.0876, 'data_size': 10000}
INFO flwr 2024-04-19 14:30:23,850 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 14:30:23,851 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1110231)[0m 2024-04-19 14:30:27.539528: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1110229)[0m 2024-04-19 14:30:27.675509: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1110229)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1110221)[0m 2024-04-19 14:30:29.708776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1110229)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1110229)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1110226)[0m 2024-04-19 14:30:28.025555: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1110226)[0m 2024-04-19 14:30:28.116748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1110226)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1110226)[0m 2024-04-19 14:30:30.076454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 14:30:49,742 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 14:30:51,101 | server.py:125 | fit progress: (1, 2.3007192611694336, {'accuracy': 0.1281, 'data_size': 10000}, 27.250396082177758)
INFO flwr 2024-04-19 14:30:51,101 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 14:30:51,102 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:31:06,023 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 14:31:07,380 | server.py:125 | fit progress: (2, 2.29436993598938, {'accuracy': 0.19, 'data_size': 10000}, 43.52893530600704)
INFO flwr 2024-04-19 14:31:07,380 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 14:31:07,380 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:31:21,659 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 14:31:22,979 | server.py:125 | fit progress: (3, 2.286573886871338, {'accuracy': 0.2747, 'data_size': 10000}, 59.12870986899361)
INFO flwr 2024-04-19 14:31:22,980 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 14:31:22,980 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:31:37,051 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 14:31:38,145 | server.py:125 | fit progress: (4, 2.2774386405944824, {'accuracy': 0.3715, 'data_size': 10000}, 74.2939462820068)
INFO flwr 2024-04-19 14:31:38,145 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 14:31:38,145 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:31:52,283 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 14:31:53,348 | server.py:125 | fit progress: (5, 2.2670094966888428, {'accuracy': 0.4549, 'data_size': 10000}, 89.49761294899508)
INFO flwr 2024-04-19 14:31:53,348 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 14:31:53,349 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:32:07,499 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 14:32:08,835 | server.py:125 | fit progress: (6, 2.2553131580352783, {'accuracy': 0.527, 'data_size': 10000}, 104.98459482099861)
INFO flwr 2024-04-19 14:32:08,836 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 14:32:08,836 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:32:21,835 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 14:32:23,139 | server.py:125 | fit progress: (7, 2.242539167404175, {'accuracy': 0.5902, 'data_size': 10000}, 119.28813980217092)
INFO flwr 2024-04-19 14:32:23,139 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 14:32:23,139 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:32:37,190 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 14:32:38,275 | server.py:125 | fit progress: (8, 2.2287988662719727, {'accuracy': 0.6424, 'data_size': 10000}, 134.42433808511123)
INFO flwr 2024-04-19 14:32:38,275 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 14:32:38,275 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:32:51,491 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 14:32:52,801 | server.py:125 | fit progress: (9, 2.214242696762085, {'accuracy': 0.6804, 'data_size': 10000}, 148.95021571801044)
INFO flwr 2024-04-19 14:32:52,801 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 14:32:52,801 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:33:06,530 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 14:33:07,848 | server.py:125 | fit progress: (10, 2.1989996433258057, {'accuracy': 0.7051, 'data_size': 10000}, 163.9975524500478)
INFO flwr 2024-04-19 14:33:07,848 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 14:33:07,849 | server.py:153 | FL finished in 163.99792215507478
INFO flwr 2024-04-19 14:33:07,849 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 14:33:07,849 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 14:33:07,849 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 14:33:07,849 | app.py:229 | app_fit: losses_centralized [(0, 2.3052616119384766), (1, 2.3007192611694336), (2, 2.29436993598938), (3, 2.286573886871338), (4, 2.2774386405944824), (5, 2.2670094966888428), (6, 2.2553131580352783), (7, 2.242539167404175), (8, 2.2287988662719727), (9, 2.214242696762085), (10, 2.1989996433258057)]
INFO flwr 2024-04-19 14:33:07,849 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0876), (1, 0.1281), (2, 0.19), (3, 0.2747), (4, 0.3715), (5, 0.4549), (6, 0.527), (7, 0.5902), (8, 0.6424), (9, 0.6804), (10, 0.7051)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7051
wandb:     loss 2.199
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_143003-9hrhanuz
wandb: Find logs at: ./wandb/offline-run-20240419_143003-9hrhanuz/logs
INFO flwr 2024-04-19 14:33:11,376 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 14:40:17,588 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1110220)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1110220)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 14:40:23,067	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 14:40:23,528	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 14:40:23,778	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 14:40:23,780	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 14:40:34,862 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58846516838.0, 'memory': 127308539290.0}
INFO flwr 2024-04-19 14:40:34,862 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 14:40:34,862 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 14:40:34,878 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 14:40:34,880 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 14:40:34,880 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 14:40:34,880 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 14:40:36,824 | server.py:94 | initial parameters (loss, other metrics): 2.3024184703826904, {'accuracy': 0.0784, 'data_size': 10000}
INFO flwr 2024-04-19 14:40:36,824 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 14:40:36,825 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1114930)[0m 2024-04-19 14:40:41.178859: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1114930)[0m 2024-04-19 14:40:41.273624: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1114930)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1114929)[0m 2024-04-19 14:40:43.261682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1114929)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1114929)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1114927)[0m 2024-04-19 14:40:41.387662: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1114934)[0m 2024-04-19 14:40:41.560022: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1114934)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1114927)[0m 2024-04-19 14:40:44.003201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 14:41:02,979 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 14:41:04,329 | server.py:125 | fit progress: (1, 2.3019487857818604, {'accuracy': 0.0806, 'data_size': 10000}, 27.50470275594853)
INFO flwr 2024-04-19 14:41:04,330 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 14:41:04,330 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:41:18,123 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 14:41:19,468 | server.py:125 | fit progress: (2, 2.3013453483581543, {'accuracy': 0.084, 'data_size': 10000}, 42.64378883410245)
INFO flwr 2024-04-19 14:41:19,469 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 14:41:19,469 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:41:33,540 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 14:41:34,579 | server.py:125 | fit progress: (3, 2.3006556034088135, {'accuracy': 0.0879, 'data_size': 10000}, 57.75490322802216)
INFO flwr 2024-04-19 14:41:34,580 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 14:41:34,580 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:41:48,322 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 14:41:49,651 | server.py:125 | fit progress: (4, 2.2998828887939453, {'accuracy': 0.0912, 'data_size': 10000}, 72.82670361013152)
INFO flwr 2024-04-19 14:41:49,652 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 14:41:49,652 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:42:02,796 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 14:42:04,093 | server.py:125 | fit progress: (5, 2.2990708351135254, {'accuracy': 0.0965, 'data_size': 10000}, 87.26877706102096)
INFO flwr 2024-04-19 14:42:04,094 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 14:42:04,094 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:42:17,958 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 14:42:19,004 | server.py:125 | fit progress: (6, 2.2982165813446045, {'accuracy': 0.1032, 'data_size': 10000}, 102.17932036798447)
INFO flwr 2024-04-19 14:42:19,004 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 14:42:19,004 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:42:32,737 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 14:42:33,781 | server.py:125 | fit progress: (7, 2.2973175048828125, {'accuracy': 0.1093, 'data_size': 10000}, 116.95671995799057)
INFO flwr 2024-04-19 14:42:33,782 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 14:42:33,782 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:42:49,997 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 14:42:51,096 | server.py:125 | fit progress: (8, 2.296382427215576, {'accuracy': 0.1142, 'data_size': 10000}, 134.27113473112695)
INFO flwr 2024-04-19 14:42:51,096 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 14:42:51,096 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:43:06,967 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 14:43:08,263 | server.py:125 | fit progress: (9, 2.2954258918762207, {'accuracy': 0.1218, 'data_size': 10000}, 151.43885106802918)
INFO flwr 2024-04-19 14:43:08,264 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 14:43:08,264 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:43:21,283 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 14:43:22,624 | server.py:125 | fit progress: (10, 2.2944414615631104, {'accuracy': 0.1299, 'data_size': 10000}, 165.79925522091798)
INFO flwr 2024-04-19 14:43:22,624 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 14:43:22,624 | server.py:153 | FL finished in 165.79965199902654
INFO flwr 2024-04-19 14:43:22,624 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 14:43:22,624 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 14:43:22,625 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 14:43:22,625 | app.py:229 | app_fit: losses_centralized [(0, 2.3024184703826904), (1, 2.3019487857818604), (2, 2.3013453483581543), (3, 2.3006556034088135), (4, 2.2998828887939453), (5, 2.2990708351135254), (6, 2.2982165813446045), (7, 2.2973175048828125), (8, 2.296382427215576), (9, 2.2954258918762207), (10, 2.2944414615631104)]
INFO flwr 2024-04-19 14:43:22,625 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0784), (1, 0.0806), (2, 0.084), (3, 0.0879), (4, 0.0912), (5, 0.0965), (6, 0.1032), (7, 0.1093), (8, 0.1142), (9, 0.1218), (10, 0.1299)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1299
wandb:     loss 2.29444
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_144017-xo9sozpc
wandb: Find logs at: ./wandb/offline-run-20240419_144017-xo9sozpc/logs
INFO flwr 2024-04-19 14:43:26,153 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 14:50:32,246 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1114927)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1114927)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 14:50:38,759	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 14:50:39,125	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 14:50:39,535	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 14:50:39,537	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 14:50:50,697 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 124288276685.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57552118579.0}
INFO flwr 2024-04-19 14:50:50,697 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 14:50:50,697 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 14:50:50,712 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 14:50:50,713 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 14:50:50,713 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 14:50:50,713 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 14:50:53,509 | server.py:94 | initial parameters (loss, other metrics): 2.3030407428741455, {'accuracy': 0.1123, 'data_size': 10000}
INFO flwr 2024-04-19 14:50:53,512 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 14:50:53,513 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1119528)[0m 2024-04-19 14:50:56.747948: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1119528)[0m 2024-04-19 14:50:56.850543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1119528)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1119528)[0m 2024-04-19 14:50:58.818082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1119530)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1119530)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1119530)[0m 2024-04-19 14:50:57.114679: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1119530)[0m 2024-04-19 14:50:57.209146: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1119530)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1119521)[0m 2024-04-19 14:50:59.397615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 14:51:18,793 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 14:51:20,184 | server.py:125 | fit progress: (1, 2.3029942512512207, {'accuracy': 0.113, 'data_size': 10000}, 26.67139474186115)
INFO flwr 2024-04-19 14:51:20,184 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 14:51:20,185 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:51:34,699 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 14:51:36,072 | server.py:125 | fit progress: (2, 2.302931308746338, {'accuracy': 0.1135, 'data_size': 10000}, 42.55913008796051)
INFO flwr 2024-04-19 14:51:36,072 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 14:51:36,072 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:51:50,262 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 14:51:51,329 | server.py:125 | fit progress: (3, 2.302858591079712, {'accuracy': 0.1145, 'data_size': 10000}, 57.8164549889043)
INFO flwr 2024-04-19 14:51:51,329 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 14:51:51,330 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:52:05,459 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 14:52:06,799 | server.py:125 | fit progress: (4, 2.302778959274292, {'accuracy': 0.1152, 'data_size': 10000}, 73.28626884287223)
INFO flwr 2024-04-19 14:52:06,799 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 14:52:06,799 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:52:19,706 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 14:52:21,051 | server.py:125 | fit progress: (5, 2.3026933670043945, {'accuracy': 0.1159, 'data_size': 10000}, 87.5384831649717)
INFO flwr 2024-04-19 14:52:21,052 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 14:52:21,052 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:52:35,077 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 14:52:36,165 | server.py:125 | fit progress: (6, 2.3026039600372314, {'accuracy': 0.1172, 'data_size': 10000}, 102.65242090402171)
INFO flwr 2024-04-19 14:52:36,165 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 14:52:36,166 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:52:51,037 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 14:52:52,126 | server.py:125 | fit progress: (7, 2.302511692047119, {'accuracy': 0.1189, 'data_size': 10000}, 118.61294645396993)
INFO flwr 2024-04-19 14:52:52,126 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 14:52:52,126 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:53:07,014 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 14:53:08,100 | server.py:125 | fit progress: (8, 2.3024163246154785, {'accuracy': 0.1197, 'data_size': 10000}, 134.58688989887014)
INFO flwr 2024-04-19 14:53:08,100 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 14:53:08,100 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:53:23,657 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 14:53:24,971 | server.py:125 | fit progress: (9, 2.3023200035095215, {'accuracy': 0.1209, 'data_size': 10000}, 151.45816990593448)
INFO flwr 2024-04-19 14:53:24,971 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 14:53:24,971 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 14:53:39,728 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 14:53:41,077 | server.py:125 | fit progress: (10, 2.30222225189209, {'accuracy': 0.1219, 'data_size': 10000}, 167.56435472192243)
INFO flwr 2024-04-19 14:53:41,077 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 14:53:41,078 | server.py:153 | FL finished in 167.56477459985763
INFO flwr 2024-04-19 14:53:41,078 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 14:53:41,078 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 14:53:41,078 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 14:53:41,078 | app.py:229 | app_fit: losses_centralized [(0, 2.3030407428741455), (1, 2.3029942512512207), (2, 2.302931308746338), (3, 2.302858591079712), (4, 2.302778959274292), (5, 2.3026933670043945), (6, 2.3026039600372314), (7, 2.302511692047119), (8, 2.3024163246154785), (9, 2.3023200035095215), (10, 2.30222225189209)]
INFO flwr 2024-04-19 14:53:41,078 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1123), (1, 0.113), (2, 0.1135), (3, 0.1145), (4, 0.1152), (5, 0.1159), (6, 0.1172), (7, 0.1189), (8, 0.1197), (9, 0.1209), (10, 0.1219)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1219
wandb:     loss 2.30222
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_145031-m9qzzbwh
wandb: Find logs at: ./wandb/offline-run-20240419_145031-m9qzzbwh/logs
INFO flwr 2024-04-19 14:53:44,549 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 15:00:50,820 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1119521)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1119521)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 15:00:55,286	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 15:00:55,847	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 15:00:56,193	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 15:00:56,194	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 15:01:07,268 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 121426105754.0, 'CPU': 64.0, 'object_store_memory': 56325473894.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-19 15:01:07,269 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 15:01:07,269 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 15:01:07,286 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 15:01:07,286 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 15:01:07,287 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 15:01:07,287 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 15:01:10,088 | server.py:94 | initial parameters (loss, other metrics): 2.305694818496704, {'accuracy': 0.1165, 'data_size': 10000}
INFO flwr 2024-04-19 15:01:10,088 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 15:01:10,089 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1124172)[0m 2024-04-19 15:01:13.255950: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1124172)[0m 2024-04-19 15:01:13.347612: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1124172)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1124176)[0m 2024-04-19 15:01:15.523397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1124179)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1124179)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1124168)[0m 2024-04-19 15:01:13.581214: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1124168)[0m 2024-04-19 15:01:13.688286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1124168)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1124173)[0m 2024-04-19 15:01:16.078283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 15:01:34,745 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 15:01:36,043 | server.py:125 | fit progress: (1, 1.8908385038375854, {'accuracy': 0.6393, 'data_size': 10000}, 25.954624607926235)
INFO flwr 2024-04-19 15:01:36,044 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 15:01:36,044 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:01:51,457 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 15:01:52,753 | server.py:125 | fit progress: (2, 1.683527946472168, {'accuracy': 0.8029, 'data_size': 10000}, 42.66446026507765)
INFO flwr 2024-04-19 15:01:52,753 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 15:01:52,754 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:02:05,573 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 15:02:06,645 | server.py:125 | fit progress: (3, 1.5889809131622314, {'accuracy': 0.8824, 'data_size': 10000}, 56.55672729504295)
INFO flwr 2024-04-19 15:02:06,646 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 15:02:06,646 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:02:20,966 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 15:02:22,275 | server.py:125 | fit progress: (4, 1.5731992721557617, {'accuracy': 0.893, 'data_size': 10000}, 72.18646777397953)
INFO flwr 2024-04-19 15:02:22,275 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 15:02:22,276 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:02:36,256 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 15:02:37,595 | server.py:125 | fit progress: (5, 1.569153904914856, {'accuracy': 0.8951, 'data_size': 10000}, 87.50637353304774)
INFO flwr 2024-04-19 15:02:37,595 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 15:02:37,595 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:02:51,346 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 15:02:52,401 | server.py:125 | fit progress: (6, 1.5676829814910889, {'accuracy': 0.8947, 'data_size': 10000}, 102.31246472196653)
INFO flwr 2024-04-19 15:02:52,401 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 15:02:52,402 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:03:07,534 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 15:03:08,610 | server.py:125 | fit progress: (7, 1.5647019147872925, {'accuracy': 0.8981, 'data_size': 10000}, 118.52157151210122)
INFO flwr 2024-04-19 15:03:08,611 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 15:03:08,611 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:03:22,968 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 15:03:24,032 | server.py:125 | fit progress: (8, 1.5612019300460815, {'accuracy': 0.9005, 'data_size': 10000}, 133.94388579693623)
INFO flwr 2024-04-19 15:03:24,033 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 15:03:24,033 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:03:37,599 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 15:03:38,938 | server.py:125 | fit progress: (9, 1.5634276866912842, {'accuracy': 0.8983, 'data_size': 10000}, 148.84983851597644)
INFO flwr 2024-04-19 15:03:38,939 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 15:03:38,939 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:03:52,886 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 15:03:54,198 | server.py:125 | fit progress: (10, 1.5587717294692993, {'accuracy': 0.9033, 'data_size': 10000}, 164.10988990194164)
INFO flwr 2024-04-19 15:03:54,199 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 15:03:54,199 | server.py:153 | FL finished in 164.11025479389355
INFO flwr 2024-04-19 15:03:54,199 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 15:03:54,199 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 15:03:54,199 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 15:03:54,199 | app.py:229 | app_fit: losses_centralized [(0, 2.305694818496704), (1, 1.8908385038375854), (2, 1.683527946472168), (3, 1.5889809131622314), (4, 1.5731992721557617), (5, 1.569153904914856), (6, 1.5676829814910889), (7, 1.5647019147872925), (8, 1.5612019300460815), (9, 1.5634276866912842), (10, 1.5587717294692993)]
INFO flwr 2024-04-19 15:03:54,199 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1165), (1, 0.6393), (2, 0.8029), (3, 0.8824), (4, 0.893), (5, 0.8951), (6, 0.8947), (7, 0.8981), (8, 0.9005), (9, 0.8983), (10, 0.9033)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9033
wandb:     loss 1.55877
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_150050-a7ubfv1h
wandb: Find logs at: ./wandb/offline-run-20240419_150050-a7ubfv1h/logs
INFO flwr 2024-04-19 15:03:57,445 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 15:11:04,795 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1124168)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1124168)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 15:11:10,384	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 15:11:10,801	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 15:11:11,209	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 15:11:11,211	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 15:11:22,403 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 115340697396.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 53717441740.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-19 15:11:22,403 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 15:11:22,403 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 15:11:22,419 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 15:11:22,420 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 15:11:22,421 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 15:11:22,421 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 15:11:24,475 | server.py:94 | initial parameters (loss, other metrics): 2.2981507778167725, {'accuracy': 0.1221, 'data_size': 10000}
INFO flwr 2024-04-19 15:11:24,475 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 15:11:24,475 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1128575)[0m 2024-04-19 15:11:28.283693: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1128575)[0m 2024-04-19 15:11:28.373740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1128575)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1128586)[0m 2024-04-19 15:11:30.588614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1128586)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1128586)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1128579)[0m 2024-04-19 15:11:29.058869: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1128578)[0m 2024-04-19 15:11:29.074530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1128578)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1128580)[0m 2024-04-19 15:11:31.553964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 15:11:52,689 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 15:11:54,052 | server.py:125 | fit progress: (1, 2.240079879760742, {'accuracy': 0.579, 'data_size': 10000}, 29.5770084138494)
INFO flwr 2024-04-19 15:11:54,053 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 15:11:54,053 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:12:08,147 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 15:12:09,582 | server.py:125 | fit progress: (2, 2.139111042022705, {'accuracy': 0.6581, 'data_size': 10000}, 45.10710507701151)
INFO flwr 2024-04-19 15:12:09,583 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 15:12:09,583 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:12:24,146 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 15:12:25,243 | server.py:125 | fit progress: (3, 2.0343949794769287, {'accuracy': 0.6799, 'data_size': 10000}, 60.76805477286689)
INFO flwr 2024-04-19 15:12:25,244 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 15:12:25,244 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:12:40,290 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 15:12:41,629 | server.py:125 | fit progress: (4, 1.948914647102356, {'accuracy': 0.7052, 'data_size': 10000}, 77.15345105202869)
INFO flwr 2024-04-19 15:12:41,629 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 15:12:41,629 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:12:55,461 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 15:12:56,914 | server.py:125 | fit progress: (5, 1.8844413757324219, {'accuracy': 0.7213, 'data_size': 10000}, 92.43893100693822)
INFO flwr 2024-04-19 15:12:56,914 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 15:12:56,915 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:13:11,449 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 15:13:12,542 | server.py:125 | fit progress: (6, 1.8322230577468872, {'accuracy': 0.7446, 'data_size': 10000}, 108.06646590400487)
INFO flwr 2024-04-19 15:13:12,542 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 15:13:12,543 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:13:26,910 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 15:13:28,003 | server.py:125 | fit progress: (7, 1.7873746156692505, {'accuracy': 0.7715, 'data_size': 10000}, 123.528146375902)
INFO flwr 2024-04-19 15:13:28,004 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 15:13:28,004 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:13:42,234 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 15:13:43,319 | server.py:125 | fit progress: (8, 1.7460980415344238, {'accuracy': 0.798, 'data_size': 10000}, 138.84397277492099)
INFO flwr 2024-04-19 15:13:43,319 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 15:13:43,320 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:13:57,303 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 15:13:58,623 | server.py:125 | fit progress: (9, 1.7092747688293457, {'accuracy': 0.8196, 'data_size': 10000}, 154.1479298248887)
INFO flwr 2024-04-19 15:13:58,623 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 15:13:58,624 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:14:12,740 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 15:14:14,086 | server.py:125 | fit progress: (10, 1.6787831783294678, {'accuracy': 0.8372, 'data_size': 10000}, 169.6103130809497)
INFO flwr 2024-04-19 15:14:14,086 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 15:14:14,086 | server.py:153 | FL finished in 169.61069795303047
INFO flwr 2024-04-19 15:14:14,086 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 15:14:14,086 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 15:14:14,086 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 15:14:14,086 | app.py:229 | app_fit: losses_centralized [(0, 2.2981507778167725), (1, 2.240079879760742), (2, 2.139111042022705), (3, 2.0343949794769287), (4, 1.948914647102356), (5, 1.8844413757324219), (6, 1.8322230577468872), (7, 1.7873746156692505), (8, 1.7460980415344238), (9, 1.7092747688293457), (10, 1.6787831783294678)]
INFO flwr 2024-04-19 15:14:14,086 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1221), (1, 0.579), (2, 0.6581), (3, 0.6799), (4, 0.7052), (5, 0.7213), (6, 0.7446), (7, 0.7715), (8, 0.798), (9, 0.8196), (10, 0.8372)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8372
wandb:     loss 1.67878
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_151104-fjk9ie5t
wandb: Find logs at: ./wandb/offline-run-20240419_151104-fjk9ie5t/logs
INFO flwr 2024-04-19 15:14:17,609 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 15:21:24,114 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1128572)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1128572)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 15:21:28,853	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 15:21:29,360	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 15:21:29,613	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 15:21:29,614	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 15:21:40,750 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 116841673728.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54360717312.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-19 15:21:40,751 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 15:21:40,751 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 15:21:40,765 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 15:21:40,767 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 15:21:40,772 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 15:21:40,772 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 15:21:43,410 | server.py:94 | initial parameters (loss, other metrics): 2.3058156967163086, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-19 15:21:43,418 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 15:21:43,420 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1132962)[0m 2024-04-19 15:21:46.873948: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1132962)[0m 2024-04-19 15:21:46.972315: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1132962)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1132952)[0m 2024-04-19 15:21:49.018421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1132958)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1132958)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1132958)[0m 2024-04-19 15:21:47.426001: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1132958)[0m 2024-04-19 15:21:47.530002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1132958)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1132949)[0m 2024-04-19 15:21:49.494099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 15:22:08,167 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 15:22:09,459 | server.py:125 | fit progress: (1, 2.301245927810669, {'accuracy': 0.1325, 'data_size': 10000}, 26.039584648096934)
INFO flwr 2024-04-19 15:22:09,460 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 15:22:09,460 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:22:23,817 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 15:22:25,138 | server.py:125 | fit progress: (2, 2.29508638381958, {'accuracy': 0.1787, 'data_size': 10000}, 41.71833526506089)
INFO flwr 2024-04-19 15:22:25,139 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 15:22:25,139 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:22:39,397 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 15:22:40,765 | server.py:125 | fit progress: (3, 2.2875096797943115, {'accuracy': 0.2465, 'data_size': 10000}, 57.34519738308154)
INFO flwr 2024-04-19 15:22:40,765 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 15:22:40,765 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:22:54,499 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 15:22:55,584 | server.py:125 | fit progress: (4, 2.278687000274658, {'accuracy': 0.329, 'data_size': 10000}, 72.16456459509209)
INFO flwr 2024-04-19 15:22:55,585 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 15:22:55,585 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:23:09,642 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 15:23:10,716 | server.py:125 | fit progress: (5, 2.2687392234802246, {'accuracy': 0.4214, 'data_size': 10000}, 87.29625513590872)
INFO flwr 2024-04-19 15:23:10,716 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 15:23:10,717 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:23:24,699 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 15:23:26,015 | server.py:125 | fit progress: (6, 2.2576823234558105, {'accuracy': 0.5134, 'data_size': 10000}, 102.59542097593658)
INFO flwr 2024-04-19 15:23:26,015 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 15:23:26,016 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:23:38,815 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 15:23:40,143 | server.py:125 | fit progress: (7, 2.24556827545166, {'accuracy': 0.5886, 'data_size': 10000}, 116.72304077306762)
INFO flwr 2024-04-19 15:23:40,143 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 15:23:40,143 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:23:54,047 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 15:23:55,113 | server.py:125 | fit progress: (8, 2.2324891090393066, {'accuracy': 0.6424, 'data_size': 10000}, 131.69329225691035)
INFO flwr 2024-04-19 15:23:55,113 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 15:23:55,113 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:24:08,046 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 15:24:09,352 | server.py:125 | fit progress: (9, 2.2183761596679688, {'accuracy': 0.679, 'data_size': 10000}, 145.93242711992934)
INFO flwr 2024-04-19 15:24:09,352 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 15:24:09,353 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:24:23,094 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 15:24:24,406 | server.py:125 | fit progress: (10, 2.2035419940948486, {'accuracy': 0.7053, 'data_size': 10000}, 160.98654158296995)
INFO flwr 2024-04-19 15:24:24,407 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 15:24:24,407 | server.py:153 | FL finished in 160.98737303493544
INFO flwr 2024-04-19 15:24:24,407 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 15:24:24,407 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 15:24:24,408 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 15:24:24,408 | app.py:229 | app_fit: losses_centralized [(0, 2.3058156967163086), (1, 2.301245927810669), (2, 2.29508638381958), (3, 2.2875096797943115), (4, 2.278687000274658), (5, 2.2687392234802246), (6, 2.2576823234558105), (7, 2.24556827545166), (8, 2.2324891090393066), (9, 2.2183761596679688), (10, 2.2035419940948486)]
INFO flwr 2024-04-19 15:24:24,408 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.1325), (2, 0.1787), (3, 0.2465), (4, 0.329), (5, 0.4214), (6, 0.5134), (7, 0.5886), (8, 0.6424), (9, 0.679), (10, 0.7053)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7053
wandb:     loss 2.20354
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_152123-qt6jbtlw
wandb: Find logs at: ./wandb/offline-run-20240419_152123-qt6jbtlw/logs
INFO flwr 2024-04-19 15:24:27,914 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 15:31:33,892 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1132949)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1132949)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 15:31:40,650	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 15:31:41,085	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 15:31:41,340	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 15:31:41,342	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 15:31:52,609 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 116691332096.0, 'node:__internal_head__': 1.0, 'object_store_memory': 54296285184.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-19 15:31:52,610 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 15:31:52,610 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 15:31:52,631 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 15:31:52,632 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 15:31:52,632 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 15:31:52,632 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 15:31:55,189 | server.py:94 | initial parameters (loss, other metrics): 2.304542303085327, {'accuracy': 0.0715, 'data_size': 10000}
INFO flwr 2024-04-19 15:31:55,189 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 15:31:55,190 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1137561)[0m 2024-04-19 15:31:58.822185: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1137561)[0m 2024-04-19 15:31:58.917127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1137561)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1137563)[0m 2024-04-19 15:32:01.075433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1137569)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1137569)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1137564)[0m 2024-04-19 15:31:59.293457: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1137564)[0m 2024-04-19 15:31:59.382970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1137564)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1137564)[0m 2024-04-19 15:32:01.952350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 15:32:20,605 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 15:32:21,887 | server.py:125 | fit progress: (1, 2.3040788173675537, {'accuracy': 0.0754, 'data_size': 10000}, 26.697959259152412)
INFO flwr 2024-04-19 15:32:21,888 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 15:32:21,888 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:32:36,182 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 15:32:37,492 | server.py:125 | fit progress: (2, 2.303473472595215, {'accuracy': 0.0784, 'data_size': 10000}, 42.30243916413747)
INFO flwr 2024-04-19 15:32:37,492 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 15:32:37,492 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:32:51,713 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 15:32:52,780 | server.py:125 | fit progress: (3, 2.302769422531128, {'accuracy': 0.0831, 'data_size': 10000}, 57.590403895126656)
INFO flwr 2024-04-19 15:32:52,780 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 15:32:52,781 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:33:06,277 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 15:33:07,603 | server.py:125 | fit progress: (4, 2.301995038986206, {'accuracy': 0.0894, 'data_size': 10000}, 72.4132286182139)
INFO flwr 2024-04-19 15:33:07,603 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 15:33:07,603 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:33:20,246 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 15:33:21,567 | server.py:125 | fit progress: (5, 2.3011627197265625, {'accuracy': 0.0954, 'data_size': 10000}, 86.37770460708998)
INFO flwr 2024-04-19 15:33:21,567 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 15:33:21,568 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:33:34,711 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 15:33:35,763 | server.py:125 | fit progress: (6, 2.300287961959839, {'accuracy': 0.1027, 'data_size': 10000}, 100.57385544013232)
INFO flwr 2024-04-19 15:33:35,764 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 15:33:35,764 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:33:49,504 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 15:33:50,585 | server.py:125 | fit progress: (7, 2.299379587173462, {'accuracy': 0.1096, 'data_size': 10000}, 115.39520827308297)
INFO flwr 2024-04-19 15:33:50,585 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 15:33:50,585 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:34:04,705 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 15:34:05,795 | server.py:125 | fit progress: (8, 2.2984578609466553, {'accuracy': 0.1186, 'data_size': 10000}, 130.60589086799882)
INFO flwr 2024-04-19 15:34:05,796 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 15:34:05,796 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:34:20,445 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 15:34:21,785 | server.py:125 | fit progress: (9, 2.2975142002105713, {'accuracy': 0.1278, 'data_size': 10000}, 146.5958116480615)
INFO flwr 2024-04-19 15:34:21,786 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 15:34:21,786 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:34:35,126 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 15:34:36,431 | server.py:125 | fit progress: (10, 2.2965524196624756, {'accuracy': 0.1361, 'data_size': 10000}, 161.24140054616146)
INFO flwr 2024-04-19 15:34:36,431 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 15:34:36,431 | server.py:153 | FL finished in 161.24177634506486
INFO flwr 2024-04-19 15:34:36,431 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 15:34:36,431 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 15:34:36,432 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 15:34:36,432 | app.py:229 | app_fit: losses_centralized [(0, 2.304542303085327), (1, 2.3040788173675537), (2, 2.303473472595215), (3, 2.302769422531128), (4, 2.301995038986206), (5, 2.3011627197265625), (6, 2.300287961959839), (7, 2.299379587173462), (8, 2.2984578609466553), (9, 2.2975142002105713), (10, 2.2965524196624756)]
INFO flwr 2024-04-19 15:34:36,432 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0715), (1, 0.0754), (2, 0.0784), (3, 0.0831), (4, 0.0894), (5, 0.0954), (6, 0.1027), (7, 0.1096), (8, 0.1186), (9, 0.1278), (10, 0.1361)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1361
wandb:     loss 2.29655
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_153133-c7rrd7aw
wandb: Find logs at: ./wandb/offline-run-20240419_153133-c7rrd7aw/logs
INFO flwr 2024-04-19 15:34:39,916 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 15:41:46,156 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1137561)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1137561)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 15:41:51,818	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 15:41:52,189	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 15:41:52,442	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 15:41:52,444	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 15:42:03,504 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 127203594036.0, 'node:__internal_head__': 1.0, 'object_store_memory': 58801540300.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-19 15:42:03,504 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 15:42:03,504 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 15:42:03,522 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 15:42:03,523 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 15:42:03,523 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 15:42:03,523 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 15:42:05,673 | server.py:94 | initial parameters (loss, other metrics): 2.3042690753936768, {'accuracy': 0.0929, 'data_size': 10000}
INFO flwr 2024-04-19 15:42:05,674 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 15:42:05,674 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1141954)[0m 2024-04-19 15:42:09.707690: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1141954)[0m 2024-04-19 15:42:09.799161: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1141954)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1141943)[0m 2024-04-19 15:42:11.809182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1141951)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1141951)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1141953)[0m 2024-04-19 15:42:10.029885: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1141953)[0m 2024-04-19 15:42:10.155753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1141953)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1141952)[0m 2024-04-19 15:42:12.151400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 15:42:31,415 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 15:42:32,767 | server.py:125 | fit progress: (1, 2.30422306060791, {'accuracy': 0.0933, 'data_size': 10000}, 27.09322105604224)
INFO flwr 2024-04-19 15:42:32,768 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 15:42:32,768 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:42:46,502 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 15:42:47,860 | server.py:125 | fit progress: (2, 2.3041629791259766, {'accuracy': 0.0935, 'data_size': 10000}, 42.186344613088295)
INFO flwr 2024-04-19 15:42:47,861 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 15:42:47,861 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:43:02,339 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 15:43:03,407 | server.py:125 | fit progress: (3, 2.304093837738037, {'accuracy': 0.0943, 'data_size': 10000}, 57.73315912508406)
INFO flwr 2024-04-19 15:43:03,407 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 15:43:03,408 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:43:16,498 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 15:43:17,876 | server.py:125 | fit progress: (4, 2.3040194511413574, {'accuracy': 0.0947, 'data_size': 10000}, 72.20147388498299)
INFO flwr 2024-04-19 15:43:17,876 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 15:43:17,876 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:43:31,736 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 15:43:33,026 | server.py:125 | fit progress: (5, 2.303940773010254, {'accuracy': 0.095, 'data_size': 10000}, 87.35181382112205)
INFO flwr 2024-04-19 15:43:33,026 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 15:43:33,026 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:43:47,544 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 15:43:48,617 | server.py:125 | fit progress: (6, 2.3038573265075684, {'accuracy': 0.0956, 'data_size': 10000}, 102.94274296192452)
INFO flwr 2024-04-19 15:43:48,617 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 15:43:48,617 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:44:03,826 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 15:44:04,901 | server.py:125 | fit progress: (7, 2.303771495819092, {'accuracy': 0.0965, 'data_size': 10000}, 119.22647722205147)
INFO flwr 2024-04-19 15:44:04,901 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 15:44:04,901 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:44:18,652 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 15:44:19,723 | server.py:125 | fit progress: (8, 2.3036837577819824, {'accuracy': 0.0987, 'data_size': 10000}, 134.04852910991758)
INFO flwr 2024-04-19 15:44:19,723 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 15:44:19,723 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:44:32,776 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 15:44:34,210 | server.py:125 | fit progress: (9, 2.303593397140503, {'accuracy': 0.0999, 'data_size': 10000}, 148.53591283410788)
INFO flwr 2024-04-19 15:44:34,210 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 15:44:34,211 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:44:48,830 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 15:44:50,217 | server.py:125 | fit progress: (10, 2.303501844406128, {'accuracy': 0.1007, 'data_size': 10000}, 164.54271845309995)
INFO flwr 2024-04-19 15:44:50,217 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 15:44:50,217 | server.py:153 | FL finished in 164.54317345796153
INFO flwr 2024-04-19 15:44:50,217 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 15:44:50,217 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 15:44:50,218 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 15:44:50,218 | app.py:229 | app_fit: losses_centralized [(0, 2.3042690753936768), (1, 2.30422306060791), (2, 2.3041629791259766), (3, 2.304093837738037), (4, 2.3040194511413574), (5, 2.303940773010254), (6, 2.3038573265075684), (7, 2.303771495819092), (8, 2.3036837577819824), (9, 2.303593397140503), (10, 2.303501844406128)]
INFO flwr 2024-04-19 15:44:50,218 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0929), (1, 0.0933), (2, 0.0935), (3, 0.0943), (4, 0.0947), (5, 0.095), (6, 0.0956), (7, 0.0965), (8, 0.0987), (9, 0.0999), (10, 0.1007)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1007
wandb:     loss 2.3035
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_154145-kpwdmj3r
wandb: Find logs at: ./wandb/offline-run-20240419_154145-kpwdmj3r/logs
INFO flwr 2024-04-19 15:44:53,735 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 15:52:00,143 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1141943)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1141943)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 15:52:04,696	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 15:52:05,131	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 15:52:05,383	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 15:52:05,384	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 15:52:16,701 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 127185983693.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58793993011.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-19 15:52:16,701 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 15:52:16,702 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 15:52:16,721 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 15:52:16,721 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 15:52:16,722 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 15:52:16,722 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 15:52:18,727 | server.py:94 | initial parameters (loss, other metrics): 2.3048839569091797, {'accuracy': 0.0672, 'data_size': 10000}
INFO flwr 2024-04-19 15:52:18,728 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 15:52:18,728 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1147579)[0m 2024-04-19 15:52:23.056799: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1147579)[0m 2024-04-19 15:52:23.152860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1147579)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1147584)[0m 2024-04-19 15:52:25.288378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1147575)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1147575)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1147571)[0m 2024-04-19 15:52:23.490409: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1147571)[0m 2024-04-19 15:52:23.586173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1147571)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1147578)[0m 2024-04-19 15:52:25.628674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 15:52:45,344 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 15:52:46,634 | server.py:125 | fit progress: (1, 1.8821121454238892, {'accuracy': 0.6611, 'data_size': 10000}, 27.906662706984207)
INFO flwr 2024-04-19 15:52:46,635 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 15:52:46,635 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:53:00,467 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 15:53:01,781 | server.py:125 | fit progress: (2, 1.6927560567855835, {'accuracy': 0.7912, 'data_size': 10000}, 43.052854019915685)
INFO flwr 2024-04-19 15:53:01,781 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 15:53:01,781 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:53:15,115 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 15:53:16,164 | server.py:125 | fit progress: (3, 1.608031988143921, {'accuracy': 0.8618, 'data_size': 10000}, 57.43667826312594)
INFO flwr 2024-04-19 15:53:16,165 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 15:53:16,165 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:53:29,570 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 15:53:30,900 | server.py:125 | fit progress: (4, 1.5774308443069458, {'accuracy': 0.8892, 'data_size': 10000}, 72.17213868699037)
INFO flwr 2024-04-19 15:53:30,900 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 15:53:30,900 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:53:44,552 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 15:53:45,866 | server.py:125 | fit progress: (5, 1.5686334371566772, {'accuracy': 0.8933, 'data_size': 10000}, 87.13834235793911)
INFO flwr 2024-04-19 15:53:45,866 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 15:53:45,867 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:54:00,174 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 15:54:01,262 | server.py:125 | fit progress: (6, 1.5607936382293701, {'accuracy': 0.9018, 'data_size': 10000}, 102.53465709998272)
INFO flwr 2024-04-19 15:54:01,263 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 15:54:01,263 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:54:13,979 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 15:54:15,063 | server.py:125 | fit progress: (7, 1.5538800954818726, {'accuracy': 0.9076, 'data_size': 10000}, 116.33491092105396)
INFO flwr 2024-04-19 15:54:15,063 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 15:54:15,063 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:54:28,675 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 15:54:29,721 | server.py:125 | fit progress: (8, 1.5546910762786865, {'accuracy': 0.9078, 'data_size': 10000}, 130.993598875124)
INFO flwr 2024-04-19 15:54:29,722 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 15:54:29,722 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:54:44,089 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 15:54:45,431 | server.py:125 | fit progress: (9, 1.5604228973388672, {'accuracy': 0.9004, 'data_size': 10000}, 146.7035470979754)
INFO flwr 2024-04-19 15:54:45,432 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 15:54:45,432 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 15:54:59,099 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 15:55:00,427 | server.py:125 | fit progress: (10, 1.556361436843872, {'accuracy': 0.9046, 'data_size': 10000}, 161.6992832799442)
INFO flwr 2024-04-19 15:55:00,427 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 15:55:00,428 | server.py:153 | FL finished in 161.69983021798544
INFO flwr 2024-04-19 15:55:00,428 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 15:55:00,428 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 15:55:00,428 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 15:55:00,428 | app.py:229 | app_fit: losses_centralized [(0, 2.3048839569091797), (1, 1.8821121454238892), (2, 1.6927560567855835), (3, 1.608031988143921), (4, 1.5774308443069458), (5, 1.5686334371566772), (6, 1.5607936382293701), (7, 1.5538800954818726), (8, 1.5546910762786865), (9, 1.5604228973388672), (10, 1.556361436843872)]
INFO flwr 2024-04-19 15:55:00,428 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0672), (1, 0.6611), (2, 0.7912), (3, 0.8618), (4, 0.8892), (5, 0.8933), (6, 0.9018), (7, 0.9076), (8, 0.9078), (9, 0.9004), (10, 0.9046)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9046
wandb:     loss 1.55636
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_155159-4ks3xkrj
wandb: Find logs at: ./wandb/offline-run-20240419_155159-4ks3xkrj/logs
INFO flwr 2024-04-19 15:55:03,949 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 16:02:10,429 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1147584)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1147584)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 16:02:15,106	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 16:02:15,679	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 16:02:16,036	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 16:02:16,038	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 16:02:27,152 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 58713598771.0, 'CPU': 64.0, 'memory': 126998397133.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-19 16:02:27,152 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 16:02:27,153 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 16:02:27,169 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 16:02:27,170 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 16:02:27,171 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 16:02:27,171 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 16:02:29,422 | server.py:94 | initial parameters (loss, other metrics): 2.3012635707855225, {'accuracy': 0.1222, 'data_size': 10000}
INFO flwr 2024-04-19 16:02:29,429 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 16:02:29,434 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1154453)[0m 2024-04-19 16:02:33.410655: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1154453)[0m 2024-04-19 16:02:33.528207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1154453)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1154455)[0m 2024-04-19 16:02:35.565307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1154455)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1154455)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1154450)[0m 2024-04-19 16:02:33.764291: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1154450)[0m 2024-04-19 16:02:33.856732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1154450)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1154450)[0m 2024-04-19 16:02:35.994817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1154460)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1154460)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
DEBUG flwr 2024-04-19 16:02:57,796 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 16:02:59,210 | server.py:125 | fit progress: (1, 2.244431495666504, {'accuracy': 0.6082, 'data_size': 10000}, 29.776129737030715)
INFO flwr 2024-04-19 16:02:59,210 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 16:02:59,210 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:03:13,300 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 16:03:14,640 | server.py:125 | fit progress: (2, 2.1448988914489746, {'accuracy': 0.6995, 'data_size': 10000}, 45.205770784989)
INFO flwr 2024-04-19 16:03:14,640 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 16:03:14,640 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:03:27,531 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 16:03:28,599 | server.py:125 | fit progress: (3, 2.032958984375, {'accuracy': 0.7213, 'data_size': 10000}, 59.16543215792626)
INFO flwr 2024-04-19 16:03:28,599 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 16:03:28,600 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:03:40,150 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 16:03:41,557 | server.py:125 | fit progress: (4, 1.940175175666809, {'accuracy': 0.742, 'data_size': 10000}, 72.12297429214232)
INFO flwr 2024-04-19 16:03:41,557 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 16:03:41,557 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:03:54,853 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 16:03:56,182 | server.py:125 | fit progress: (5, 1.8688445091247559, {'accuracy': 0.7643, 'data_size': 10000}, 86.74865261791274)
INFO flwr 2024-04-19 16:03:56,183 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 16:03:56,183 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:04:09,792 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 16:04:10,876 | server.py:125 | fit progress: (6, 1.8149211406707764, {'accuracy': 0.7797, 'data_size': 10000}, 101.44212315091863)
INFO flwr 2024-04-19 16:04:10,876 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 16:04:10,876 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:04:25,984 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 16:04:27,064 | server.py:125 | fit progress: (7, 1.7733232975006104, {'accuracy': 0.7929, 'data_size': 10000}, 117.63034122996032)
INFO flwr 2024-04-19 16:04:27,064 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 16:04:27,065 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:04:41,000 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 16:04:42,088 | server.py:125 | fit progress: (8, 1.7387402057647705, {'accuracy': 0.8091, 'data_size': 10000}, 132.65376514708623)
INFO flwr 2024-04-19 16:04:42,088 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 16:04:42,088 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:04:56,192 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 16:04:57,566 | server.py:125 | fit progress: (9, 1.7109571695327759, {'accuracy': 0.8196, 'data_size': 10000}, 148.13176249014214)
INFO flwr 2024-04-19 16:04:57,566 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 16:04:57,566 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:05:11,179 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 16:05:12,583 | server.py:125 | fit progress: (10, 1.6880978345870972, {'accuracy': 0.8292, 'data_size': 10000}, 163.14891535113566)
INFO flwr 2024-04-19 16:05:12,583 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 16:05:12,583 | server.py:153 | FL finished in 163.14929366693832
INFO flwr 2024-04-19 16:05:12,583 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 16:05:12,583 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 16:05:12,583 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 16:05:12,584 | app.py:229 | app_fit: losses_centralized [(0, 2.3012635707855225), (1, 2.244431495666504), (2, 2.1448988914489746), (3, 2.032958984375), (4, 1.940175175666809), (5, 1.8688445091247559), (6, 1.8149211406707764), (7, 1.7733232975006104), (8, 1.7387402057647705), (9, 1.7109571695327759), (10, 1.6880978345870972)]
INFO flwr 2024-04-19 16:05:12,584 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1222), (1, 0.6082), (2, 0.6995), (3, 0.7213), (4, 0.742), (5, 0.7643), (6, 0.7797), (7, 0.7929), (8, 0.8091), (9, 0.8196), (10, 0.8292)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8292
wandb:     loss 1.6881
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_160210-osjmk5wn
wandb: Find logs at: ./wandb/offline-run-20240419_160210-osjmk5wn/logs
INFO flwr 2024-04-19 16:05:16,065 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 16:12:21,828 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1154456)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1154456)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
2024-04-19 16:12:26,745	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 16:12:27,115	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 16:12:27,450	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 16:12:27,451	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 16:12:38,660 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 126954055885.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 58694595379.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-19 16:12:38,661 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 16:12:38,661 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 16:12:38,674 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 16:12:38,675 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 16:12:38,675 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 16:12:38,676 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 16:12:40,824 | server.py:94 | initial parameters (loss, other metrics): 2.3025295734405518, {'accuracy': 0.081, 'data_size': 10000}
INFO flwr 2024-04-19 16:12:40,825 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 16:12:40,825 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1158969)[0m 2024-04-19 16:12:44.861368: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1158969)[0m 2024-04-19 16:12:44.959045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1158969)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1158969)[0m 2024-04-19 16:12:47.159741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1158972)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1158972)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1158972)[0m 2024-04-19 16:12:45.108208: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1158972)[0m 2024-04-19 16:12:45.218885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1158972)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1158974)[0m 2024-04-19 16:12:47.386437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 16:13:06,502 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 16:13:07,792 | server.py:125 | fit progress: (1, 2.298039674758911, {'accuracy': 0.1365, 'data_size': 10000}, 26.967349532991648)
INFO flwr 2024-04-19 16:13:07,793 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 16:13:07,793 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:13:22,706 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 16:13:24,028 | server.py:125 | fit progress: (2, 2.2917938232421875, {'accuracy': 0.2279, 'data_size': 10000}, 43.2032531502191)
INFO flwr 2024-04-19 16:13:24,029 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 16:13:24,029 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:13:38,057 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 16:13:39,109 | server.py:125 | fit progress: (3, 2.2840793132781982, {'accuracy': 0.3249, 'data_size': 10000}, 58.28358144615777)
INFO flwr 2024-04-19 16:13:39,109 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 16:13:39,109 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:13:53,250 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 16:13:54,626 | server.py:125 | fit progress: (4, 2.2749507427215576, {'accuracy': 0.427, 'data_size': 10000}, 73.80110026011243)
INFO flwr 2024-04-19 16:13:54,626 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 16:13:54,627 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:14:08,820 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 16:14:10,167 | server.py:125 | fit progress: (5, 2.2646992206573486, {'accuracy': 0.515, 'data_size': 10000}, 89.34149123821408)
INFO flwr 2024-04-19 16:14:10,167 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 16:14:10,167 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:14:24,020 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 16:14:25,093 | server.py:125 | fit progress: (6, 2.253244400024414, {'accuracy': 0.5835, 'data_size': 10000}, 104.26779440720566)
INFO flwr 2024-04-19 16:14:25,093 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 16:14:25,094 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:14:39,366 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 16:14:40,436 | server.py:125 | fit progress: (7, 2.240677833557129, {'accuracy': 0.6357, 'data_size': 10000}, 119.61050111916848)
INFO flwr 2024-04-19 16:14:40,436 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 16:14:40,436 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:14:53,352 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 16:14:54,385 | server.py:125 | fit progress: (8, 2.2271595001220703, {'accuracy': 0.6773, 'data_size': 10000}, 133.56033104402013)
INFO flwr 2024-04-19 16:14:54,386 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 16:14:54,386 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:15:07,855 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 16:15:09,189 | server.py:125 | fit progress: (9, 2.2129077911376953, {'accuracy': 0.7069, 'data_size': 10000}, 148.36378271202557)
INFO flwr 2024-04-19 16:15:09,189 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 16:15:09,189 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:15:23,303 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 16:15:24,607 | server.py:125 | fit progress: (10, 2.198138475418091, {'accuracy': 0.7326, 'data_size': 10000}, 163.78208714118227)
INFO flwr 2024-04-19 16:15:24,607 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 16:15:24,608 | server.py:153 | FL finished in 163.7825318481773
INFO flwr 2024-04-19 16:15:24,608 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 16:15:24,608 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 16:15:24,608 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 16:15:24,608 | app.py:229 | app_fit: losses_centralized [(0, 2.3025295734405518), (1, 2.298039674758911), (2, 2.2917938232421875), (3, 2.2840793132781982), (4, 2.2749507427215576), (5, 2.2646992206573486), (6, 2.253244400024414), (7, 2.240677833557129), (8, 2.2271595001220703), (9, 2.2129077911376953), (10, 2.198138475418091)]
INFO flwr 2024-04-19 16:15:24,608 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.081), (1, 0.1365), (2, 0.2279), (3, 0.3249), (4, 0.427), (5, 0.515), (6, 0.5835), (7, 0.6357), (8, 0.6773), (9, 0.7069), (10, 0.7326)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7326
wandb:     loss 2.19814
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_161221-znu72tpd
wandb: Find logs at: ./wandb/offline-run-20240419_161221-znu72tpd/logs
INFO flwr 2024-04-19 16:15:28,117 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.0001, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 16:22:34,378 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1158969)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1158969)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 16:22:39,077	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 16:22:39,527	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 16:22:39,899	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 16:22:39,900	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
INFO flwr 2024-04-19 16:22:50,904 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 126803487744.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58630066176.0}
INFO flwr 2024-04-19 16:22:50,905 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-19 16:22:50,905 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-19 16:22:50,919 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-19 16:22:50,920 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-19 16:22:50,920 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-19 16:22:50,920 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-19 16:22:53,003 | server.py:94 | initial parameters (loss, other metrics): 2.3045308589935303, {'accuracy': 0.0968, 'data_size': 10000}
INFO flwr 2024-04-19 16:22:53,008 | server.py:104 | FL starting
DEBUG flwr 2024-04-19 16:22:53,009 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1163339)[0m 2024-04-19 16:22:57.139689: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1163339)[0m 2024-04-19 16:22:57.234616: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1163339)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1163339)[0m 2024-04-19 16:22:59.221484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1163333)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1163333)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1163340)[0m 2024-04-19 16:22:57.555309: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1163340)[0m 2024-04-19 16:22:57.653173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1163340)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1163340)[0m 2024-04-19 16:22:59.823229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-19 16:23:20,949 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-19 16:23:22,331 | server.py:125 | fit progress: (1, 2.304088830947876, {'accuracy': 0.1011, 'data_size': 10000}, 29.321672586956993)
INFO flwr 2024-04-19 16:23:22,331 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-19 16:23:22,332 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:23:37,069 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-19 16:23:38,409 | server.py:125 | fit progress: (2, 2.303492307662964, {'accuracy': 0.108, 'data_size': 10000}, 45.40049172495492)
INFO flwr 2024-04-19 16:23:38,410 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-19 16:23:38,410 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:23:51,914 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-19 16:23:52,974 | server.py:125 | fit progress: (3, 2.3028085231781006, {'accuracy': 0.1158, 'data_size': 10000}, 59.96537144994363)
INFO flwr 2024-04-19 16:23:52,975 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-19 16:23:52,975 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:24:06,864 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-19 16:24:08,198 | server.py:125 | fit progress: (4, 2.3020524978637695, {'accuracy': 0.1244, 'data_size': 10000}, 75.18879831582308)
INFO flwr 2024-04-19 16:24:08,198 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-19 16:24:08,198 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:24:22,513 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-19 16:24:23,825 | server.py:125 | fit progress: (5, 2.301239252090454, {'accuracy': 0.1351, 'data_size': 10000}, 90.81571014784276)
INFO flwr 2024-04-19 16:24:23,825 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-19 16:24:23,825 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:24:39,226 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-19 16:24:40,356 | server.py:125 | fit progress: (6, 2.3003783226013184, {'accuracy': 0.1469, 'data_size': 10000}, 107.34687999193557)
INFO flwr 2024-04-19 16:24:40,356 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-19 16:24:40,356 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:24:54,399 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-19 16:24:55,491 | server.py:125 | fit progress: (7, 2.299485683441162, {'accuracy': 0.158, 'data_size': 10000}, 122.48202426894568)
INFO flwr 2024-04-19 16:24:55,491 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-19 16:24:55,491 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:25:09,551 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-19 16:25:10,615 | server.py:125 | fit progress: (8, 2.298577070236206, {'accuracy': 0.171, 'data_size': 10000}, 137.60617747786455)
INFO flwr 2024-04-19 16:25:10,615 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-19 16:25:10,616 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:25:25,408 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-19 16:25:26,719 | server.py:125 | fit progress: (9, 2.2976486682891846, {'accuracy': 0.1824, 'data_size': 10000}, 153.70999520784244)
INFO flwr 2024-04-19 16:25:26,719 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-19 16:25:26,719 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-19 16:25:40,094 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-19 16:25:41,463 | server.py:125 | fit progress: (10, 2.2967026233673096, {'accuracy': 0.1953, 'data_size': 10000}, 168.454045265913)
INFO flwr 2024-04-19 16:25:41,463 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-19 16:25:41,463 | server.py:153 | FL finished in 168.45443058293313
INFO flwr 2024-04-19 16:25:41,463 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-19 16:25:41,464 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-19 16:25:41,464 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-19 16:25:41,464 | app.py:229 | app_fit: losses_centralized [(0, 2.3045308589935303), (1, 2.304088830947876), (2, 2.303492307662964), (3, 2.3028085231781006), (4, 2.3020524978637695), (5, 2.301239252090454), (6, 2.3003783226013184), (7, 2.299485683441162), (8, 2.298577070236206), (9, 2.2976486682891846), (10, 2.2967026233673096)]
INFO flwr 2024-04-19 16:25:41,464 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0968), (1, 0.1011), (2, 0.108), (3, 0.1158), (4, 0.1244), (5, 0.1351), (6, 0.1469), (7, 0.158), (8, 0.171), (9, 0.1824), (10, 0.1953)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1953
wandb:     loss 2.2967
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240419_162234-d8k5o9hw
wandb: Find logs at: ./wandb/offline-run-20240419_162234-d8k5o9hw/logs
INFO flwr 2024-04-19 16:25:44,985 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 1e-05, 'betas': [0.9, 0.99], 'eps': 1e-09, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-19 16:32:53,201 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1163332)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1163332)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-19 16:33:10,879	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-19 16:33:21,190	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-19 16:33:22,234	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip' (0.18MiB) to Ray cluster...
2024-04-19 16:33:22,237	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d38c73ec4bc6b26f.zip'.
slurmstepd-ctit088: error: *** STEP 281922.0 ON ctit088 CANCELLED AT 2024-04-22T10:21:18 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
*** SIGTERM received at time=1713774078 on cpu 62 ***
PC: @     0x7f3b2a4aa35c  (unknown)  recv
    @     0x7f3b2a3cd090  (unknown)  (unknown)
[2024-04-22 10:21:18,337 E 1028030 1028030] logging.cc:361: *** SIGTERM received at time=1713774078 on cpu 62 ***
[2024-04-22 10:21:18,338 E 1028030 1028030] logging.cc:361: PC: @     0x7f3b2a4aa35c  (unknown)  recv
[2024-04-22 10:21:18,338 E 1028030 1028030] logging.cc:361:     @     0x7f3b2a3cd090  (unknown)  (unknown)
slurmstepd-ctit088: error: *** JOB 281922 ON ctit088 CANCELLED AT 2024-04-22T10:21:18 ***
