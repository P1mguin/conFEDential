ctit084
2024-04-12 11:25:03.441157: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-12 11:25:04.097403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-12 11:25:05.923770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-12 11:25:16,727 | batch_run_simulation.py:80 | Loaded 36 configs with name MINST-CNN-FEDADAM, running...
INFO flwr 2024-04-12 11:25:16,727 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': [0.2, 0.25, 0.3]}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-12 11:32:35,713 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-12 11:32:44,769	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-12 11:33:01,107	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-12 11:33:01,586	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_57db9ab65c7076c7.zip' (12.69MiB) to Ray cluster...
2024-04-12 11:33:01,629	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_57db9ab65c7076c7.zip'.
INFO flwr 2024-04-12 11:33:12,755 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77929275801.0, 'node:__internal_head__': 1.0, 'accelerator_type:RTX': 1.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0, 'memory': 171834976871.0, 'CPU': 64.0}
INFO flwr 2024-04-12 11:33:12,755 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-12 11:33:12,755 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-12 11:33:12,773 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-12 11:33:12,774 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-12 11:33:12,774 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-12 11:33:12,774 | server.py:91 | Evaluating initial parameters
ERROR flwr 2024-04-12 11:33:18,613 | app.py:313 | Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10000, 784]
[2m[36m(pid=1307201)[0m 2024-04-12 11:33:18.616365: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1307208)[0m 2024-04-12 11:33:18.651059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1307208)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
ERROR flwr 2024-04-12 11:33:18,767 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 131, in evaluate
    loss, accuracy, data_size = Strategy.test(parameters, test_loader, run_config)
  File "/home/s2240084/conFEDential/src/training/strategies/Strategy.py", line 81, in test
    outputs = net(features)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/src/utils/configs/Model.py", line 43, in forward
    x = self.layers(x)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10000, 784]

ERROR flwr 2024-04-12 11:33:18,768 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240412_113232-4d3rpzio
wandb: Find logs at: ./wandb/offline-run-20240412_113232-4d3rpzio/logs
[2m[36m(pid=1307201)[0m 2024-04-12 11:33:22.081593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-12 11:33:22,439 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': [0.2, 0.25, 0.3]}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-12 11:40:26,695 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(pid=1307210)[0m 2024-04-12 11:33:18.654862: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1307210)[0m 2024-04-12 11:33:18.764201: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1307210)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1307204)[0m 2024-04-12 11:33:22.081608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
2024-04-12 11:40:31,127	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-12 11:40:31,623	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-12 11:40:32,087	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_428419eee0ea215e.zip' (13.86MiB) to Ray cluster...
2024-04-12 11:40:32,123	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_428419eee0ea215e.zip'.
INFO flwr 2024-04-12 11:40:41,793 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.14': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 74880357580.0, 'CPU': 64.0, 'accelerator_type:RTX': 1.0, 'memory': 164720834356.0}
INFO flwr 2024-04-12 11:40:41,793 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-12 11:40:41,794 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-12 11:40:41,810 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-12 11:40:41,812 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-12 11:40:41,812 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-12 11:40:41,812 | server.py:91 | Evaluating initial parameters
ERROR flwr 2024-04-12 11:40:43,759 | app.py:313 | Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10000, 784]
ERROR flwr 2024-04-12 11:40:43,766 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 131, in evaluate
    loss, accuracy, data_size = Strategy.test(parameters, test_loader, run_config)
  File "/home/s2240084/conFEDential/src/training/strategies/Strategy.py", line 81, in test
    outputs = net(features)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/src/utils/configs/Model.py", line 43, in forward
    x = self.layers(x)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10000, 784]

ERROR flwr 2024-04-12 11:40:43,766 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240412_114026-z172xiei
wandb: Find logs at: ./wandb/offline-run-20240412_114026-z172xiei/logs
[2m[36m(pid=1313291)[0m 2024-04-12 11:40:47.113982: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1313291)[0m 2024-04-12 11:40:47.204991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1313291)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-12 11:40:48,412 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': [0.2, 0.25, 0.3]}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
[2m[36m(pid=1313287)[0m 2024-04-12 11:40:48.871255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-12 11:47:52,939 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(pid=1313293)[0m 2024-04-12 11:40:47.279781: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1313293)[0m 2024-04-12 11:40:47.366018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1313293)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1313293)[0m 2024-04-12 11:40:48.920449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
2024-04-12 11:48:01,885	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-12 11:48:03,125	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-12 11:48:03,595	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ddd9c10462bec382.zip' (15.04MiB) to Ray cluster...
2024-04-12 11:48:03,642	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ddd9c10462bec382.zip'.
INFO flwr 2024-04-12 11:48:13,185 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 166757899674.0, 'object_store_memory': 75753385574.0, 'CPU': 64.0, 'accelerator_type:RTX': 1.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-12 11:48:13,186 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-12 11:48:13,186 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-12 11:48:13,200 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-12 11:48:13,202 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-12 11:48:13,202 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-12 11:48:13,203 | server.py:91 | Evaluating initial parameters
ERROR flwr 2024-04-12 11:48:16,108 | app.py:313 | Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10000, 784]
ERROR flwr 2024-04-12 11:48:16,112 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 131, in evaluate
    loss, accuracy, data_size = Strategy.test(parameters, test_loader, run_config)
  File "/home/s2240084/conFEDential/src/training/strategies/Strategy.py", line 81, in test
    outputs = net(features)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/src/utils/configs/Model.py", line 43, in forward
    x = self.layers(x)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10000, 784]

ERROR flwr 2024-04-12 11:48:16,112 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240412_114752-6kza2yu8
wandb: Find logs at: ./wandb/offline-run-20240412_114752-6kza2yu8/logs
[2m[36m(pid=1320628)[0m 2024-04-12 11:48:18.851232: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1320628)[0m 2024-04-12 11:48:18.969650: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1320628)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-12 11:48:20,159 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': [0.2, 0.25, 0.3]}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
[2m[36m(pid=1320628)[0m 2024-04-12 11:48:21.536635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-12 11:55:23,805 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(pid=1320623)[0m 2024-04-12 11:48:18.851062: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1320623)[0m 2024-04-12 11:48:18.979309: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1320623)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1320623)[0m 2024-04-12 11:48:21.536623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
2024-04-12 11:55:28,084	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-12 11:55:28,624	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-12 11:55:29,145	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fe233eb6e199cbcf.zip' (16.21MiB) to Ray cluster...
2024-04-12 11:55:29,195	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fe233eb6e199cbcf.zip'.
INFO flwr 2024-04-12 11:55:38,855 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 75598888550.0, 'accelerator_type:RTX': 1.0, 'memory': 166397406618.0, 'CPU': 64.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0}
INFO flwr 2024-04-12 11:55:38,855 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-12 11:55:38,855 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-12 11:55:38,871 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-12 11:55:38,872 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-12 11:55:38,872 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-12 11:55:38,873 | server.py:91 | Evaluating initial parameters
ERROR flwr 2024-04-12 11:55:41,163 | app.py:313 | Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10000, 784]
ERROR flwr 2024-04-12 11:55:41,167 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 131, in evaluate
    loss, accuracy, data_size = Strategy.test(parameters, test_loader, run_config)
  File "/home/s2240084/conFEDential/src/training/strategies/Strategy.py", line 81, in test
    outputs = net(features)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/src/utils/configs/Model.py", line 43, in forward
    x = self.layers(x)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [10000, 784]

ERROR flwr 2024-04-12 11:55:41,167 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240412_115523-e8ka3gpy
wandb: Find logs at: ./wandb/offline-run-20240412_115523-e8ka3gpy/logs
[2m[36m(pid=1327257)[0m 2024-04-12 11:55:44.196733: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1327250)[0m 2024-04-12 11:55:44.285185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1327250)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-12 11:55:45,050 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': [0.2, 0.25, 0.3]}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
[2m[36m(pid=1327257)[0m 2024-04-12 11:55:45.980440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-ctit084: error: *** STEP 280506.0 ON ctit084 CANCELLED AT 2024-04-12T12:00:16 ***
*** SIGTERM received at time=1712916016 on cpu 63 ***
slurmstepd-ctit084: error: *** JOB 280506 ON ctit084 CANCELLED AT 2024-04-12T12:00:16 ***
PC: @     0x7f4c2a1e499f  (unknown)  poll
    @     0x7f4c2a115090  (unknown)  (unknown)
    @ ... and at least 7 more frames
[2024-04-12 12:00:16,026 E 1301824 1301824] logging.cc:361: *** SIGTERM received at time=1712916016 on cpu 63 ***
[2024-04-12 12:00:16,026 E 1301824 1301824] logging.cc:361: PC: @     0x7f4c2a1e499f  (unknown)  poll
[2024-04-12 12:00:16,026 E 1301824 1301824] logging.cc:361:     @     0x7f4c2a115090  (unknown)  (unknown)
[2024-04-12 12:00:16,026 E 1301824 1301824] logging.cc:361:     @ ... and at least 7 more frames
