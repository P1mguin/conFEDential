ctit082
2024-04-15 02:18:29.594530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-15 02:18:53.694303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-15 02:21:09,564 | batch_run_simulation.py:80 | Loaded 144 configs with name MINST-LOGISTICREGRESSION-FEDADAM, running...
INFO flwr 2024-04-15 02:21:09,564 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 02:28:36,546 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-15 02:28:45,024	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 02:29:05,699	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 02:29:06,248	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 02:29:06,427	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 02:29:06,606	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 02:29:06,710	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (17.79MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 02:29:07,053	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_60c41ee6e014b53f.zip' (214.44MiB) to Ray cluster...
2024-04-15 02:29:07,981	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_60c41ee6e014b53f.zip'.
INFO flwr 2024-04-15 02:29:21,160 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 170893907354.0, 'object_store_memory': 77525960294.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-15 02:29:21,161 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 02:29:21,162 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 02:29:21,187 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 02:29:21,189 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 02:29:21,189 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 02:29:21,190 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1438953)[0m 2024-04-15 02:29:28.061415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1438953)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-15 02:29:30,272 | server.py:94 | initial parameters (loss, other metrics): 2.3044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-04-15 02:29:30,272 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 02:29:30,272 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1438951)[0m 2024-04-15 02:29:30.213888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1438948)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1438948)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1438949)[0m 2024-04-15 02:29:28.238654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1438949)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1438955)[0m 2024-04-15 02:29:30.346159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 02:29:58,374 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 02:29:59,894 | server.py:125 | fit progress: (1, 1.9954404830932617, {'accuracy': 0.5859, 'data_size': 10000}, 29.621254186989972)
INFO flwr 2024-04-15 02:29:59,894 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 02:29:59,895 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:30:09,974 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 02:30:11,457 | server.py:125 | fit progress: (2, 1.78217613697052, {'accuracy': 0.7386, 'data_size': 10000}, 41.184770797990495)
INFO flwr 2024-04-15 02:30:11,457 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 02:30:11,458 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:30:20,672 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 02:30:21,964 | server.py:125 | fit progress: (3, 1.679571270942688, {'accuracy': 0.8126, 'data_size': 10000}, 51.691663403995335)
INFO flwr 2024-04-15 02:30:21,964 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 02:30:21,964 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:30:31,258 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 02:30:32,549 | server.py:125 | fit progress: (4, 1.6144518852233887, {'accuracy': 0.8683, 'data_size': 10000}, 62.276666048011975)
INFO flwr 2024-04-15 02:30:32,549 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 02:30:32,549 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:30:41,671 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 02:30:43,201 | server.py:125 | fit progress: (5, 1.593704104423523, {'accuracy': 0.8791, 'data_size': 10000}, 72.92862906499067)
INFO flwr 2024-04-15 02:30:43,201 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 02:30:43,201 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:30:52,441 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 02:30:53,960 | server.py:125 | fit progress: (6, 1.5881599187850952, {'accuracy': 0.8827, 'data_size': 10000}, 83.6878614530142)
INFO flwr 2024-04-15 02:30:53,960 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 02:30:53,961 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:31:02,948 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 02:31:04,544 | server.py:125 | fit progress: (7, 1.5760079622268677, {'accuracy': 0.8914, 'data_size': 10000}, 94.27210195100633)
INFO flwr 2024-04-15 02:31:04,545 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 02:31:04,545 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:31:14,436 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 02:31:15,758 | server.py:125 | fit progress: (8, 1.5818567276000977, {'accuracy': 0.8837, 'data_size': 10000}, 105.48544111300725)
INFO flwr 2024-04-15 02:31:15,758 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 02:31:15,758 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:31:25,649 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 02:31:27,182 | server.py:125 | fit progress: (9, 1.6040599346160889, {'accuracy': 0.8618, 'data_size': 10000}, 116.90954040701035)
INFO flwr 2024-04-15 02:31:27,182 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 02:31:27,182 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:31:36,773 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 02:31:38,362 | server.py:125 | fit progress: (10, 1.5949751138687134, {'accuracy': 0.8703, 'data_size': 10000}, 128.0894090490183)
INFO flwr 2024-04-15 02:31:38,362 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 02:31:38,362 | server.py:153 | FL finished in 128.0899836079916
INFO flwr 2024-04-15 02:31:38,363 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 02:31:38,363 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 02:31:38,363 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 02:31:38,363 | app.py:229 | app_fit: losses_centralized [(0, 2.3044753074645996), (1, 1.9954404830932617), (2, 1.78217613697052), (3, 1.679571270942688), (4, 1.6144518852233887), (5, 1.593704104423523), (6, 1.5881599187850952), (7, 1.5760079622268677), (8, 1.5818567276000977), (9, 1.6040599346160889), (10, 1.5949751138687134)]
INFO flwr 2024-04-15 02:31:38,363 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0654), (1, 0.5859), (2, 0.7386), (3, 0.8126), (4, 0.8683), (5, 0.8791), (6, 0.8827), (7, 0.8914), (8, 0.8837), (9, 0.8618), (10, 0.8703)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8703
wandb:     loss 1.59498
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_022832-qwpq685k
wandb: Find logs at: ./wandb/offline-run-20240415_022832-qwpq685k/logs
INFO flwr 2024-04-15 02:31:41,881 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 02:38:55,942 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1438949)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1438949)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 02:39:00,744	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 02:39:02,089	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 02:39:02,588	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 02:39:02,781	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 02:39:02,964	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 02:39:03,068	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (17.81MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 02:39:03,408	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a481c53443634851.zip' (214.48MiB) to Ray cluster...
2024-04-15 02:39:04,289	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a481c53443634851.zip'.
INFO flwr 2024-04-15 02:39:17,653 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 78602497228.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 173405826868.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-15 02:39:17,654 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 02:39:17,654 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 02:39:17,681 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 02:39:17,682 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 02:39:17,682 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 02:39:17,682 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 02:39:21,877 | server.py:94 | initial parameters (loss, other metrics): 2.3022866249084473, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-15 02:39:21,877 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 02:39:21,878 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1443872)[0m 2024-04-15 02:39:24.444227: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1443872)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1443872)[0m 2024-04-15 02:39:26.774871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1443872)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1443872)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1443863)[0m 2024-04-15 02:39:24.688041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1443863)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1443863)[0m 2024-04-15 02:39:27.878122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1443864)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1443864)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-15 02:39:43,721 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 02:39:45,244 | server.py:125 | fit progress: (1, 1.8922033309936523, {'accuracy': 0.6042, 'data_size': 10000}, 23.366900730005)
INFO flwr 2024-04-15 02:39:45,245 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 02:39:45,245 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:39:54,507 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 02:39:55,988 | server.py:125 | fit progress: (2, 1.7237111330032349, {'accuracy': 0.7432, 'data_size': 10000}, 34.11011366802268)
INFO flwr 2024-04-15 02:39:55,988 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 02:39:55,988 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:40:05,514 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 02:40:06,795 | server.py:125 | fit progress: (3, 1.6721669435501099, {'accuracy': 0.7942, 'data_size': 10000}, 44.91713820601581)
INFO flwr 2024-04-15 02:40:06,795 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 02:40:06,795 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:40:15,500 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 02:40:17,007 | server.py:125 | fit progress: (4, 1.6131325960159302, {'accuracy': 0.8541, 'data_size': 10000}, 55.12928455401561)
INFO flwr 2024-04-15 02:40:17,007 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 02:40:17,007 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:40:26,618 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 02:40:28,133 | server.py:125 | fit progress: (5, 1.6383823156356812, {'accuracy': 0.8255, 'data_size': 10000}, 66.25556463902467)
INFO flwr 2024-04-15 02:40:28,133 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 02:40:28,134 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:40:37,684 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 02:40:39,350 | server.py:125 | fit progress: (6, 1.5830178260803223, {'accuracy': 0.8809, 'data_size': 10000}, 77.47228320201975)
INFO flwr 2024-04-15 02:40:39,350 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 02:40:39,350 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:40:49,144 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 02:40:50,413 | server.py:125 | fit progress: (7, 1.581230878829956, {'accuracy': 0.8808, 'data_size': 10000}, 88.53588410202065)
INFO flwr 2024-04-15 02:40:50,414 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 02:40:50,414 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:41:00,720 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 02:41:02,333 | server.py:125 | fit progress: (8, 1.5692782402038574, {'accuracy': 0.8928, 'data_size': 10000}, 100.45585990900872)
INFO flwr 2024-04-15 02:41:02,334 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 02:41:02,334 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:41:12,688 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 02:41:14,016 | server.py:125 | fit progress: (9, 1.5711705684661865, {'accuracy': 0.8917, 'data_size': 10000}, 112.13796857200214)
INFO flwr 2024-04-15 02:41:14,016 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 02:41:14,016 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:41:24,517 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 02:41:25,737 | server.py:125 | fit progress: (10, 1.5626906156539917, {'accuracy': 0.8996, 'data_size': 10000}, 123.85921463900013)
INFO flwr 2024-04-15 02:41:25,737 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 02:41:25,737 | server.py:153 | FL finished in 123.85969832001138
INFO flwr 2024-04-15 02:41:25,737 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 02:41:25,738 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 02:41:25,738 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 02:41:25,738 | app.py:229 | app_fit: losses_centralized [(0, 2.3022866249084473), (1, 1.8922033309936523), (2, 1.7237111330032349), (3, 1.6721669435501099), (4, 1.6131325960159302), (5, 1.6383823156356812), (6, 1.5830178260803223), (7, 1.581230878829956), (8, 1.5692782402038574), (9, 1.5711705684661865), (10, 1.5626906156539917)]
INFO flwr 2024-04-15 02:41:25,738 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.6042), (2, 0.7432), (3, 0.7942), (4, 0.8541), (5, 0.8255), (6, 0.8809), (7, 0.8808), (8, 0.8928), (9, 0.8917), (10, 0.8996)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8996
wandb:     loss 1.56269
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_023855-zornvkrg
wandb: Find logs at: ./wandb/offline-run-20240415_023855-zornvkrg/logs
INFO flwr 2024-04-15 02:41:29,290 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 02:48:39,910 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1443863)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1443863)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-15 02:48:45,059	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 02:48:46,303	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 02:48:46,868	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 02:48:47,049	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 02:48:47,230	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 02:48:47,327	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (17.96MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 02:48:47,646	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6a187da855453667.zip' (214.64MiB) to Ray cluster...
2024-04-15 02:48:48,552	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6a187da855453667.zip'.
INFO flwr 2024-04-15 02:49:00,622 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 78609422745.0, 'memory': 173421986407.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-15 02:49:00,623 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 02:49:00,623 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 02:49:00,644 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 02:49:00,645 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 02:49:00,645 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 02:49:00,645 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 02:49:04,170 | server.py:94 | initial parameters (loss, other metrics): 2.302816867828369, {'accuracy': 0.0734, 'data_size': 10000}
INFO flwr 2024-04-15 02:49:04,170 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 02:49:04,171 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1448792)[0m 2024-04-15 02:49:07.211366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1448792)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1448792)[0m 2024-04-15 02:49:09.695968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1448795)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1448795)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1448794)[0m 2024-04-15 02:49:07.881048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1448794)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1448798)[0m 2024-04-15 02:49:10.932883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 02:49:25,367 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 02:49:26,866 | server.py:125 | fit progress: (1, 1.823991060256958, {'accuracy': 0.6407, 'data_size': 10000}, 22.695285375986714)
INFO flwr 2024-04-15 02:49:26,866 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 02:49:26,867 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:49:36,699 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 02:49:38,122 | server.py:125 | fit progress: (2, 1.8196841478347778, {'accuracy': 0.6383, 'data_size': 10000}, 33.95103134398232)
INFO flwr 2024-04-15 02:49:38,122 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 02:49:38,122 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:49:47,057 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 02:49:48,482 | server.py:125 | fit progress: (3, 1.6906298398971558, {'accuracy': 0.7719, 'data_size': 10000}, 44.3114948469738)
INFO flwr 2024-04-15 02:49:48,482 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 02:49:48,483 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:49:56,397 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 02:49:57,804 | server.py:125 | fit progress: (4, 1.6957943439483643, {'accuracy': 0.7644, 'data_size': 10000}, 53.63389515297604)
INFO flwr 2024-04-15 02:49:57,805 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 02:49:57,805 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:50:06,971 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 02:50:08,197 | server.py:125 | fit progress: (5, 1.6142488718032837, {'accuracy': 0.8474, 'data_size': 10000}, 64.02652540599229)
INFO flwr 2024-04-15 02:50:08,197 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 02:50:08,198 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:50:17,393 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 02:50:18,880 | server.py:125 | fit progress: (6, 1.6034377813339233, {'accuracy': 0.857, 'data_size': 10000}, 74.70923055597814)
INFO flwr 2024-04-15 02:50:18,880 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 02:50:18,880 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:50:27,813 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 02:50:29,038 | server.py:125 | fit progress: (7, 1.6054636240005493, {'accuracy': 0.8547, 'data_size': 10000}, 84.86752379999962)
INFO flwr 2024-04-15 02:50:29,038 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 02:50:29,039 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:50:37,945 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 02:50:39,133 | server.py:125 | fit progress: (8, 1.608103632926941, {'accuracy': 0.8527, 'data_size': 10000}, 94.96286392299226)
INFO flwr 2024-04-15 02:50:39,134 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 02:50:39,134 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:50:48,893 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 02:50:50,349 | server.py:125 | fit progress: (9, 1.6040281057357788, {'accuracy': 0.8566, 'data_size': 10000}, 106.17791562099592)
INFO flwr 2024-04-15 02:50:50,349 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 02:50:50,349 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:50:59,257 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 02:51:00,668 | server.py:125 | fit progress: (10, 1.5952858924865723, {'accuracy': 0.8656, 'data_size': 10000}, 116.49763890198665)
INFO flwr 2024-04-15 02:51:00,669 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 02:51:00,669 | server.py:153 | FL finished in 116.49815578397829
INFO flwr 2024-04-15 02:51:00,669 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 02:51:00,669 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 02:51:00,669 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 02:51:00,669 | app.py:229 | app_fit: losses_centralized [(0, 2.302816867828369), (1, 1.823991060256958), (2, 1.8196841478347778), (3, 1.6906298398971558), (4, 1.6957943439483643), (5, 1.6142488718032837), (6, 1.6034377813339233), (7, 1.6054636240005493), (8, 1.608103632926941), (9, 1.6040281057357788), (10, 1.5952858924865723)]
INFO flwr 2024-04-15 02:51:00,669 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0734), (1, 0.6407), (2, 0.6383), (3, 0.7719), (4, 0.7644), (5, 0.8474), (6, 0.857), (7, 0.8547), (8, 0.8527), (9, 0.8566), (10, 0.8656)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8656
wandb:     loss 1.59529
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_024839-n10gwxph
wandb: Find logs at: ./wandb/offline-run-20240415_024839-n10gwxph/logs
INFO flwr 2024-04-15 02:51:04,221 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 02:58:15,584 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1448791)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1448791)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 02:58:20,739	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 02:58:22,193	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 02:58:22,701	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 02:58:22,876	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 02:58:23,051	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 02:58:23,150	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (18.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 02:58:23,475	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_36d4ca10bda7efd0.zip' (214.81MiB) to Ray cluster...
2024-04-15 02:58:24,336	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_36d4ca10bda7efd0.zip'.
INFO flwr 2024-04-15 02:58:36,706 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 76663256678.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 168880932250.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-15 02:58:36,706 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 02:58:36,706 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 02:58:36,723 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 02:58:36,725 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 02:58:36,725 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 02:58:36,726 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 02:58:39,740 | server.py:94 | initial parameters (loss, other metrics): 2.304152250289917, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-15 02:58:39,741 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 02:58:39,742 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1453789)[0m 2024-04-15 02:58:43.393978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1453789)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1453785)[0m 2024-04-15 02:58:45.871758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1453788)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1453788)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1453784)[0m 2024-04-15 02:58:43.572955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1453784)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1453784)[0m 2024-04-15 02:58:46.224433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 02:59:01,888 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 02:59:03,404 | server.py:125 | fit progress: (1, 2.0067975521087646, {'accuracy': 0.6179, 'data_size': 10000}, 23.661667905020295)
INFO flwr 2024-04-15 02:59:03,404 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 02:59:03,404 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:59:13,332 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 02:59:14,768 | server.py:125 | fit progress: (2, 1.7355531454086304, {'accuracy': 0.8101, 'data_size': 10000}, 35.02612394600874)
INFO flwr 2024-04-15 02:59:14,768 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 02:59:14,769 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:59:23,579 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 02:59:24,986 | server.py:125 | fit progress: (3, 1.6417649984359741, {'accuracy': 0.8572, 'data_size': 10000}, 45.244292998017045)
INFO flwr 2024-04-15 02:59:24,987 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 02:59:24,987 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:59:33,713 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 02:59:35,114 | server.py:125 | fit progress: (4, 1.6041913032531738, {'accuracy': 0.8755, 'data_size': 10000}, 55.37174017401412)
INFO flwr 2024-04-15 02:59:35,114 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 02:59:35,114 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:59:44,041 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 02:59:45,258 | server.py:125 | fit progress: (5, 1.5837167501449585, {'accuracy': 0.8881, 'data_size': 10000}, 65.51565095901606)
INFO flwr 2024-04-15 02:59:45,258 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 02:59:45,258 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 02:59:55,037 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 02:59:56,243 | server.py:125 | fit progress: (6, 1.5736312866210938, {'accuracy': 0.8957, 'data_size': 10000}, 76.50136829100666)
INFO flwr 2024-04-15 02:59:56,244 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 02:59:56,244 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:00:05,126 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 03:00:06,525 | server.py:125 | fit progress: (7, 1.5719901323318481, {'accuracy': 0.895, 'data_size': 10000}, 86.78259616301511)
INFO flwr 2024-04-15 03:00:06,525 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 03:00:06,525 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:00:15,998 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 03:00:17,476 | server.py:125 | fit progress: (8, 1.5718740224838257, {'accuracy': 0.8934, 'data_size': 10000}, 97.7336340630136)
INFO flwr 2024-04-15 03:00:17,476 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 03:00:17,476 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:00:26,500 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 03:00:27,711 | server.py:125 | fit progress: (9, 1.5723379850387573, {'accuracy': 0.8936, 'data_size': 10000}, 107.96854094401351)
INFO flwr 2024-04-15 03:00:27,711 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 03:00:27,711 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:00:38,214 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 03:00:39,670 | server.py:125 | fit progress: (10, 1.5671583414077759, {'accuracy': 0.8988, 'data_size': 10000}, 119.92775651600095)
INFO flwr 2024-04-15 03:00:39,670 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 03:00:39,670 | server.py:153 | FL finished in 119.92821634601569
INFO flwr 2024-04-15 03:00:39,670 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 03:00:39,670 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 03:00:39,671 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 03:00:39,671 | app.py:229 | app_fit: losses_centralized [(0, 2.304152250289917), (1, 2.0067975521087646), (2, 1.7355531454086304), (3, 1.6417649984359741), (4, 1.6041913032531738), (5, 1.5837167501449585), (6, 1.5736312866210938), (7, 1.5719901323318481), (8, 1.5718740224838257), (9, 1.5723379850387573), (10, 1.5671583414077759)]
INFO flwr 2024-04-15 03:00:39,671 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.6179), (2, 0.8101), (3, 0.8572), (4, 0.8755), (5, 0.8881), (6, 0.8957), (7, 0.895), (8, 0.8934), (9, 0.8936), (10, 0.8988)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8988
wandb:     loss 1.56716
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_025815-mrsl3wwt
wandb: Find logs at: ./wandb/offline-run-20240415_025815-mrsl3wwt/logs
INFO flwr 2024-04-15 03:00:43,294 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 03:07:54,633 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1453783)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1453783)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 03:07:59,747	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 03:08:01,004	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 03:08:01,526	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 03:08:01,696	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 03:08:01,869	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 03:08:01,966	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (18.27MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 03:08:02,291	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b9093e5843f437ce.zip' (214.97MiB) to Ray cluster...
2024-04-15 03:08:03,133	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b9093e5843f437ce.zip'.
INFO flwr 2024-04-15 03:08:16,959 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 169448543232.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 76906518528.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-15 03:08:16,959 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 03:08:16,960 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 03:08:16,979 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 03:08:16,981 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 03:08:16,981 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 03:08:16,982 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 03:08:21,568 | server.py:94 | initial parameters (loss, other metrics): 2.3035645484924316, {'accuracy': 0.0768, 'data_size': 10000}
INFO flwr 2024-04-15 03:08:21,568 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 03:08:21,568 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1458171)[0m 2024-04-15 03:08:23.374918: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1458171)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1458164)[0m 2024-04-15 03:08:26.082585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1458166)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1458166)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1458162)[0m 2024-04-15 03:08:23.828663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1458162)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1458162)[0m 2024-04-15 03:08:26.347752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 03:08:41,893 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 03:08:43,326 | server.py:125 | fit progress: (1, 1.8885856866836548, {'accuracy': 0.6162, 'data_size': 10000}, 21.757701353984885)
INFO flwr 2024-04-15 03:08:43,326 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 03:08:43,326 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:08:52,736 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 03:08:53,947 | server.py:125 | fit progress: (2, 1.6727089881896973, {'accuracy': 0.805, 'data_size': 10000}, 32.37836087998585)
INFO flwr 2024-04-15 03:08:53,947 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 03:08:53,947 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:09:02,946 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 03:09:04,139 | server.py:125 | fit progress: (3, 1.5984504222869873, {'accuracy': 0.8736, 'data_size': 10000}, 42.57044082597713)
INFO flwr 2024-04-15 03:09:04,139 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 03:09:04,139 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:09:12,994 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 03:09:14,226 | server.py:125 | fit progress: (4, 1.5847117900848389, {'accuracy': 0.8795, 'data_size': 10000}, 52.658085204981035)
INFO flwr 2024-04-15 03:09:14,227 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 03:09:14,227 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:09:22,340 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 03:09:23,804 | server.py:125 | fit progress: (5, 1.5804873704910278, {'accuracy': 0.8853, 'data_size': 10000}, 62.23580903498805)
INFO flwr 2024-04-15 03:09:23,804 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 03:09:23,804 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:09:32,621 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 03:09:34,090 | server.py:125 | fit progress: (6, 1.5711731910705566, {'accuracy': 0.891, 'data_size': 10000}, 72.52163146800012)
INFO flwr 2024-04-15 03:09:34,090 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 03:09:34,090 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:09:42,459 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 03:09:43,675 | server.py:125 | fit progress: (7, 1.5707684755325317, {'accuracy': 0.891, 'data_size': 10000}, 82.10679470398463)
INFO flwr 2024-04-15 03:09:43,675 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 03:09:43,676 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:09:52,552 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 03:09:53,740 | server.py:125 | fit progress: (8, 1.572978138923645, {'accuracy': 0.8894, 'data_size': 10000}, 92.17218579998007)
INFO flwr 2024-04-15 03:09:53,741 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 03:09:53,741 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:10:02,047 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 03:10:03,466 | server.py:125 | fit progress: (9, 1.5618563890457153, {'accuracy': 0.8991, 'data_size': 10000}, 101.89808907097904)
INFO flwr 2024-04-15 03:10:03,467 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 03:10:03,467 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:10:12,006 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 03:10:13,414 | server.py:125 | fit progress: (10, 1.5564961433410645, {'accuracy': 0.905, 'data_size': 10000}, 111.84612474398455)
INFO flwr 2024-04-15 03:10:13,415 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 03:10:13,415 | server.py:153 | FL finished in 111.84657835098915
INFO flwr 2024-04-15 03:10:13,415 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 03:10:13,415 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 03:10:13,415 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 03:10:13,415 | app.py:229 | app_fit: losses_centralized [(0, 2.3035645484924316), (1, 1.8885856866836548), (2, 1.6727089881896973), (3, 1.5984504222869873), (4, 1.5847117900848389), (5, 1.5804873704910278), (6, 1.5711731910705566), (7, 1.5707684755325317), (8, 1.572978138923645), (9, 1.5618563890457153), (10, 1.5564961433410645)]
INFO flwr 2024-04-15 03:10:13,415 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0768), (1, 0.6162), (2, 0.805), (3, 0.8736), (4, 0.8795), (5, 0.8853), (6, 0.891), (7, 0.891), (8, 0.8894), (9, 0.8991), (10, 0.905)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.905
wandb:     loss 1.5565
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_030754-lebbhyc1
wandb: Find logs at: ./wandb/offline-run-20240415_030754-lebbhyc1/logs
INFO flwr 2024-04-15 03:10:16,934 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 03:17:28,267 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1458156)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1458156)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 03:17:33,566	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 03:17:35,780	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 03:17:36,297	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 03:17:36,470	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 03:17:36,650	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 03:17:36,751	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (18.43MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 03:17:37,079	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f055c5de4a7ea463.zip' (215.14MiB) to Ray cluster...
2024-04-15 03:17:37,917	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f055c5de4a7ea463.zip'.
INFO flwr 2024-04-15 03:17:50,416 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 168746810368.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'object_store_memory': 76605775872.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-15 03:17:50,416 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 03:17:50,416 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 03:17:50,436 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 03:17:50,441 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 03:17:50,441 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 03:17:50,441 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 03:17:53,868 | server.py:94 | initial parameters (loss, other metrics): 2.304069757461548, {'accuracy': 0.0882, 'data_size': 10000}
INFO flwr 2024-04-15 03:17:53,869 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 03:17:53,869 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1462786)[0m 2024-04-15 03:17:57.188008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1462786)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1462786)[0m 2024-04-15 03:17:59.793531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1462786)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1462786)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1462791)[0m 2024-04-15 03:17:57.675026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1462791)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1462791)[0m 2024-04-15 03:18:00.530912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 03:18:16,675 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 03:18:17,930 | server.py:125 | fit progress: (1, 1.8427810668945312, {'accuracy': 0.623, 'data_size': 10000}, 24.060989457997493)
INFO flwr 2024-04-15 03:18:17,930 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 03:18:17,931 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:18:27,609 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 03:18:28,819 | server.py:125 | fit progress: (2, 1.6486550569534302, {'accuracy': 0.8162, 'data_size': 10000}, 34.94989396899473)
INFO flwr 2024-04-15 03:18:28,819 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 03:18:28,819 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:18:37,775 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 03:18:39,192 | server.py:125 | fit progress: (3, 1.6407572031021118, {'accuracy': 0.8211, 'data_size': 10000}, 45.32282777799992)
INFO flwr 2024-04-15 03:18:39,192 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 03:18:39,192 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:18:48,370 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 03:18:49,840 | server.py:125 | fit progress: (4, 1.6027792692184448, {'accuracy': 0.8594, 'data_size': 10000}, 55.97103721299209)
INFO flwr 2024-04-15 03:18:49,840 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 03:18:49,840 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:18:58,816 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 03:19:00,296 | server.py:125 | fit progress: (5, 1.5885628461837769, {'accuracy': 0.8751, 'data_size': 10000}, 66.42698128000484)
INFO flwr 2024-04-15 03:19:00,296 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 03:19:00,296 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:19:09,924 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 03:19:11,353 | server.py:125 | fit progress: (6, 1.6084873676300049, {'accuracy': 0.8531, 'data_size': 10000}, 77.48360535601387)
INFO flwr 2024-04-15 03:19:11,353 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 03:19:11,353 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:19:20,428 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 03:19:21,624 | server.py:125 | fit progress: (7, 1.5675840377807617, {'accuracy': 0.8935, 'data_size': 10000}, 87.75485590301105)
INFO flwr 2024-04-15 03:19:21,624 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 03:19:21,624 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:19:31,058 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 03:19:32,261 | server.py:125 | fit progress: (8, 1.5872621536254883, {'accuracy': 0.8739, 'data_size': 10000}, 98.3919793110108)
INFO flwr 2024-04-15 03:19:32,261 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 03:19:32,262 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:19:41,794 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 03:19:43,036 | server.py:125 | fit progress: (9, 1.5638049840927124, {'accuracy': 0.8968, 'data_size': 10000}, 109.16740981099429)
INFO flwr 2024-04-15 03:19:43,037 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 03:19:43,037 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:19:52,132 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 03:19:53,564 | server.py:125 | fit progress: (10, 1.57445228099823, {'accuracy': 0.887, 'data_size': 10000}, 119.69468291598605)
INFO flwr 2024-04-15 03:19:53,564 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 03:19:53,564 | server.py:153 | FL finished in 119.69514021801297
INFO flwr 2024-04-15 03:19:53,564 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 03:19:53,564 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 03:19:53,564 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 03:19:53,565 | app.py:229 | app_fit: losses_centralized [(0, 2.304069757461548), (1, 1.8427810668945312), (2, 1.6486550569534302), (3, 1.6407572031021118), (4, 1.6027792692184448), (5, 1.5885628461837769), (6, 1.6084873676300049), (7, 1.5675840377807617), (8, 1.5872621536254883), (9, 1.5638049840927124), (10, 1.57445228099823)]
INFO flwr 2024-04-15 03:19:53,565 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0882), (1, 0.623), (2, 0.8162), (3, 0.8211), (4, 0.8594), (5, 0.8751), (6, 0.8531), (7, 0.8935), (8, 0.8739), (9, 0.8968), (10, 0.887)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.887
wandb:     loss 1.57445
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_031727-p0l8efy0
wandb: Find logs at: ./wandb/offline-run-20240415_031727-p0l8efy0/logs
INFO flwr 2024-04-15 03:19:57,186 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 03:27:08,197 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1462778)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1462778)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 03:27:14,157	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 03:27:15,477	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 03:27:15,986	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 03:27:16,158	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 03:27:16,332	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 03:27:16,431	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (18.58MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 03:27:16,760	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_095956a983e17a4a.zip' (215.31MiB) to Ray cluster...
2024-04-15 03:27:17,618	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_095956a983e17a4a.zip'.
INFO flwr 2024-04-15 03:27:29,629 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 78597616435.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 173394438349.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-15 03:27:29,629 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 03:27:29,629 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 03:27:29,646 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 03:27:29,647 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 03:27:29,647 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 03:27:29,648 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 03:27:33,207 | server.py:94 | initial parameters (loss, other metrics): 2.299149513244629, {'accuracy': 0.1618, 'data_size': 10000}
INFO flwr 2024-04-15 03:27:33,207 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 03:27:33,207 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1467523)[0m 2024-04-15 03:27:36.662904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1467523)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1467519)[0m 2024-04-15 03:27:38.266260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1467523)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1467523)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1467529)[0m 2024-04-15 03:27:36.872541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1467529)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1467521)[0m 2024-04-15 03:27:40.883253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 03:27:55,256 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 03:27:56,474 | server.py:125 | fit progress: (1, 1.9735146760940552, {'accuracy': 0.6795, 'data_size': 10000}, 23.26642041199375)
INFO flwr 2024-04-15 03:27:56,474 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 03:27:56,474 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:28:06,060 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 03:28:07,574 | server.py:125 | fit progress: (2, 1.7837121486663818, {'accuracy': 0.7417, 'data_size': 10000}, 34.36653008798021)
INFO flwr 2024-04-15 03:28:07,574 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 03:28:07,574 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:28:15,924 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 03:28:17,325 | server.py:125 | fit progress: (3, 1.6887544393539429, {'accuracy': 0.7984, 'data_size': 10000}, 44.117859096993925)
INFO flwr 2024-04-15 03:28:17,325 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 03:28:17,326 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:28:25,681 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 03:28:27,104 | server.py:125 | fit progress: (4, 1.6271369457244873, {'accuracy': 0.8505, 'data_size': 10000}, 53.89711388698197)
INFO flwr 2024-04-15 03:28:27,105 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 03:28:27,105 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:28:35,578 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 03:28:36,757 | server.py:125 | fit progress: (5, 1.5895977020263672, {'accuracy': 0.8819, 'data_size': 10000}, 63.55017920100363)
INFO flwr 2024-04-15 03:28:36,758 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 03:28:36,758 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:28:45,179 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 03:28:46,592 | server.py:125 | fit progress: (6, 1.5763087272644043, {'accuracy': 0.8914, 'data_size': 10000}, 73.38493020599708)
INFO flwr 2024-04-15 03:28:46,592 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 03:28:46,593 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:28:55,102 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 03:28:56,505 | server.py:125 | fit progress: (7, 1.57417631149292, {'accuracy': 0.8919, 'data_size': 10000}, 83.29756780798198)
INFO flwr 2024-04-15 03:28:56,505 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 03:28:56,505 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:29:05,123 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 03:29:06,540 | server.py:125 | fit progress: (8, 1.568023443222046, {'accuracy': 0.8959, 'data_size': 10000}, 93.33320149598876)
INFO flwr 2024-04-15 03:29:06,541 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 03:29:06,541 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:29:15,534 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 03:29:16,722 | server.py:125 | fit progress: (9, 1.5641610622406006, {'accuracy': 0.8998, 'data_size': 10000}, 103.51507225900423)
INFO flwr 2024-04-15 03:29:16,723 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 03:29:16,723 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:29:25,213 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 03:29:26,420 | server.py:125 | fit progress: (10, 1.562557339668274, {'accuracy': 0.901, 'data_size': 10000}, 113.21316504100105)
INFO flwr 2024-04-15 03:29:26,421 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 03:29:26,421 | server.py:153 | FL finished in 113.21363058299175
INFO flwr 2024-04-15 03:29:26,421 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 03:29:26,421 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 03:29:26,421 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 03:29:26,421 | app.py:229 | app_fit: losses_centralized [(0, 2.299149513244629), (1, 1.9735146760940552), (2, 1.7837121486663818), (3, 1.6887544393539429), (4, 1.6271369457244873), (5, 1.5895977020263672), (6, 1.5763087272644043), (7, 1.57417631149292), (8, 1.568023443222046), (9, 1.5641610622406006), (10, 1.562557339668274)]
INFO flwr 2024-04-15 03:29:26,421 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1618), (1, 0.6795), (2, 0.7417), (3, 0.7984), (4, 0.8505), (5, 0.8819), (6, 0.8914), (7, 0.8919), (8, 0.8959), (9, 0.8998), (10, 0.901)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.901
wandb:     loss 1.56256
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_032707-27j42o4d
wandb: Find logs at: ./wandb/offline-run-20240415_032707-27j42o4d/logs
INFO flwr 2024-04-15 03:29:29,914 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 03:36:41,469 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1467530)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1467530)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 03:36:47,965	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 03:36:49,140	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 03:36:49,687	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 03:36:49,952	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 03:36:50,202	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 03:36:50,329	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (18.74MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 03:36:50,665	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_43ec69fd644e8f77.zip' (215.48MiB) to Ray cluster...
2024-04-15 03:36:51,579	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_43ec69fd644e8f77.zip'.
INFO flwr 2024-04-15 03:37:05,420 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 76529412096.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 168568628224.0}
INFO flwr 2024-04-15 03:37:05,420 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 03:37:05,420 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 03:37:05,442 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 03:37:05,443 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 03:37:05,444 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 03:37:05,444 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 03:37:09,971 | server.py:94 | initial parameters (loss, other metrics): 2.3048603534698486, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-15 03:37:09,971 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 03:37:09,972 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1471546)[0m 2024-04-15 03:37:12.200028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1471546)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1471537)[0m 2024-04-15 03:37:14.690813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1471543)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1471543)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1471542)[0m 2024-04-15 03:37:12.703638: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1471542)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1471542)[0m 2024-04-15 03:37:15.652545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 03:37:30,816 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 03:37:32,380 | server.py:125 | fit progress: (1, 1.8074856996536255, {'accuracy': 0.7557, 'data_size': 10000}, 22.4081236209895)
INFO flwr 2024-04-15 03:37:32,380 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 03:37:32,380 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:37:42,431 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 03:37:43,866 | server.py:125 | fit progress: (2, 1.654447078704834, {'accuracy': 0.8265, 'data_size': 10000}, 33.8942684349895)
INFO flwr 2024-04-15 03:37:43,866 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 03:37:43,867 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:37:52,749 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 03:37:53,924 | server.py:125 | fit progress: (3, 1.6070789098739624, {'accuracy': 0.8607, 'data_size': 10000}, 43.951852328988025)
INFO flwr 2024-04-15 03:37:53,924 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 03:37:53,924 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:38:02,472 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 03:38:04,035 | server.py:125 | fit progress: (4, 1.5827323198318481, {'accuracy': 0.8839, 'data_size': 10000}, 54.063358173007146)
INFO flwr 2024-04-15 03:38:04,035 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 03:38:04,036 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:38:12,428 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 03:38:13,824 | server.py:125 | fit progress: (5, 1.5785576105117798, {'accuracy': 0.8836, 'data_size': 10000}, 63.85216070400202)
INFO flwr 2024-04-15 03:38:13,824 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 03:38:13,824 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:38:21,931 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 03:38:23,391 | server.py:125 | fit progress: (6, 1.575175166130066, {'accuracy': 0.8871, 'data_size': 10000}, 73.41924381398712)
INFO flwr 2024-04-15 03:38:23,391 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 03:38:23,392 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:38:31,836 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 03:38:33,057 | server.py:125 | fit progress: (7, 1.570812463760376, {'accuracy': 0.8917, 'data_size': 10000}, 83.08545236798818)
INFO flwr 2024-04-15 03:38:33,057 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 03:38:33,058 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:38:41,665 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 03:38:43,057 | server.py:125 | fit progress: (8, 1.5696542263031006, {'accuracy': 0.8924, 'data_size': 10000}, 93.08541617600713)
INFO flwr 2024-04-15 03:38:43,057 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 03:38:43,058 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:38:51,744 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 03:38:52,955 | server.py:125 | fit progress: (9, 1.5610202550888062, {'accuracy': 0.8999, 'data_size': 10000}, 102.98286628301139)
INFO flwr 2024-04-15 03:38:52,955 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 03:38:52,955 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:39:01,884 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 03:39:03,092 | server.py:125 | fit progress: (10, 1.5594158172607422, {'accuracy': 0.9014, 'data_size': 10000}, 113.12033147498732)
INFO flwr 2024-04-15 03:39:03,092 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 03:39:03,092 | server.py:153 | FL finished in 113.12080406499445
INFO flwr 2024-04-15 03:39:03,093 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 03:39:03,093 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 03:39:03,093 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 03:39:03,093 | app.py:229 | app_fit: losses_centralized [(0, 2.3048603534698486), (1, 1.8074856996536255), (2, 1.654447078704834), (3, 1.6070789098739624), (4, 1.5827323198318481), (5, 1.5785576105117798), (6, 1.575175166130066), (7, 1.570812463760376), (8, 1.5696542263031006), (9, 1.5610202550888062), (10, 1.5594158172607422)]
INFO flwr 2024-04-15 03:39:03,093 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.7557), (2, 0.8265), (3, 0.8607), (4, 0.8839), (5, 0.8836), (6, 0.8871), (7, 0.8917), (8, 0.8924), (9, 0.8999), (10, 0.9014)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9014
wandb:     loss 1.55942
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_033641-yla7q1kn
wandb: Find logs at: ./wandb/offline-run-20240415_033641-yla7q1kn/logs
INFO flwr 2024-04-15 03:39:06,635 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 03:46:17,401 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1471534)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1471534)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 03:46:22,700	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 03:46:23,998	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 03:46:24,621	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 03:46:24,793	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 03:46:24,967	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 03:46:25,065	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (18.89MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 03:46:25,394	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4205efeaca1fe1a7.zip' (215.65MiB) to Ray cluster...
2024-04-15 03:46:26,313	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4205efeaca1fe1a7.zip'.
INFO flwr 2024-04-15 03:46:38,370 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 78566017843.0, 'accelerator_type:TITAN': 1.0, 'memory': 173320708301.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-15 03:46:38,370 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 03:46:38,370 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 03:46:38,393 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 03:46:38,395 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 03:46:38,395 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 03:46:38,395 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 03:46:42,089 | server.py:94 | initial parameters (loss, other metrics): 2.3016185760498047, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-15 03:46:42,090 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 03:46:42,090 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1475909)[0m 2024-04-15 03:46:45.154254: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1475909)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1475909)[0m 2024-04-15 03:46:47.780846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1475911)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1475911)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1475906)[0m 2024-04-15 03:46:45.689443: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1475906)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1475906)[0m 2024-04-15 03:46:48.658737: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 03:47:03,093 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 03:47:04,549 | server.py:125 | fit progress: (1, 1.7388314008712769, {'accuracy': 0.7412, 'data_size': 10000}, 22.45837614199263)
INFO flwr 2024-04-15 03:47:04,549 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 03:47:04,549 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:47:13,946 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 03:47:15,404 | server.py:125 | fit progress: (2, 1.6764129400253296, {'accuracy': 0.7853, 'data_size': 10000}, 33.31404291500803)
INFO flwr 2024-04-15 03:47:15,405 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 03:47:15,405 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:47:23,803 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 03:47:25,236 | server.py:125 | fit progress: (3, 1.6055548191070557, {'accuracy': 0.8564, 'data_size': 10000}, 43.14589939801954)
INFO flwr 2024-04-15 03:47:25,236 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 03:47:25,237 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:47:33,430 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 03:47:34,922 | server.py:125 | fit progress: (4, 1.5802987813949585, {'accuracy': 0.8798, 'data_size': 10000}, 52.832016186002875)
INFO flwr 2024-04-15 03:47:34,922 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 03:47:34,923 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:47:43,233 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 03:47:44,419 | server.py:125 | fit progress: (5, 1.5914143323898315, {'accuracy': 0.8697, 'data_size': 10000}, 62.32927446899703)
INFO flwr 2024-04-15 03:47:44,420 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 03:47:44,420 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:47:52,592 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 03:47:53,995 | server.py:125 | fit progress: (6, 1.6609385013580322, {'accuracy': 0.798, 'data_size': 10000}, 71.90464264299953)
INFO flwr 2024-04-15 03:47:53,995 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 03:47:53,995 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:48:02,833 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 03:48:04,056 | server.py:125 | fit progress: (7, 1.573309302330017, {'accuracy': 0.8875, 'data_size': 10000}, 81.96546807599952)
INFO flwr 2024-04-15 03:48:04,056 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 03:48:04,056 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:48:12,705 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 03:48:13,914 | server.py:125 | fit progress: (8, 1.5925981998443604, {'accuracy': 0.8683, 'data_size': 10000}, 91.82355647301301)
INFO flwr 2024-04-15 03:48:13,914 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 03:48:13,914 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:48:22,643 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 03:48:24,063 | server.py:125 | fit progress: (9, 1.573936104774475, {'accuracy': 0.8869, 'data_size': 10000}, 101.97258222999517)
INFO flwr 2024-04-15 03:48:24,063 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 03:48:24,063 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:48:33,006 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 03:48:34,431 | server.py:125 | fit progress: (10, 1.5769165754318237, {'accuracy': 0.884, 'data_size': 10000}, 112.34064911000314)
INFO flwr 2024-04-15 03:48:34,431 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 03:48:34,431 | server.py:153 | FL finished in 112.3411826530064
INFO flwr 2024-04-15 03:48:34,431 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 03:48:34,432 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 03:48:34,432 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 03:48:34,432 | app.py:229 | app_fit: losses_centralized [(0, 2.3016185760498047), (1, 1.7388314008712769), (2, 1.6764129400253296), (3, 1.6055548191070557), (4, 1.5802987813949585), (5, 1.5914143323898315), (6, 1.6609385013580322), (7, 1.573309302330017), (8, 1.5925981998443604), (9, 1.573936104774475), (10, 1.5769165754318237)]
INFO flwr 2024-04-15 03:48:34,432 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.7412), (2, 0.7853), (3, 0.8564), (4, 0.8798), (5, 0.8697), (6, 0.798), (7, 0.8875), (8, 0.8683), (9, 0.8869), (10, 0.884)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.884
wandb:     loss 1.57692
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_034617-zmflpnmp
wandb: Find logs at: ./wandb/offline-run-20240415_034617-zmflpnmp/logs
INFO flwr 2024-04-15 03:48:37,911 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 03:55:48,821 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1475905)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1475905)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 03:55:54,201	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 03:55:55,384	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 03:55:55,895	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 03:55:56,069	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 03:55:56,244	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 03:55:56,343	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.05MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 03:55:56,675	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3a3c886a185f9ab3.zip' (215.82MiB) to Ray cluster...
2024-04-15 03:55:57,573	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3a3c886a185f9ab3.zip'.
INFO flwr 2024-04-15 03:56:10,467 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 76630650470.0, 'memory': 168804851098.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-15 03:56:10,467 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 03:56:10,468 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 03:56:10,496 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 03:56:10,497 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 03:56:10,497 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 03:56:10,498 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 03:56:13,637 | server.py:94 | initial parameters (loss, other metrics): 2.3034725189208984, {'accuracy': 0.077, 'data_size': 10000}
INFO flwr 2024-04-15 03:56:13,638 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 03:56:13,642 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1480887)[0m 2024-04-15 03:56:17.387216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1480887)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1480892)[0m 2024-04-15 03:56:19.663124: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1480888)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1480888)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1480883)[0m 2024-04-15 03:56:17.640419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1480883)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1480883)[0m 2024-04-15 03:56:21.011800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 03:56:40,985 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 03:56:42,392 | server.py:125 | fit progress: (1, 1.9828611612319946, {'accuracy': 0.7387, 'data_size': 10000}, 28.753523559018504)
INFO flwr 2024-04-15 03:56:42,393 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 03:56:42,393 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:56:58,044 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 03:56:59,521 | server.py:125 | fit progress: (2, 1.7534788846969604, {'accuracy': 0.8005, 'data_size': 10000}, 45.88194535000366)
INFO flwr 2024-04-15 03:56:59,521 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 03:56:59,521 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:57:13,160 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 03:57:14,525 | server.py:125 | fit progress: (3, 1.6508431434631348, {'accuracy': 0.8483, 'data_size': 10000}, 60.88675082701957)
INFO flwr 2024-04-15 03:57:14,526 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 03:57:14,526 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:57:28,415 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 03:57:29,860 | server.py:125 | fit progress: (4, 1.6041780710220337, {'accuracy': 0.8778, 'data_size': 10000}, 76.22095775601338)
INFO flwr 2024-04-15 03:57:29,860 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 03:57:29,860 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:57:44,187 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 03:57:45,437 | server.py:125 | fit progress: (5, 1.5829823017120361, {'accuracy': 0.8912, 'data_size': 10000}, 91.79808806002256)
INFO flwr 2024-04-15 03:57:45,437 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 03:57:45,437 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:58:00,469 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 03:58:01,698 | server.py:125 | fit progress: (6, 1.5727665424346924, {'accuracy': 0.8987, 'data_size': 10000}, 108.05875687501975)
INFO flwr 2024-04-15 03:58:01,698 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 03:58:01,698 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:58:15,426 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 03:58:16,843 | server.py:125 | fit progress: (7, 1.5690350532531738, {'accuracy': 0.8993, 'data_size': 10000}, 123.20397842899547)
INFO flwr 2024-04-15 03:58:16,843 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 03:58:16,843 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:58:31,901 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 03:58:33,314 | server.py:125 | fit progress: (8, 1.5621417760849, {'accuracy': 0.9035, 'data_size': 10000}, 139.6750097910117)
INFO flwr 2024-04-15 03:58:33,314 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 03:58:33,314 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:58:46,488 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 03:58:47,690 | server.py:125 | fit progress: (9, 1.5628806352615356, {'accuracy': 0.9026, 'data_size': 10000}, 154.05130713799736)
INFO flwr 2024-04-15 03:58:47,690 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 03:58:47,691 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 03:59:02,010 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 03:59:03,441 | server.py:125 | fit progress: (10, 1.5725195407867432, {'accuracy': 0.8923, 'data_size': 10000}, 169.8023284310184)
INFO flwr 2024-04-15 03:59:03,441 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 03:59:03,442 | server.py:153 | FL finished in 169.80278913900838
INFO flwr 2024-04-15 03:59:03,442 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 03:59:03,442 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 03:59:03,442 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 03:59:03,442 | app.py:229 | app_fit: losses_centralized [(0, 2.3034725189208984), (1, 1.9828611612319946), (2, 1.7534788846969604), (3, 1.6508431434631348), (4, 1.6041780710220337), (5, 1.5829823017120361), (6, 1.5727665424346924), (7, 1.5690350532531738), (8, 1.5621417760849), (9, 1.5628806352615356), (10, 1.5725195407867432)]
INFO flwr 2024-04-15 03:59:03,442 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.077), (1, 0.7387), (2, 0.8005), (3, 0.8483), (4, 0.8778), (5, 0.8912), (6, 0.8987), (7, 0.8993), (8, 0.9035), (9, 0.9026), (10, 0.8923)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8923
wandb:     loss 1.57252
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_035548-ptpwd1nt
wandb: Find logs at: ./wandb/offline-run-20240415_035548-ptpwd1nt/logs
INFO flwr 2024-04-15 03:59:06,971 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 04:06:18,575 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1480883)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1480883)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 04:06:23,787	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 04:06:25,216	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 04:06:25,749	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 04:06:25,924	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 04:06:26,100	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 04:06:26,199	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.21MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 04:06:26,528	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a8ec78c971a87ac3.zip' (215.99MiB) to Ray cluster...
2024-04-15 04:06:27,372	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a8ec78c971a87ac3.zip'.
INFO flwr 2024-04-15 04:06:40,477 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 78504886272.0, 'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 173178067968.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-15 04:06:40,477 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 04:06:40,477 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 04:06:40,494 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 04:06:40,500 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 04:06:40,500 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 04:06:40,500 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 04:06:44,585 | server.py:94 | initial parameters (loss, other metrics): 2.30568265914917, {'accuracy': 0.0528, 'data_size': 10000}
INFO flwr 2024-04-15 04:06:44,586 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 04:06:44,587 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1485275)[0m 2024-04-15 04:06:47.171584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1485275)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1485277)[0m 2024-04-15 04:06:49.875105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1485280)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1485280)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1485280)[0m 2024-04-15 04:06:47.445385: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1485280)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1485281)[0m 2024-04-15 04:06:50.117541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 04:07:11,011 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 04:07:12,500 | server.py:125 | fit progress: (1, 1.8394169807434082, {'accuracy': 0.7074, 'data_size': 10000}, 27.913812949991552)
INFO flwr 2024-04-15 04:07:12,500 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 04:07:12,501 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:07:25,629 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 04:07:27,026 | server.py:125 | fit progress: (2, 1.7030445337295532, {'accuracy': 0.7744, 'data_size': 10000}, 42.44026633197791)
INFO flwr 2024-04-15 04:07:27,027 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 04:07:27,027 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:07:41,268 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 04:07:42,718 | server.py:125 | fit progress: (3, 1.6784511804580688, {'accuracy': 0.7908, 'data_size': 10000}, 58.13195106299827)
INFO flwr 2024-04-15 04:07:42,718 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 04:07:42,719 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:07:56,738 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 04:07:58,165 | server.py:125 | fit progress: (4, 1.6668423414230347, {'accuracy': 0.7981, 'data_size': 10000}, 73.5785213309864)
INFO flwr 2024-04-15 04:07:58,165 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 04:07:58,165 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:08:12,583 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 04:08:13,739 | server.py:125 | fit progress: (5, 1.6460431814193726, {'accuracy': 0.8168, 'data_size': 10000}, 89.15232983799069)
INFO flwr 2024-04-15 04:08:13,739 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 04:08:13,739 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:08:28,137 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 04:08:29,381 | server.py:125 | fit progress: (6, 1.6424065828323364, {'accuracy': 0.8196, 'data_size': 10000}, 104.79453006997937)
INFO flwr 2024-04-15 04:08:29,381 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 04:08:29,381 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:08:43,776 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 04:08:44,977 | server.py:125 | fit progress: (7, 1.646034598350525, {'accuracy': 0.8155, 'data_size': 10000}, 120.39108354097698)
INFO flwr 2024-04-15 04:08:44,978 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 04:08:44,978 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:09:00,361 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 04:09:01,862 | server.py:125 | fit progress: (8, 1.6578121185302734, {'accuracy': 0.8041, 'data_size': 10000}, 137.2757941649761)
INFO flwr 2024-04-15 04:09:01,862 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 04:09:01,863 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:09:16,106 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 04:09:17,584 | server.py:125 | fit progress: (9, 1.641013264656067, {'accuracy': 0.82, 'data_size': 10000}, 152.99765753999236)
INFO flwr 2024-04-15 04:09:17,584 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 04:09:17,584 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:09:31,668 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 04:09:32,890 | server.py:125 | fit progress: (10, 1.6331369876861572, {'accuracy': 0.8272, 'data_size': 10000}, 168.30430590198375)
INFO flwr 2024-04-15 04:09:32,891 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 04:09:32,891 | server.py:153 | FL finished in 168.304822252976
INFO flwr 2024-04-15 04:09:32,891 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 04:09:32,891 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 04:09:32,891 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 04:09:32,892 | app.py:229 | app_fit: losses_centralized [(0, 2.30568265914917), (1, 1.8394169807434082), (2, 1.7030445337295532), (3, 1.6784511804580688), (4, 1.6668423414230347), (5, 1.6460431814193726), (6, 1.6424065828323364), (7, 1.646034598350525), (8, 1.6578121185302734), (9, 1.641013264656067), (10, 1.6331369876861572)]
INFO flwr 2024-04-15 04:09:32,892 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0528), (1, 0.7074), (2, 0.7744), (3, 0.7908), (4, 0.7981), (5, 0.8168), (6, 0.8196), (7, 0.8155), (8, 0.8041), (9, 0.82), (10, 0.8272)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8272
wandb:     loss 1.63314
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_040618-62v6jmlk
wandb: Find logs at: ./wandb/offline-run-20240415_040618-62v6jmlk/logs
INFO flwr 2024-04-15 04:09:36,427 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 04:16:48,392 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1485270)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1485270)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 04:16:53,085	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 04:16:54,564	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 04:16:55,056	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 04:16:55,231	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 04:16:55,407	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 04:16:55,508	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.36MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 04:16:55,856	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_619e26c86120ce2b.zip' (216.16MiB) to Ray cluster...
2024-04-15 04:16:56,769	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_619e26c86120ce2b.zip'.
INFO flwr 2024-04-15 04:17:09,013 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'memory': 168399365940.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 76456871116.0}
INFO flwr 2024-04-15 04:17:09,014 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 04:17:09,014 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 04:17:09,032 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 04:17:09,033 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 04:17:09,034 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 04:17:09,034 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 04:17:11,901 | server.py:94 | initial parameters (loss, other metrics): 2.3044581413269043, {'accuracy': 0.0625, 'data_size': 10000}
INFO flwr 2024-04-15 04:17:11,902 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 04:17:11,903 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1490433)[0m 2024-04-15 04:17:15.826448: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1490433)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1490433)[0m 2024-04-15 04:17:17.994446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1490432)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1490432)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1490428)[0m 2024-04-15 04:17:16.192989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1490428)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1490428)[0m 2024-04-15 04:17:19.253638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 04:17:42,463 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 04:17:43,959 | server.py:125 | fit progress: (1, 1.8870218992233276, {'accuracy': 0.5794, 'data_size': 10000}, 32.05642862900277)
INFO flwr 2024-04-15 04:17:43,959 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 04:17:43,960 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:17:59,214 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 04:18:00,718 | server.py:125 | fit progress: (2, 1.6395896673202515, {'accuracy': 0.8303, 'data_size': 10000}, 48.81527532800101)
INFO flwr 2024-04-15 04:18:00,718 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 04:18:00,718 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:18:13,852 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 04:18:15,233 | server.py:125 | fit progress: (3, 1.7073744535446167, {'accuracy': 0.755, 'data_size': 10000}, 63.33003214298515)
INFO flwr 2024-04-15 04:18:15,233 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 04:18:15,233 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:18:28,946 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 04:18:30,369 | server.py:125 | fit progress: (4, 1.597720980644226, {'accuracy': 0.8646, 'data_size': 10000}, 78.46692926200922)
INFO flwr 2024-04-15 04:18:30,370 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 04:18:30,370 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:18:44,682 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 04:18:45,896 | server.py:125 | fit progress: (5, 1.6089991331100464, {'accuracy': 0.8516, 'data_size': 10000}, 93.99305553498561)
INFO flwr 2024-04-15 04:18:45,896 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 04:18:45,896 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:19:00,330 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 04:19:01,558 | server.py:125 | fit progress: (6, 1.562930703163147, {'accuracy': 0.8994, 'data_size': 10000}, 109.65502216000459)
INFO flwr 2024-04-15 04:19:01,558 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 04:19:01,558 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:19:16,903 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 04:19:18,122 | server.py:125 | fit progress: (7, 1.587772011756897, {'accuracy': 0.8729, 'data_size': 10000}, 126.21937380498275)
INFO flwr 2024-04-15 04:19:18,122 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 04:19:18,122 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:19:32,695 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 04:19:34,148 | server.py:125 | fit progress: (8, 1.583794116973877, {'accuracy': 0.8766, 'data_size': 10000}, 142.24573918498936)
INFO flwr 2024-04-15 04:19:34,149 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 04:19:34,149 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:19:48,322 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 04:19:49,799 | server.py:125 | fit progress: (9, 1.5612438917160034, {'accuracy': 0.901, 'data_size': 10000}, 157.89639470700058)
INFO flwr 2024-04-15 04:19:49,799 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 04:19:49,799 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:20:04,226 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 04:20:05,423 | server.py:125 | fit progress: (10, 1.5553910732269287, {'accuracy': 0.9061, 'data_size': 10000}, 173.5206762799935)
INFO flwr 2024-04-15 04:20:05,424 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 04:20:05,424 | server.py:153 | FL finished in 173.52113233099226
INFO flwr 2024-04-15 04:20:05,424 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 04:20:05,424 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 04:20:05,424 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 04:20:05,424 | app.py:229 | app_fit: losses_centralized [(0, 2.3044581413269043), (1, 1.8870218992233276), (2, 1.6395896673202515), (3, 1.7073744535446167), (4, 1.597720980644226), (5, 1.6089991331100464), (6, 1.562930703163147), (7, 1.587772011756897), (8, 1.583794116973877), (9, 1.5612438917160034), (10, 1.5553910732269287)]
INFO flwr 2024-04-15 04:20:05,424 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0625), (1, 0.5794), (2, 0.8303), (3, 0.755), (4, 0.8646), (5, 0.8516), (6, 0.8994), (7, 0.8729), (8, 0.8766), (9, 0.901), (10, 0.9061)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9061
wandb:     loss 1.55539
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_041648-o41629b9
wandb: Find logs at: ./wandb/offline-run-20240415_041648-o41629b9/logs
INFO flwr 2024-04-15 04:20:08,962 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 04:27:20,719 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1490424)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1490424)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 04:27:25,467	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 04:27:26,586	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 04:27:27,229	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 04:27:27,473	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 04:27:27,729	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 04:27:27,861	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 04:27:28,249	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_44db9c3ecba2c418.zip' (216.18MiB) to Ray cluster...
2024-04-15 04:27:29,115	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_44db9c3ecba2c418.zip'.
INFO flwr 2024-04-15 04:27:42,151 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 78085372723.0, 'memory': 172199203021.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-15 04:27:42,152 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 04:27:42,152 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 04:27:42,171 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 04:27:42,172 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 04:27:42,172 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 04:27:42,172 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 04:27:46,233 | server.py:94 | initial parameters (loss, other metrics): 2.3048858642578125, {'accuracy': 0.0696, 'data_size': 10000}
INFO flwr 2024-04-15 04:27:46,234 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 04:27:46,237 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1495697)[0m 2024-04-15 04:27:48.906661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1495697)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1495705)[0m 2024-04-15 04:27:51.339368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1495710)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1495710)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1495702)[0m 2024-04-15 04:27:49.225700: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1495702)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1495707)[0m 2024-04-15 04:27:52.396814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 04:28:12,884 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 04:28:14,398 | server.py:125 | fit progress: (1, 1.9888677597045898, {'accuracy': 0.6862, 'data_size': 10000}, 28.162055419990793)
INFO flwr 2024-04-15 04:28:14,399 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 04:28:14,399 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:28:28,393 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 04:28:29,807 | server.py:125 | fit progress: (2, 1.8079136610031128, {'accuracy': 0.7415, 'data_size': 10000}, 43.57026765900082)
INFO flwr 2024-04-15 04:28:29,807 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 04:28:29,807 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:28:43,762 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 04:28:44,964 | server.py:125 | fit progress: (3, 1.6924372911453247, {'accuracy': 0.8088, 'data_size': 10000}, 58.72747722399072)
INFO flwr 2024-04-15 04:28:44,964 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 04:28:44,965 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:28:59,294 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 04:29:00,734 | server.py:125 | fit progress: (4, 1.6238757371902466, {'accuracy': 0.8561, 'data_size': 10000}, 74.49774334899848)
INFO flwr 2024-04-15 04:29:00,734 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 04:29:00,735 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:29:14,124 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 04:29:15,548 | server.py:125 | fit progress: (5, 1.5846099853515625, {'accuracy': 0.8885, 'data_size': 10000}, 89.31192348801414)
INFO flwr 2024-04-15 04:29:15,549 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 04:29:15,549 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:29:28,968 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 04:29:30,350 | server.py:125 | fit progress: (6, 1.5712895393371582, {'accuracy': 0.8981, 'data_size': 10000}, 104.11376801200095)
INFO flwr 2024-04-15 04:29:30,350 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 04:29:30,351 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:29:44,577 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 04:29:45,768 | server.py:125 | fit progress: (7, 1.5650134086608887, {'accuracy': 0.901, 'data_size': 10000}, 119.53219334199093)
INFO flwr 2024-04-15 04:29:45,769 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 04:29:45,769 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:30:00,011 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 04:30:01,424 | server.py:125 | fit progress: (8, 1.561737060546875, {'accuracy': 0.9042, 'data_size': 10000}, 135.18736923800316)
INFO flwr 2024-04-15 04:30:01,424 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 04:30:01,424 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:30:15,320 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 04:30:16,502 | server.py:125 | fit progress: (9, 1.5588696002960205, {'accuracy': 0.9058, 'data_size': 10000}, 150.26568764299736)
INFO flwr 2024-04-15 04:30:16,502 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 04:30:16,503 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:30:31,412 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 04:30:32,598 | server.py:125 | fit progress: (10, 1.5594851970672607, {'accuracy': 0.9059, 'data_size': 10000}, 166.36130148198572)
INFO flwr 2024-04-15 04:30:32,598 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 04:30:32,598 | server.py:153 | FL finished in 166.36175366101088
INFO flwr 2024-04-15 04:30:32,598 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 04:30:32,598 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 04:30:32,598 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 04:30:32,599 | app.py:229 | app_fit: losses_centralized [(0, 2.3048858642578125), (1, 1.9888677597045898), (2, 1.8079136610031128), (3, 1.6924372911453247), (4, 1.6238757371902466), (5, 1.5846099853515625), (6, 1.5712895393371582), (7, 1.5650134086608887), (8, 1.561737060546875), (9, 1.5588696002960205), (10, 1.5594851970672607)]
INFO flwr 2024-04-15 04:30:32,599 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0696), (1, 0.6862), (2, 0.7415), (3, 0.8088), (4, 0.8561), (5, 0.8885), (6, 0.8981), (7, 0.901), (8, 0.9042), (9, 0.9058), (10, 0.9059)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9059
wandb:     loss 1.55949
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_042720-2udh3bo8
wandb: Find logs at: ./wandb/offline-run-20240415_042720-2udh3bo8/logs
INFO flwr 2024-04-15 04:30:36,194 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 04:37:47,892 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1495702)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1495702)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 04:37:53,410	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 04:37:54,684	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 04:37:55,295	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 04:37:55,480	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 04:37:55,654	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 04:37:55,753	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 04:37:56,080	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_854add1b39000e08.zip' (216.19MiB) to Ray cluster...
2024-04-15 04:37:56,914	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_854add1b39000e08.zip'.
INFO flwr 2024-04-15 04:38:09,747 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 78131229081.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 172306201191.0}
INFO flwr 2024-04-15 04:38:09,747 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 04:38:09,748 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 04:38:09,768 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 04:38:09,770 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 04:38:09,770 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 04:38:09,770 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 04:38:12,351 | server.py:94 | initial parameters (loss, other metrics): 2.3002538681030273, {'accuracy': 0.1356, 'data_size': 10000}
INFO flwr 2024-04-15 04:38:12,352 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 04:38:12,353 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1501231)[0m 2024-04-15 04:38:16.535106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1501231)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1501237)[0m 2024-04-15 04:38:19.325671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1501233)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1501233)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1501232)[0m 2024-04-15 04:38:17.049084: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1501232)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1501230)[0m 2024-04-15 04:38:20.084688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 04:38:42,124 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 04:38:43,713 | server.py:125 | fit progress: (1, 1.8406579494476318, {'accuracy': 0.7004, 'data_size': 10000}, 31.35966268600896)
INFO flwr 2024-04-15 04:38:43,713 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 04:38:43,714 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:38:58,916 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 04:39:00,366 | server.py:125 | fit progress: (2, 1.6802458763122559, {'accuracy': 0.7973, 'data_size': 10000}, 48.01271260200883)
INFO flwr 2024-04-15 04:39:00,366 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 04:39:00,366 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:39:15,248 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 04:39:16,708 | server.py:125 | fit progress: (3, 1.5958796739578247, {'accuracy': 0.8763, 'data_size': 10000}, 64.35480494299554)
INFO flwr 2024-04-15 04:39:16,708 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 04:39:16,708 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:39:30,634 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 04:39:32,117 | server.py:125 | fit progress: (4, 1.5711555480957031, {'accuracy': 0.8973, 'data_size': 10000}, 79.76375917502446)
INFO flwr 2024-04-15 04:39:32,117 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 04:39:32,117 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:39:46,214 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 04:39:47,446 | server.py:125 | fit progress: (5, 1.5709155797958374, {'accuracy': 0.8928, 'data_size': 10000}, 95.09307812000043)
INFO flwr 2024-04-15 04:39:47,446 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 04:39:47,446 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:40:01,489 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 04:40:02,675 | server.py:125 | fit progress: (6, 1.5798720121383667, {'accuracy': 0.8821, 'data_size': 10000}, 110.32214449401363)
INFO flwr 2024-04-15 04:40:02,675 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 04:40:02,676 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:40:17,258 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 04:40:18,477 | server.py:125 | fit progress: (7, 1.5579313039779663, {'accuracy': 0.905, 'data_size': 10000}, 126.12454208699637)
INFO flwr 2024-04-15 04:40:18,478 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 04:40:18,478 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:40:32,176 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 04:40:33,601 | server.py:125 | fit progress: (8, 1.5566095113754272, {'accuracy': 0.9064, 'data_size': 10000}, 141.24827210401418)
INFO flwr 2024-04-15 04:40:33,601 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 04:40:33,602 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:40:47,610 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 04:40:49,036 | server.py:125 | fit progress: (9, 1.555773138999939, {'accuracy': 0.9067, 'data_size': 10000}, 156.68334083200898)
INFO flwr 2024-04-15 04:40:49,036 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 04:40:49,037 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:41:02,899 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 04:41:04,085 | server.py:125 | fit progress: (10, 1.555130124092102, {'accuracy': 0.9084, 'data_size': 10000}, 171.73170798202045)
INFO flwr 2024-04-15 04:41:04,085 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 04:41:04,085 | server.py:153 | FL finished in 171.7323335900146
INFO flwr 2024-04-15 04:41:04,085 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 04:41:04,085 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 04:41:04,086 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 04:41:04,086 | app.py:229 | app_fit: losses_centralized [(0, 2.3002538681030273), (1, 1.8406579494476318), (2, 1.6802458763122559), (3, 1.5958796739578247), (4, 1.5711555480957031), (5, 1.5709155797958374), (6, 1.5798720121383667), (7, 1.5579313039779663), (8, 1.5566095113754272), (9, 1.555773138999939), (10, 1.555130124092102)]
INFO flwr 2024-04-15 04:41:04,086 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1356), (1, 0.7004), (2, 0.7973), (3, 0.8763), (4, 0.8973), (5, 0.8928), (6, 0.8821), (7, 0.905), (8, 0.9064), (9, 0.9067), (10, 0.9084)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9084
wandb:     loss 1.55513
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_043747-t48qfd9o
wandb: Find logs at: ./wandb/offline-run-20240415_043747-t48qfd9o/logs
INFO flwr 2024-04-15 04:41:07,683 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 04:48:19,537 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1501229)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1501229)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 04:48:24,716	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 04:48:26,150	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 04:48:26,705	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 04:48:26,883	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 04:48:27,061	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 04:48:27,162	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 04:48:27,498	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a9d9ac3ed4508001.zip' (216.21MiB) to Ray cluster...
2024-04-15 04:48:28,334	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a9d9ac3ed4508001.zip'.
INFO flwr 2024-04-15 04:48:40,957 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'memory': 167651499828.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 76136357068.0}
INFO flwr 2024-04-15 04:48:40,958 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 04:48:40,958 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 04:48:40,978 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 04:48:40,979 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 04:48:40,979 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 04:48:40,980 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 04:48:43,888 | server.py:94 | initial parameters (loss, other metrics): 2.3023762702941895, {'accuracy': 0.0925, 'data_size': 10000}
INFO flwr 2024-04-15 04:48:43,889 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 04:48:43,889 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1505884)[0m 2024-04-15 04:48:48.058354: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1505884)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1505881)[0m 2024-04-15 04:48:50.618085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1505882)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1505882)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1505883)[0m 2024-04-15 04:48:48.291555: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1505883)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1505883)[0m 2024-04-15 04:48:51.244620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 04:49:13,499 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 04:49:15,013 | server.py:125 | fit progress: (1, 1.6945381164550781, {'accuracy': 0.7997, 'data_size': 10000}, 31.123700435011415)
INFO flwr 2024-04-15 04:49:15,013 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 04:49:15,013 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:49:30,973 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 04:49:32,430 | server.py:125 | fit progress: (2, 1.6638062000274658, {'accuracy': 0.8001, 'data_size': 10000}, 48.54076102201361)
INFO flwr 2024-04-15 04:49:32,430 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 04:49:32,430 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:49:46,157 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 04:49:47,621 | server.py:125 | fit progress: (3, 1.5700418949127197, {'accuracy': 0.8923, 'data_size': 10000}, 63.7321835400071)
INFO flwr 2024-04-15 04:49:47,621 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 04:49:47,622 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:50:01,316 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 04:50:02,735 | server.py:125 | fit progress: (4, 1.5743762254714966, {'accuracy': 0.8878, 'data_size': 10000}, 78.8456905670173)
INFO flwr 2024-04-15 04:50:02,735 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 04:50:02,735 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:50:15,992 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 04:50:17,195 | server.py:125 | fit progress: (5, 1.5605629682540894, {'accuracy': 0.9009, 'data_size': 10000}, 93.3063312730228)
INFO flwr 2024-04-15 04:50:17,195 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 04:50:17,196 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:50:31,626 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 04:50:32,827 | server.py:125 | fit progress: (6, 1.5691571235656738, {'accuracy': 0.8926, 'data_size': 10000}, 108.93850709401886)
INFO flwr 2024-04-15 04:50:32,828 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 04:50:32,828 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:50:47,973 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 04:50:49,190 | server.py:125 | fit progress: (7, 1.57169771194458, {'accuracy': 0.889, 'data_size': 10000}, 125.30132817500271)
INFO flwr 2024-04-15 04:50:49,190 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 04:50:49,191 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:51:03,529 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 04:51:04,987 | server.py:125 | fit progress: (8, 1.5550669431686401, {'accuracy': 0.9062, 'data_size': 10000}, 141.09809516201494)
INFO flwr 2024-04-15 04:51:04,987 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 04:51:04,987 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:51:18,868 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 04:51:20,275 | server.py:125 | fit progress: (9, 1.5642260313034058, {'accuracy': 0.8965, 'data_size': 10000}, 156.38595910801087)
INFO flwr 2024-04-15 04:51:20,275 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 04:51:20,275 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 04:51:36,707 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 04:51:37,901 | server.py:125 | fit progress: (10, 1.5542664527893066, {'accuracy': 0.9066, 'data_size': 10000}, 174.0126677119988)
INFO flwr 2024-04-15 04:51:37,902 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 04:51:37,902 | server.py:153 | FL finished in 174.0131099630089
INFO flwr 2024-04-15 04:51:37,902 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 04:51:37,902 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 04:51:37,902 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 04:51:37,902 | app.py:229 | app_fit: losses_centralized [(0, 2.3023762702941895), (1, 1.6945381164550781), (2, 1.6638062000274658), (3, 1.5700418949127197), (4, 1.5743762254714966), (5, 1.5605629682540894), (6, 1.5691571235656738), (7, 1.57169771194458), (8, 1.5550669431686401), (9, 1.5642260313034058), (10, 1.5542664527893066)]
INFO flwr 2024-04-15 04:51:37,903 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0925), (1, 0.7997), (2, 0.8001), (3, 0.8923), (4, 0.8878), (5, 0.9009), (6, 0.8926), (7, 0.889), (8, 0.9062), (9, 0.8965), (10, 0.9066)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9066
wandb:     loss 1.55427
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_044819-gk0qi2qi
wandb: Find logs at: ./wandb/offline-run-20240415_044819-gk0qi2qi/logs
INFO flwr 2024-04-15 04:51:41,481 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 04:58:53,694 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1505884)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1505884)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 04:59:00,078	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 04:59:01,260	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 04:59:01,882	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 04:59:02,128	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 04:59:02,380	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 04:59:02,510	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 04:59:02,852	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6ddcfaeeed0f41c8.zip' (216.22MiB) to Ray cluster...
2024-04-15 04:59:03,694	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6ddcfaeeed0f41c8.zip'.
INFO flwr 2024-04-15 04:59:17,803 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 76166317670.0, 'accelerator_type:TITAN': 1.0, 'memory': 167721407898.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-15 04:59:17,803 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 04:59:17,803 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 04:59:17,829 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 04:59:17,831 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 04:59:17,831 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 04:59:17,831 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 04:59:20,423 | server.py:94 | initial parameters (loss, other metrics): 2.3067264556884766, {'accuracy': 0.0637, 'data_size': 10000}
INFO flwr 2024-04-15 04:59:20,424 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 04:59:20,424 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1510871)[0m 2024-04-15 04:59:24.888858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1510871)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1510871)[0m 2024-04-15 04:59:27.872398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1510871)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1510871)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1510869)[0m 2024-04-15 04:59:25.434559: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1510869)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1510864)[0m 2024-04-15 04:59:27.980667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 04:59:51,144 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 04:59:52,639 | server.py:125 | fit progress: (1, 1.9878336191177368, {'accuracy': 0.7481, 'data_size': 10000}, 32.21502259501722)
INFO flwr 2024-04-15 04:59:52,640 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 04:59:52,640 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:00:07,208 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 05:00:08,602 | server.py:125 | fit progress: (2, 1.766719102859497, {'accuracy': 0.7972, 'data_size': 10000}, 48.178064605017425)
INFO flwr 2024-04-15 05:00:08,603 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 05:00:08,603 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:00:24,185 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 05:00:25,589 | server.py:125 | fit progress: (3, 1.6782077550888062, {'accuracy': 0.8249, 'data_size': 10000}, 65.16477831100929)
INFO flwr 2024-04-15 05:00:25,589 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 05:00:25,590 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:00:39,588 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 05:00:41,028 | server.py:125 | fit progress: (4, 1.6255053281784058, {'accuracy': 0.8565, 'data_size': 10000}, 80.6035050250066)
INFO flwr 2024-04-15 05:00:41,028 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 05:00:41,028 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:00:55,035 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 05:00:56,487 | server.py:125 | fit progress: (5, 1.5952597856521606, {'accuracy': 0.8766, 'data_size': 10000}, 96.06239406700479)
INFO flwr 2024-04-15 05:00:56,487 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 05:00:56,487 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:01:11,003 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 05:01:12,243 | server.py:125 | fit progress: (6, 1.5773156881332397, {'accuracy': 0.8929, 'data_size': 10000}, 111.81838130601682)
INFO flwr 2024-04-15 05:01:12,243 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 05:01:12,243 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:01:26,361 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 05:01:27,598 | server.py:125 | fit progress: (7, 1.5669300556182861, {'accuracy': 0.8998, 'data_size': 10000}, 127.17420556402067)
INFO flwr 2024-04-15 05:01:27,599 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 05:01:27,599 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:01:41,741 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 05:01:42,959 | server.py:125 | fit progress: (8, 1.5621135234832764, {'accuracy': 0.9013, 'data_size': 10000}, 142.5351002830139)
INFO flwr 2024-04-15 05:01:42,960 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 05:01:42,960 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:01:57,011 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 05:01:58,501 | server.py:125 | fit progress: (9, 1.5613141059875488, {'accuracy': 0.9014, 'data_size': 10000}, 158.07687096201698)
INFO flwr 2024-04-15 05:01:58,501 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 05:01:58,502 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:02:12,716 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 05:02:14,208 | server.py:125 | fit progress: (10, 1.5575002431869507, {'accuracy': 0.9048, 'data_size': 10000}, 173.78328754700487)
INFO flwr 2024-04-15 05:02:14,208 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 05:02:14,208 | server.py:153 | FL finished in 173.78377250701305
INFO flwr 2024-04-15 05:02:14,208 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 05:02:14,208 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 05:02:14,209 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 05:02:14,209 | app.py:229 | app_fit: losses_centralized [(0, 2.3067264556884766), (1, 1.9878336191177368), (2, 1.766719102859497), (3, 1.6782077550888062), (4, 1.6255053281784058), (5, 1.5952597856521606), (6, 1.5773156881332397), (7, 1.5669300556182861), (8, 1.5621135234832764), (9, 1.5613141059875488), (10, 1.5575002431869507)]
INFO flwr 2024-04-15 05:02:14,209 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0637), (1, 0.7481), (2, 0.7972), (3, 0.8249), (4, 0.8565), (5, 0.8766), (6, 0.8929), (7, 0.8998), (8, 0.9013), (9, 0.9014), (10, 0.9048)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9048
wandb:     loss 1.5575
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_045853-97b4u18b
wandb: Find logs at: ./wandb/offline-run-20240415_045853-97b4u18b/logs
INFO flwr 2024-04-15 05:02:17,805 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 05:09:29,893 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1510863)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1510863)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 05:09:35,379	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 05:09:36,706	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 05:09:37,192	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 05:09:37,421	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 05:09:37,612	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 05:09:37,723	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 05:09:38,079	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ea57c38d943e7fd2.zip' (216.23MiB) to Ray cluster...
2024-04-15 05:09:38,926	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ea57c38d943e7fd2.zip'.
INFO flwr 2024-04-15 05:09:52,368 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 167408817152.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'object_store_memory': 76032350208.0}
INFO flwr 2024-04-15 05:09:52,368 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 05:09:52,368 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 05:09:52,386 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 05:09:52,387 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 05:09:52,387 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 05:09:52,388 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 05:09:56,690 | server.py:94 | initial parameters (loss, other metrics): 2.3047380447387695, {'accuracy': 0.0849, 'data_size': 10000}
INFO flwr 2024-04-15 05:09:56,690 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 05:09:56,691 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1515248)[0m 2024-04-15 05:09:59.384019: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1515248)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1515242)[0m 2024-04-15 05:10:02.036285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1515251)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1515251)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1515249)[0m 2024-04-15 05:09:59.770656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1515249)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1515250)[0m 2024-04-15 05:10:02.748707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 05:10:25,690 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 05:10:27,206 | server.py:125 | fit progress: (1, 1.9324214458465576, {'accuracy': 0.5669, 'data_size': 10000}, 30.51471839300939)
INFO flwr 2024-04-15 05:10:27,206 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 05:10:27,206 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:10:41,691 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 05:10:43,124 | server.py:125 | fit progress: (2, 1.6968107223510742, {'accuracy': 0.7824, 'data_size': 10000}, 46.433278819022235)
INFO flwr 2024-04-15 05:10:43,124 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 05:10:43,125 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:10:57,644 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 05:10:59,085 | server.py:125 | fit progress: (3, 1.5993956327438354, {'accuracy': 0.8724, 'data_size': 10000}, 62.39466822199756)
INFO flwr 2024-04-15 05:10:59,086 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 05:10:59,086 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:11:13,877 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 05:11:15,305 | server.py:125 | fit progress: (4, 1.581445574760437, {'accuracy': 0.8849, 'data_size': 10000}, 78.61461964301998)
INFO flwr 2024-04-15 05:11:15,306 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 05:11:15,306 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:11:29,223 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 05:11:30,433 | server.py:125 | fit progress: (5, 1.56883704662323, {'accuracy': 0.8933, 'data_size': 10000}, 93.74260680901352)
INFO flwr 2024-04-15 05:11:30,434 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 05:11:30,434 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:11:45,361 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 05:11:46,751 | server.py:125 | fit progress: (6, 1.5626025199890137, {'accuracy': 0.9002, 'data_size': 10000}, 110.06067895601154)
INFO flwr 2024-04-15 05:11:46,752 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 05:11:46,752 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:12:03,767 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 05:12:04,961 | server.py:125 | fit progress: (7, 1.55925714969635, {'accuracy': 0.9029, 'data_size': 10000}, 128.2703241360141)
INFO flwr 2024-04-15 05:12:04,961 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 05:12:04,962 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:12:20,105 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 05:12:21,256 | server.py:125 | fit progress: (8, 1.5561848878860474, {'accuracy': 0.9054, 'data_size': 10000}, 144.56544664900866)
INFO flwr 2024-04-15 05:12:21,257 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 05:12:21,257 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:12:35,102 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 05:12:36,527 | server.py:125 | fit progress: (9, 1.5528984069824219, {'accuracy': 0.9097, 'data_size': 10000}, 159.8361669339938)
INFO flwr 2024-04-15 05:12:36,527 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 05:12:36,527 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:12:51,700 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 05:12:53,164 | server.py:125 | fit progress: (10, 1.5492033958435059, {'accuracy': 0.9118, 'data_size': 10000}, 176.47281007101992)
INFO flwr 2024-04-15 05:12:53,164 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 05:12:53,164 | server.py:153 | FL finished in 176.473249142
INFO flwr 2024-04-15 05:12:53,164 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 05:12:53,164 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 05:12:53,164 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 05:12:53,165 | app.py:229 | app_fit: losses_centralized [(0, 2.3047380447387695), (1, 1.9324214458465576), (2, 1.6968107223510742), (3, 1.5993956327438354), (4, 1.581445574760437), (5, 1.56883704662323), (6, 1.5626025199890137), (7, 1.55925714969635), (8, 1.5561848878860474), (9, 1.5528984069824219), (10, 1.5492033958435059)]
INFO flwr 2024-04-15 05:12:53,165 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0849), (1, 0.5669), (2, 0.7824), (3, 0.8724), (4, 0.8849), (5, 0.8933), (6, 0.9002), (7, 0.9029), (8, 0.9054), (9, 0.9097), (10, 0.9118)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9118
wandb:     loss 1.5492
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_050929-m9qxnlad
wandb: Find logs at: ./wandb/offline-run-20240415_050929-m9qxnlad/logs
INFO flwr 2024-04-15 05:12:56,662 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 05:20:07,801 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1515244)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1515244)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 05:20:12,572	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 05:20:13,969	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 05:20:14,502	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 05:20:14,680	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 05:20:14,860	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 05:20:14,960	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 05:20:15,295	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1508c29deb020c37.zip' (216.24MiB) to Ray cluster...
2024-04-15 05:20:16,149	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1508c29deb020c37.zip'.
INFO flwr 2024-04-15 05:20:28,673 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 76121873203.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 167617704141.0}
INFO flwr 2024-04-15 05:20:28,673 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 05:20:28,673 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 05:20:28,693 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 05:20:28,694 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 05:20:28,694 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 05:20:28,695 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 05:20:31,700 | server.py:94 | initial parameters (loss, other metrics): 2.3052616119384766, {'accuracy': 0.0876, 'data_size': 10000}
INFO flwr 2024-04-15 05:20:31,701 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 05:20:31,701 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1519904)[0m 2024-04-15 05:20:35.334102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1519904)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1519914)[0m 2024-04-15 05:20:37.716003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1519914)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1519914)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1519913)[0m 2024-04-15 05:20:35.989241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1519913)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1519900)[0m 2024-04-15 05:20:38.555212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 05:21:01,792 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 05:21:03,250 | server.py:125 | fit progress: (1, 1.7461509704589844, {'accuracy': 0.732, 'data_size': 10000}, 31.549141944997245)
INFO flwr 2024-04-15 05:21:03,250 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 05:21:03,250 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:21:18,773 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 05:21:20,244 | server.py:125 | fit progress: (2, 1.6368557214736938, {'accuracy': 0.8274, 'data_size': 10000}, 48.542769205989316)
INFO flwr 2024-04-15 05:21:20,244 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 05:21:20,244 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:21:34,503 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 05:21:35,945 | server.py:125 | fit progress: (3, 1.6050652265548706, {'accuracy': 0.8567, 'data_size': 10000}, 64.24414142398746)
INFO flwr 2024-04-15 05:21:35,945 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 05:21:35,945 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:21:50,070 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 05:21:51,535 | server.py:125 | fit progress: (4, 1.5850515365600586, {'accuracy': 0.8773, 'data_size': 10000}, 79.83417493698653)
INFO flwr 2024-04-15 05:21:51,535 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 05:21:51,536 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:22:05,999 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 05:22:07,410 | server.py:125 | fit progress: (5, 1.56093430519104, {'accuracy': 0.9009, 'data_size': 10000}, 95.70952319700154)
INFO flwr 2024-04-15 05:22:07,411 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 05:22:07,411 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:22:21,889 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 05:22:23,390 | server.py:125 | fit progress: (6, 1.5788512229919434, {'accuracy': 0.8821, 'data_size': 10000}, 111.688989399001)
INFO flwr 2024-04-15 05:22:23,390 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 05:22:23,390 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:22:36,646 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 05:22:37,866 | server.py:125 | fit progress: (7, 1.5612123012542725, {'accuracy': 0.9003, 'data_size': 10000}, 126.16486577899195)
INFO flwr 2024-04-15 05:22:37,866 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 05:22:37,866 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:22:52,127 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 05:22:53,576 | server.py:125 | fit progress: (8, 1.5608474016189575, {'accuracy': 0.9012, 'data_size': 10000}, 141.87464564398397)
INFO flwr 2024-04-15 05:22:53,576 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 05:22:53,576 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:23:07,240 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 05:23:08,430 | server.py:125 | fit progress: (9, 1.5609972476959229, {'accuracy': 0.9012, 'data_size': 10000}, 156.72942956499173)
INFO flwr 2024-04-15 05:23:08,431 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 05:23:08,431 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:23:23,328 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 05:23:24,648 | server.py:125 | fit progress: (10, 1.5606892108917236, {'accuracy': 0.8993, 'data_size': 10000}, 172.9474822639895)
INFO flwr 2024-04-15 05:23:24,649 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 05:23:24,649 | server.py:153 | FL finished in 172.948048027989
INFO flwr 2024-04-15 05:23:24,649 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 05:23:24,649 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 05:23:24,649 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 05:23:24,650 | app.py:229 | app_fit: losses_centralized [(0, 2.3052616119384766), (1, 1.7461509704589844), (2, 1.6368557214736938), (3, 1.6050652265548706), (4, 1.5850515365600586), (5, 1.56093430519104), (6, 1.5788512229919434), (7, 1.5612123012542725), (8, 1.5608474016189575), (9, 1.5609972476959229), (10, 1.5606892108917236)]
INFO flwr 2024-04-15 05:23:24,650 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0876), (1, 0.732), (2, 0.8274), (3, 0.8567), (4, 0.8773), (5, 0.9009), (6, 0.8821), (7, 0.9003), (8, 0.9012), (9, 0.9012), (10, 0.8993)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8993
wandb:     loss 1.56069
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_052007-c9g6z08p
wandb: Find logs at: ./wandb/offline-run-20240415_052007-c9g6z08p/logs
INFO flwr 2024-04-15 05:23:28,229 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 05:30:40,194 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1519900)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1519900)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 05:30:46,047	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 05:30:47,156	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 05:30:47,677	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 05:30:47,847	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 05:30:48,022	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 05:30:48,119	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 05:30:48,447	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7b70b0da898990b2.zip' (216.26MiB) to Ray cluster...
2024-04-15 05:30:49,345	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7b70b0da898990b2.zip'.
INFO flwr 2024-04-15 05:31:01,416 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 76208711270.0, 'accelerator_type:TITAN': 1.0, 'memory': 167820326298.0}
INFO flwr 2024-04-15 05:31:01,416 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 05:31:01,416 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 05:31:01,438 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 05:31:01,439 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 05:31:01,440 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 05:31:01,440 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 05:31:04,676 | server.py:94 | initial parameters (loss, other metrics): 2.3024184703826904, {'accuracy': 0.0784, 'data_size': 10000}
INFO flwr 2024-04-15 05:31:04,677 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 05:31:04,678 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1524639)[0m 2024-04-15 05:31:07.987666: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1524639)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1524639)[0m 2024-04-15 05:31:10.441229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1524641)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1524641)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1524640)[0m 2024-04-15 05:31:08.426938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1524640)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1524632)[0m 2024-04-15 05:31:10.857073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 05:31:40,911 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 05:31:42,342 | server.py:125 | fit progress: (1, 2.0011017322540283, {'accuracy': 0.6715, 'data_size': 10000}, 37.664300775999436)
INFO flwr 2024-04-15 05:31:42,342 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 05:31:42,342 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:32:05,152 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 05:32:06,679 | server.py:125 | fit progress: (2, 1.7835345268249512, {'accuracy': 0.7525, 'data_size': 10000}, 62.00192719997722)
INFO flwr 2024-04-15 05:32:06,680 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 05:32:06,680 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:32:29,990 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 05:32:31,455 | server.py:125 | fit progress: (3, 1.710611343383789, {'accuracy': 0.7763, 'data_size': 10000}, 86.77799776598113)
INFO flwr 2024-04-15 05:32:31,456 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 05:32:31,456 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:32:53,945 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 05:32:55,336 | server.py:125 | fit progress: (4, 1.6409801244735718, {'accuracy': 0.8384, 'data_size': 10000}, 110.65854498298722)
INFO flwr 2024-04-15 05:32:55,336 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 05:32:55,336 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:33:17,492 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 05:33:18,782 | server.py:125 | fit progress: (5, 1.5921143293380737, {'accuracy': 0.8828, 'data_size': 10000}, 134.1046737729921)
INFO flwr 2024-04-15 05:33:18,782 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 05:33:18,783 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:33:41,912 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 05:33:43,114 | server.py:125 | fit progress: (6, 1.5720038414001465, {'accuracy': 0.8992, 'data_size': 10000}, 158.43660243297927)
INFO flwr 2024-04-15 05:33:43,114 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 05:33:43,115 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:34:04,212 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 05:34:05,440 | server.py:125 | fit progress: (7, 1.5691134929656982, {'accuracy': 0.9007, 'data_size': 10000}, 180.76217905399972)
INFO flwr 2024-04-15 05:34:05,440 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 05:34:05,440 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:34:32,262 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 05:34:33,772 | server.py:125 | fit progress: (8, 1.5644534826278687, {'accuracy': 0.9031, 'data_size': 10000}, 209.0941634499759)
INFO flwr 2024-04-15 05:34:33,772 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 05:34:33,772 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:35:02,456 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 05:35:03,972 | server.py:125 | fit progress: (9, 1.5604304075241089, {'accuracy': 0.9057, 'data_size': 10000}, 239.2950790989853)
INFO flwr 2024-04-15 05:35:03,973 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 05:35:03,973 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:35:25,564 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 05:35:26,784 | server.py:125 | fit progress: (10, 1.5550835132598877, {'accuracy': 0.9095, 'data_size': 10000}, 262.10628151797573)
INFO flwr 2024-04-15 05:35:26,784 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 05:35:26,784 | server.py:153 | FL finished in 262.1068215580017
INFO flwr 2024-04-15 05:35:26,784 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 05:35:26,785 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 05:35:26,785 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 05:35:26,785 | app.py:229 | app_fit: losses_centralized [(0, 2.3024184703826904), (1, 2.0011017322540283), (2, 1.7835345268249512), (3, 1.710611343383789), (4, 1.6409801244735718), (5, 1.5921143293380737), (6, 1.5720038414001465), (7, 1.5691134929656982), (8, 1.5644534826278687), (9, 1.5604304075241089), (10, 1.5550835132598877)]
INFO flwr 2024-04-15 05:35:26,785 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0784), (1, 0.6715), (2, 0.7525), (3, 0.7763), (4, 0.8384), (5, 0.8828), (6, 0.8992), (7, 0.9007), (8, 0.9031), (9, 0.9057), (10, 0.9095)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9095
wandb:     loss 1.55508
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_053039-bfyudyfn
wandb: Find logs at: ./wandb/offline-run-20240415_053039-bfyudyfn/logs
INFO flwr 2024-04-15 05:35:30,311 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 05:42:41,195 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1524629)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1524629)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 05:42:46,190	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 05:42:47,356	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 05:42:47,857	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 05:42:48,045	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 05:42:48,218	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 05:42:48,317	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 05:42:48,652	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_af4a393694f196bb.zip' (216.27MiB) to Ray cluster...
2024-04-15 05:42:49,585	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_af4a393694f196bb.zip'.
INFO flwr 2024-04-15 05:43:03,306 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'object_store_memory': 76072766668.0, 'memory': 167503122228.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-04-15 05:43:03,307 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 05:43:03,307 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 05:43:03,327 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 05:43:03,328 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 05:43:03,328 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 05:43:03,329 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 05:43:06,493 | server.py:94 | initial parameters (loss, other metrics): 2.3030407428741455, {'accuracy': 0.1123, 'data_size': 10000}
INFO flwr 2024-04-15 05:43:06,493 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 05:43:06,494 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1529300)[0m 2024-04-15 05:43:10.141414: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1529300)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1529301)[0m 2024-04-15 05:43:12.778625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1529301)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1529301)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1529298)[0m 2024-04-15 05:43:10.645605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1529298)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1529298)[0m 2024-04-15 05:43:13.567892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 05:43:45,529 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 05:43:46,967 | server.py:125 | fit progress: (1, 1.816626787185669, {'accuracy': 0.7581, 'data_size': 10000}, 40.47378420300083)
INFO flwr 2024-04-15 05:43:46,968 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 05:43:46,968 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:44:10,098 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 05:44:11,483 | server.py:125 | fit progress: (2, 1.6643210649490356, {'accuracy': 0.8248, 'data_size': 10000}, 64.98937057002331)
INFO flwr 2024-04-15 05:44:11,483 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 05:44:11,483 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:44:36,742 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 05:44:38,178 | server.py:125 | fit progress: (3, 1.5984705686569214, {'accuracy': 0.8724, 'data_size': 10000}, 91.68452550302027)
INFO flwr 2024-04-15 05:44:38,178 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 05:44:38,179 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:45:02,318 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 05:45:03,813 | server.py:125 | fit progress: (4, 1.5690670013427734, {'accuracy': 0.8983, 'data_size': 10000}, 117.31918490602402)
INFO flwr 2024-04-15 05:45:03,813 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 05:45:03,813 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:45:24,422 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 05:45:25,899 | server.py:125 | fit progress: (5, 1.5645030736923218, {'accuracy': 0.9, 'data_size': 10000}, 139.405731218023)
INFO flwr 2024-04-15 05:45:25,900 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 05:45:25,900 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:45:48,096 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 05:45:49,567 | server.py:125 | fit progress: (6, 1.5609487295150757, {'accuracy': 0.902, 'data_size': 10000}, 163.0730299820134)
INFO flwr 2024-04-15 05:45:49,567 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 05:45:49,567 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:46:12,969 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 05:46:14,187 | server.py:125 | fit progress: (7, 1.5572925806045532, {'accuracy': 0.9063, 'data_size': 10000}, 187.69319036000525)
INFO flwr 2024-04-15 05:46:14,187 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 05:46:14,187 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:46:37,694 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 05:46:39,101 | server.py:125 | fit progress: (8, 1.5625683069229126, {'accuracy': 0.9006, 'data_size': 10000}, 212.6070743750024)
INFO flwr 2024-04-15 05:46:39,101 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 05:46:39,101 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:47:06,083 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 05:47:07,291 | server.py:125 | fit progress: (9, 1.5592479705810547, {'accuracy': 0.903, 'data_size': 10000}, 240.79772272802074)
INFO flwr 2024-04-15 05:47:07,292 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 05:47:07,292 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:47:31,103 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 05:47:32,298 | server.py:125 | fit progress: (10, 1.5530427694320679, {'accuracy': 0.9086, 'data_size': 10000}, 265.8044278380112)
INFO flwr 2024-04-15 05:47:32,298 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 05:47:32,298 | server.py:153 | FL finished in 265.8048966550268
INFO flwr 2024-04-15 05:47:32,299 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 05:47:32,299 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 05:47:32,299 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 05:47:32,299 | app.py:229 | app_fit: losses_centralized [(0, 2.3030407428741455), (1, 1.816626787185669), (2, 1.6643210649490356), (3, 1.5984705686569214), (4, 1.5690670013427734), (5, 1.5645030736923218), (6, 1.5609487295150757), (7, 1.5572925806045532), (8, 1.5625683069229126), (9, 1.5592479705810547), (10, 1.5530427694320679)]
INFO flwr 2024-04-15 05:47:32,299 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1123), (1, 0.7581), (2, 0.8248), (3, 0.8724), (4, 0.8983), (5, 0.9), (6, 0.902), (7, 0.9063), (8, 0.9006), (9, 0.903), (10, 0.9086)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9086
wandb:     loss 1.55304
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_054240-25p6ud55
wandb: Find logs at: ./wandb/offline-run-20240415_054240-25p6ud55/logs
INFO flwr 2024-04-15 05:47:35,889 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 05:54:47,056 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1529294)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1529294)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 05:54:52,938	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 05:54:54,043	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 05:54:54,578	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 05:54:54,752	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 05:54:54,928	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 05:54:55,028	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 05:54:55,366	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_26ab9c7bd06ab5c8.zip' (216.28MiB) to Ray cluster...
2024-04-15 05:54:56,351	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_26ab9c7bd06ab5c8.zip'.
INFO flwr 2024-04-15 05:55:08,659 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 76045986201.0, 'memory': 167440634471.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-15 05:55:08,659 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 05:55:08,660 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 05:55:08,682 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 05:55:08,683 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 05:55:08,683 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 05:55:08,684 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 05:55:11,904 | server.py:94 | initial parameters (loss, other metrics): 2.305694818496704, {'accuracy': 0.1165, 'data_size': 10000}
INFO flwr 2024-04-15 05:55:11,907 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 05:55:11,908 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1534915)[0m 2024-04-15 05:55:15.724407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1534915)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1534915)[0m 2024-04-15 05:55:18.106741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1534915)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1534915)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1534906)[0m 2024-04-15 05:55:16.077003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1534906)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1534913)[0m 2024-04-15 05:55:18.873105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 05:55:48,645 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 05:55:50,087 | server.py:125 | fit progress: (1, 1.8435487747192383, {'accuracy': 0.63, 'data_size': 10000}, 38.17992534098448)
INFO flwr 2024-04-15 05:55:50,088 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 05:55:50,088 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:56:14,622 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 05:56:16,040 | server.py:125 | fit progress: (2, 1.6183221340179443, {'accuracy': 0.8503, 'data_size': 10000}, 64.13287501499872)
INFO flwr 2024-04-15 05:56:16,041 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 05:56:16,041 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:56:38,586 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 05:56:40,004 | server.py:125 | fit progress: (3, 1.579942226409912, {'accuracy': 0.884, 'data_size': 10000}, 88.09646560499095)
INFO flwr 2024-04-15 05:56:40,004 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 05:56:40,004 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:57:02,629 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 05:57:04,083 | server.py:125 | fit progress: (4, 1.588786005973816, {'accuracy': 0.8731, 'data_size': 10000}, 112.17595334898215)
INFO flwr 2024-04-15 05:57:04,084 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 05:57:04,084 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:57:26,533 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 05:57:27,968 | server.py:125 | fit progress: (5, 1.5660948753356934, {'accuracy': 0.8961, 'data_size': 10000}, 136.06081699600327)
INFO flwr 2024-04-15 05:57:27,968 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 05:57:27,969 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:57:50,258 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 05:57:51,719 | server.py:125 | fit progress: (6, 1.5783675909042358, {'accuracy': 0.8835, 'data_size': 10000}, 159.8119442879979)
INFO flwr 2024-04-15 05:57:51,720 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 05:57:51,720 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:58:16,840 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 05:58:18,060 | server.py:125 | fit progress: (7, 1.5680803060531616, {'accuracy': 0.8928, 'data_size': 10000}, 186.15305459900992)
INFO flwr 2024-04-15 05:58:18,061 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 05:58:18,061 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:58:41,280 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 05:58:42,700 | server.py:125 | fit progress: (8, 1.5610755681991577, {'accuracy': 0.9003, 'data_size': 10000}, 210.79246416300884)
INFO flwr 2024-04-15 05:58:42,700 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 05:58:42,700 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:59:05,077 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 05:59:06,261 | server.py:125 | fit progress: (9, 1.5560322999954224, {'accuracy': 0.9047, 'data_size': 10000}, 234.35350926598767)
INFO flwr 2024-04-15 05:59:06,261 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 05:59:06,261 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 05:59:30,567 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 05:59:31,795 | server.py:125 | fit progress: (10, 1.5541677474975586, {'accuracy': 0.9061, 'data_size': 10000}, 259.8873978880001)
INFO flwr 2024-04-15 05:59:31,795 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 05:59:31,795 | server.py:153 | FL finished in 259.88788868300617
INFO flwr 2024-04-15 05:59:31,795 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 05:59:31,796 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 05:59:31,796 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 05:59:31,796 | app.py:229 | app_fit: losses_centralized [(0, 2.305694818496704), (1, 1.8435487747192383), (2, 1.6183221340179443), (3, 1.579942226409912), (4, 1.588786005973816), (5, 1.5660948753356934), (6, 1.5783675909042358), (7, 1.5680803060531616), (8, 1.5610755681991577), (9, 1.5560322999954224), (10, 1.5541677474975586)]
INFO flwr 2024-04-15 05:59:31,796 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1165), (1, 0.63), (2, 0.8503), (3, 0.884), (4, 0.8731), (5, 0.8961), (6, 0.8835), (7, 0.8928), (8, 0.9003), (9, 0.9047), (10, 0.9061)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9061
wandb:     loss 1.55417
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_055446-jdhtjwu6
wandb: Find logs at: ./wandb/offline-run-20240415_055446-jdhtjwu6/logs
INFO flwr 2024-04-15 05:59:35,362 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 06:06:46,137 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1534906)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1534906)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 06:06:51,049	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 06:06:52,280	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 06:06:52,810	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 06:06:52,983	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 06:06:53,156	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 06:06:53,253	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 06:06:53,599	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c558fb3d7fae6cf0.zip' (216.29MiB) to Ray cluster...
2024-04-15 06:06:54,499	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c558fb3d7fae6cf0.zip'.
INFO flwr 2024-04-15 06:07:06,818 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 75900939878.0, 'memory': 167102193050.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-15 06:07:06,818 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 06:07:06,818 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 06:07:06,838 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 06:07:06,840 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 06:07:06,840 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 06:07:06,840 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 06:07:10,079 | server.py:94 | initial parameters (loss, other metrics): 2.2981507778167725, {'accuracy': 0.1221, 'data_size': 10000}
INFO flwr 2024-04-15 06:07:10,080 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 06:07:10,080 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1539313)[0m 2024-04-15 06:07:13.537131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1539313)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1539313)[0m 2024-04-15 06:07:16.102730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1539313)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1539313)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1539309)[0m 2024-04-15 06:07:14.093142: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1539309)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1539309)[0m 2024-04-15 06:07:16.731156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 06:07:51,274 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 06:07:52,732 | server.py:125 | fit progress: (1, 1.98014497756958, {'accuracy': 0.6886, 'data_size': 10000}, 42.65171252802247)
INFO flwr 2024-04-15 06:07:52,732 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 06:07:52,732 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:08:15,580 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 06:08:17,026 | server.py:125 | fit progress: (2, 1.8001224994659424, {'accuracy': 0.7336, 'data_size': 10000}, 66.9462240730063)
INFO flwr 2024-04-15 06:08:17,027 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 06:08:17,027 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:08:41,718 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 06:08:43,206 | server.py:125 | fit progress: (3, 1.7135963439941406, {'accuracy': 0.7749, 'data_size': 10000}, 93.12630819602055)
INFO flwr 2024-04-15 06:08:43,207 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 06:08:43,207 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:09:09,109 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 06:09:10,567 | server.py:125 | fit progress: (4, 1.6486130952835083, {'accuracy': 0.8284, 'data_size': 10000}, 120.48736580001423)
INFO flwr 2024-04-15 06:09:10,568 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 06:09:10,568 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:09:36,842 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 06:09:38,313 | server.py:125 | fit progress: (5, 1.607690453529358, {'accuracy': 0.8676, 'data_size': 10000}, 148.23300712302444)
INFO flwr 2024-04-15 06:09:38,313 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 06:09:38,314 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:10:02,761 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 06:10:04,177 | server.py:125 | fit progress: (6, 1.580502986907959, {'accuracy': 0.8898, 'data_size': 10000}, 174.09722838300513)
INFO flwr 2024-04-15 06:10:04,178 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 06:10:04,178 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:10:27,051 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 06:10:28,262 | server.py:125 | fit progress: (7, 1.566683053970337, {'accuracy': 0.8998, 'data_size': 10000}, 198.18198039202252)
INFO flwr 2024-04-15 06:10:28,262 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 06:10:28,263 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:10:51,957 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 06:10:53,353 | server.py:125 | fit progress: (8, 1.564553141593933, {'accuracy': 0.902, 'data_size': 10000}, 223.27267119000317)
INFO flwr 2024-04-15 06:10:53,353 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 06:10:53,353 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:11:16,347 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 06:11:17,542 | server.py:125 | fit progress: (9, 1.5647222995758057, {'accuracy': 0.8991, 'data_size': 10000}, 247.4622775700118)
INFO flwr 2024-04-15 06:11:17,543 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 06:11:17,543 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:11:40,535 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 06:11:41,742 | server.py:125 | fit progress: (10, 1.5625345706939697, {'accuracy': 0.9014, 'data_size': 10000}, 271.6619911210146)
INFO flwr 2024-04-15 06:11:41,742 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 06:11:41,742 | server.py:153 | FL finished in 271.6624482090119
INFO flwr 2024-04-15 06:11:41,743 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 06:11:41,743 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 06:11:41,743 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 06:11:41,743 | app.py:229 | app_fit: losses_centralized [(0, 2.2981507778167725), (1, 1.98014497756958), (2, 1.8001224994659424), (3, 1.7135963439941406), (4, 1.6486130952835083), (5, 1.607690453529358), (6, 1.580502986907959), (7, 1.566683053970337), (8, 1.564553141593933), (9, 1.5647222995758057), (10, 1.5625345706939697)]
INFO flwr 2024-04-15 06:11:41,743 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1221), (1, 0.6886), (2, 0.7336), (3, 0.7749), (4, 0.8284), (5, 0.8676), (6, 0.8898), (7, 0.8998), (8, 0.902), (9, 0.8991), (10, 0.9014)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9014
wandb:     loss 1.56253
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_060645-itk5eiyv
wandb: Find logs at: ./wandb/offline-run-20240415_060645-itk5eiyv/logs
INFO flwr 2024-04-15 06:11:45,362 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 06:18:56,047 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1539307)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1539307)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 06:19:00,659	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 06:19:02,125	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 06:19:02,687	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 06:19:02,859	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 06:19:03,032	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 06:19:03,130	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 06:19:03,459	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2a22ac82900220ba.zip' (216.31MiB) to Ray cluster...
2024-04-15 06:19:04,294	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2a22ac82900220ba.zip'.
INFO flwr 2024-04-15 06:19:16,680 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77727532646.0, 'CPU': 64.0, 'GPU': 1.0, 'memory': 171364242842.0, 'accelerator_type:TITAN': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-15 06:19:16,680 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 06:19:16,680 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 06:19:16,696 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 06:19:16,697 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 06:19:16,697 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 06:19:16,697 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 06:19:20,342 | server.py:94 | initial parameters (loss, other metrics): 2.3058156967163086, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-15 06:19:20,347 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 06:19:20,348 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1545367)[0m 2024-04-15 06:19:23.373239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1545367)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1545359)[0m 2024-04-15 06:19:25.672721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1545369)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1545369)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1545375)[0m 2024-04-15 06:19:23.574854: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1545375)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1545364)[0m 2024-04-15 06:19:27.178279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 06:19:55,696 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 06:19:57,154 | server.py:125 | fit progress: (1, 1.8559380769729614, {'accuracy': 0.6865, 'data_size': 10000}, 36.80600152601255)
INFO flwr 2024-04-15 06:19:57,154 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 06:19:57,154 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:20:19,009 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 06:20:20,467 | server.py:125 | fit progress: (2, 1.7022205591201782, {'accuracy': 0.7831, 'data_size': 10000}, 60.11973338999087)
INFO flwr 2024-04-15 06:20:20,468 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 06:20:20,468 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:20:45,122 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 06:20:46,517 | server.py:125 | fit progress: (3, 1.592530608177185, {'accuracy': 0.8811, 'data_size': 10000}, 86.16961591300787)
INFO flwr 2024-04-15 06:20:46,518 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 06:20:46,518 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:21:08,540 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 06:21:09,969 | server.py:125 | fit progress: (4, 1.568969964981079, {'accuracy': 0.8972, 'data_size': 10000}, 109.62159553798847)
INFO flwr 2024-04-15 06:21:09,970 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 06:21:09,970 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:21:32,121 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 06:21:33,503 | server.py:125 | fit progress: (5, 1.5659981966018677, {'accuracy': 0.8973, 'data_size': 10000}, 133.15502029500203)
INFO flwr 2024-04-15 06:21:33,503 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 06:21:33,503 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:21:56,160 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 06:21:57,578 | server.py:125 | fit progress: (6, 1.565736174583435, {'accuracy': 0.8975, 'data_size': 10000}, 157.2303461029951)
INFO flwr 2024-04-15 06:21:57,578 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 06:21:57,579 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:22:18,401 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 06:22:19,577 | server.py:125 | fit progress: (7, 1.5693745613098145, {'accuracy': 0.893, 'data_size': 10000}, 179.2292456649884)
INFO flwr 2024-04-15 06:22:19,577 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 06:22:19,577 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:22:45,395 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 06:22:46,824 | server.py:125 | fit progress: (8, 1.568204641342163, {'accuracy': 0.8931, 'data_size': 10000}, 206.47597924698493)
INFO flwr 2024-04-15 06:22:46,824 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 06:22:46,824 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:23:06,059 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 06:23:07,237 | server.py:125 | fit progress: (9, 1.5537863969802856, {'accuracy': 0.9092, 'data_size': 10000}, 226.88979010700132)
INFO flwr 2024-04-15 06:23:07,238 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 06:23:07,238 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:23:29,270 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 06:23:30,536 | server.py:125 | fit progress: (10, 1.5542339086532593, {'accuracy': 0.9075, 'data_size': 10000}, 250.18870994300232)
INFO flwr 2024-04-15 06:23:30,537 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 06:23:30,537 | server.py:153 | FL finished in 250.18926229901263
INFO flwr 2024-04-15 06:23:30,537 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 06:23:30,537 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 06:23:30,538 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 06:23:30,538 | app.py:229 | app_fit: losses_centralized [(0, 2.3058156967163086), (1, 1.8559380769729614), (2, 1.7022205591201782), (3, 1.592530608177185), (4, 1.568969964981079), (5, 1.5659981966018677), (6, 1.565736174583435), (7, 1.5693745613098145), (8, 1.568204641342163), (9, 1.5537863969802856), (10, 1.5542339086532593)]
INFO flwr 2024-04-15 06:23:30,538 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.6865), (2, 0.7831), (3, 0.8811), (4, 0.8972), (5, 0.8973), (6, 0.8975), (7, 0.893), (8, 0.8931), (9, 0.9092), (10, 0.9075)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9075
wandb:     loss 1.55423
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_061855-1rbeis4j
wandb: Find logs at: ./wandb/offline-run-20240415_061855-1rbeis4j/logs
INFO flwr 2024-04-15 06:23:34,046 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 06:30:44,897 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1545359)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1545359)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 06:30:52,339	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 06:30:53,483	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 06:30:54,022	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 06:30:54,211	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 06:30:54,400	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 06:30:54,510	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 06:30:54,849	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6945badcb510052d.zip' (216.32MiB) to Ray cluster...
2024-04-15 06:30:55,694	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6945badcb510052d.zip'.
INFO flwr 2024-04-15 06:31:08,838 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 75933714432.0, 'GPU': 1.0, 'memory': 167178667008.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-15 06:31:08,839 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 06:31:08,839 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 06:31:08,859 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 06:31:08,860 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 06:31:08,861 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 06:31:08,861 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 06:31:11,867 | server.py:94 | initial parameters (loss, other metrics): 2.304542303085327, {'accuracy': 0.0715, 'data_size': 10000}
INFO flwr 2024-04-15 06:31:11,867 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 06:31:11,868 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1550441)[0m 2024-04-15 06:31:15.722384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1550441)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1550441)[0m 2024-04-15 06:31:18.435918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1550439)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1550439)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1550444)[0m 2024-04-15 06:31:16.241888: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1550444)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1550444)[0m 2024-04-15 06:31:18.954645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 06:31:49,938 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 06:31:51,413 | server.py:125 | fit progress: (1, 1.695168137550354, {'accuracy': 0.797, 'data_size': 10000}, 39.545291554997675)
INFO flwr 2024-04-15 06:31:51,413 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 06:31:51,414 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:32:15,500 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 06:32:16,962 | server.py:125 | fit progress: (2, 1.597122073173523, {'accuracy': 0.8708, 'data_size': 10000}, 65.09492476601736)
INFO flwr 2024-04-15 06:32:16,963 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 06:32:16,963 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:32:39,865 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 06:32:41,317 | server.py:125 | fit progress: (3, 1.5888949632644653, {'accuracy': 0.8749, 'data_size': 10000}, 89.44941326300614)
INFO flwr 2024-04-15 06:32:41,317 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 06:32:41,317 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:33:04,659 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 06:33:06,193 | server.py:125 | fit progress: (4, 1.568211555480957, {'accuracy': 0.8952, 'data_size': 10000}, 114.32547609900939)
INFO flwr 2024-04-15 06:33:06,193 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 06:33:06,193 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:33:27,096 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 06:33:28,525 | server.py:125 | fit progress: (5, 1.5720062255859375, {'accuracy': 0.8905, 'data_size': 10000}, 136.65768744499655)
INFO flwr 2024-04-15 06:33:28,525 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 06:33:28,525 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:33:50,408 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 06:33:51,939 | server.py:125 | fit progress: (6, 1.5534799098968506, {'accuracy': 0.9077, 'data_size': 10000}, 160.07166132400744)
INFO flwr 2024-04-15 06:33:51,939 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 06:33:51,939 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:34:14,531 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 06:34:15,748 | server.py:125 | fit progress: (7, 1.576759934425354, {'accuracy': 0.8834, 'data_size': 10000}, 183.88053151001805)
INFO flwr 2024-04-15 06:34:15,748 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 06:34:15,748 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:34:38,792 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 06:34:40,304 | server.py:125 | fit progress: (8, 1.5955893993377686, {'accuracy': 0.8645, 'data_size': 10000}, 208.4362487780163)
INFO flwr 2024-04-15 06:34:40,304 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 06:34:40,304 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:35:04,992 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 06:35:06,198 | server.py:125 | fit progress: (9, 1.563410758972168, {'accuracy': 0.8973, 'data_size': 10000}, 234.33066446901648)
INFO flwr 2024-04-15 06:35:06,198 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 06:35:06,198 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:35:28,117 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 06:35:29,331 | server.py:125 | fit progress: (10, 1.5529836416244507, {'accuracy': 0.9082, 'data_size': 10000}, 257.46375961601734)
INFO flwr 2024-04-15 06:35:29,331 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 06:35:29,332 | server.py:153 | FL finished in 257.4642380489968
INFO flwr 2024-04-15 06:35:29,332 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 06:35:29,332 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 06:35:29,332 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 06:35:29,332 | app.py:229 | app_fit: losses_centralized [(0, 2.304542303085327), (1, 1.695168137550354), (2, 1.597122073173523), (3, 1.5888949632644653), (4, 1.568211555480957), (5, 1.5720062255859375), (6, 1.5534799098968506), (7, 1.576759934425354), (8, 1.5955893993377686), (9, 1.563410758972168), (10, 1.5529836416244507)]
INFO flwr 2024-04-15 06:35:29,332 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0715), (1, 0.797), (2, 0.8708), (3, 0.8749), (4, 0.8952), (5, 0.8905), (6, 0.9077), (7, 0.8834), (8, 0.8645), (9, 0.8973), (10, 0.9082)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9082
wandb:     loss 1.55298
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_063044-jdgzmejn
wandb: Find logs at: ./wandb/offline-run-20240415_063044-jdgzmejn/logs
INFO flwr 2024-04-15 06:35:32,910 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 06:42:44,571 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1550435)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1550435)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 06:42:49,979	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 06:42:51,138	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 06:42:51,795	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 06:42:51,995	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 06:42:52,167	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 06:42:52,281	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 06:42:52,758	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_98e723b86d5841e3.zip' (216.33MiB) to Ray cluster...
2024-04-15 06:42:53,694	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_98e723b86d5841e3.zip'.
INFO flwr 2024-04-15 06:43:06,707 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 77852216524.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 171655171892.0}
INFO flwr 2024-04-15 06:43:06,708 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 06:43:06,708 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 06:43:06,740 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 06:43:06,741 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 06:43:06,742 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 06:43:06,742 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 06:43:09,445 | server.py:94 | initial parameters (loss, other metrics): 2.3042690753936768, {'accuracy': 0.0929, 'data_size': 10000}
INFO flwr 2024-04-15 06:43:09,446 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 06:43:09,446 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1554915)[0m 2024-04-15 06:43:13.103292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1554915)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1554915)[0m 2024-04-15 06:43:16.351862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1554913)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1554913)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1554916)[0m 2024-04-15 06:43:14.215966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1554916)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1554916)[0m 2024-04-15 06:43:17.282991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 06:43:48,134 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 06:43:49,713 | server.py:125 | fit progress: (1, 1.9752769470214844, {'accuracy': 0.7266, 'data_size': 10000}, 40.26740854099626)
INFO flwr 2024-04-15 06:43:49,714 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 06:43:49,714 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:44:11,576 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 06:44:13,018 | server.py:125 | fit progress: (2, 1.7802128791809082, {'accuracy': 0.7894, 'data_size': 10000}, 63.5720174619928)
INFO flwr 2024-04-15 06:44:13,018 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 06:44:13,018 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:44:34,796 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 06:44:36,261 | server.py:125 | fit progress: (3, 1.6964054107666016, {'accuracy': 0.812, 'data_size': 10000}, 86.81555498598027)
INFO flwr 2024-04-15 06:44:36,262 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 06:44:36,262 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:44:57,216 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 06:44:58,692 | server.py:125 | fit progress: (4, 1.6582218408584595, {'accuracy': 0.8262, 'data_size': 10000}, 109.24624465598026)
INFO flwr 2024-04-15 06:44:58,692 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 06:44:58,693 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:45:23,893 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 06:45:25,321 | server.py:125 | fit progress: (5, 1.6257874965667725, {'accuracy': 0.8472, 'data_size': 10000}, 135.875652341987)
INFO flwr 2024-04-15 06:45:25,322 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 06:45:25,322 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:45:49,147 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 06:45:50,622 | server.py:125 | fit progress: (6, 1.5932356119155884, {'accuracy': 0.8769, 'data_size': 10000}, 161.1764776139753)
INFO flwr 2024-04-15 06:45:50,623 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 06:45:50,623 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:46:16,162 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 06:46:17,363 | server.py:125 | fit progress: (7, 1.57589590549469, {'accuracy': 0.8899, 'data_size': 10000}, 187.9176407949999)
INFO flwr 2024-04-15 06:46:17,364 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 06:46:17,364 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:46:38,391 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 06:46:39,858 | server.py:125 | fit progress: (8, 1.5656986236572266, {'accuracy': 0.9006, 'data_size': 10000}, 210.41230907000136)
INFO flwr 2024-04-15 06:46:39,858 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 06:46:39,859 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:47:00,028 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 06:47:01,253 | server.py:125 | fit progress: (9, 1.5575506687164307, {'accuracy': 0.9079, 'data_size': 10000}, 231.80737711198162)
INFO flwr 2024-04-15 06:47:01,254 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 06:47:01,254 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:47:23,920 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 06:47:25,105 | server.py:125 | fit progress: (10, 1.5542761087417603, {'accuracy': 0.9114, 'data_size': 10000}, 255.6588743259781)
INFO flwr 2024-04-15 06:47:25,105 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 06:47:25,105 | server.py:153 | FL finished in 255.65935562399682
INFO flwr 2024-04-15 06:47:25,105 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 06:47:25,105 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 06:47:25,106 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 06:47:25,106 | app.py:229 | app_fit: losses_centralized [(0, 2.3042690753936768), (1, 1.9752769470214844), (2, 1.7802128791809082), (3, 1.6964054107666016), (4, 1.6582218408584595), (5, 1.6257874965667725), (6, 1.5932356119155884), (7, 1.57589590549469), (8, 1.5656986236572266), (9, 1.5575506687164307), (10, 1.5542761087417603)]
INFO flwr 2024-04-15 06:47:25,106 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0929), (1, 0.7266), (2, 0.7894), (3, 0.812), (4, 0.8262), (5, 0.8472), (6, 0.8769), (7, 0.8899), (8, 0.9006), (9, 0.9079), (10, 0.9114)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9114
wandb:     loss 1.55428
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_064243-xo2sx0k0
wandb: Find logs at: ./wandb/offline-run-20240415_064243-xo2sx0k0/logs
INFO flwr 2024-04-15 06:47:28,686 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 06:54:39,531 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1554916)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1554916)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 06:54:44,859	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 06:54:46,207	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 06:54:46,817	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 06:54:46,995	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 06:54:47,175	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 06:54:47,276	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 06:54:47,616	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4303994c0a2ab4ca.zip' (216.34MiB) to Ray cluster...
2024-04-15 06:54:48,553	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4303994c0a2ab4ca.zip'.
INFO flwr 2024-04-15 06:55:01,558 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75916111872.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 167137594368.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-15 06:55:01,559 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 06:55:01,559 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 06:55:01,579 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 06:55:01,580 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 06:55:01,580 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 06:55:01,580 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 06:55:05,204 | server.py:94 | initial parameters (loss, other metrics): 2.3048839569091797, {'accuracy': 0.0672, 'data_size': 10000}
INFO flwr 2024-04-15 06:55:05,205 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 06:55:05,206 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1560517)[0m 2024-04-15 06:55:08.405538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1560517)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1560517)[0m 2024-04-15 06:55:11.317641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1560514)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1560514)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1560512)[0m 2024-04-15 06:55:08.828768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1560512)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1560512)[0m 2024-04-15 06:55:11.444262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 06:55:42,394 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 06:55:43,923 | server.py:125 | fit progress: (1, 1.8780184984207153, {'accuracy': 0.6497, 'data_size': 10000}, 38.71757945601712)
INFO flwr 2024-04-15 06:55:43,924 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 06:55:43,924 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:56:07,708 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 06:56:09,221 | server.py:125 | fit progress: (2, 1.691510558128357, {'accuracy': 0.7974, 'data_size': 10000}, 64.01531669299584)
INFO flwr 2024-04-15 06:56:09,221 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 06:56:09,222 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:56:32,592 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 06:56:34,072 | server.py:125 | fit progress: (3, 1.6093302965164185, {'accuracy': 0.861, 'data_size': 10000}, 88.86655437300215)
INFO flwr 2024-04-15 06:56:34,073 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 06:56:34,073 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:56:55,875 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 06:56:57,315 | server.py:125 | fit progress: (4, 1.5790823698043823, {'accuracy': 0.8885, 'data_size': 10000}, 112.1096247509995)
INFO flwr 2024-04-15 06:56:57,316 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 06:56:57,316 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:57:19,274 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 06:57:20,707 | server.py:125 | fit progress: (5, 1.5680681467056274, {'accuracy': 0.8945, 'data_size': 10000}, 135.50131722699734)
INFO flwr 2024-04-15 06:57:20,707 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 06:57:20,708 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:57:43,251 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 06:57:44,737 | server.py:125 | fit progress: (6, 1.5593082904815674, {'accuracy': 0.9022, 'data_size': 10000}, 159.5308542099956)
INFO flwr 2024-04-15 06:57:44,737 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 06:57:44,737 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:58:04,786 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 06:58:05,963 | server.py:125 | fit progress: (7, 1.553141474723816, {'accuracy': 0.9079, 'data_size': 10000}, 180.75691293200362)
INFO flwr 2024-04-15 06:58:05,963 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 06:58:05,963 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:58:27,366 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 06:58:28,792 | server.py:125 | fit progress: (8, 1.5516127347946167, {'accuracy': 0.9094, 'data_size': 10000}, 203.5861121659982)
INFO flwr 2024-04-15 06:58:28,792 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 06:58:28,792 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:58:52,523 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 06:58:53,794 | server.py:125 | fit progress: (9, 1.5549057722091675, {'accuracy': 0.9074, 'data_size': 10000}, 228.58853557301336)
INFO flwr 2024-04-15 06:58:53,795 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 06:58:53,795 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 06:59:16,877 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 06:59:18,086 | server.py:125 | fit progress: (10, 1.5549190044403076, {'accuracy': 0.9071, 'data_size': 10000}, 252.88048613999854)
INFO flwr 2024-04-15 06:59:18,087 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 06:59:18,087 | server.py:153 | FL finished in 252.88094158199965
INFO flwr 2024-04-15 06:59:18,087 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 06:59:18,087 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 06:59:18,087 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 06:59:18,087 | app.py:229 | app_fit: losses_centralized [(0, 2.3048839569091797), (1, 1.8780184984207153), (2, 1.691510558128357), (3, 1.6093302965164185), (4, 1.5790823698043823), (5, 1.5680681467056274), (6, 1.5593082904815674), (7, 1.553141474723816), (8, 1.5516127347946167), (9, 1.5549057722091675), (10, 1.5549190044403076)]
INFO flwr 2024-04-15 06:59:18,087 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0672), (1, 0.6497), (2, 0.7974), (3, 0.861), (4, 0.8885), (5, 0.8945), (6, 0.9022), (7, 0.9079), (8, 0.9094), (9, 0.9074), (10, 0.9071)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9071
wandb:     loss 1.55492
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_065439-064slfan
wandb: Find logs at: ./wandb/offline-run-20240415_065439-064slfan/logs
INFO flwr 2024-04-15 06:59:21,569 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 07:06:32,342 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1560509)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1560509)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 07:06:37,331	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 07:06:38,769	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 07:06:39,293	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 07:06:39,469	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 07:06:39,647	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 07:06:39,747	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 07:06:40,085	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a261b41e803a8279.zip' (216.35MiB) to Ray cluster...
2024-04-15 07:06:40,945	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a261b41e803a8279.zip'.
INFO flwr 2024-04-15 07:06:53,442 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75785893478.0, 'memory': 166833751450.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-15 07:06:53,442 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 07:06:53,443 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 07:06:53,462 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 07:06:53,463 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 07:06:53,463 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 07:06:53,463 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 07:06:56,579 | server.py:94 | initial parameters (loss, other metrics): 2.3012635707855225, {'accuracy': 0.1222, 'data_size': 10000}
INFO flwr 2024-04-15 07:06:56,580 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 07:06:56,580 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1565168)[0m 2024-04-15 07:07:00.251945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1565168)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1565176)[0m 2024-04-15 07:07:02.964469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1565176)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1565176)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1565179)[0m 2024-04-15 07:07:00.661017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1565179)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1565174)[0m 2024-04-15 07:07:03.219219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 07:07:36,343 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 07:07:37,802 | server.py:125 | fit progress: (1, 1.7735828161239624, {'accuracy': 0.7038, 'data_size': 10000}, 41.22258800599957)
INFO flwr 2024-04-15 07:07:37,803 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 07:07:37,803 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:08:01,510 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 07:08:02,936 | server.py:125 | fit progress: (2, 1.6171385049819946, {'accuracy': 0.8488, 'data_size': 10000}, 66.35591998999007)
INFO flwr 2024-04-15 07:08:02,936 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 07:08:02,936 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:08:24,892 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 07:08:26,355 | server.py:125 | fit progress: (3, 1.5750370025634766, {'accuracy': 0.8875, 'data_size': 10000}, 89.77543905799394)
INFO flwr 2024-04-15 07:08:26,356 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 07:08:26,356 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:08:45,262 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 07:08:46,799 | server.py:125 | fit progress: (4, 1.5660815238952637, {'accuracy': 0.8952, 'data_size': 10000}, 110.21878577797906)
INFO flwr 2024-04-15 07:08:46,799 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 07:08:46,799 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:09:09,349 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 07:09:10,838 | server.py:125 | fit progress: (5, 1.5900115966796875, {'accuracy': 0.8714, 'data_size': 10000}, 134.2583035719872)
INFO flwr 2024-04-15 07:09:10,838 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 07:09:10,839 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:09:33,313 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 07:09:34,802 | server.py:125 | fit progress: (6, 1.583695650100708, {'accuracy': 0.8764, 'data_size': 10000}, 158.22209786699386)
INFO flwr 2024-04-15 07:09:34,802 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 07:09:34,802 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:09:58,380 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 07:09:59,585 | server.py:125 | fit progress: (7, 1.5569075345993042, {'accuracy': 0.9041, 'data_size': 10000}, 183.0052221090009)
INFO flwr 2024-04-15 07:09:59,585 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 07:09:59,586 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:10:21,801 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 07:10:23,255 | server.py:125 | fit progress: (8, 1.5630754232406616, {'accuracy': 0.8981, 'data_size': 10000}, 206.67551093598013)
INFO flwr 2024-04-15 07:10:23,256 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 07:10:23,256 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:10:46,182 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 07:10:47,382 | server.py:125 | fit progress: (9, 1.5574440956115723, {'accuracy': 0.9037, 'data_size': 10000}, 230.80234496298363)
INFO flwr 2024-04-15 07:10:47,382 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 07:10:47,383 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:11:09,772 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 07:11:10,975 | server.py:125 | fit progress: (10, 1.5542476177215576, {'accuracy': 0.9067, 'data_size': 10000}, 254.3949857239786)
INFO flwr 2024-04-15 07:11:10,975 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 07:11:10,975 | server.py:153 | FL finished in 254.39554035800393
INFO flwr 2024-04-15 07:11:10,976 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 07:11:10,976 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 07:11:10,976 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 07:11:10,976 | app.py:229 | app_fit: losses_centralized [(0, 2.3012635707855225), (1, 1.7735828161239624), (2, 1.6171385049819946), (3, 1.5750370025634766), (4, 1.5660815238952637), (5, 1.5900115966796875), (6, 1.583695650100708), (7, 1.5569075345993042), (8, 1.5630754232406616), (9, 1.5574440956115723), (10, 1.5542476177215576)]
INFO flwr 2024-04-15 07:11:10,976 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1222), (1, 0.7038), (2, 0.8488), (3, 0.8875), (4, 0.8952), (5, 0.8714), (6, 0.8764), (7, 0.9041), (8, 0.8981), (9, 0.9037), (10, 0.9067)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9067
wandb:     loss 1.55425
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_070632-thexcmsl
wandb: Find logs at: ./wandb/offline-run-20240415_070632-thexcmsl/logs
INFO flwr 2024-04-15 07:11:14,526 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 07:18:25,662 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1565168)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1565168)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 07:18:30,589	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 07:18:31,769	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 07:18:32,460	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 07:18:32,668	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 07:18:32,857	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 07:18:32,959	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 07:18:33,302	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6cc47948c58729c8.zip' (216.37MiB) to Ray cluster...
2024-04-15 07:18:34,151	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6cc47948c58729c8.zip'.
INFO flwr 2024-04-15 07:18:46,441 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 171357533594.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 77724657254.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-15 07:18:46,442 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 07:18:46,442 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 07:18:46,463 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 07:18:46,464 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 07:18:46,464 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 07:18:46,465 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 07:18:49,525 | server.py:94 | initial parameters (loss, other metrics): 2.3025295734405518, {'accuracy': 0.081, 'data_size': 10000}
INFO flwr 2024-04-15 07:18:49,525 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 07:18:49,525 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1569891)[0m 2024-04-15 07:18:53.068379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1569891)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1569885)[0m 2024-04-15 07:18:55.306536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1569897)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1569897)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1569877)[0m 2024-04-15 07:18:53.481930: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1569877)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1569877)[0m 2024-04-15 07:18:56.707532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 07:19:41,804 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 07:19:43,322 | server.py:125 | fit progress: (1, 2.042422294616699, {'accuracy': 0.5623, 'data_size': 10000}, 53.79664277599659)
INFO flwr 2024-04-15 07:19:43,322 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 07:19:43,322 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:20:25,066 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 07:20:26,507 | server.py:125 | fit progress: (2, 1.8231761455535889, {'accuracy': 0.7182, 'data_size': 10000}, 96.98205022900947)
INFO flwr 2024-04-15 07:20:26,508 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 07:20:26,508 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:21:09,543 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 07:21:11,081 | server.py:125 | fit progress: (3, 1.6852296590805054, {'accuracy': 0.8208, 'data_size': 10000}, 141.55615136001143)
INFO flwr 2024-04-15 07:21:11,082 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 07:21:11,082 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:21:53,830 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 07:21:55,321 | server.py:125 | fit progress: (4, 1.6296205520629883, {'accuracy': 0.8531, 'data_size': 10000}, 185.79575508399284)
INFO flwr 2024-04-15 07:21:55,321 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 07:21:55,321 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:22:37,995 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 07:22:39,449 | server.py:125 | fit progress: (5, 1.6047812700271606, {'accuracy': 0.8701, 'data_size': 10000}, 229.92426728099235)
INFO flwr 2024-04-15 07:22:39,450 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 07:22:39,450 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:23:19,960 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 07:23:21,417 | server.py:125 | fit progress: (6, 1.5857125520706177, {'accuracy': 0.8852, 'data_size': 10000}, 271.8922883310006)
INFO flwr 2024-04-15 07:23:21,418 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 07:23:21,418 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:24:06,029 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 07:24:07,269 | server.py:125 | fit progress: (7, 1.5710235834121704, {'accuracy': 0.8958, 'data_size': 10000}, 317.74344178501633)
INFO flwr 2024-04-15 07:24:07,269 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 07:24:07,269 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:24:42,022 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 07:24:43,563 | server.py:125 | fit progress: (8, 1.5617479085922241, {'accuracy': 0.9034, 'data_size': 10000}, 354.03793625198887)
INFO flwr 2024-04-15 07:24:43,563 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 07:24:43,564 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:25:24,514 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 07:25:25,727 | server.py:125 | fit progress: (9, 1.5580297708511353, {'accuracy': 0.9058, 'data_size': 10000}, 396.20218640600797)
INFO flwr 2024-04-15 07:25:25,728 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 07:25:25,728 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:26:09,451 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 07:26:10,665 | server.py:125 | fit progress: (10, 1.556827187538147, {'accuracy': 0.9063, 'data_size': 10000}, 441.13948869201704)
INFO flwr 2024-04-15 07:26:10,665 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 07:26:10,665 | server.py:153 | FL finished in 441.1399669699895
INFO flwr 2024-04-15 07:26:10,665 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 07:26:10,665 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 07:26:10,666 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 07:26:10,666 | app.py:229 | app_fit: losses_centralized [(0, 2.3025295734405518), (1, 2.042422294616699), (2, 1.8231761455535889), (3, 1.6852296590805054), (4, 1.6296205520629883), (5, 1.6047812700271606), (6, 1.5857125520706177), (7, 1.5710235834121704), (8, 1.5617479085922241), (9, 1.5580297708511353), (10, 1.556827187538147)]
INFO flwr 2024-04-15 07:26:10,666 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.081), (1, 0.5623), (2, 0.7182), (3, 0.8208), (4, 0.8531), (5, 0.8701), (6, 0.8852), (7, 0.8958), (8, 0.9034), (9, 0.9058), (10, 0.9063)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9063
wandb:     loss 1.55683
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_071824-u0ww5fjt
wandb: Find logs at: ./wandb/offline-run-20240415_071824-u0ww5fjt/logs
INFO flwr 2024-04-15 07:26:14,194 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 07:33:24,649 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1569877)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1569877)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 07:33:29,518	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 07:33:30,835	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 07:33:31,344	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 07:33:31,526	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 07:33:31,703	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 07:33:31,804	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 07:33:32,143	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7000bc564dc8f48c.zip' (216.38MiB) to Ray cluster...
2024-04-15 07:33:33,078	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7000bc564dc8f48c.zip'.
INFO flwr 2024-04-15 07:33:45,507 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 75670291660.0, 'memory': 166564013876.0, 'accelerator_type:TITAN': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-15 07:33:45,508 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 07:33:45,508 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 07:33:45,530 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 07:33:45,531 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 07:33:45,531 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 07:33:45,531 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 07:33:50,031 | server.py:94 | initial parameters (loss, other metrics): 2.3045308589935303, {'accuracy': 0.0968, 'data_size': 10000}
INFO flwr 2024-04-15 07:33:50,032 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 07:33:50,033 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1575205)[0m 2024-04-15 07:33:52.283995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1575205)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1575204)[0m 2024-04-15 07:33:54.885721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1575211)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1575211)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1575210)[0m 2024-04-15 07:33:52.745790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1575210)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1575209)[0m 2024-04-15 07:33:55.541037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1575204)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1575204)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 07:34:48,339 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 07:34:49,768 | server.py:125 | fit progress: (1, 1.8946298360824585, {'accuracy': 0.6193, 'data_size': 10000}, 59.735481367999455)
INFO flwr 2024-04-15 07:34:49,769 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 07:34:49,769 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:35:33,179 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 07:35:34,643 | server.py:125 | fit progress: (2, 1.6785361766815186, {'accuracy': 0.8087, 'data_size': 10000}, 104.6100513410056)
INFO flwr 2024-04-15 07:35:34,643 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 07:35:34,643 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:36:11,414 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 07:36:12,827 | server.py:125 | fit progress: (3, 1.6016141176223755, {'accuracy': 0.8695, 'data_size': 10000}, 142.79424499100423)
INFO flwr 2024-04-15 07:36:12,827 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 07:36:12,827 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:36:53,287 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 07:36:54,714 | server.py:125 | fit progress: (4, 1.5694187879562378, {'accuracy': 0.8978, 'data_size': 10000}, 184.6809091430041)
INFO flwr 2024-04-15 07:36:54,714 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 07:36:54,714 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:37:37,311 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 07:37:38,784 | server.py:125 | fit progress: (5, 1.5614746809005737, {'accuracy': 0.9037, 'data_size': 10000}, 228.75095699200756)
INFO flwr 2024-04-15 07:37:38,784 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 07:37:38,784 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:38:29,351 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 07:38:30,864 | server.py:125 | fit progress: (6, 1.562193512916565, {'accuracy': 0.9007, 'data_size': 10000}, 280.83112661199993)
INFO flwr 2024-04-15 07:38:30,864 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 07:38:30,864 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:39:11,769 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 07:39:12,987 | server.py:125 | fit progress: (7, 1.5625860691070557, {'accuracy': 0.9001, 'data_size': 10000}, 322.95412660200964)
INFO flwr 2024-04-15 07:39:12,987 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 07:39:12,987 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:39:51,072 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 07:39:52,552 | server.py:125 | fit progress: (8, 1.5612481832504272, {'accuracy': 0.9014, 'data_size': 10000}, 362.5194843230129)
INFO flwr 2024-04-15 07:39:52,553 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 07:39:52,553 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:40:39,916 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 07:40:41,127 | server.py:125 | fit progress: (9, 1.552566409111023, {'accuracy': 0.9105, 'data_size': 10000}, 411.09382831602124)
INFO flwr 2024-04-15 07:40:41,127 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 07:40:41,127 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:41:23,432 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 07:41:24,670 | server.py:125 | fit progress: (10, 1.548952341079712, {'accuracy': 0.9121, 'data_size': 10000}, 454.63744491300895)
INFO flwr 2024-04-15 07:41:24,671 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 07:41:24,671 | server.py:153 | FL finished in 454.6380581819976
INFO flwr 2024-04-15 07:41:24,671 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 07:41:24,671 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 07:41:24,671 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 07:41:24,672 | app.py:229 | app_fit: losses_centralized [(0, 2.3045308589935303), (1, 1.8946298360824585), (2, 1.6785361766815186), (3, 1.6016141176223755), (4, 1.5694187879562378), (5, 1.5614746809005737), (6, 1.562193512916565), (7, 1.5625860691070557), (8, 1.5612481832504272), (9, 1.552566409111023), (10, 1.548952341079712)]
INFO flwr 2024-04-15 07:41:24,672 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0968), (1, 0.6193), (2, 0.8087), (3, 0.8695), (4, 0.8978), (5, 0.9037), (6, 0.9007), (7, 0.9001), (8, 0.9014), (9, 0.9105), (10, 0.9121)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9121
wandb:     loss 1.54895
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_073324-0f81mx5s
wandb: Find logs at: ./wandb/offline-run-20240415_073324-0f81mx5s/logs
INFO flwr 2024-04-15 07:41:28,199 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 07:48:38,840 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-15 07:48:43,800	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 07:48:45,106	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 07:48:45,672	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 07:48:45,843	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 07:48:46,021	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 07:48:46,121	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 07:48:46,460	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6701cfdd8a6e953c.zip' (216.39MiB) to Ray cluster...
2024-04-15 07:48:47,384	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6701cfdd8a6e953c.zip'.
INFO flwr 2024-04-15 07:48:59,863 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'object_store_memory': 75752982528.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 166756959232.0}
INFO flwr 2024-04-15 07:48:59,864 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 07:48:59,864 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 07:48:59,882 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 07:48:59,883 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 07:48:59,883 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 07:48:59,883 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 07:49:03,339 | server.py:94 | initial parameters (loss, other metrics): 2.3030409812927246, {'accuracy': 0.1002, 'data_size': 10000}
INFO flwr 2024-04-15 07:49:03,340 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 07:49:03,341 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1581114)[0m 2024-04-15 07:49:06.635557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1581114)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1581115)[0m 2024-04-15 07:49:08.955306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1581110)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1581110)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1581110)[0m 2024-04-15 07:49:07.019515: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1581110)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1581111)[0m 2024-04-15 07:49:10.661614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 07:49:58,295 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 07:49:59,795 | server.py:125 | fit progress: (1, 1.7256783246994019, {'accuracy': 0.7665, 'data_size': 10000}, 56.45396954001626)
INFO flwr 2024-04-15 07:49:59,795 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 07:49:59,795 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:50:40,175 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 07:50:41,654 | server.py:125 | fit progress: (2, 1.6120948791503906, {'accuracy': 0.8558, 'data_size': 10000}, 98.3132753210084)
INFO flwr 2024-04-15 07:50:41,654 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 07:50:41,654 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:51:23,592 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 07:51:25,024 | server.py:125 | fit progress: (3, 1.5759005546569824, {'accuracy': 0.8871, 'data_size': 10000}, 141.6834960039996)
INFO flwr 2024-04-15 07:51:25,024 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 07:51:25,025 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:52:04,166 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 07:52:05,627 | server.py:125 | fit progress: (4, 1.5665949583053589, {'accuracy': 0.8959, 'data_size': 10000}, 182.28643682299298)
INFO flwr 2024-04-15 07:52:05,627 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 07:52:05,628 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:52:44,209 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 07:52:45,648 | server.py:125 | fit progress: (5, 1.5793583393096924, {'accuracy': 0.8831, 'data_size': 10000}, 222.3071388209937)
INFO flwr 2024-04-15 07:52:45,648 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 07:52:45,648 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:53:27,435 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 07:53:28,919 | server.py:125 | fit progress: (6, 1.580670714378357, {'accuracy': 0.881, 'data_size': 10000}, 265.57876730299904)
INFO flwr 2024-04-15 07:53:28,920 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 07:53:28,920 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:54:13,124 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 07:54:14,338 | server.py:125 | fit progress: (7, 1.5643337965011597, {'accuracy': 0.8967, 'data_size': 10000}, 310.99757315800525)
INFO flwr 2024-04-15 07:54:14,338 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 07:54:14,339 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:54:54,650 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 07:54:56,119 | server.py:125 | fit progress: (8, 1.5691850185394287, {'accuracy': 0.8919, 'data_size': 10000}, 352.77875192600186)
INFO flwr 2024-04-15 07:54:56,120 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 07:54:56,120 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:55:38,851 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 07:55:40,088 | server.py:125 | fit progress: (9, 1.5549207925796509, {'accuracy': 0.9063, 'data_size': 10000}, 396.7475895259995)
INFO flwr 2024-04-15 07:55:40,088 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 07:55:40,089 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 07:56:22,122 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 07:56:23,310 | server.py:125 | fit progress: (10, 1.5467329025268555, {'accuracy': 0.9147, 'data_size': 10000}, 439.9698150480108)
INFO flwr 2024-04-15 07:56:23,311 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 07:56:23,311 | server.py:153 | FL finished in 439.9702817359939
INFO flwr 2024-04-15 07:56:23,311 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 07:56:23,311 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 07:56:23,311 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 07:56:23,311 | app.py:229 | app_fit: losses_centralized [(0, 2.3030409812927246), (1, 1.7256783246994019), (2, 1.6120948791503906), (3, 1.5759005546569824), (4, 1.5665949583053589), (5, 1.5793583393096924), (6, 1.580670714378357), (7, 1.5643337965011597), (8, 1.5691850185394287), (9, 1.5549207925796509), (10, 1.5467329025268555)]
INFO flwr 2024-04-15 07:56:23,312 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1002), (1, 0.7665), (2, 0.8558), (3, 0.8871), (4, 0.8959), (5, 0.8831), (6, 0.881), (7, 0.8967), (8, 0.8919), (9, 0.9063), (10, 0.9147)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9147
wandb:     loss 1.54673
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_074838-m5iit49j
wandb: Find logs at: ./wandb/offline-run-20240415_074838-m5iit49j/logs
INFO flwr 2024-04-15 07:56:26,811 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 08:03:37,479 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1581108)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1581108)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 08:03:42,287	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 08:03:43,772	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 08:03:44,292	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 08:03:44,465	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 08:03:44,640	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 08:03:44,738	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 08:03:45,074	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4706939bad396f07.zip' (216.40MiB) to Ray cluster...
2024-04-15 08:03:45,969	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4706939bad396f07.zip'.
INFO flwr 2024-04-15 08:03:58,548 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170468801946.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'object_store_memory': 77343772262.0}
INFO flwr 2024-04-15 08:03:58,549 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 08:03:58,549 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 08:03:58,573 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 08:03:58,574 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 08:03:58,574 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 08:03:58,574 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 08:04:02,125 | server.py:94 | initial parameters (loss, other metrics): 2.3033287525177, {'accuracy': 0.0786, 'data_size': 10000}
INFO flwr 2024-04-15 08:04:02,126 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 08:04:02,126 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1587561)[0m 2024-04-15 08:04:05.299093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1587561)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1587557)[0m 2024-04-15 08:04:07.698500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1587565)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1587565)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1587566)[0m 2024-04-15 08:04:05.539241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1587566)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1587561)[0m 2024-04-15 08:04:08.244949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1587558)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1587558)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-15 08:04:57,271 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 08:04:58,687 | server.py:125 | fit progress: (1, 1.9820386171340942, {'accuracy': 0.7597, 'data_size': 10000}, 56.5607798419951)
INFO flwr 2024-04-15 08:04:58,687 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 08:04:58,687 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:05:36,264 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 08:05:37,720 | server.py:125 | fit progress: (2, 1.7689208984375, {'accuracy': 0.7933, 'data_size': 10000}, 95.5937725380063)
INFO flwr 2024-04-15 08:05:37,720 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 08:05:37,720 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:06:19,012 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 08:06:20,516 | server.py:125 | fit progress: (3, 1.6790083646774292, {'accuracy': 0.8232, 'data_size': 10000}, 138.3906722610118)
INFO flwr 2024-04-15 08:06:20,517 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 08:06:20,517 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:06:57,988 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 08:06:59,446 | server.py:125 | fit progress: (4, 1.6273359060287476, {'accuracy': 0.8547, 'data_size': 10000}, 177.3204380160023)
INFO flwr 2024-04-15 08:06:59,446 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 08:06:59,447 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:07:38,117 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 08:07:39,651 | server.py:125 | fit progress: (5, 1.5946332216262817, {'accuracy': 0.8796, 'data_size': 10000}, 217.52517915301723)
INFO flwr 2024-04-15 08:07:39,651 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 08:07:39,652 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:08:18,900 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 08:08:20,351 | server.py:125 | fit progress: (6, 1.5771983861923218, {'accuracy': 0.8911, 'data_size': 10000}, 258.2249145170208)
INFO flwr 2024-04-15 08:08:20,351 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 08:08:20,351 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:08:56,575 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 08:08:57,830 | server.py:125 | fit progress: (7, 1.5675486326217651, {'accuracy': 0.9, 'data_size': 10000}, 295.7046532499953)
INFO flwr 2024-04-15 08:08:57,831 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 08:08:57,831 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:09:38,596 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 08:09:40,056 | server.py:125 | fit progress: (8, 1.5627802610397339, {'accuracy': 0.9039, 'data_size': 10000}, 337.9304679320194)
INFO flwr 2024-04-15 08:09:40,057 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 08:09:40,057 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:10:17,610 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 08:10:18,783 | server.py:125 | fit progress: (9, 1.5566015243530273, {'accuracy': 0.9091, 'data_size': 10000}, 376.65739562700037)
INFO flwr 2024-04-15 08:10:18,783 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 08:10:18,784 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:11:00,240 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 08:11:01,438 | server.py:125 | fit progress: (10, 1.5515228509902954, {'accuracy': 0.9135, 'data_size': 10000}, 419.3122786680178)
INFO flwr 2024-04-15 08:11:01,438 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 08:11:01,438 | server.py:153 | FL finished in 419.3127580170112
INFO flwr 2024-04-15 08:11:01,439 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 08:11:01,439 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 08:11:01,439 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 08:11:01,439 | app.py:229 | app_fit: losses_centralized [(0, 2.3033287525177), (1, 1.9820386171340942), (2, 1.7689208984375), (3, 1.6790083646774292), (4, 1.6273359060287476), (5, 1.5946332216262817), (6, 1.5771983861923218), (7, 1.5675486326217651), (8, 1.5627802610397339), (9, 1.5566015243530273), (10, 1.5515228509902954)]
INFO flwr 2024-04-15 08:11:01,439 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0786), (1, 0.7597), (2, 0.7933), (3, 0.8232), (4, 0.8547), (5, 0.8796), (6, 0.8911), (7, 0.9), (8, 0.9039), (9, 0.9091), (10, 0.9135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9135
wandb:     loss 1.55152
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_080337-224cso7v
wandb: Find logs at: ./wandb/offline-run-20240415_080337-224cso7v/logs
INFO flwr 2024-04-15 08:11:04,989 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 08:18:15,832 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1587556)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1587556)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-15 08:18:23,452	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 08:18:24,692	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 08:18:25,236	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 08:18:25,474	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 08:18:25,716	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 08:18:25,827	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 08:18:26,182	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0585dd38374524bb.zip' (216.42MiB) to Ray cluster...
2024-04-15 08:18:27,036	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0585dd38374524bb.zip'.
INFO flwr 2024-04-15 08:18:41,800 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 75633994137.0, 'GPU': 1.0, 'memory': 166479319655.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-15 08:18:41,800 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 08:18:41,801 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 08:18:41,818 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 08:18:41,818 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 08:18:41,819 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 08:18:41,819 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 08:18:44,168 | server.py:94 | initial parameters (loss, other metrics): 2.3034565448760986, {'accuracy': 0.0994, 'data_size': 10000}
INFO flwr 2024-04-15 08:18:44,169 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 08:18:44,169 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1592240)[0m 2024-04-15 08:18:59.651652: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1592240)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1592240)[0m 2024-04-15 08:19:01.850152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1592241)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1592241)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1592248)[0m 2024-04-15 08:19:00.005292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1592248)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1592249)[0m 2024-04-15 08:19:02.327895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 08:19:48,495 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 08:19:50,034 | server.py:125 | fit progress: (1, 1.8653922080993652, {'accuracy': 0.735, 'data_size': 10000}, 65.86442814400652)
INFO flwr 2024-04-15 08:19:50,034 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 08:19:50,034 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:20:26,598 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 08:20:28,033 | server.py:125 | fit progress: (2, 1.689315676689148, {'accuracy': 0.8054, 'data_size': 10000}, 103.86363899701973)
INFO flwr 2024-04-15 08:20:28,033 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 08:20:28,033 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:21:10,897 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 08:21:12,297 | server.py:125 | fit progress: (3, 1.635938048362732, {'accuracy': 0.83, 'data_size': 10000}, 148.12822665600106)
INFO flwr 2024-04-15 08:21:12,298 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 08:21:12,298 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:21:47,766 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 08:21:49,214 | server.py:125 | fit progress: (4, 1.6085577011108398, {'accuracy': 0.8557, 'data_size': 10000}, 185.04474402501364)
INFO flwr 2024-04-15 08:21:49,214 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 08:21:49,214 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:22:35,109 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 08:22:36,530 | server.py:125 | fit progress: (5, 1.5782058238983154, {'accuracy': 0.8846, 'data_size': 10000}, 232.3609946250217)
INFO flwr 2024-04-15 08:22:36,530 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 08:22:36,531 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:23:28,281 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 08:23:29,838 | server.py:125 | fit progress: (6, 1.556087851524353, {'accuracy': 0.9058, 'data_size': 10000}, 285.6691112840199)
INFO flwr 2024-04-15 08:23:29,838 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 08:23:29,839 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:24:14,877 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 08:24:16,157 | server.py:125 | fit progress: (7, 1.5545997619628906, {'accuracy': 0.9082, 'data_size': 10000}, 331.9877993700211)
INFO flwr 2024-04-15 08:24:16,157 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 08:24:16,157 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:24:55,253 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 08:24:56,715 | server.py:125 | fit progress: (8, 1.5573208332061768, {'accuracy': 0.9041, 'data_size': 10000}, 372.5461961640103)
INFO flwr 2024-04-15 08:24:56,716 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 08:24:56,716 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:25:39,902 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 08:25:41,088 | server.py:125 | fit progress: (9, 1.554485559463501, {'accuracy': 0.9069, 'data_size': 10000}, 416.9191026549961)
INFO flwr 2024-04-15 08:25:41,088 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 08:25:41,089 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:26:21,291 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 08:26:22,490 | server.py:125 | fit progress: (10, 1.5500540733337402, {'accuracy': 0.9125, 'data_size': 10000}, 458.32140986801824)
INFO flwr 2024-04-15 08:26:22,491 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 08:26:22,491 | server.py:153 | FL finished in 458.3219121020229
INFO flwr 2024-04-15 08:26:22,491 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 08:26:22,491 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 08:26:22,491 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 08:26:22,492 | app.py:229 | app_fit: losses_centralized [(0, 2.3034565448760986), (1, 1.8653922080993652), (2, 1.689315676689148), (3, 1.635938048362732), (4, 1.6085577011108398), (5, 1.5782058238983154), (6, 1.556087851524353), (7, 1.5545997619628906), (8, 1.5573208332061768), (9, 1.554485559463501), (10, 1.5500540733337402)]
INFO flwr 2024-04-15 08:26:22,492 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0994), (1, 0.735), (2, 0.8054), (3, 0.83), (4, 0.8557), (5, 0.8846), (6, 0.9058), (7, 0.9082), (8, 0.9041), (9, 0.9069), (10, 0.9125)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9125
wandb:     loss 1.55005
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_081815-1y264ye1
wandb: Find logs at: ./wandb/offline-run-20240415_081815-1y264ye1/logs
INFO flwr 2024-04-15 08:26:26,029 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.1}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 08:33:36,756 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1592242)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1592242)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 08:33:41,659	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 08:33:42,919	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 08:33:43,554	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 08:33:43,744	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 08:33:43,949	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 08:33:44,049	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 08:33:44,388	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ca70fcd211ed0b6a.zip' (216.43MiB) to Ray cluster...
2024-04-15 08:33:45,230	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ca70fcd211ed0b6a.zip'.
INFO flwr 2024-04-15 08:33:57,844 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 166322624308.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 75566838988.0}
INFO flwr 2024-04-15 08:33:57,844 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 08:33:57,845 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 08:33:57,861 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 08:33:57,862 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 08:33:57,863 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 08:33:57,863 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 08:34:00,587 | server.py:94 | initial parameters (loss, other metrics): 2.30328106880188, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-15 08:34:00,588 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 08:34:00,588 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1597911)[0m 2024-04-15 08:34:04.805873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1597911)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1597915)[0m 2024-04-15 08:34:07.500079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1597912)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1597912)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1597912)[0m 2024-04-15 08:34:05.363004: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1597912)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1597912)[0m 2024-04-15 08:34:07.768788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 08:34:58,164 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 08:34:59,649 | server.py:125 | fit progress: (1, 1.7445379495620728, {'accuracy': 0.7565, 'data_size': 10000}, 59.06130569198285)
INFO flwr 2024-04-15 08:34:59,650 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 08:34:59,650 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:35:37,503 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 08:35:38,973 | server.py:125 | fit progress: (2, 1.6467796564102173, {'accuracy': 0.8168, 'data_size': 10000}, 98.38535943700117)
INFO flwr 2024-04-15 08:35:38,974 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 08:35:38,974 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:36:16,443 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 08:36:17,919 | server.py:125 | fit progress: (3, 1.5950062274932861, {'accuracy': 0.8669, 'data_size': 10000}, 137.3304691700032)
INFO flwr 2024-04-15 08:36:17,919 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 08:36:17,919 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:37:01,496 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 08:37:02,956 | server.py:125 | fit progress: (4, 1.5706405639648438, {'accuracy': 0.8922, 'data_size': 10000}, 182.36782310999115)
INFO flwr 2024-04-15 08:37:02,956 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 08:37:02,956 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:37:42,136 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 08:37:43,543 | server.py:125 | fit progress: (5, 1.5559866428375244, {'accuracy': 0.906, 'data_size': 10000}, 222.95503558998462)
INFO flwr 2024-04-15 08:37:43,543 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 08:37:43,544 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:38:25,685 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 08:38:27,108 | server.py:125 | fit progress: (6, 1.566347599029541, {'accuracy': 0.8956, 'data_size': 10000}, 266.52025329499156)
INFO flwr 2024-04-15 08:38:27,109 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 08:38:27,109 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:39:10,316 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 08:39:11,602 | server.py:125 | fit progress: (7, 1.5994832515716553, {'accuracy': 0.861, 'data_size': 10000}, 311.01405225598137)
INFO flwr 2024-04-15 08:39:11,602 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 08:39:11,603 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:39:54,325 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 08:39:55,766 | server.py:125 | fit progress: (8, 1.5768097639083862, {'accuracy': 0.8837, 'data_size': 10000}, 355.17753002999234)
INFO flwr 2024-04-15 08:39:55,766 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 08:39:55,766 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:40:37,284 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 08:40:38,491 | server.py:125 | fit progress: (9, 1.552487850189209, {'accuracy': 0.909, 'data_size': 10000}, 397.9034664199862)
INFO flwr 2024-04-15 08:40:38,492 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 08:40:38,492 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:41:23,145 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 08:41:24,370 | server.py:125 | fit progress: (10, 1.574313998222351, {'accuracy': 0.8857, 'data_size': 10000}, 443.7815157919831)
INFO flwr 2024-04-15 08:41:24,370 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 08:41:24,370 | server.py:153 | FL finished in 443.7819660779787
INFO flwr 2024-04-15 08:41:24,370 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 08:41:24,370 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 08:41:24,370 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 08:41:24,371 | app.py:229 | app_fit: losses_centralized [(0, 2.30328106880188), (1, 1.7445379495620728), (2, 1.6467796564102173), (3, 1.5950062274932861), (4, 1.5706405639648438), (5, 1.5559866428375244), (6, 1.566347599029541), (7, 1.5994832515716553), (8, 1.5768097639083862), (9, 1.552487850189209), (10, 1.574313998222351)]
INFO flwr 2024-04-15 08:41:24,371 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.7565), (2, 0.8168), (3, 0.8669), (4, 0.8922), (5, 0.906), (6, 0.8956), (7, 0.861), (8, 0.8837), (9, 0.909), (10, 0.8857)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8857
wandb:     loss 1.57431
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_083336-f4nx47jm
wandb: Find logs at: ./wandb/offline-run-20240415_083336-f4nx47jm/logs
INFO flwr 2024-04-15 08:41:27,887 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 08:48:38,222 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1597909)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1597909)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 08:48:43,146	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 08:48:44,483	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 08:48:45,063	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 08:48:45,237	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 08:48:45,411	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 08:48:45,510	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 08:48:45,848	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ab752a38acedb450.zip' (216.44MiB) to Ray cluster...
2024-04-15 08:48:46,690	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ab752a38acedb450.zip'.
INFO flwr 2024-04-15 08:48:59,186 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 166312422810.0, 'object_store_memory': 75562466918.0, 'CPU': 64.0}
INFO flwr 2024-04-15 08:48:59,186 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 08:48:59,186 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 08:48:59,206 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 08:48:59,207 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 08:48:59,208 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 08:48:59,208 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 08:49:02,052 | server.py:94 | initial parameters (loss, other metrics): 2.298717498779297, {'accuracy': 0.1272, 'data_size': 10000}
INFO flwr 2024-04-15 08:49:02,053 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 08:49:02,053 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1603204)[0m 2024-04-15 08:49:06.145218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1603204)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1603206)[0m 2024-04-15 08:49:08.507228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1603206)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1603206)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1603323)[0m 2024-04-15 08:49:06.462314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1603323)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1603203)[0m 2024-04-15 08:49:09.422162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 08:49:58,059 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 08:49:59,473 | server.py:125 | fit progress: (1, 1.9909595251083374, {'accuracy': 0.7034, 'data_size': 10000}, 57.419877570995595)
INFO flwr 2024-04-15 08:49:59,473 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 08:49:59,474 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:50:39,515 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 08:50:41,037 | server.py:125 | fit progress: (2, 1.8030247688293457, {'accuracy': 0.7576, 'data_size': 10000}, 98.98373158602044)
INFO flwr 2024-04-15 08:50:41,037 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 08:50:41,037 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:51:24,311 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 08:51:25,747 | server.py:125 | fit progress: (3, 1.7031830549240112, {'accuracy': 0.8061, 'data_size': 10000}, 143.6936481820012)
INFO flwr 2024-04-15 08:51:25,747 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 08:51:25,747 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:52:06,228 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 08:52:07,743 | server.py:125 | fit progress: (4, 1.639654517173767, {'accuracy': 0.8441, 'data_size': 10000}, 185.69005189600284)
INFO flwr 2024-04-15 08:52:07,743 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 08:52:07,744 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:52:47,483 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 08:52:48,987 | server.py:125 | fit progress: (5, 1.601987361907959, {'accuracy': 0.8727, 'data_size': 10000}, 226.93408800600446)
INFO flwr 2024-04-15 08:52:48,987 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 08:52:48,988 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:53:21,247 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 08:53:22,690 | server.py:125 | fit progress: (6, 1.5838780403137207, {'accuracy': 0.885, 'data_size': 10000}, 260.6372164259956)
INFO flwr 2024-04-15 08:53:22,691 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 08:53:22,691 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:54:07,901 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 08:54:09,110 | server.py:125 | fit progress: (7, 1.5720518827438354, {'accuracy': 0.8942, 'data_size': 10000}, 307.0565397310129)
INFO flwr 2024-04-15 08:54:09,110 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 08:54:09,110 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:54:54,475 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 08:54:55,958 | server.py:125 | fit progress: (8, 1.563230037689209, {'accuracy': 0.9022, 'data_size': 10000}, 353.9046833140019)
INFO flwr 2024-04-15 08:54:55,958 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 08:54:55,958 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:55:38,080 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 08:55:39,325 | server.py:125 | fit progress: (9, 1.557210087776184, {'accuracy': 0.9062, 'data_size': 10000}, 397.2721040620236)
INFO flwr 2024-04-15 08:55:39,326 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 08:55:39,326 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 08:56:25,866 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 08:56:27,081 | server.py:125 | fit progress: (10, 1.554223895072937, {'accuracy': 0.9094, 'data_size': 10000}, 445.02783842702047)
INFO flwr 2024-04-15 08:56:27,081 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 08:56:27,081 | server.py:153 | FL finished in 445.02830136200646
INFO flwr 2024-04-15 08:56:27,082 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 08:56:27,082 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 08:56:27,082 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 08:56:27,082 | app.py:229 | app_fit: losses_centralized [(0, 2.298717498779297), (1, 1.9909595251083374), (2, 1.8030247688293457), (3, 1.7031830549240112), (4, 1.639654517173767), (5, 1.601987361907959), (6, 1.5838780403137207), (7, 1.5720518827438354), (8, 1.563230037689209), (9, 1.557210087776184), (10, 1.554223895072937)]
INFO flwr 2024-04-15 08:56:27,082 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1272), (1, 0.7034), (2, 0.7576), (3, 0.8061), (4, 0.8441), (5, 0.8727), (6, 0.885), (7, 0.8942), (8, 0.9022), (9, 0.9062), (10, 0.9094)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9094
wandb:     loss 1.55422
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_084837-mad9ubvw
wandb: Find logs at: ./wandb/offline-run-20240415_084837-mad9ubvw/logs
INFO flwr 2024-04-15 08:56:30,574 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 09:03:40,930 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1603203)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1603203)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 09:03:45,614	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 09:03:47,111	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 09:03:47,631	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 09:03:47,805	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 09:03:47,981	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 09:03:48,080	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 09:03:48,419	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_98a5e90f001ede38.zip' (216.45MiB) to Ray cluster...
2024-04-15 09:03:49,281	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_98a5e90f001ede38.zip'.
INFO flwr 2024-04-15 09:04:01,299 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 77424932044.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170658174772.0, 'CPU': 64.0}
INFO flwr 2024-04-15 09:04:01,300 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 09:04:01,300 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 09:04:01,318 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 09:04:01,319 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 09:04:01,319 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 09:04:01,319 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 09:04:04,220 | server.py:94 | initial parameters (loss, other metrics): 2.307264804840088, {'accuracy': 0.0483, 'data_size': 10000}
INFO flwr 2024-04-15 09:04:04,221 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 09:04:04,222 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1608607)[0m 2024-04-15 09:04:07.859925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1608607)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1608609)[0m 2024-04-15 09:04:10.224914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1608604)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1608604)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1608600)[0m 2024-04-15 09:04:08.535532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1608600)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1608600)[0m 2024-04-15 09:04:10.894121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 09:04:55,940 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 09:04:57,353 | server.py:125 | fit progress: (1, 1.8353400230407715, {'accuracy': 0.7298, 'data_size': 10000}, 53.13145251700189)
INFO flwr 2024-04-15 09:04:57,354 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 09:04:57,354 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:05:32,729 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 09:05:34,215 | server.py:125 | fit progress: (2, 1.6776195764541626, {'accuracy': 0.8145, 'data_size': 10000}, 89.99315774301067)
INFO flwr 2024-04-15 09:05:34,215 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 09:05:34,216 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:06:15,299 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 09:06:16,762 | server.py:125 | fit progress: (3, 1.6106394529342651, {'accuracy': 0.8592, 'data_size': 10000}, 132.54035455500707)
INFO flwr 2024-04-15 09:06:16,763 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 09:06:16,763 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:06:59,658 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 09:07:01,155 | server.py:125 | fit progress: (4, 1.5812002420425415, {'accuracy': 0.8831, 'data_size': 10000}, 176.93324062498868)
INFO flwr 2024-04-15 09:07:01,156 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 09:07:01,156 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:07:42,939 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 09:07:44,432 | server.py:125 | fit progress: (5, 1.5658764839172363, {'accuracy': 0.8968, 'data_size': 10000}, 220.21044209698448)
INFO flwr 2024-04-15 09:07:44,433 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 09:07:44,433 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:08:25,609 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 09:08:27,105 | server.py:125 | fit progress: (6, 1.5580010414123535, {'accuracy': 0.9041, 'data_size': 10000}, 262.88272732900805)
INFO flwr 2024-04-15 09:08:27,105 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 09:08:27,105 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:09:07,288 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 09:09:08,502 | server.py:125 | fit progress: (7, 1.5544161796569824, {'accuracy': 0.9084, 'data_size': 10000}, 304.27972009498626)
INFO flwr 2024-04-15 09:09:08,502 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 09:09:08,502 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:09:52,809 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 09:09:54,317 | server.py:125 | fit progress: (8, 1.552595853805542, {'accuracy': 0.9106, 'data_size': 10000}, 350.0946001250122)
INFO flwr 2024-04-15 09:09:54,317 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 09:09:54,317 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:10:37,655 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 09:10:38,853 | server.py:125 | fit progress: (9, 1.553167700767517, {'accuracy': 0.9095, 'data_size': 10000}, 394.631293110986)
INFO flwr 2024-04-15 09:10:38,854 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 09:10:38,854 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:11:20,387 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 09:11:21,593 | server.py:125 | fit progress: (10, 1.5505824089050293, {'accuracy': 0.9112, 'data_size': 10000}, 437.37086793000344)
INFO flwr 2024-04-15 09:11:21,593 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 09:11:21,593 | server.py:153 | FL finished in 437.37136836600257
INFO flwr 2024-04-15 09:11:21,594 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 09:11:21,594 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 09:11:21,594 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 09:11:21,594 | app.py:229 | app_fit: losses_centralized [(0, 2.307264804840088), (1, 1.8353400230407715), (2, 1.6776195764541626), (3, 1.6106394529342651), (4, 1.5812002420425415), (5, 1.5658764839172363), (6, 1.5580010414123535), (7, 1.5544161796569824), (8, 1.552595853805542), (9, 1.553167700767517), (10, 1.5505824089050293)]
INFO flwr 2024-04-15 09:11:21,594 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0483), (1, 0.7298), (2, 0.8145), (3, 0.8592), (4, 0.8831), (5, 0.8968), (6, 0.9041), (7, 0.9084), (8, 0.9106), (9, 0.9095), (10, 0.9112)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9112
wandb:     loss 1.55058
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_090340-dy3j86v3
wandb: Find logs at: ./wandb/offline-run-20240415_090340-dy3j86v3/logs
INFO flwr 2024-04-15 09:11:25,123 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: FedAdam
			local: {'lr': 0.2}
			global: {'lr': 0.2, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 09:18:35,666 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1608596)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1608596)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 09:18:42,013	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 09:18:43,117	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 09:18:43,633	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 09:18:43,805	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 09:18:43,979	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 09:18:44,077	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 09:18:44,414	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4bd04d0087aa85e8.zip' (216.47MiB) to Ray cluster...
2024-04-15 09:18:45,264	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4bd04d0087aa85e8.zip'.
INFO flwr 2024-04-15 09:18:57,697 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75326163763.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 165761048781.0}
INFO flwr 2024-04-15 09:18:57,697 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 09:18:57,697 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 09:18:57,715 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 09:18:57,716 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 09:18:57,717 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 09:18:57,717 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 09:19:00,462 | server.py:94 | initial parameters (loss, other metrics): 2.302804708480835, {'accuracy': 0.0685, 'data_size': 10000}
INFO flwr 2024-04-15 09:19:00,462 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 09:19:00,463 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1613867)[0m 2024-04-15 09:19:04.526740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1613867)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1613867)[0m 2024-04-15 09:19:06.972054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1613875)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1613875)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1613872)[0m 2024-04-15 09:19:05.007232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1613872)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1613872)[0m 2024-04-15 09:19:08.106814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 09:19:53,816 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 09:19:55,323 | server.py:125 | fit progress: (1, 1.8408173322677612, {'accuracy': 0.6286, 'data_size': 10000}, 54.860058154008584)
INFO flwr 2024-04-15 09:19:55,323 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 09:19:55,324 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:20:35,699 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 09:20:37,157 | server.py:125 | fit progress: (2, 1.6335439682006836, {'accuracy': 0.8321, 'data_size': 10000}, 96.69401043900871)
INFO flwr 2024-04-15 09:20:37,157 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 09:20:37,157 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:21:18,169 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 09:21:19,722 | server.py:125 | fit progress: (3, 1.5685291290283203, {'accuracy': 0.894, 'data_size': 10000}, 139.25926606700523)
INFO flwr 2024-04-15 09:21:19,722 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 09:21:19,723 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:21:57,659 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 09:21:59,088 | server.py:125 | fit progress: (4, 1.5678126811981201, {'accuracy': 0.8949, 'data_size': 10000}, 178.6250478879956)
INFO flwr 2024-04-15 09:21:59,088 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 09:21:59,088 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:22:40,143 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 09:22:41,581 | server.py:125 | fit progress: (5, 1.556823492050171, {'accuracy': 0.9049, 'data_size': 10000}, 221.11776553900563)
INFO flwr 2024-04-15 09:22:41,581 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 09:22:41,581 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:23:26,167 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 09:23:27,644 | server.py:125 | fit progress: (6, 1.5572965145111084, {'accuracy': 0.9035, 'data_size': 10000}, 267.18133830599254)
INFO flwr 2024-04-15 09:23:27,645 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 09:23:27,645 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:24:07,143 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 09:24:08,351 | server.py:125 | fit progress: (7, 1.5757476091384888, {'accuracy': 0.8843, 'data_size': 10000}, 307.8881626020011)
INFO flwr 2024-04-15 09:24:08,351 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 09:24:08,352 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:24:49,422 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 09:24:50,895 | server.py:125 | fit progress: (8, 1.5626991987228394, {'accuracy': 0.8987, 'data_size': 10000}, 350.4320759859984)
INFO flwr 2024-04-15 09:24:50,895 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 09:24:50,896 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:25:28,398 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 09:25:29,605 | server.py:125 | fit progress: (9, 1.5502129793167114, {'accuracy': 0.911, 'data_size': 10000}, 389.1418485259928)
INFO flwr 2024-04-15 09:25:29,605 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 09:25:29,605 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:26:07,706 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 09:26:08,910 | server.py:125 | fit progress: (10, 1.5478004217147827, {'accuracy': 0.9127, 'data_size': 10000}, 428.44751708500553)
INFO flwr 2024-04-15 09:26:08,911 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 09:26:08,911 | server.py:153 | FL finished in 428.4483129220025
INFO flwr 2024-04-15 09:26:08,911 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 09:26:08,912 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 09:26:08,912 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 09:26:08,912 | app.py:229 | app_fit: losses_centralized [(0, 2.302804708480835), (1, 1.8408173322677612), (2, 1.6335439682006836), (3, 1.5685291290283203), (4, 1.5678126811981201), (5, 1.556823492050171), (6, 1.5572965145111084), (7, 1.5757476091384888), (8, 1.5626991987228394), (9, 1.5502129793167114), (10, 1.5478004217147827)]
INFO flwr 2024-04-15 09:26:08,912 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0685), (1, 0.6286), (2, 0.8321), (3, 0.894), (4, 0.8949), (5, 0.9049), (6, 0.9035), (7, 0.8843), (8, 0.8987), (9, 0.911), (10, 0.9127)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9127
wandb:     loss 1.5478
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_091835-tq57310i
wandb: Find logs at: ./wandb/offline-run-20240415_091835-tq57310i/logs
INFO flwr 2024-04-15 09:26:12,457 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.05, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-15 09:33:23,608 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1613867)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1613867)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-15 09:33:28,597	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-15 09:33:29,984	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-15 09:33:30,532	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out is very large (70.12MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280555.out']})`
2024-04-15 09:33:30,709	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out is very large (70.06MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280556.out']})`
2024-04-15 09:33:30,888	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out is very large (40.11MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280669.out']})`
2024-04-15 09:33:30,988	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out is very large (19.38MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/hpc_runs/slurm_outputs/mnist/slurm-280671.out']})`
2024-04-15 09:33:31,331	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_241391ad3d1a3b5f.zip' (216.48MiB) to Ray cluster...
2024-04-15 09:33:32,249	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_241391ad3d1a3b5f.zip'.
INFO flwr 2024-04-15 09:33:44,776 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 75306066739.0, 'GPU': 1.0, 'CPU': 64.0, 'accelerator_type:TITAN': 1.0, 'memory': 165714155725.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-15 09:33:44,776 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-15 09:33:44,776 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-15 09:33:44,798 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-15 09:33:44,798 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-15 09:33:44,799 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-15 09:33:44,799 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-15 09:33:48,375 | server.py:94 | initial parameters (loss, other metrics): 2.3021957874298096, {'accuracy': 0.0855, 'data_size': 10000}
INFO flwr 2024-04-15 09:33:48,376 | server.py:104 | FL starting
DEBUG flwr 2024-04-15 09:33:48,377 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1620156)[0m 2024-04-15 09:33:51.603936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1620156)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1620156)[0m 2024-04-15 09:33:54.102302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1620159)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1620159)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1620161)[0m 2024-04-15 09:33:51.868748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1620161)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1620158)[0m 2024-04-15 09:33:54.649372: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-15 09:34:09,514 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-04-15 09:34:10,977 | server.py:125 | fit progress: (1, 2.030548572540283, {'accuracy': 0.5121, 'data_size': 10000}, 22.601269695995143)
INFO flwr 2024-04-15 09:34:10,978 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-15 09:34:10,978 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:34:19,743 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-15 09:34:21,161 | server.py:125 | fit progress: (2, 1.8318017721176147, {'accuracy': 0.6478, 'data_size': 10000}, 32.78472292498918)
INFO flwr 2024-04-15 09:34:21,161 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-15 09:34:21,161 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:34:29,274 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-15 09:34:30,802 | server.py:125 | fit progress: (3, 1.6734102964401245, {'accuracy': 0.8261, 'data_size': 10000}, 42.42580725401058)
INFO flwr 2024-04-15 09:34:30,802 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-15 09:34:30,802 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:34:38,897 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-15 09:34:40,552 | server.py:125 | fit progress: (4, 1.656838297843933, {'accuracy': 0.8283, 'data_size': 10000}, 52.17610607799725)
INFO flwr 2024-04-15 09:34:40,553 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-15 09:34:40,553 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:34:49,893 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-15 09:34:51,460 | server.py:125 | fit progress: (5, 1.624703288078308, {'accuracy': 0.8517, 'data_size': 10000}, 63.08358799299458)
INFO flwr 2024-04-15 09:34:51,460 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-15 09:34:51,460 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:34:59,642 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-15 09:35:01,122 | server.py:125 | fit progress: (6, 1.5893616676330566, {'accuracy': 0.8816, 'data_size': 10000}, 72.74613104900345)
INFO flwr 2024-04-15 09:35:01,123 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-15 09:35:01,123 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:35:09,738 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-15 09:35:10,944 | server.py:125 | fit progress: (7, 1.6124968528747559, {'accuracy': 0.8543, 'data_size': 10000}, 82.56785173699609)
INFO flwr 2024-04-15 09:35:10,944 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-15 09:35:10,944 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:35:19,529 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-15 09:35:20,954 | server.py:125 | fit progress: (8, 1.5955530405044556, {'accuracy': 0.8713, 'data_size': 10000}, 92.57811918700463)
INFO flwr 2024-04-15 09:35:20,954 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-15 09:35:20,955 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:35:30,036 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-15 09:35:31,238 | server.py:125 | fit progress: (9, 1.5749154090881348, {'accuracy': 0.8906, 'data_size': 10000}, 102.8621627370012)
INFO flwr 2024-04-15 09:35:31,238 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-15 09:35:31,239 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-15 09:35:40,239 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-15 09:35:41,446 | server.py:125 | fit progress: (10, 1.5689150094985962, {'accuracy': 0.8972, 'data_size': 10000}, 113.06953940400854)
INFO flwr 2024-04-15 09:35:41,446 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-15 09:35:41,446 | server.py:153 | FL finished in 113.07001527398825
INFO flwr 2024-04-15 09:35:41,446 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-15 09:35:41,446 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-15 09:35:41,447 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-15 09:35:41,447 | app.py:229 | app_fit: losses_centralized [(0, 2.3021957874298096), (1, 2.030548572540283), (2, 1.8318017721176147), (3, 1.6734102964401245), (4, 1.656838297843933), (5, 1.624703288078308), (6, 1.5893616676330566), (7, 1.6124968528747559), (8, 1.5955530405044556), (9, 1.5749154090881348), (10, 1.5689150094985962)]
INFO flwr 2024-04-15 09:35:41,447 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0855), (1, 0.5121), (2, 0.6478), (3, 0.8261), (4, 0.8283), (5, 0.8517), (6, 0.8816), (7, 0.8543), (8, 0.8713), (9, 0.8906), (10, 0.8972)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8972
wandb:     loss 1.56892
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240415_093323-01mjm3ko
wandb: Find logs at: ./wandb/offline-run-20240415_093323-01mjm3ko/logs
INFO flwr 2024-04-15 09:35:45,050 | run_simulation.py:153 | 
Running with Config
	Simulation
		batch_size: 2
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: FedAdam
			local: {'lr': 0.05}
			global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-08, 'weight_decay': 0.9999}
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
