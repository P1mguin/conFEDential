ctit085
2024-04-03 08:33:29.305623: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-03 08:33:35.279466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-03 08:33:55.537189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-03 08:41:51,030 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-03 08:41:57,817	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-03 08:42:01,937	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-03 08:42:02,195	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6edf60eeafec3338.zip' (7.60MiB) to Ray cluster...
2024-04-03 08:42:02,213	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6edf60eeafec3338.zip'.
INFO flwr 2024-04-03 08:42:12,135 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.15': 1.0, 'object_store_memory': 75225103564.0, 'accelerator_type:RTX': 1.0, 'GPU': 1.0, 'memory': 165525241652.0, 'CPU': 64.0}
INFO flwr 2024-04-03 08:42:12,135 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-03 08:42:12,136 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-03 08:42:12,157 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-03 08:42:12,158 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-03 08:42:12,159 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-03 08:42:12,159 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=3081329)[0m 2024-04-03 08:42:17.777204: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3081324)[0m 2024-04-03 08:42:17.870973: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3081324)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3081323)[0m 2024-04-03 08:42:19.911086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-03 08:42:33,173 | server.py:94 | initial parameters (loss, other metrics): 2.3025906085968018, {'accuracy': 0.146, 'data_size': 10000}
INFO flwr 2024-04-03 08:42:33,173 | server.py:104 | FL starting
DEBUG flwr 2024-04-03 08:42:33,174 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=3081323)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3081323)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3081333)[0m 2024-04-03 08:42:18.019149: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=3081333)[0m 2024-04-03 08:42:18.103053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3081333)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=3081335)[0m 2024-04-03 08:42:20.059554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-03 08:42:58,472 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-03 08:43:01,829 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-03 08:43:07,639 | server.py:125 | fit progress: (1, 2.3025906085968018, {'accuracy': 0.146, 'data_size': 10000}, 34.46516590099782)
INFO flwr 2024-04-03 08:43:07,639 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-03 08:43:07,639 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-03 08:43:15,318 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-03 08:43:30,277 | server.py:125 | fit progress: (2, 2.3025906085968018, {'accuracy': 0.146, 'data_size': 10000}, 57.1033903619973)
INFO flwr 2024-04-03 08:43:30,277 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-03 08:43:30,278 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-03 08:43:37,315 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-03 08:43:57,591 | server.py:125 | fit progress: (3, 2.3025906085968018, {'accuracy': 0.1459, 'data_size': 10000}, 84.41728899799637)
INFO flwr 2024-04-03 08:43:57,591 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-03 08:43:57,591 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-03 08:44:04,650 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-03 08:44:36,643 | server.py:125 | fit progress: (4, 2.3025906085968018, {'accuracy': 0.1461, 'data_size': 10000}, 123.46953864299576)
INFO flwr 2024-04-03 08:44:36,644 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-03 08:44:36,644 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-03 08:44:43,641 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-03 08:45:22,146 | server.py:125 | fit progress: (5, 2.3025906085968018, {'accuracy': 0.1462, 'data_size': 10000}, 168.9726228909858)
INFO flwr 2024-04-03 08:45:22,147 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-03 08:45:22,147 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-03 08:45:29,768 | server.py:236 | fit_round 6 received 10 results and 0 failures
slurmstepd-ctit085: error: *** JOB 278666 ON ctit085 CANCELLED AT 2024-04-03T08:46:46 ***
