ctit088
2024-04-05 18:40:11.175593: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-05 18:40:16.930114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-05 18:40:42.668683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-05 18:42:50,017 | batch_run_simulation.py:63 | Loaded 140 configs, running...
INFO flwr 2024-04-05 18:42:50,018 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 18:50:12,147 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-05 18:50:20,885	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 18:50:29,779	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 18:50:30,090	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f4205761b2e4b5d0.zip' (7.45MiB) to Ray cluster...
2024-04-05 18:50:30,118	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f4205761b2e4b5d0.zip'.
INFO flwr 2024-04-05 18:50:41,807 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 75443141836.0, 'node:10.20.240.18': 1.0, 'memory': 166033997620.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-05 18:50:41,808 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 18:50:41,808 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 18:50:41,826 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 18:50:41,827 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 18:50:41,828 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 18:50:41,828 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=445787)[0m 2024-04-05 18:50:47.737550: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=445787)[0m 2024-04-05 18:50:47.834766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=445787)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=445787)[0m 2024-04-05 18:50:49.684898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-05 18:50:50,210 | server.py:94 | initial parameters (loss, other metrics): 2.302842617034912, {'accuracy': 0.0938, 'data_size': 10000}
INFO flwr 2024-04-05 18:50:50,210 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 18:50:50,211 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=445787)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=445787)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=445772)[0m 2024-04-05 18:50:47.782670: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=445793)[0m 2024-04-05 18:50:47.826137: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=445793)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=445782)[0m 2024-04-05 18:50:49.724395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 18:51:14,855 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 18:51:15,597 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 18:51:16,636 | server.py:125 | fit progress: (1, 2.302842617034912, {'accuracy': 0.0938, 'data_size': 10000}, 26.425070075012627)
INFO flwr 2024-04-05 18:51:16,636 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 18:51:16,637 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:51:25,725 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 18:51:28,574 | server.py:125 | fit progress: (2, 2.302842617034912, {'accuracy': 0.094, 'data_size': 10000}, 38.363496122008655)
INFO flwr 2024-04-05 18:51:28,574 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 18:51:28,575 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:51:37,204 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 18:51:40,915 | server.py:125 | fit progress: (3, 2.302842617034912, {'accuracy': 0.0939, 'data_size': 10000}, 50.70404656200844)
INFO flwr 2024-04-05 18:51:40,915 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 18:51:40,915 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:51:49,103 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 18:51:53,656 | server.py:125 | fit progress: (4, 2.302842140197754, {'accuracy': 0.0939, 'data_size': 10000}, 63.44514820800396)
INFO flwr 2024-04-05 18:51:53,656 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 18:51:53,656 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:52:02,148 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 18:52:07,015 | server.py:125 | fit progress: (5, 2.302842140197754, {'accuracy': 0.0939, 'data_size': 10000}, 76.80459290101135)
INFO flwr 2024-04-05 18:52:07,015 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 18:52:07,016 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:52:15,406 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 18:52:21,339 | server.py:125 | fit progress: (6, 2.302842378616333, {'accuracy': 0.094, 'data_size': 10000}, 91.12800605400116)
INFO flwr 2024-04-05 18:52:21,339 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 18:52:21,339 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:52:29,441 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 18:52:36,442 | server.py:125 | fit progress: (7, 2.302842140197754, {'accuracy': 0.094, 'data_size': 10000}, 106.2309576000116)
INFO flwr 2024-04-05 18:52:36,442 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 18:52:36,443 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:52:44,271 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 18:52:51,401 | server.py:125 | fit progress: (8, 2.302842140197754, {'accuracy': 0.094, 'data_size': 10000}, 121.1905780800007)
INFO flwr 2024-04-05 18:52:51,402 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 18:52:51,402 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:52:59,028 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 18:53:07,281 | server.py:125 | fit progress: (9, 2.302842140197754, {'accuracy': 0.094, 'data_size': 10000}, 137.07058574400435)
INFO flwr 2024-04-05 18:53:07,281 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 18:53:07,282 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:53:14,741 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 18:53:24,061 | server.py:125 | fit progress: (10, 2.302841901779175, {'accuracy': 0.094, 'data_size': 10000}, 153.85079201000917)
INFO flwr 2024-04-05 18:53:24,062 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 18:53:24,062 | server.py:153 | FL finished in 153.85118249400693
INFO flwr 2024-04-05 18:53:24,062 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 18:53:24,062 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 18:53:24,062 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 18:53:24,062 | app.py:229 | app_fit: losses_centralized [(0, 2.302842617034912), (1, 2.302842617034912), (2, 2.302842617034912), (3, 2.302842617034912), (4, 2.302842140197754), (5, 2.302842140197754), (6, 2.302842378616333), (7, 2.302842140197754), (8, 2.302842140197754), (9, 2.302842140197754), (10, 2.302841901779175)]
INFO flwr 2024-04-05 18:53:24,063 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0938), (1, 0.0938), (2, 0.094), (3, 0.0939), (4, 0.0939), (5, 0.0939), (6, 0.094), (7, 0.094), (8, 0.094), (9, 0.094), (10, 0.094)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.094
wandb:     loss 2.30284
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_185008-obx047o9
wandb: Find logs at: ./wandb/offline-run-20240405_185008-obx047o9/logs
INFO flwr 2024-04-05 18:53:27,645 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:00:32,333 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=445793)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=445793)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 19:00:36,840	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:00:37,101	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:00:37,515	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6f47a0dddb008339.zip' (7.48MiB) to Ray cluster...
2024-04-05 19:00:37,533	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6f47a0dddb008339.zip'.
INFO flwr 2024-04-05 19:00:47,700 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 161076964352.0, 'object_store_memory': 73318699008.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 19:00:47,700 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:00:47,700 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:00:47,714 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:00:47,715 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:00:47,716 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:00:47,716 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 19:00:49,616 | server.py:94 | initial parameters (loss, other metrics): 2.302441120147705, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-05 19:00:49,616 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:00:49,616 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=452262)[0m 2024-04-05 19:00:53.208658: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=452263)[0m 2024-04-05 19:00:53.318835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=452263)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=452250)[0m 2024-04-05 19:00:55.463945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=452261)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=452261)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=452255)[0m 2024-04-05 19:00:53.895846: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=452257)[0m 2024-04-05 19:00:53.919560: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=452257)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=452254)[0m 2024-04-05 19:00:56.174497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 19:01:07,627 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:01:08,107 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:01:09,301 | server.py:125 | fit progress: (1, 2.302435874938965, {'accuracy': 0.1249, 'data_size': 10000}, 19.684965453998302)
INFO flwr 2024-04-05 19:01:09,301 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:01:09,302 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:01:17,569 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:01:19,832 | server.py:125 | fit progress: (2, 2.302431344985962, {'accuracy': 0.1239, 'data_size': 10000}, 30.215434379002545)
INFO flwr 2024-04-05 19:01:19,832 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:01:19,832 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:01:26,775 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:01:30,062 | server.py:125 | fit progress: (3, 2.30242657661438, {'accuracy': 0.1241, 'data_size': 10000}, 40.446286217993475)
INFO flwr 2024-04-05 19:01:30,063 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:01:30,063 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:01:37,367 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:01:41,467 | server.py:125 | fit progress: (4, 2.3024237155914307, {'accuracy': 0.1247, 'data_size': 10000}, 51.85083930200199)
INFO flwr 2024-04-05 19:01:41,467 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:01:41,467 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:01:48,440 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:01:53,102 | server.py:125 | fit progress: (5, 2.3024184703826904, {'accuracy': 0.1228, 'data_size': 10000}, 63.48567524099781)
INFO flwr 2024-04-05 19:01:53,102 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:01:53,102 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:02:00,678 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:02:06,381 | server.py:125 | fit progress: (6, 2.3024158477783203, {'accuracy': 0.1228, 'data_size': 10000}, 76.76516069899662)
INFO flwr 2024-04-05 19:02:06,382 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:02:06,382 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:02:13,820 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:02:20,094 | server.py:125 | fit progress: (7, 2.302412748336792, {'accuracy': 0.123, 'data_size': 10000}, 90.47803311099415)
INFO flwr 2024-04-05 19:02:20,095 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:02:20,095 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:02:27,247 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:02:35,005 | server.py:125 | fit progress: (8, 2.3024089336395264, {'accuracy': 0.124, 'data_size': 10000}, 105.3888780289999)
INFO flwr 2024-04-05 19:02:35,005 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:02:35,006 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:02:42,528 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:02:51,085 | server.py:125 | fit progress: (9, 2.302403450012207, {'accuracy': 0.1228, 'data_size': 10000}, 121.46894538399647)
INFO flwr 2024-04-05 19:02:51,085 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:02:51,086 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:02:58,507 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:03:07,229 | server.py:125 | fit progress: (10, 2.3023998737335205, {'accuracy': 0.123, 'data_size': 10000}, 137.61300141799438)
INFO flwr 2024-04-05 19:03:07,229 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:03:07,230 | server.py:153 | FL finished in 137.61333335499512
INFO flwr 2024-04-05 19:03:07,230 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:03:07,230 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:03:07,230 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:03:07,230 | app.py:229 | app_fit: losses_centralized [(0, 2.302441120147705), (1, 2.302435874938965), (2, 2.302431344985962), (3, 2.30242657661438), (4, 2.3024237155914307), (5, 2.3024184703826904), (6, 2.3024158477783203), (7, 2.302412748336792), (8, 2.3024089336395264), (9, 2.302403450012207), (10, 2.3023998737335205)]
INFO flwr 2024-04-05 19:03:07,230 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.1249), (2, 0.1239), (3, 0.1241), (4, 0.1247), (5, 0.1228), (6, 0.1228), (7, 0.123), (8, 0.124), (9, 0.1228), (10, 0.123)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.123
wandb:     loss 2.3024
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_190031-t9n5geew
wandb: Find logs at: ./wandb/offline-run-20240405_190031-t9n5geew/logs
INFO flwr 2024-04-05 19:03:10,787 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:10:16,879 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=452249)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=452249)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 19:10:22,437	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:10:22,825	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:10:23,204	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c7fcd1a3099b9366.zip' (7.50MiB) to Ray cluster...
2024-04-05 19:10:23,229	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c7fcd1a3099b9366.zip'.
INFO flwr 2024-04-05 19:10:33,386 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 149316277658.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68278404710.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-05 19:10:33,386 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:10:33,386 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:10:33,399 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:10:33,400 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:10:33,401 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:10:33,401 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 19:10:36,154 | server.py:94 | initial parameters (loss, other metrics): 2.3025121688842773, {'accuracy': 0.0851, 'data_size': 10000}
INFO flwr 2024-04-05 19:10:36,154 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:10:36,155 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=459154)[0m 2024-04-05 19:10:41.281681: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=459154)[0m 2024-04-05 19:10:41.340967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=459154)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=459154)[0m 2024-04-05 19:10:43.469060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=459140)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=459140)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=459145)[0m 2024-04-05 19:10:41.789524: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=459145)[0m 2024-04-05 19:10:41.895158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=459145)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=459142)[0m 2024-04-05 19:10:43.876930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 19:10:55,328 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:10:55,847 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:10:57,141 | server.py:125 | fit progress: (1, 2.3025028705596924, {'accuracy': 0.0858, 'data_size': 10000}, 20.986241667997092)
INFO flwr 2024-04-05 19:10:57,141 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:10:57,141 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:11:05,623 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:11:07,982 | server.py:125 | fit progress: (2, 2.3024940490722656, {'accuracy': 0.0877, 'data_size': 10000}, 31.827216017991304)
INFO flwr 2024-04-05 19:11:07,982 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:11:07,982 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:11:15,726 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:11:19,178 | server.py:125 | fit progress: (3, 2.3024837970733643, {'accuracy': 0.0891, 'data_size': 10000}, 43.02299434298766)
INFO flwr 2024-04-05 19:11:19,178 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:11:19,178 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:11:26,390 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:11:30,323 | server.py:125 | fit progress: (4, 2.302476644515991, {'accuracy': 0.0874, 'data_size': 10000}, 54.16790511399449)
INFO flwr 2024-04-05 19:11:30,323 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:11:30,323 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:11:38,692 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:11:43,684 | server.py:125 | fit progress: (5, 2.302468776702881, {'accuracy': 0.0891, 'data_size': 10000}, 67.52907803599373)
INFO flwr 2024-04-05 19:11:43,684 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:11:43,684 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:11:51,416 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:11:57,093 | server.py:125 | fit progress: (6, 2.302457809448242, {'accuracy': 0.0893, 'data_size': 10000}, 80.93782309998642)
INFO flwr 2024-04-05 19:11:57,093 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:11:57,093 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:12:05,052 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:12:11,780 | server.py:125 | fit progress: (7, 2.3024487495422363, {'accuracy': 0.0926, 'data_size': 10000}, 95.62559173699992)
INFO flwr 2024-04-05 19:12:11,781 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:12:11,781 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:12:19,441 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:12:26,947 | server.py:125 | fit progress: (8, 2.3024380207061768, {'accuracy': 0.0943, 'data_size': 10000}, 110.79191297299985)
INFO flwr 2024-04-05 19:12:26,947 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:12:26,950 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:12:34,411 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:12:44,532 | server.py:125 | fit progress: (9, 2.30242919921875, {'accuracy': 0.0942, 'data_size': 10000}, 128.37706079799682)
INFO flwr 2024-04-05 19:12:44,532 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:12:44,532 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:12:52,285 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:13:01,412 | server.py:125 | fit progress: (10, 2.3024215698242188, {'accuracy': 0.0966, 'data_size': 10000}, 145.25688886699209)
INFO flwr 2024-04-05 19:13:01,412 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:13:01,412 | server.py:153 | FL finished in 145.25728538399562
INFO flwr 2024-04-05 19:13:01,412 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:13:01,412 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:13:01,412 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:13:01,412 | app.py:229 | app_fit: losses_centralized [(0, 2.3025121688842773), (1, 2.3025028705596924), (2, 2.3024940490722656), (3, 2.3024837970733643), (4, 2.302476644515991), (5, 2.302468776702881), (6, 2.302457809448242), (7, 2.3024487495422363), (8, 2.3024380207061768), (9, 2.30242919921875), (10, 2.3024215698242188)]
INFO flwr 2024-04-05 19:13:01,413 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0851), (1, 0.0858), (2, 0.0877), (3, 0.0891), (4, 0.0874), (5, 0.0891), (6, 0.0893), (7, 0.0926), (8, 0.0943), (9, 0.0942), (10, 0.0966)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0966
wandb:     loss 2.30242
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_191016-rge3kb2c
wandb: Find logs at: ./wandb/offline-run-20240405_191016-rge3kb2c/logs
INFO flwr 2024-04-05 19:13:04,938 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:20:18,477 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=459143)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=459143)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 19:20:42,142	ERROR services.py:1207 -- Failed to start the dashboard 
2024-04-05 19:20:42,143	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-04-05 19:20:42,144	ERROR services.py:1276 -- 
The last 20 lines of /tmp/ray/session_2024-04-05_19-20-20_681181_440919/logs/dashboard.log (it contains the error message from the dashboard): 
2024-04-05 19:20:40,822	INFO utils.py:123 -- Module ray.dashboard.modules.actor.actor_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-04-05 19:20:41,002	INFO utils.py:123 -- Module ray.dashboard.modules.event.event_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-04-05 19:20:41,027	INFO utils.py:123 -- Module ray.dashboard.modules.healthz.healthz_agent cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-04-05 19:20:41,030	INFO utils.py:123 -- Module ray.dashboard.modules.healthz.healthz_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-04-05 19:20:41,707	INFO utils.py:123 -- Module ray.dashboard.modules.job.job_agent cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-04-05 19:20:41,723	INFO utils.py:123 -- Module ray.dashboard.modules.job.job_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-04-05 19:20:41,801	INFO utils.py:123 -- Module ray.dashboard.modules.log.log_agent cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'
2024-04-05 19:20:41,805	INFO utils.py:123 -- Module ray.dashboard.modules.log.log_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'

2024-04-05 19:20:42,418	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:20:49,092	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:20:49,398	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_986742f902e36d08.zip' (7.52MiB) to Ray cluster...
2024-04-05 19:20:49,421	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_986742f902e36d08.zip'.
INFO flwr 2024-04-05 19:21:00,363 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 158687488615.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 72294637977.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 19:21:00,363 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:21:00,363 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:21:00,382 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:21:00,383 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:21:00,384 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:21:00,384 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 19:21:02,339 | server.py:94 | initial parameters (loss, other metrics): 2.3025853633880615, {'accuracy': 0.0866, 'data_size': 10000}
INFO flwr 2024-04-05 19:21:02,340 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:21:02,340 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=464594)[0m 2024-04-05 19:21:13.189833: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=464595)[0m 2024-04-05 19:21:13.307670: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=464595)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=464595)[0m 2024-04-05 19:21:36.293900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=464603)[0m 2024-04-05 19:21:13.204695: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=464605)[0m 2024-04-05 19:21:13.309405: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=464605)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=464594)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=464594)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=464605)[0m 2024-04-05 19:21:36.293882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 19:22:51,574 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:22:52,115 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:22:53,367 | server.py:125 | fit progress: (1, 2.3025739192962646, {'accuracy': 0.0859, 'data_size': 10000}, 111.0270059260074)
INFO flwr 2024-04-05 19:22:53,367 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:22:53,368 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:23:01,645 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:23:04,056 | server.py:125 | fit progress: (2, 2.3025619983673096, {'accuracy': 0.0872, 'data_size': 10000}, 121.71612922300119)
INFO flwr 2024-04-05 19:23:04,056 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:23:04,056 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:23:12,161 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:23:15,382 | server.py:125 | fit progress: (3, 2.302549362182617, {'accuracy': 0.0866, 'data_size': 10000}, 133.041954558008)
INFO flwr 2024-04-05 19:23:15,382 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:23:15,382 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:23:22,973 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:23:27,275 | server.py:125 | fit progress: (4, 2.3025338649749756, {'accuracy': 0.0886, 'data_size': 10000}, 144.93534124700818)
INFO flwr 2024-04-05 19:23:27,275 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:23:27,276 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:23:34,936 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:23:40,763 | server.py:125 | fit progress: (5, 2.302521228790283, {'accuracy': 0.087, 'data_size': 10000}, 158.42319446800684)
INFO flwr 2024-04-05 19:23:40,763 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:23:40,764 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:23:48,973 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:23:55,146 | server.py:125 | fit progress: (6, 2.30250883102417, {'accuracy': 0.0891, 'data_size': 10000}, 172.8057602619956)
INFO flwr 2024-04-05 19:23:55,146 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:23:55,146 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:24:02,825 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:24:09,706 | server.py:125 | fit progress: (7, 2.302497148513794, {'accuracy': 0.0874, 'data_size': 10000}, 187.36624565400416)
INFO flwr 2024-04-05 19:24:09,706 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:24:09,706 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:24:17,553 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:24:25,239 | server.py:125 | fit progress: (8, 2.302483558654785, {'accuracy': 0.0887, 'data_size': 10000}, 202.89919093399658)
INFO flwr 2024-04-05 19:24:25,239 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:24:25,239 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:24:33,921 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:24:42,412 | server.py:125 | fit progress: (9, 2.3024728298187256, {'accuracy': 0.0912, 'data_size': 10000}, 220.07182843600458)
INFO flwr 2024-04-05 19:24:42,412 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:24:42,412 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:24:50,266 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:25:00,059 | server.py:125 | fit progress: (10, 2.3024604320526123, {'accuracy': 0.0938, 'data_size': 10000}, 237.7188910190016)
INFO flwr 2024-04-05 19:25:00,059 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:25:00,059 | server.py:153 | FL finished in 237.71938996300742
INFO flwr 2024-04-05 19:25:00,059 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:25:00,059 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:25:00,059 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:25:00,060 | app.py:229 | app_fit: losses_centralized [(0, 2.3025853633880615), (1, 2.3025739192962646), (2, 2.3025619983673096), (3, 2.302549362182617), (4, 2.3025338649749756), (5, 2.302521228790283), (6, 2.30250883102417), (7, 2.302497148513794), (8, 2.302483558654785), (9, 2.3024728298187256), (10, 2.3024604320526123)]
INFO flwr 2024-04-05 19:25:00,060 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0866), (1, 0.0859), (2, 0.0872), (3, 0.0866), (4, 0.0886), (5, 0.087), (6, 0.0891), (7, 0.0874), (8, 0.0887), (9, 0.0912), (10, 0.0938)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0938
wandb:     loss 2.30246
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_192015-3axlpypu
wandb: Find logs at: ./wandb/offline-run-20240405_192015-3axlpypu/logs
INFO flwr 2024-04-05 19:25:03,628 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:32:09,309 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=464605)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=464605)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 19:32:14,101	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:32:14,488	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:32:14,811	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2667c3df534531b.zip' (7.54MiB) to Ray cluster...
2024-04-05 19:32:14,829	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2667c3df534531b.zip'.
INFO flwr 2024-04-05 19:32:25,613 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 72082148966.0, 'node:10.20.240.18': 1.0, 'memory': 158191680922.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-05 19:32:25,613 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:32:25,613 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:32:25,627 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:32:25,627 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:32:25,627 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:32:25,628 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 19:32:27,808 | server.py:94 | initial parameters (loss, other metrics): 2.3025784492492676, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-05 19:32:27,809 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:32:27,809 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=472524)[0m 2024-04-05 19:32:31.165820: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=472524)[0m 2024-04-05 19:32:31.328757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=472524)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=472522)[0m 2024-04-05 19:32:33.657629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=472522)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=472522)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=472518)[0m 2024-04-05 19:32:32.104355: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=472518)[0m 2024-04-05 19:32:32.204631: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=472518)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=472518)[0m 2024-04-05 19:32:34.258866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 19:32:45,732 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:32:46,243 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:32:47,504 | server.py:125 | fit progress: (1, 2.302561044692993, {'accuracy': 0.1135, 'data_size': 10000}, 19.694731985990074)
INFO flwr 2024-04-05 19:32:47,504 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:32:47,504 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:32:55,945 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:32:58,603 | server.py:125 | fit progress: (2, 2.302541494369507, {'accuracy': 0.1135, 'data_size': 10000}, 30.79363276300137)
INFO flwr 2024-04-05 19:32:58,603 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:32:58,603 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:33:06,586 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:33:09,809 | server.py:125 | fit progress: (3, 2.3025214672088623, {'accuracy': 0.1135, 'data_size': 10000}, 41.999451112991665)
INFO flwr 2024-04-05 19:33:09,809 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:33:09,809 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:33:17,196 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:33:21,484 | server.py:125 | fit progress: (4, 2.3025012016296387, {'accuracy': 0.1135, 'data_size': 10000}, 53.675219905999256)
INFO flwr 2024-04-05 19:33:21,485 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:33:21,485 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:33:29,132 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:33:34,336 | server.py:125 | fit progress: (5, 2.302478551864624, {'accuracy': 0.1135, 'data_size': 10000}, 66.52728511299938)
INFO flwr 2024-04-05 19:33:34,337 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:33:34,337 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:33:42,165 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:33:47,896 | server.py:125 | fit progress: (6, 2.302459478378296, {'accuracy': 0.1135, 'data_size': 10000}, 80.08638763098861)
INFO flwr 2024-04-05 19:33:47,896 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:33:47,896 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:33:55,642 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:34:02,491 | server.py:125 | fit progress: (7, 2.3024377822875977, {'accuracy': 0.1135, 'data_size': 10000}, 94.68162728699099)
INFO flwr 2024-04-05 19:34:02,491 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:34:02,491 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:34:10,079 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:34:17,505 | server.py:125 | fit progress: (8, 2.302414894104004, {'accuracy': 0.1135, 'data_size': 10000}, 109.6961591559957)
INFO flwr 2024-04-05 19:34:17,506 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:34:17,506 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:34:25,652 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:34:34,017 | server.py:125 | fit progress: (9, 2.302389144897461, {'accuracy': 0.1135, 'data_size': 10000}, 126.20774614300171)
INFO flwr 2024-04-05 19:34:34,017 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:34:34,017 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:34:41,468 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:34:50,864 | server.py:125 | fit progress: (10, 2.3023693561553955, {'accuracy': 0.1135, 'data_size': 10000}, 143.0547425650002)
INFO flwr 2024-04-05 19:34:50,864 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:34:50,864 | server.py:153 | FL finished in 143.05520858199452
INFO flwr 2024-04-05 19:34:50,864 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:34:50,865 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:34:50,865 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:34:50,865 | app.py:229 | app_fit: losses_centralized [(0, 2.3025784492492676), (1, 2.302561044692993), (2, 2.302541494369507), (3, 2.3025214672088623), (4, 2.3025012016296387), (5, 2.302478551864624), (6, 2.302459478378296), (7, 2.3024377822875977), (8, 2.302414894104004), (9, 2.302389144897461), (10, 2.3023693561553955)]
INFO flwr 2024-04-05 19:34:50,865 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1135), (2, 0.1135), (3, 0.1135), (4, 0.1135), (5, 0.1135), (6, 0.1135), (7, 0.1135), (8, 0.1135), (9, 0.1135), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.30237
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_193208-kmms4rrq
wandb: Find logs at: ./wandb/offline-run-20240405_193208-kmms4rrq/logs
INFO flwr 2024-04-05 19:34:54,448 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:42:01,398 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=472518)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=472518)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 19:42:06,669	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:42:07,238	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:42:07,725	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f29dccf66a4fc796.zip' (7.56MiB) to Ray cluster...
2024-04-05 19:42:07,749	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f29dccf66a4fc796.zip'.
INFO flwr 2024-04-05 19:42:18,793 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 149490554676.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 68353094860.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 19:42:18,794 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:42:18,794 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:42:18,815 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:42:18,817 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:42:18,817 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:42:18,817 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 19:42:20,995 | server.py:94 | initial parameters (loss, other metrics): 2.3024845123291016, {'accuracy': 0.1249, 'data_size': 10000}
INFO flwr 2024-04-05 19:42:20,995 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:42:20,996 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=480016)[0m 2024-04-05 19:42:25.282587: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=480016)[0m 2024-04-05 19:42:25.379317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=480016)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=480020)[0m 2024-04-05 19:42:27.412613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=480017)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=480017)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=480017)[0m 2024-04-05 19:42:25.507362: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=480017)[0m 2024-04-05 19:42:25.598622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=480017)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=480018)[0m 2024-04-05 19:42:27.902161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 19:42:40,220 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:42:40,750 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:42:42,185 | server.py:125 | fit progress: (1, 2.302464723587036, {'accuracy': 0.1237, 'data_size': 10000}, 21.189696755987825)
INFO flwr 2024-04-05 19:42:42,186 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:42:42,186 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:42:51,712 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:42:54,536 | server.py:125 | fit progress: (2, 2.3024442195892334, {'accuracy': 0.123, 'data_size': 10000}, 33.53995446499903)
INFO flwr 2024-04-05 19:42:54,536 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:42:54,536 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:43:02,956 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:43:06,285 | server.py:125 | fit progress: (3, 2.302422285079956, {'accuracy': 0.1238, 'data_size': 10000}, 45.28917814599117)
INFO flwr 2024-04-05 19:43:06,285 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:43:06,285 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:43:14,458 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:43:18,977 | server.py:125 | fit progress: (4, 2.3024070262908936, {'accuracy': 0.1238, 'data_size': 10000}, 57.98189599599573)
INFO flwr 2024-04-05 19:43:18,978 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:43:18,978 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:43:27,343 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:43:32,374 | server.py:125 | fit progress: (5, 2.3023853302001953, {'accuracy': 0.1308, 'data_size': 10000}, 71.37843733499176)
INFO flwr 2024-04-05 19:43:32,374 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:43:32,374 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:43:40,926 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:43:46,843 | server.py:125 | fit progress: (6, 2.3023698329925537, {'accuracy': 0.1334, 'data_size': 10000}, 85.8472583959956)
INFO flwr 2024-04-05 19:43:46,843 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:43:46,843 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:43:54,921 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:44:01,573 | server.py:125 | fit progress: (7, 2.302344799041748, {'accuracy': 0.1358, 'data_size': 10000}, 100.57749941399379)
INFO flwr 2024-04-05 19:44:01,573 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:44:01,573 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:44:10,128 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:44:17,807 | server.py:125 | fit progress: (8, 2.3023276329040527, {'accuracy': 0.1421, 'data_size': 10000}, 116.81191290999413)
INFO flwr 2024-04-05 19:44:17,808 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:44:17,808 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:44:25,600 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:44:33,943 | server.py:125 | fit progress: (9, 2.3023064136505127, {'accuracy': 0.1501, 'data_size': 10000}, 132.94792051499826)
INFO flwr 2024-04-05 19:44:33,944 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:44:33,944 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:44:41,935 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:44:51,427 | server.py:125 | fit progress: (10, 2.3022828102111816, {'accuracy': 0.1499, 'data_size': 10000}, 150.43119902799663)
INFO flwr 2024-04-05 19:44:51,427 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:44:51,427 | server.py:153 | FL finished in 150.43158400099492
INFO flwr 2024-04-05 19:44:51,427 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:44:51,427 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:44:51,427 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:44:51,428 | app.py:229 | app_fit: losses_centralized [(0, 2.3024845123291016), (1, 2.302464723587036), (2, 2.3024442195892334), (3, 2.302422285079956), (4, 2.3024070262908936), (5, 2.3023853302001953), (6, 2.3023698329925537), (7, 2.302344799041748), (8, 2.3023276329040527), (9, 2.3023064136505127), (10, 2.3022828102111816)]
INFO flwr 2024-04-05 19:44:51,428 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1249), (1, 0.1237), (2, 0.123), (3, 0.1238), (4, 0.1238), (5, 0.1308), (6, 0.1334), (7, 0.1358), (8, 0.1421), (9, 0.1501), (10, 0.1499)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1499
wandb:     loss 2.30228
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_194200-o806ma54
wandb: Find logs at: ./wandb/offline-run-20240405_194200-o806ma54/logs
INFO flwr 2024-04-05 19:44:54,951 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:52:01,834 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=480012)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=480012)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 19:52:07,605	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:52:07,873	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:52:08,138	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4d9a6a5576c0d3d7.zip' (7.58MiB) to Ray cluster...
2024-04-05 19:52:08,156	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4d9a6a5576c0d3d7.zip'.
INFO flwr 2024-04-05 19:52:18,993 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 157673801524.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 71860200652.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-05 19:52:18,994 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:52:18,994 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:52:19,010 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:52:19,011 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:52:19,011 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:52:19,011 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 19:52:22,874 | server.py:94 | initial parameters (loss, other metrics): 2.3026721477508545, {'accuracy': 0.0954, 'data_size': 10000}
INFO flwr 2024-04-05 19:52:22,875 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:52:22,875 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=484419)[0m 2024-04-05 19:52:24.718269: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=484419)[0m 2024-04-05 19:52:24.818870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=484419)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=484419)[0m 2024-04-05 19:52:26.906981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=484419)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=484419)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=484415)[0m 2024-04-05 19:52:25.305002: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=484415)[0m 2024-04-05 19:52:25.401327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=484415)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=484417)[0m 2024-04-05 19:52:27.645005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 19:52:38,535 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:52:39,070 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:52:40,121 | server.py:125 | fit progress: (1, 2.3026468753814697, {'accuracy': 0.0952, 'data_size': 10000}, 17.24569667498872)
INFO flwr 2024-04-05 19:52:40,121 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:52:40,121 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:52:48,485 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:52:51,139 | server.py:125 | fit progress: (2, 2.302625894546509, {'accuracy': 0.093, 'data_size': 10000}, 28.26410543899692)
INFO flwr 2024-04-05 19:52:51,139 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:52:51,140 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:52:59,245 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:53:02,788 | server.py:125 | fit progress: (3, 2.302612781524658, {'accuracy': 0.1035, 'data_size': 10000}, 39.91267152399814)
INFO flwr 2024-04-05 19:53:02,788 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:53:02,788 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:53:11,117 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:53:15,342 | server.py:125 | fit progress: (4, 2.302597761154175, {'accuracy': 0.1147, 'data_size': 10000}, 52.466566805989714)
INFO flwr 2024-04-05 19:53:15,342 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:53:15,342 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:53:23,172 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:53:30,167 | server.py:125 | fit progress: (5, 2.302577257156372, {'accuracy': 0.1189, 'data_size': 10000}, 67.29179628599377)
INFO flwr 2024-04-05 19:53:30,167 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:53:30,168 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:53:38,105 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:53:44,124 | server.py:125 | fit progress: (6, 2.3025572299957275, {'accuracy': 0.1274, 'data_size': 10000}, 81.24895113499952)
INFO flwr 2024-04-05 19:53:44,124 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:53:44,124 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:53:52,016 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:53:58,831 | server.py:125 | fit progress: (7, 2.302539825439453, {'accuracy': 0.1367, 'data_size': 10000}, 95.95646177099843)
INFO flwr 2024-04-05 19:53:58,832 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:53:58,832 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:54:06,773 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:54:14,855 | server.py:125 | fit progress: (8, 2.302518129348755, {'accuracy': 0.1239, 'data_size': 10000}, 111.97999347098812)
INFO flwr 2024-04-05 19:54:14,855 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:54:14,855 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:54:22,798 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:54:31,323 | server.py:125 | fit progress: (9, 2.3024964332580566, {'accuracy': 0.1455, 'data_size': 10000}, 128.44831562899344)
INFO flwr 2024-04-05 19:54:31,324 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:54:31,324 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:54:39,409 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:54:51,095 | server.py:125 | fit progress: (10, 2.3024721145629883, {'accuracy': 0.1492, 'data_size': 10000}, 148.22017717998824)
INFO flwr 2024-04-05 19:54:51,096 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:54:51,096 | server.py:153 | FL finished in 148.22104979099822
INFO flwr 2024-04-05 19:54:51,096 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:54:51,096 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:54:51,096 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:54:51,097 | app.py:229 | app_fit: losses_centralized [(0, 2.3026721477508545), (1, 2.3026468753814697), (2, 2.302625894546509), (3, 2.302612781524658), (4, 2.302597761154175), (5, 2.302577257156372), (6, 2.3025572299957275), (7, 2.302539825439453), (8, 2.302518129348755), (9, 2.3024964332580566), (10, 2.3024721145629883)]
INFO flwr 2024-04-05 19:54:51,097 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0954), (1, 0.0952), (2, 0.093), (3, 0.1035), (4, 0.1147), (5, 0.1189), (6, 0.1274), (7, 0.1367), (8, 0.1239), (9, 0.1455), (10, 0.1492)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1492
wandb:     loss 2.30247
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_195200-l0lmafbm
wandb: Find logs at: ./wandb/offline-run-20240405_195200-l0lmafbm/logs
INFO flwr 2024-04-05 19:54:54,993 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:02:01,295 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=484411)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=484411)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:02:07,502	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:02:07,796	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:02:08,110	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fab6b3ac6671596a.zip' (7.60MiB) to Ray cluster...
2024-04-05 20:02:08,136	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fab6b3ac6671596a.zip'.
INFO flwr 2024-04-05 20:02:18,776 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151650886656.0, 'node:__internal_head__': 1.0, 'object_store_memory': 69278951424.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-05 20:02:18,776 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:02:18,776 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:02:18,790 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:02:18,791 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:02:18,791 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:02:18,791 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 20:02:22,059 | server.py:94 | initial parameters (loss, other metrics): 2.30263352394104, {'accuracy': 0.1007, 'data_size': 10000}
INFO flwr 2024-04-05 20:02:22,059 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:02:22,059 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=492259)[0m 2024-04-05 20:02:24.361751: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=492259)[0m 2024-04-05 20:02:24.461814: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=492259)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=492259)[0m 2024-04-05 20:02:26.551975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=492259)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=492259)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=492256)[0m 2024-04-05 20:02:25.126262: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=492256)[0m 2024-04-05 20:02:25.224149: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=492256)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=492256)[0m 2024-04-05 20:02:28.049813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:02:39,515 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:02:40,061 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:02:41,068 | server.py:125 | fit progress: (1, 2.302633047103882, {'accuracy': 0.1007, 'data_size': 10000}, 19.00885393099452)
INFO flwr 2024-04-05 20:02:41,068 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:02:41,068 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:02:49,759 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:02:52,196 | server.py:125 | fit progress: (2, 2.3026323318481445, {'accuracy': 0.1007, 'data_size': 10000}, 30.137215638998896)
INFO flwr 2024-04-05 20:02:52,197 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:02:52,197 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:02:59,736 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:03:03,402 | server.py:125 | fit progress: (3, 2.3026318550109863, {'accuracy': 0.1007, 'data_size': 10000}, 41.34242551999341)
INFO flwr 2024-04-05 20:03:03,402 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:03:03,402 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:03:11,321 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:03:15,461 | server.py:125 | fit progress: (4, 2.302631378173828, {'accuracy': 0.1007, 'data_size': 10000}, 53.40162607698585)
INFO flwr 2024-04-05 20:03:15,461 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:03:15,461 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:03:22,880 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:03:28,042 | server.py:125 | fit progress: (5, 2.302630662918091, {'accuracy': 0.1008, 'data_size': 10000}, 65.98327685899858)
INFO flwr 2024-04-05 20:03:28,043 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:03:28,043 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:03:35,743 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:03:41,539 | server.py:125 | fit progress: (6, 2.302630662918091, {'accuracy': 0.1008, 'data_size': 10000}, 79.47964022599626)
INFO flwr 2024-04-05 20:03:41,539 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:03:41,539 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:03:49,453 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:03:56,237 | server.py:125 | fit progress: (7, 2.3026301860809326, {'accuracy': 0.1007, 'data_size': 10000}, 94.17740465099632)
INFO flwr 2024-04-05 20:03:56,237 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:03:56,237 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:04:03,820 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:04:11,534 | server.py:125 | fit progress: (8, 2.3026294708251953, {'accuracy': 0.1008, 'data_size': 10000}, 109.47498676998657)
INFO flwr 2024-04-05 20:04:11,534 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:04:11,535 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:04:19,184 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:04:27,508 | server.py:125 | fit progress: (9, 2.302629232406616, {'accuracy': 0.1008, 'data_size': 10000}, 125.44855585698679)
INFO flwr 2024-04-05 20:04:27,508 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:04:27,508 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:04:35,542 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:04:44,899 | server.py:125 | fit progress: (10, 2.302629232406616, {'accuracy': 0.1008, 'data_size': 10000}, 142.8402139369864)
INFO flwr 2024-04-05 20:04:44,900 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:04:44,900 | server.py:153 | FL finished in 142.84057095598837
INFO flwr 2024-04-05 20:04:44,900 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:04:44,900 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:04:44,900 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:04:44,900 | app.py:229 | app_fit: losses_centralized [(0, 2.30263352394104), (1, 2.302633047103882), (2, 2.3026323318481445), (3, 2.3026318550109863), (4, 2.302631378173828), (5, 2.302630662918091), (6, 2.302630662918091), (7, 2.3026301860809326), (8, 2.3026294708251953), (9, 2.302629232406616), (10, 2.302629232406616)]
INFO flwr 2024-04-05 20:04:44,900 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1007), (1, 0.1007), (2, 0.1007), (3, 0.1007), (4, 0.1007), (5, 0.1008), (6, 0.1008), (7, 0.1007), (8, 0.1008), (9, 0.1008), (10, 0.1008)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1008
wandb:     loss 2.30263
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_200200-e1uth2wi
wandb: Find logs at: ./wandb/offline-run-20240405_200200-e1uth2wi/logs
INFO flwr 2024-04-05 20:04:48,453 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:11:55,111 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=492246)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=492246)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:12:00,030	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:12:00,394	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:12:00,658	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_823ba04fa33e47f3.zip' (7.62MiB) to Ray cluster...
2024-04-05 20:12:00,682	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_823ba04fa33e47f3.zip'.
INFO flwr 2024-04-05 20:12:11,739 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 145921805312.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66823630848.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-05 20:12:11,739 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:12:11,739 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:12:11,754 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:12:11,756 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:12:11,757 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:12:11,757 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 20:12:13,999 | server.py:94 | initial parameters (loss, other metrics): 2.3025307655334473, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-05 20:12:14,000 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:12:14,000 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=499665)[0m 2024-04-05 20:12:18.346984: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=499665)[0m 2024-04-05 20:12:18.466013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=499665)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=499664)[0m 2024-04-05 20:12:20.645480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=499665)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=499665)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=499670)[0m 2024-04-05 20:12:18.822397: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=499670)[0m 2024-04-05 20:12:18.922381: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=499670)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=499668)[0m 2024-04-05 20:12:21.467226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:12:33,070 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:12:33,622 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:12:34,945 | server.py:125 | fit progress: (1, 2.3025124073028564, {'accuracy': 0.0974, 'data_size': 10000}, 20.944858048998867)
INFO flwr 2024-04-05 20:12:34,945 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:12:34,945 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:12:43,595 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:12:46,235 | server.py:125 | fit progress: (2, 2.3024919033050537, {'accuracy': 0.0974, 'data_size': 10000}, 32.23492074100068)
INFO flwr 2024-04-05 20:12:46,235 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:12:46,235 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:12:54,440 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:12:58,284 | server.py:125 | fit progress: (3, 2.3024637699127197, {'accuracy': 0.0974, 'data_size': 10000}, 44.28413554999861)
INFO flwr 2024-04-05 20:12:58,284 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:12:58,284 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:13:05,805 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:13:10,678 | server.py:125 | fit progress: (4, 2.3024418354034424, {'accuracy': 0.0974, 'data_size': 10000}, 56.678458342998056)
INFO flwr 2024-04-05 20:13:10,679 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:13:10,679 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:13:18,524 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:13:23,936 | server.py:125 | fit progress: (5, 2.3024179935455322, {'accuracy': 0.0974, 'data_size': 10000}, 69.9360639059887)
INFO flwr 2024-04-05 20:13:23,936 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:13:23,937 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:13:33,223 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:13:39,358 | server.py:125 | fit progress: (6, 2.3023953437805176, {'accuracy': 0.0974, 'data_size': 10000}, 85.3583780409972)
INFO flwr 2024-04-05 20:13:39,359 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:13:39,359 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:13:47,387 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:13:54,405 | server.py:125 | fit progress: (7, 2.3023681640625, {'accuracy': 0.0974, 'data_size': 10000}, 100.40539290499873)
INFO flwr 2024-04-05 20:13:54,406 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:13:54,406 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:14:02,510 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:14:10,012 | server.py:125 | fit progress: (8, 2.302337408065796, {'accuracy': 0.0974, 'data_size': 10000}, 116.01235696199001)
INFO flwr 2024-04-05 20:14:10,013 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:14:10,013 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:14:17,931 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:14:26,786 | server.py:125 | fit progress: (9, 2.302321195602417, {'accuracy': 0.0974, 'data_size': 10000}, 132.78587648199755)
INFO flwr 2024-04-05 20:14:26,786 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:14:26,786 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:14:34,810 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:14:45,924 | server.py:125 | fit progress: (10, 2.3022923469543457, {'accuracy': 0.0974, 'data_size': 10000}, 151.9236797009944)
INFO flwr 2024-04-05 20:14:45,924 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:14:45,924 | server.py:153 | FL finished in 151.92406509199645
INFO flwr 2024-04-05 20:14:45,924 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:14:45,924 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:14:45,924 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:14:45,924 | app.py:229 | app_fit: losses_centralized [(0, 2.3025307655334473), (1, 2.3025124073028564), (2, 2.3024919033050537), (3, 2.3024637699127197), (4, 2.3024418354034424), (5, 2.3024179935455322), (6, 2.3023953437805176), (7, 2.3023681640625), (8, 2.302337408065796), (9, 2.302321195602417), (10, 2.3022923469543457)]
INFO flwr 2024-04-05 20:14:45,924 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.0974), (2, 0.0974), (3, 0.0974), (4, 0.0974), (5, 0.0974), (6, 0.0974), (7, 0.0974), (8, 0.0974), (9, 0.0974), (10, 0.0974)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0974
wandb:     loss 2.30229
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_201154-kzd3cp3z
wandb: Find logs at: ./wandb/offline-run-20240405_201154-kzd3cp3z/logs
INFO flwr 2024-04-05 20:14:49,501 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:21:55,596 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=499670)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=499670)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:22:00,567	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:22:00,859	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:22:01,196	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_08030da4d6415587.zip' (7.64MiB) to Ray cluster...
2024-04-05 20:22:01,214	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_08030da4d6415587.zip'.
INFO flwr 2024-04-05 20:22:11,962 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 157084141773.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 71607489331.0}
INFO flwr 2024-04-05 20:22:11,962 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:22:11,962 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:22:11,978 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:22:11,979 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:22:11,980 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:22:11,980 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 20:22:15,517 | server.py:94 | initial parameters (loss, other metrics): 2.3024537563323975, {'accuracy': 0.1429, 'data_size': 10000}
INFO flwr 2024-04-05 20:22:15,518 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:22:15,518 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=504054)[0m 2024-04-05 20:22:17.117220: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=504054)[0m 2024-04-05 20:22:17.211362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=504054)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=504061)[0m 2024-04-05 20:22:19.680988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=504053)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=504053)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=504050)[0m 2024-04-05 20:22:18.621461: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=504050)[0m 2024-04-05 20:22:18.705533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=504050)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=504050)[0m 2024-04-05 20:22:20.862862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:22:31,985 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:22:32,511 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:22:33,540 | server.py:125 | fit progress: (1, 2.3024096488952637, {'accuracy': 0.137, 'data_size': 10000}, 18.021986506995745)
INFO flwr 2024-04-05 20:22:33,540 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:22:33,540 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:22:42,134 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:22:44,678 | server.py:125 | fit progress: (2, 2.3023550510406494, {'accuracy': 0.1532, 'data_size': 10000}, 29.159938643002533)
INFO flwr 2024-04-05 20:22:44,678 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:22:44,679 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:22:52,748 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:22:55,998 | server.py:125 | fit progress: (3, 2.3023204803466797, {'accuracy': 0.1468, 'data_size': 10000}, 40.4801295340003)
INFO flwr 2024-04-05 20:22:55,998 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:22:55,999 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:23:03,679 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:23:08,071 | server.py:125 | fit progress: (4, 2.3022778034210205, {'accuracy': 0.1399, 'data_size': 10000}, 52.55329621800047)
INFO flwr 2024-04-05 20:23:08,072 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:23:08,072 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:23:16,613 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:23:21,652 | server.py:125 | fit progress: (5, 2.3022332191467285, {'accuracy': 0.1121, 'data_size': 10000}, 66.13440302500385)
INFO flwr 2024-04-05 20:23:21,653 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:23:21,653 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:23:29,263 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:23:35,354 | server.py:125 | fit progress: (6, 2.302184581756592, {'accuracy': 0.101, 'data_size': 10000}, 79.83593397300865)
INFO flwr 2024-04-05 20:23:35,354 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:23:35,354 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:23:43,206 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:23:50,067 | server.py:125 | fit progress: (7, 2.302147626876831, {'accuracy': 0.1077, 'data_size': 10000}, 94.54945458300062)
INFO flwr 2024-04-05 20:23:50,068 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:23:50,068 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:23:58,054 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:24:05,806 | server.py:125 | fit progress: (8, 2.3021037578582764, {'accuracy': 0.1569, 'data_size': 10000}, 110.2877352659998)
INFO flwr 2024-04-05 20:24:05,806 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:24:05,806 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:24:13,715 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:24:22,354 | server.py:125 | fit progress: (9, 2.3020753860473633, {'accuracy': 0.1566, 'data_size': 10000}, 126.83624373099883)
INFO flwr 2024-04-05 20:24:22,354 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:24:22,355 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:24:30,942 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:24:40,046 | server.py:125 | fit progress: (10, 2.302042007446289, {'accuracy': 0.1466, 'data_size': 10000}, 144.52823158599494)
INFO flwr 2024-04-05 20:24:40,046 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:24:40,047 | server.py:153 | FL finished in 144.52861406700686
INFO flwr 2024-04-05 20:24:40,047 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:24:40,047 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:24:40,047 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:24:40,047 | app.py:229 | app_fit: losses_centralized [(0, 2.3024537563323975), (1, 2.3024096488952637), (2, 2.3023550510406494), (3, 2.3023204803466797), (4, 2.3022778034210205), (5, 2.3022332191467285), (6, 2.302184581756592), (7, 2.302147626876831), (8, 2.3021037578582764), (9, 2.3020753860473633), (10, 2.302042007446289)]
INFO flwr 2024-04-05 20:24:40,047 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1429), (1, 0.137), (2, 0.1532), (3, 0.1468), (4, 0.1399), (5, 0.1121), (6, 0.101), (7, 0.1077), (8, 0.1569), (9, 0.1566), (10, 0.1466)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1466
wandb:     loss 2.30204
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_202155-d3xla8on
wandb: Find logs at: ./wandb/offline-run-20240405_202155-d3xla8on/logs
INFO flwr 2024-04-05 20:24:43,633 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:31:49,756 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=504050)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=504050)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:31:55,703	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:31:56,026	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:31:56,377	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_201d85278266ef60.zip' (7.66MiB) to Ray cluster...
2024-04-05 20:31:56,401	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_201d85278266ef60.zip'.
INFO flwr 2024-04-05 20:32:07,152 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 150922950452.0, 'CPU': 64.0, 'object_store_memory': 68966978764.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 20:32:07,152 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:32:07,153 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:32:07,177 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:32:07,179 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:32:07,179 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:32:07,180 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 20:32:10,971 | server.py:94 | initial parameters (loss, other metrics): 2.3025310039520264, {'accuracy': 0.1006, 'data_size': 10000}
INFO flwr 2024-04-05 20:32:10,971 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:32:10,972 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=511902)[0m 2024-04-05 20:32:12.988261: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=511902)[0m 2024-04-05 20:32:13.085666: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=511902)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=511899)[0m 2024-04-05 20:32:15.216688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=511899)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=511899)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=511897)[0m 2024-04-05 20:32:13.480491: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=511897)[0m 2024-04-05 20:32:13.572736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=511897)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=511900)[0m 2024-04-05 20:32:15.530191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:32:27,421 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:32:27,928 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:32:29,255 | server.py:125 | fit progress: (1, 2.3024604320526123, {'accuracy': 0.1, 'data_size': 10000}, 18.283102289002272)
INFO flwr 2024-04-05 20:32:29,255 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:32:29,255 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:32:37,721 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:32:40,062 | server.py:125 | fit progress: (2, 2.302361249923706, {'accuracy': 0.1015, 'data_size': 10000}, 29.090502084989566)
INFO flwr 2024-04-05 20:32:40,062 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:32:40,063 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:32:48,393 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:32:51,980 | server.py:125 | fit progress: (3, 2.3022444248199463, {'accuracy': 0.101, 'data_size': 10000}, 41.007934077002574)
INFO flwr 2024-04-05 20:32:51,980 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:32:51,980 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:32:59,330 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:33:03,332 | server.py:125 | fit progress: (4, 2.3021793365478516, {'accuracy': 0.1015, 'data_size': 10000}, 52.360570820994326)
INFO flwr 2024-04-05 20:33:03,333 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:33:03,333 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:33:11,070 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:33:16,180 | server.py:125 | fit progress: (5, 2.3020691871643066, {'accuracy': 0.101, 'data_size': 10000}, 65.20879227599653)
INFO flwr 2024-04-05 20:33:16,181 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:33:16,181 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:33:23,608 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:33:29,340 | server.py:125 | fit progress: (6, 2.301988363265991, {'accuracy': 0.1019, 'data_size': 10000}, 78.36813832398911)
INFO flwr 2024-04-05 20:33:29,340 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:33:29,340 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:33:37,262 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:33:44,005 | server.py:125 | fit progress: (7, 2.301882266998291, {'accuracy': 0.1379, 'data_size': 10000}, 93.032948854001)
INFO flwr 2024-04-05 20:33:44,005 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:33:44,005 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:33:51,830 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:33:59,540 | server.py:125 | fit progress: (8, 2.3017842769622803, {'accuracy': 0.101, 'data_size': 10000}, 108.56845447899832)
INFO flwr 2024-04-05 20:33:59,540 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:33:59,541 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:34:07,227 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:34:15,735 | server.py:125 | fit progress: (9, 2.3016819953918457, {'accuracy': 0.1132, 'data_size': 10000}, 124.76303523300157)
INFO flwr 2024-04-05 20:34:15,735 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:34:15,735 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:34:23,501 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:34:32,542 | server.py:125 | fit progress: (10, 2.301612615585327, {'accuracy': 0.1285, 'data_size': 10000}, 141.5707167609944)
INFO flwr 2024-04-05 20:34:32,543 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:34:32,543 | server.py:153 | FL finished in 141.57112322299508
INFO flwr 2024-04-05 20:34:32,543 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:34:32,543 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:34:32,543 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:34:32,543 | app.py:229 | app_fit: losses_centralized [(0, 2.3025310039520264), (1, 2.3024604320526123), (2, 2.302361249923706), (3, 2.3022444248199463), (4, 2.3021793365478516), (5, 2.3020691871643066), (6, 2.301988363265991), (7, 2.301882266998291), (8, 2.3017842769622803), (9, 2.3016819953918457), (10, 2.301612615585327)]
INFO flwr 2024-04-05 20:34:32,543 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1006), (1, 0.1), (2, 0.1015), (3, 0.101), (4, 0.1015), (5, 0.101), (6, 0.1019), (7, 0.1379), (8, 0.101), (9, 0.1132), (10, 0.1285)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1285
wandb:     loss 2.30161
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_203149-dzs8hqrb
wandb: Find logs at: ./wandb/offline-run-20240405_203149-dzs8hqrb/logs
INFO flwr 2024-04-05 20:34:36,041 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:41:42,840 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=511893)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=511893)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:41:47,733	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:41:48,066	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:41:48,469	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_edde5fbce5291ba2.zip' (7.68MiB) to Ray cluster...
2024-04-05 20:41:48,489	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_edde5fbce5291ba2.zip'.
INFO flwr 2024-04-05 20:41:59,455 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 146181891892.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 66935096524.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-05 20:41:59,455 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:41:59,455 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:41:59,469 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:41:59,469 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:41:59,469 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:41:59,470 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 20:42:01,683 | server.py:94 | initial parameters (loss, other metrics): 2.302619457244873, {'accuracy': 0.1031, 'data_size': 10000}
INFO flwr 2024-04-05 20:42:01,684 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:42:01,684 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=519312)[0m 2024-04-05 20:42:05.385935: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=519312)[0m 2024-04-05 20:42:05.484075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=519312)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=519304)[0m 2024-04-05 20:42:07.632866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=519313)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=519313)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=519310)[0m 2024-04-05 20:42:05.775826: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=519310)[0m 2024-04-05 20:42:05.889078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=519310)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=519308)[0m 2024-04-05 20:42:08.106948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:42:20,277 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:42:20,809 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:42:22,139 | server.py:125 | fit progress: (1, 2.3024938106536865, {'accuracy': 0.1008, 'data_size': 10000}, 20.454782578002778)
INFO flwr 2024-04-05 20:42:22,139 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:42:22,139 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:42:31,265 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:42:33,948 | server.py:125 | fit progress: (2, 2.302410364151001, {'accuracy': 0.1027, 'data_size': 10000}, 32.26440497000294)
INFO flwr 2024-04-05 20:42:33,949 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:42:33,949 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:42:41,965 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:42:45,432 | server.py:125 | fit progress: (3, 2.302325487136841, {'accuracy': 0.102, 'data_size': 10000}, 43.747815876995446)
INFO flwr 2024-04-05 20:42:45,432 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:42:45,432 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:42:54,269 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:42:58,721 | server.py:125 | fit progress: (4, 2.3022401332855225, {'accuracy': 0.1043, 'data_size': 10000}, 57.03724642599991)
INFO flwr 2024-04-05 20:42:58,721 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:42:58,722 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:43:06,596 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:43:11,536 | server.py:125 | fit progress: (5, 2.3021247386932373, {'accuracy': 0.1031, 'data_size': 10000}, 69.85167532399646)
INFO flwr 2024-04-05 20:43:11,536 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:43:11,536 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:43:19,941 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:43:26,052 | server.py:125 | fit progress: (6, 2.302065372467041, {'accuracy': 0.1031, 'data_size': 10000}, 84.36847641599888)
INFO flwr 2024-04-05 20:43:26,053 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:43:26,053 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:43:34,366 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:43:41,299 | server.py:125 | fit progress: (7, 2.301966905593872, {'accuracy': 0.1342, 'data_size': 10000}, 99.61481986498984)
INFO flwr 2024-04-05 20:43:41,299 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:43:41,299 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:43:49,433 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:43:57,214 | server.py:125 | fit progress: (8, 2.301884651184082, {'accuracy': 0.132, 'data_size': 10000}, 115.53056805799133)
INFO flwr 2024-04-05 20:43:57,215 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:43:57,215 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:44:05,400 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:44:13,795 | server.py:125 | fit progress: (9, 2.3017849922180176, {'accuracy': 0.1345, 'data_size': 10000}, 132.110972622002)
INFO flwr 2024-04-05 20:44:13,795 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:44:13,795 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:44:21,654 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:44:32,601 | server.py:125 | fit progress: (10, 2.301669120788574, {'accuracy': 0.1719, 'data_size': 10000}, 150.9167657789949)
INFO flwr 2024-04-05 20:44:32,601 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:44:32,601 | server.py:153 | FL finished in 150.9171143269923
INFO flwr 2024-04-05 20:44:32,601 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:44:32,601 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:44:32,601 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:44:32,601 | app.py:229 | app_fit: losses_centralized [(0, 2.302619457244873), (1, 2.3024938106536865), (2, 2.302410364151001), (3, 2.302325487136841), (4, 2.3022401332855225), (5, 2.3021247386932373), (6, 2.302065372467041), (7, 2.301966905593872), (8, 2.301884651184082), (9, 2.3017849922180176), (10, 2.301669120788574)]
INFO flwr 2024-04-05 20:44:32,601 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1031), (1, 0.1008), (2, 0.1027), (3, 0.102), (4, 0.1043), (5, 0.1031), (6, 0.1031), (7, 0.1342), (8, 0.132), (9, 0.1345), (10, 0.1719)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1719
wandb:     loss 2.30167
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_204142-t2kbtj4u
wandb: Find logs at: ./wandb/offline-run-20240405_204142-t2kbtj4u/logs
INFO flwr 2024-04-05 20:44:36,123 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:51:42,003 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=519303)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=519303)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:51:48,159	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:51:48,421	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:51:48,804	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d025d967605fda8a.zip' (7.70MiB) to Ray cluster...
2024-04-05 20:51:48,821	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d025d967605fda8a.zip'.
INFO flwr 2024-04-05 20:51:59,647 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 71300404838.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 156367611290.0, 'CPU': 64.0}
INFO flwr 2024-04-05 20:51:59,648 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:51:59,648 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:51:59,664 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:51:59,665 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:51:59,666 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:51:59,666 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 20:52:03,243 | server.py:94 | initial parameters (loss, other metrics): 2.3024065494537354, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-05 20:52:03,244 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:52:03,244 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=523692)[0m 2024-04-05 20:52:05.086793: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=523692)[0m 2024-04-05 20:52:05.182183: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=523692)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=523683)[0m 2024-04-05 20:52:07.628580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=523692)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=523692)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=523693)[0m 2024-04-05 20:52:05.818869: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=523693)[0m 2024-04-05 20:52:05.919285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=523693)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=523695)[0m 2024-04-05 20:52:07.822679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:52:20,005 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:52:20,506 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:52:21,562 | server.py:125 | fit progress: (1, 2.302338123321533, {'accuracy': 0.0982, 'data_size': 10000}, 18.317945344999316)
INFO flwr 2024-04-05 20:52:21,562 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:52:21,562 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:52:29,961 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:52:32,653 | server.py:125 | fit progress: (2, 2.3022334575653076, {'accuracy': 0.1106, 'data_size': 10000}, 29.40949146200728)
INFO flwr 2024-04-05 20:52:32,654 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:52:32,654 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:52:40,357 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:52:43,721 | server.py:125 | fit progress: (3, 2.3021368980407715, {'accuracy': 0.0982, 'data_size': 10000}, 40.47724813000241)
INFO flwr 2024-04-05 20:52:43,721 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:52:43,721 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:52:51,620 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:52:55,745 | server.py:125 | fit progress: (4, 2.302013635635376, {'accuracy': 0.0982, 'data_size': 10000}, 52.50137845000427)
INFO flwr 2024-04-05 20:52:55,746 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:52:55,746 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:53:03,663 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:53:08,831 | server.py:125 | fit progress: (5, 2.3018763065338135, {'accuracy': 0.0982, 'data_size': 10000}, 65.58696562401019)
INFO flwr 2024-04-05 20:53:08,831 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:53:08,831 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:53:16,454 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:53:22,239 | server.py:125 | fit progress: (6, 2.3017759323120117, {'accuracy': 0.1024, 'data_size': 10000}, 78.99557350399846)
INFO flwr 2024-04-05 20:53:22,240 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:53:22,240 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:53:29,950 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:53:36,913 | server.py:125 | fit progress: (7, 2.3016655445098877, {'accuracy': 0.119, 'data_size': 10000}, 93.66961120499764)
INFO flwr 2024-04-05 20:53:36,914 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:53:36,914 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:53:44,752 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:53:52,697 | server.py:125 | fit progress: (8, 2.3015379905700684, {'accuracy': 0.1524, 'data_size': 10000}, 109.45277464100218)
INFO flwr 2024-04-05 20:53:52,697 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:53:52,697 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:54:00,522 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:54:09,015 | server.py:125 | fit progress: (9, 2.3013803958892822, {'accuracy': 0.2076, 'data_size': 10000}, 125.77095954900142)
INFO flwr 2024-04-05 20:54:09,015 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:54:09,015 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:54:17,249 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:54:26,772 | server.py:125 | fit progress: (10, 2.301243305206299, {'accuracy': 0.2382, 'data_size': 10000}, 143.52839824100374)
INFO flwr 2024-04-05 20:54:26,773 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:54:26,773 | server.py:153 | FL finished in 143.52881502000673
INFO flwr 2024-04-05 20:54:26,773 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:54:26,773 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:54:26,773 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:54:26,773 | app.py:229 | app_fit: losses_centralized [(0, 2.3024065494537354), (1, 2.302338123321533), (2, 2.3022334575653076), (3, 2.3021368980407715), (4, 2.302013635635376), (5, 2.3018763065338135), (6, 2.3017759323120117), (7, 2.3016655445098877), (8, 2.3015379905700684), (9, 2.3013803958892822), (10, 2.301243305206299)]
INFO flwr 2024-04-05 20:54:26,773 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.0982), (2, 0.1106), (3, 0.0982), (4, 0.0982), (5, 0.0982), (6, 0.1024), (7, 0.119), (8, 0.1524), (9, 0.2076), (10, 0.2382)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2382
wandb:     loss 2.30124
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_205141-8o09kepu
wandb: Find logs at: ./wandb/offline-run-20240405_205141-8o09kepu/logs
INFO flwr 2024-04-05 20:54:30,301 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:01:37,455 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=523683)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=523683)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 21:01:45,907	ERROR node.py:605 -- Failed to connect to GCS. Please check `gcs_server.out` for more details.
2024-04-05 21:02:00,301	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:02:00,635	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:02:01,019	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5394fc471e27440b.zip' (7.72MiB) to Ray cluster...
2024-04-05 21:02:01,038	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5394fc471e27440b.zip'.
INFO flwr 2024-04-05 21:02:12,077 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 150688238592.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68866387968.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 21:02:12,078 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:02:12,078 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:02:12,095 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:02:12,096 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:02:12,097 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:02:12,097 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 21:02:15,186 | server.py:94 | initial parameters (loss, other metrics): 2.302610397338867, {'accuracy': 0.0977, 'data_size': 10000}
INFO flwr 2024-04-05 21:02:15,188 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:02:15,189 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=531603)[0m 2024-04-05 21:02:18.565032: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=531603)[0m 2024-04-05 21:02:18.711721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=531603)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=531603)[0m 2024-04-05 21:02:21.562251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=531603)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=531603)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=531596)[0m 2024-04-05 21:02:18.733605: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=531596)[0m 2024-04-05 21:02:18.805750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=531596)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=531595)[0m 2024-04-05 21:02:21.563598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 21:02:37,792 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:02:38,355 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:02:39,372 | server.py:125 | fit progress: (1, 2.3024303913116455, {'accuracy': 0.1403, 'data_size': 10000}, 24.182520696995198)
INFO flwr 2024-04-05 21:02:39,372 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:02:39,372 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:02:47,882 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:02:50,552 | server.py:125 | fit progress: (2, 2.302260637283325, {'accuracy': 0.1161, 'data_size': 10000}, 35.362689380999655)
INFO flwr 2024-04-05 21:02:50,552 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:02:50,552 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:02:57,726 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:03:01,143 | server.py:125 | fit progress: (3, 2.3020310401916504, {'accuracy': 0.1135, 'data_size': 10000}, 45.953692312992644)
INFO flwr 2024-04-05 21:03:01,143 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:03:01,143 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:03:08,355 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:03:12,469 | server.py:125 | fit progress: (4, 2.3019180297851562, {'accuracy': 0.1229, 'data_size': 10000}, 57.27985435500159)
INFO flwr 2024-04-05 21:03:12,469 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:03:12,469 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:03:20,293 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:03:25,485 | server.py:125 | fit progress: (5, 2.3017494678497314, {'accuracy': 0.1181, 'data_size': 10000}, 70.29610634199344)
INFO flwr 2024-04-05 21:03:25,485 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:03:25,486 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:03:33,354 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:03:39,131 | server.py:125 | fit progress: (6, 2.3016021251678467, {'accuracy': 0.13, 'data_size': 10000}, 83.94190748900292)
INFO flwr 2024-04-05 21:03:39,131 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:03:39,131 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:03:46,751 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:03:53,593 | server.py:125 | fit progress: (7, 2.3014211654663086, {'accuracy': 0.1139, 'data_size': 10000}, 98.40404392300115)
INFO flwr 2024-04-05 21:03:53,593 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:03:53,594 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:04:01,647 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:04:09,331 | server.py:125 | fit progress: (8, 2.301224946975708, {'accuracy': 0.125, 'data_size': 10000}, 114.14236019799137)
INFO flwr 2024-04-05 21:04:09,332 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:04:09,332 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:04:17,091 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:04:25,410 | server.py:125 | fit progress: (9, 2.301029920578003, {'accuracy': 0.1688, 'data_size': 10000}, 130.22136714299268)
INFO flwr 2024-04-05 21:04:25,411 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:04:25,411 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:04:33,736 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:04:43,127 | server.py:125 | fit progress: (10, 2.300814151763916, {'accuracy': 0.2698, 'data_size': 10000}, 147.93798082700232)
INFO flwr 2024-04-05 21:04:43,127 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:04:43,127 | server.py:153 | FL finished in 147.93836469299276
INFO flwr 2024-04-05 21:04:43,128 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:04:43,128 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:04:43,128 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:04:43,128 | app.py:229 | app_fit: losses_centralized [(0, 2.302610397338867), (1, 2.3024303913116455), (2, 2.302260637283325), (3, 2.3020310401916504), (4, 2.3019180297851562), (5, 2.3017494678497314), (6, 2.3016021251678467), (7, 2.3014211654663086), (8, 2.301224946975708), (9, 2.301029920578003), (10, 2.300814151763916)]
INFO flwr 2024-04-05 21:04:43,128 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0977), (1, 0.1403), (2, 0.1161), (3, 0.1135), (4, 0.1229), (5, 0.1181), (6, 0.13), (7, 0.1139), (8, 0.125), (9, 0.1688), (10, 0.2698)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2698
wandb:     loss 2.30081
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_210137-jslxjzph
wandb: Find logs at: ./wandb/offline-run-20240405_210137-jslxjzph/logs
INFO flwr 2024-04-05 21:04:46,744 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:11:54,135 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=531596)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=531596)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 21:11:59,825	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:12:00,089	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:12:00,381	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d94e19cc77e6a5f2.zip' (7.74MiB) to Ray cluster...
2024-04-05 21:12:00,412	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d94e19cc77e6a5f2.zip'.
INFO flwr 2024-04-05 21:12:11,582 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 146738203239.0, 'object_store_memory': 67173515673.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-05 21:12:11,582 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:12:11,582 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:12:11,597 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:12:11,599 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:12:11,599 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:12:11,599 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 21:12:15,380 | server.py:94 | initial parameters (loss, other metrics): 2.302450656890869, {'accuracy': 0.1029, 'data_size': 10000}
INFO flwr 2024-04-05 21:12:15,380 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:12:15,381 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=538990)[0m 2024-04-05 21:12:17.246144: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=538990)[0m 2024-04-05 21:12:17.359137: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=538990)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=538990)[0m 2024-04-05 21:12:19.547116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=538993)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=538993)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=538991)[0m 2024-04-05 21:12:17.697484: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=538991)[0m 2024-04-05 21:12:17.801132: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=538991)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=538994)[0m 2024-04-05 21:12:20.133851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 21:12:31,878 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:12:32,469 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:12:33,516 | server.py:125 | fit progress: (1, 2.3024497032165527, {'accuracy': 0.1028, 'data_size': 10000}, 18.13506057098857)
INFO flwr 2024-04-05 21:12:33,516 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:12:33,516 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:12:42,220 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:12:44,865 | server.py:125 | fit progress: (2, 2.302448034286499, {'accuracy': 0.1029, 'data_size': 10000}, 29.484417657993617)
INFO flwr 2024-04-05 21:12:44,865 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:12:44,865 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:12:52,788 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:12:56,407 | server.py:125 | fit progress: (3, 2.302447557449341, {'accuracy': 0.1028, 'data_size': 10000}, 41.02675441399333)
INFO flwr 2024-04-05 21:12:56,408 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:12:56,408 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:13:05,937 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:13:10,484 | server.py:125 | fit progress: (4, 2.3024466037750244, {'accuracy': 0.1028, 'data_size': 10000}, 55.10394903199631)
INFO flwr 2024-04-05 21:13:10,485 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:13:10,485 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:13:18,428 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:13:25,290 | server.py:125 | fit progress: (5, 2.3024463653564453, {'accuracy': 0.1027, 'data_size': 10000}, 69.90914659899136)
INFO flwr 2024-04-05 21:13:25,290 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:13:25,290 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:13:33,421 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:13:39,635 | server.py:125 | fit progress: (6, 2.302445650100708, {'accuracy': 0.1027, 'data_size': 10000}, 84.25438726399443)
INFO flwr 2024-04-05 21:13:39,635 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:13:39,635 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:13:47,660 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:13:55,599 | server.py:125 | fit progress: (7, 2.3024446964263916, {'accuracy': 0.1027, 'data_size': 10000}, 100.21871951798676)
INFO flwr 2024-04-05 21:13:55,600 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:13:55,600 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:14:03,654 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:14:12,465 | server.py:125 | fit progress: (8, 2.302443504333496, {'accuracy': 0.1027, 'data_size': 10000}, 117.08405284499167)
INFO flwr 2024-04-05 21:14:12,465 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:14:12,465 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:14:20,510 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:14:30,392 | server.py:125 | fit progress: (9, 2.3024425506591797, {'accuracy': 0.1028, 'data_size': 10000}, 135.01114328199765)
INFO flwr 2024-04-05 21:14:30,392 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:14:30,392 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:14:38,118 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:14:47,587 | server.py:125 | fit progress: (10, 2.302440881729126, {'accuracy': 0.1029, 'data_size': 10000}, 152.20642294899153)
INFO flwr 2024-04-05 21:14:47,587 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:14:47,587 | server.py:153 | FL finished in 152.2067957299878
INFO flwr 2024-04-05 21:14:47,587 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:14:47,587 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:14:47,588 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:14:47,588 | app.py:229 | app_fit: losses_centralized [(0, 2.302450656890869), (1, 2.3024497032165527), (2, 2.302448034286499), (3, 2.302447557449341), (4, 2.3024466037750244), (5, 2.3024463653564453), (6, 2.302445650100708), (7, 2.3024446964263916), (8, 2.302443504333496), (9, 2.3024425506591797), (10, 2.302440881729126)]
INFO flwr 2024-04-05 21:14:47,588 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1029), (1, 0.1028), (2, 0.1029), (3, 0.1028), (4, 0.1028), (5, 0.1027), (6, 0.1027), (7, 0.1027), (8, 0.1027), (9, 0.1028), (10, 0.1029)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1029
wandb:     loss 2.30244
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_211153-gtyddtkj
wandb: Find logs at: ./wandb/offline-run-20240405_211153-gtyddtkj/logs
INFO flwr 2024-04-05 21:14:51,172 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:21:57,302 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=538985)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=538985)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 21:22:03,425	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:22:03,785	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:22:04,094	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_46cea3b8e85ac126.zip' (7.76MiB) to Ray cluster...
2024-04-05 21:22:04,112	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_46cea3b8e85ac126.zip'.
INFO flwr 2024-04-05 21:22:14,844 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 155873168384.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 71088500736.0}
INFO flwr 2024-04-05 21:22:14,844 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:22:14,844 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:22:14,858 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:22:14,859 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:22:14,859 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:22:14,859 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 21:22:18,600 | server.py:94 | initial parameters (loss, other metrics): 2.3023674488067627, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-05 21:22:18,600 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:22:18,600 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=543405)[0m 2024-04-05 21:22:19.971327: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=543403)[0m 2024-04-05 21:22:20.258039: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=543403)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=543405)[0m 2024-04-05 21:22:22.663346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=543405)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=543405)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=543406)[0m 2024-04-05 21:22:20.986633: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=543401)[0m 2024-04-05 21:22:21.062425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=543401)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=543407)[0m 2024-04-05 21:22:23.148913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 21:22:35,111 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:22:35,622 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:22:36,620 | server.py:125 | fit progress: (1, 2.3023130893707275, {'accuracy': 0.101, 'data_size': 10000}, 18.019939289006288)
INFO flwr 2024-04-05 21:22:36,621 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:22:36,621 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:22:44,748 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:22:47,323 | server.py:125 | fit progress: (2, 2.3022773265838623, {'accuracy': 0.101, 'data_size': 10000}, 28.722739075004938)
INFO flwr 2024-04-05 21:22:47,323 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:22:47,323 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:22:55,267 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:22:58,717 | server.py:125 | fit progress: (3, 2.302217721939087, {'accuracy': 0.101, 'data_size': 10000}, 40.11685354099609)
INFO flwr 2024-04-05 21:22:58,717 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:22:58,718 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:23:06,287 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:23:10,300 | server.py:125 | fit progress: (4, 2.302175760269165, {'accuracy': 0.101, 'data_size': 10000}, 51.6992307939945)
INFO flwr 2024-04-05 21:23:10,300 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:23:10,300 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:23:18,163 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:23:23,258 | server.py:125 | fit progress: (5, 2.3021159172058105, {'accuracy': 0.101, 'data_size': 10000}, 64.65715908000129)
INFO flwr 2024-04-05 21:23:23,258 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:23:23,258 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:23:31,083 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:23:36,805 | server.py:125 | fit progress: (6, 2.3020684719085693, {'accuracy': 0.101, 'data_size': 10000}, 78.20444332899933)
INFO flwr 2024-04-05 21:23:36,805 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:23:36,805 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:23:44,665 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:23:51,471 | server.py:125 | fit progress: (7, 2.302008867263794, {'accuracy': 0.101, 'data_size': 10000}, 92.87090073400759)
INFO flwr 2024-04-05 21:23:51,471 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:23:51,472 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:23:59,105 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:24:06,542 | server.py:125 | fit progress: (8, 2.30195689201355, {'accuracy': 0.101, 'data_size': 10000}, 107.94139936599822)
INFO flwr 2024-04-05 21:24:06,542 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:24:06,542 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:24:14,290 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:24:23,164 | server.py:125 | fit progress: (9, 2.3018977642059326, {'accuracy': 0.101, 'data_size': 10000}, 124.5634680820076)
INFO flwr 2024-04-05 21:24:23,164 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:24:23,164 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:24:31,174 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:24:40,922 | server.py:125 | fit progress: (10, 2.3018264770507812, {'accuracy': 0.101, 'data_size': 10000}, 142.3211820150027)
INFO flwr 2024-04-05 21:24:40,922 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:24:40,922 | server.py:153 | FL finished in 142.32157362300495
INFO flwr 2024-04-05 21:24:40,922 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:24:40,922 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:24:40,922 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:24:40,922 | app.py:229 | app_fit: losses_centralized [(0, 2.3023674488067627), (1, 2.3023130893707275), (2, 2.3022773265838623), (3, 2.302217721939087), (4, 2.302175760269165), (5, 2.3021159172058105), (6, 2.3020684719085693), (7, 2.302008867263794), (8, 2.30195689201355), (9, 2.3018977642059326), (10, 2.3018264770507812)]
INFO flwr 2024-04-05 21:24:40,922 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.101), (2, 0.101), (3, 0.101), (4, 0.101), (5, 0.101), (6, 0.101), (7, 0.101), (8, 0.101), (9, 0.101), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.30183
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_212156-pnygaxr3
wandb: Find logs at: ./wandb/offline-run-20240405_212156-pnygaxr3/logs
INFO flwr 2024-04-05 21:24:44,424 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:31:52,358 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=543400)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=543400)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 21:31:58,152	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:31:58,544	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:31:58,934	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_597f014b64afcfa5.zip' (7.78MiB) to Ray cluster...
2024-04-05 21:31:58,960	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_597f014b64afcfa5.zip'.
INFO flwr 2024-04-05 21:32:09,782 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 142787660391.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65480425881.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 21:32:09,782 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:32:09,783 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:32:09,797 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:32:09,798 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:32:09,799 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:32:09,799 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 21:32:12,310 | server.py:94 | initial parameters (loss, other metrics): 2.3025574684143066, {'accuracy': 0.1282, 'data_size': 10000}
INFO flwr 2024-04-05 21:32:12,310 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:32:12,311 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=551140)[0m 2024-04-05 21:32:16.902528: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=551140)[0m 2024-04-05 21:32:16.961113: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=551140)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=551140)[0m 2024-04-05 21:32:18.899155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=551128)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=551128)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=551136)[0m 2024-04-05 21:32:17.758982: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=551136)[0m 2024-04-05 21:32:17.863419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=551136)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=551136)[0m 2024-04-05 21:32:19.870154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 21:32:31,190 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:32:31,697 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:32:32,995 | server.py:125 | fit progress: (1, 2.302426815032959, {'accuracy': 0.1289, 'data_size': 10000}, 20.68496026500361)
INFO flwr 2024-04-05 21:32:32,995 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:32:32,996 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:32:41,603 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:32:44,071 | server.py:125 | fit progress: (2, 2.302297353744507, {'accuracy': 0.1512, 'data_size': 10000}, 31.76107708500058)
INFO flwr 2024-04-05 21:32:44,072 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:32:44,072 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:32:52,077 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:32:55,775 | server.py:125 | fit progress: (3, 2.3021631240844727, {'accuracy': 0.1631, 'data_size': 10000}, 43.46523490600521)
INFO flwr 2024-04-05 21:32:55,776 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:32:55,776 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:33:03,658 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:33:08,209 | server.py:125 | fit progress: (4, 2.302035331726074, {'accuracy': 0.1707, 'data_size': 10000}, 55.89835181100352)
INFO flwr 2024-04-05 21:33:08,209 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:33:08,209 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:33:16,506 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:33:22,200 | server.py:125 | fit progress: (5, 2.3019089698791504, {'accuracy': 0.1123, 'data_size': 10000}, 69.88962524100498)
INFO flwr 2024-04-05 21:33:22,200 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:33:22,200 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:33:30,673 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:33:37,575 | server.py:125 | fit progress: (6, 2.3017454147338867, {'accuracy': 0.1421, 'data_size': 10000}, 85.26463635300752)
INFO flwr 2024-04-05 21:33:37,575 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:33:37,575 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:33:46,426 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:33:54,181 | server.py:125 | fit progress: (7, 2.3015899658203125, {'accuracy': 0.1394, 'data_size': 10000}, 101.87066852299904)
INFO flwr 2024-04-05 21:33:54,181 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:33:54,181 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:34:01,841 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:34:10,690 | server.py:125 | fit progress: (8, 2.301398515701294, {'accuracy': 0.1398, 'data_size': 10000}, 118.37931757800106)
INFO flwr 2024-04-05 21:34:10,690 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:34:10,690 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:34:18,747 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:34:28,266 | server.py:125 | fit progress: (9, 2.301279306411743, {'accuracy': 0.1135, 'data_size': 10000}, 135.95565760700265)
INFO flwr 2024-04-05 21:34:28,266 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:34:28,267 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:34:36,503 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:34:47,183 | server.py:125 | fit progress: (10, 2.3011209964752197, {'accuracy': 0.1815, 'data_size': 10000}, 154.87307681499806)
INFO flwr 2024-04-05 21:34:47,184 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:34:47,184 | server.py:153 | FL finished in 154.8734542470047
INFO flwr 2024-04-05 21:34:47,184 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:34:47,184 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:34:47,184 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:34:47,184 | app.py:229 | app_fit: losses_centralized [(0, 2.3025574684143066), (1, 2.302426815032959), (2, 2.302297353744507), (3, 2.3021631240844727), (4, 2.302035331726074), (5, 2.3019089698791504), (6, 2.3017454147338867), (7, 2.3015899658203125), (8, 2.301398515701294), (9, 2.301279306411743), (10, 2.3011209964752197)]
INFO flwr 2024-04-05 21:34:47,184 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1282), (1, 0.1289), (2, 0.1512), (3, 0.1631), (4, 0.1707), (5, 0.1123), (6, 0.1421), (7, 0.1394), (8, 0.1398), (9, 0.1135), (10, 0.1815)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1815
wandb:     loss 2.30112
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_213151-xn9nbny4
wandb: Find logs at: ./wandb/offline-run-20240405_213151-xn9nbny4/logs
INFO flwr 2024-04-05 21:34:50,745 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:41:59,056 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=551136)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=551136)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 21:42:04,132	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:42:04,407	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:42:04,688	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6e82b94f70c44ea4.zip' (7.80MiB) to Ray cluster...
2024-04-05 21:42:04,707	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6e82b94f70c44ea4.zip'.
INFO flwr 2024-04-05 21:42:15,752 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 69573719654.0, 'node:10.20.240.18': 1.0, 'memory': 152338679194.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-05 21:42:15,752 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:42:15,752 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:42:15,766 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:42:15,767 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:42:15,767 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:42:15,768 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 21:42:18,187 | server.py:94 | initial parameters (loss, other metrics): 2.3025667667388916, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-05 21:42:18,188 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:42:18,188 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=558528)[0m 2024-04-05 21:42:21.518594: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=558528)[0m 2024-04-05 21:42:21.622547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=558528)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=558528)[0m 2024-04-05 21:42:23.513952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=558528)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=558528)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=558526)[0m 2024-04-05 21:42:22.232794: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=558526)[0m 2024-04-05 21:42:22.328181: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=558526)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=558526)[0m 2024-04-05 21:42:24.470121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 21:42:36,353 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:42:36,912 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:42:38,165 | server.py:125 | fit progress: (1, 2.3024024963378906, {'accuracy': 0.1028, 'data_size': 10000}, 19.976931697994587)
INFO flwr 2024-04-05 21:42:38,165 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:42:38,165 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:42:46,720 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:42:49,504 | server.py:125 | fit progress: (2, 2.3022539615631104, {'accuracy': 0.1028, 'data_size': 10000}, 31.315724254003726)
INFO flwr 2024-04-05 21:42:49,504 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:42:49,504 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:42:57,182 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:43:00,728 | server.py:125 | fit progress: (3, 2.3021128177642822, {'accuracy': 0.1042, 'data_size': 10000}, 42.539842451995355)
INFO flwr 2024-04-05 21:43:00,728 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:43:00,728 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:43:08,534 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:43:13,266 | server.py:125 | fit progress: (4, 2.3019702434539795, {'accuracy': 0.1185, 'data_size': 10000}, 55.0782631040056)
INFO flwr 2024-04-05 21:43:13,267 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:43:13,267 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:43:21,138 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:43:26,727 | server.py:125 | fit progress: (5, 2.3018240928649902, {'accuracy': 0.1448, 'data_size': 10000}, 68.53891290500178)
INFO flwr 2024-04-05 21:43:26,727 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:43:26,727 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:43:34,907 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:43:41,831 | server.py:125 | fit progress: (6, 2.301692247390747, {'accuracy': 0.1644, 'data_size': 10000}, 83.64333844599605)
INFO flwr 2024-04-05 21:43:41,832 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:43:41,832 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:43:49,596 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:43:57,138 | server.py:125 | fit progress: (7, 2.3015596866607666, {'accuracy': 0.167, 'data_size': 10000}, 98.94975131200044)
INFO flwr 2024-04-05 21:43:57,138 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:43:57,138 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:44:04,773 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:44:15,091 | server.py:125 | fit progress: (8, 2.3013477325439453, {'accuracy': 0.1508, 'data_size': 10000}, 116.9025833820051)
INFO flwr 2024-04-05 21:44:15,091 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:44:15,091 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:44:23,154 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:44:33,021 | server.py:125 | fit progress: (9, 2.301177501678467, {'accuracy': 0.1535, 'data_size': 10000}, 134.83250762200623)
INFO flwr 2024-04-05 21:44:33,021 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:44:33,021 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:44:41,216 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:44:51,833 | server.py:125 | fit progress: (10, 2.3009064197540283, {'accuracy': 0.1037, 'data_size': 10000}, 153.64495130399882)
INFO flwr 2024-04-05 21:44:51,833 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:44:51,833 | server.py:153 | FL finished in 153.64530842300155
INFO flwr 2024-04-05 21:44:51,833 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:44:51,834 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:44:51,834 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:44:51,834 | app.py:229 | app_fit: losses_centralized [(0, 2.3025667667388916), (1, 2.3024024963378906), (2, 2.3022539615631104), (3, 2.3021128177642822), (4, 2.3019702434539795), (5, 2.3018240928649902), (6, 2.301692247390747), (7, 2.3015596866607666), (8, 2.3013477325439453), (9, 2.301177501678467), (10, 2.3009064197540283)]
INFO flwr 2024-04-05 21:44:51,834 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.1028), (2, 0.1028), (3, 0.1042), (4, 0.1185), (5, 0.1448), (6, 0.1644), (7, 0.167), (8, 0.1508), (9, 0.1535), (10, 0.1037)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1037
wandb:     loss 2.30091
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_214158-qw9gdpvu
wandb: Find logs at: ./wandb/offline-run-20240405_214158-qw9gdpvu/logs
INFO flwr 2024-04-05 21:44:55,348 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:52:01,752 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=558524)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=558524)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 21:52:07,404	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:52:07,678	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:52:07,947	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_25d79b01f8efbf88.zip' (7.82MiB) to Ray cluster...
2024-04-05 21:52:07,965	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_25d79b01f8efbf88.zip'.
INFO flwr 2024-04-05 21:52:18,904 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 155697492173.0, 'node:__internal_head__': 1.0, 'object_store_memory': 71013210931.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-05 21:52:18,904 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:52:18,905 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:52:18,925 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:52:18,926 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:52:18,927 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:52:18,927 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 21:52:21,347 | server.py:94 | initial parameters (loss, other metrics): 2.3027219772338867, {'accuracy': 0.0744, 'data_size': 10000}
INFO flwr 2024-04-05 21:52:21,347 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:52:21,348 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=563014)[0m 2024-04-05 21:52:24.626346: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=563014)[0m 2024-04-05 21:52:24.716759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=563014)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=563011)[0m 2024-04-05 21:52:26.410806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=563011)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=563011)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=563007)[0m 2024-04-05 21:52:25.079194: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=563007)[0m 2024-04-05 21:52:25.139817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=563007)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=563007)[0m 2024-04-05 21:52:27.690117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 21:52:38,881 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:52:39,409 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:52:40,626 | server.py:125 | fit progress: (1, 2.3025598526000977, {'accuracy': 0.0947, 'data_size': 10000}, 19.278041225989)
INFO flwr 2024-04-05 21:52:40,626 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:52:40,626 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:52:48,929 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:52:51,516 | server.py:125 | fit progress: (2, 2.302278995513916, {'accuracy': 0.0958, 'data_size': 10000}, 30.16846374499437)
INFO flwr 2024-04-05 21:52:51,516 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:52:51,516 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:52:59,438 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:53:02,907 | server.py:125 | fit progress: (3, 2.30208158493042, {'accuracy': 0.0958, 'data_size': 10000}, 41.559552698992775)
INFO flwr 2024-04-05 21:53:02,907 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:53:02,907 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:53:10,959 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:53:15,041 | server.py:125 | fit progress: (4, 2.301781415939331, {'accuracy': 0.0958, 'data_size': 10000}, 53.69330223900033)
INFO flwr 2024-04-05 21:53:15,041 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:53:15,053 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:53:23,243 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:53:28,603 | server.py:125 | fit progress: (5, 2.301506519317627, {'accuracy': 0.0976, 'data_size': 10000}, 67.25541302798956)
INFO flwr 2024-04-05 21:53:28,603 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:53:28,603 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:53:36,502 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:53:42,731 | server.py:125 | fit progress: (6, 2.301241397857666, {'accuracy': 0.0958, 'data_size': 10000}, 81.38399861699145)
INFO flwr 2024-04-05 21:53:42,732 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:53:42,732 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:53:50,592 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:53:57,868 | server.py:125 | fit progress: (7, 2.3009262084960938, {'accuracy': 0.0958, 'data_size': 10000}, 96.52036871400196)
INFO flwr 2024-04-05 21:53:57,868 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:53:57,868 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:54:05,734 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:54:13,658 | server.py:125 | fit progress: (8, 2.3005738258361816, {'accuracy': 0.0965, 'data_size': 10000}, 112.31092410600104)
INFO flwr 2024-04-05 21:54:13,659 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:54:13,659 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:54:21,565 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:54:30,560 | server.py:125 | fit progress: (9, 2.300013303756714, {'accuracy': 0.0958, 'data_size': 10000}, 129.2124426829978)
INFO flwr 2024-04-05 21:54:30,560 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:54:30,560 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:54:38,491 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:54:47,931 | server.py:125 | fit progress: (10, 2.299513101577759, {'accuracy': 0.0958, 'data_size': 10000}, 146.583992838001)
INFO flwr 2024-04-05 21:54:47,932 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:54:47,932 | server.py:153 | FL finished in 146.58440780299134
INFO flwr 2024-04-05 21:54:47,932 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:54:47,932 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:54:47,932 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:54:47,932 | app.py:229 | app_fit: losses_centralized [(0, 2.3027219772338867), (1, 2.3025598526000977), (2, 2.302278995513916), (3, 2.30208158493042), (4, 2.301781415939331), (5, 2.301506519317627), (6, 2.301241397857666), (7, 2.3009262084960938), (8, 2.3005738258361816), (9, 2.300013303756714), (10, 2.299513101577759)]
INFO flwr 2024-04-05 21:54:47,933 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0744), (1, 0.0947), (2, 0.0958), (3, 0.0958), (4, 0.0958), (5, 0.0976), (6, 0.0958), (7, 0.0958), (8, 0.0965), (9, 0.0958), (10, 0.0958)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0958
wandb:     loss 2.29951
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_215201-36yrhrxy
wandb: Find logs at: ./wandb/offline-run-20240405_215201-36yrhrxy/logs
INFO flwr 2024-04-05 21:54:51,460 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:01:58,630 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=563007)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=563007)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 22:02:03,419	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:02:03,799	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:02:04,101	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dc7a2633ea3dd2f0.zip' (7.84MiB) to Ray cluster...
2024-04-05 22:02:04,119	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dc7a2633ea3dd2f0.zip'.
INFO flwr 2024-04-05 22:02:15,562 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 141579637351.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64962701721.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-05 22:02:15,562 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:02:15,563 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:02:15,582 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:02:15,583 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:02:15,583 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:02:15,583 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 22:02:18,556 | server.py:94 | initial parameters (loss, other metrics): 2.302467107772827, {'accuracy': 0.1042, 'data_size': 10000}
INFO flwr 2024-04-05 22:02:18,556 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:02:18,557 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=570827)[0m 2024-04-05 22:02:21.160782: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=570827)[0m 2024-04-05 22:02:21.273240: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=570827)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=570819)[0m 2024-04-05 22:02:23.474562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=570825)[0m 2024-04-05 22:02:22.217975: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=570819)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=570819)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=570825)[0m 2024-04-05 22:02:22.308063: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=570825)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=570825)[0m 2024-04-05 22:02:26.244378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 22:02:38,363 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:02:38,865 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:02:40,114 | server.py:125 | fit progress: (1, 2.3022818565368652, {'accuracy': 0.1029, 'data_size': 10000}, 21.557689830005984)
INFO flwr 2024-04-05 22:02:40,115 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:02:40,115 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:02:49,021 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:02:51,513 | server.py:125 | fit progress: (2, 2.3020925521850586, {'accuracy': 0.1009, 'data_size': 10000}, 32.95649365001009)
INFO flwr 2024-04-05 22:02:51,513 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:02:51,513 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:02:59,724 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:03:03,460 | server.py:125 | fit progress: (3, 2.301790714263916, {'accuracy': 0.1009, 'data_size': 10000}, 44.903428606005036)
INFO flwr 2024-04-05 22:03:03,460 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:03:03,460 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:03:11,282 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:03:15,864 | server.py:125 | fit progress: (4, 2.301408052444458, {'accuracy': 0.1009, 'data_size': 10000}, 57.307331372998306)
INFO flwr 2024-04-05 22:03:15,864 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:03:15,864 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:03:23,816 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:03:29,632 | server.py:125 | fit progress: (5, 2.3010125160217285, {'accuracy': 0.1009, 'data_size': 10000}, 71.07534331300121)
INFO flwr 2024-04-05 22:03:29,632 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:03:29,632 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:03:37,690 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:03:44,573 | server.py:125 | fit progress: (6, 2.3005261421203613, {'accuracy': 0.1009, 'data_size': 10000}, 86.0160983740061)
INFO flwr 2024-04-05 22:03:44,573 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:03:44,573 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:03:52,630 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:04:01,597 | server.py:125 | fit progress: (7, 2.2997796535491943, {'accuracy': 0.1009, 'data_size': 10000}, 103.04041611900902)
INFO flwr 2024-04-05 22:04:01,597 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:04:01,597 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:04:09,159 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:04:17,695 | server.py:125 | fit progress: (8, 2.2988266944885254, {'accuracy': 0.1009, 'data_size': 10000}, 119.13880720200541)
INFO flwr 2024-04-05 22:04:17,696 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:04:17,696 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:04:25,612 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:04:35,368 | server.py:125 | fit progress: (9, 2.2969040870666504, {'accuracy': 0.1009, 'data_size': 10000}, 136.81169145500462)
INFO flwr 2024-04-05 22:04:35,369 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:04:35,369 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:04:43,375 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:04:53,761 | server.py:125 | fit progress: (10, 2.2946019172668457, {'accuracy': 0.101, 'data_size': 10000}, 155.20475897401047)
INFO flwr 2024-04-05 22:04:53,762 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:04:53,762 | server.py:153 | FL finished in 155.20531025300443
INFO flwr 2024-04-05 22:04:53,762 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:04:53,762 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:04:53,762 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:04:53,762 | app.py:229 | app_fit: losses_centralized [(0, 2.302467107772827), (1, 2.3022818565368652), (2, 2.3020925521850586), (3, 2.301790714263916), (4, 2.301408052444458), (5, 2.3010125160217285), (6, 2.3005261421203613), (7, 2.2997796535491943), (8, 2.2988266944885254), (9, 2.2969040870666504), (10, 2.2946019172668457)]
INFO flwr 2024-04-05 22:04:53,763 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1042), (1, 0.1029), (2, 0.1009), (3, 0.1009), (4, 0.1009), (5, 0.1009), (6, 0.1009), (7, 0.1009), (8, 0.1009), (9, 0.1009), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.2946
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_220158-zo9bn4co
wandb: Find logs at: ./wandb/offline-run-20240405_220158-zo9bn4co/logs
INFO flwr 2024-04-05 22:04:57,405 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:12:04,315 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=570827)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=570827)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 22:12:09,064	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:12:09,355	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:12:09,664	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_35f660e851f53995.zip' (7.86MiB) to Ray cluster...
2024-04-05 22:12:09,689	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_35f660e851f53995.zip'.
INFO flwr 2024-04-05 22:12:20,546 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 155423465268.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 70895770828.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 22:12:20,546 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:12:20,546 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:12:20,560 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:12:20,562 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:12:20,562 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:12:20,562 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 22:12:22,509 | server.py:94 | initial parameters (loss, other metrics): 2.3024063110351562, {'accuracy': 0.1184, 'data_size': 10000}
INFO flwr 2024-04-05 22:12:22,509 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:12:22,510 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=575206)[0m 2024-04-05 22:12:26.942817: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=575206)[0m 2024-04-05 22:12:27.035082: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=575206)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=575206)[0m 2024-04-05 22:12:29.245292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=575203)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=575203)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=575197)[0m 2024-04-05 22:12:27.515276: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=575192)[0m 2024-04-05 22:12:27.691883: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=575192)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=575197)[0m 2024-04-05 22:12:29.639379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 22:12:41,353 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:12:41,884 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:12:43,350 | server.py:125 | fit progress: (1, 2.3020591735839844, {'accuracy': 0.1107, 'data_size': 10000}, 20.840540597986546)
INFO flwr 2024-04-05 22:12:43,350 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:12:43,351 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:12:53,106 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:12:55,606 | server.py:125 | fit progress: (2, 2.3015575408935547, {'accuracy': 0.0974, 'data_size': 10000}, 33.09618122498796)
INFO flwr 2024-04-05 22:12:55,606 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:12:55,606 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:13:04,001 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:13:07,292 | server.py:125 | fit progress: (3, 2.301058292388916, {'accuracy': 0.1658, 'data_size': 10000}, 44.78190857399022)
INFO flwr 2024-04-05 22:13:07,292 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:13:07,292 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:13:14,771 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:13:19,053 | server.py:125 | fit progress: (4, 2.30033016204834, {'accuracy': 0.1009, 'data_size': 10000}, 56.5432579240005)
INFO flwr 2024-04-05 22:13:19,053 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:13:19,053 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:13:27,049 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:13:32,335 | server.py:125 | fit progress: (5, 2.2992122173309326, {'accuracy': 0.1009, 'data_size': 10000}, 69.8253853339993)
INFO flwr 2024-04-05 22:13:32,335 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:13:32,335 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:13:40,586 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:13:46,637 | server.py:125 | fit progress: (6, 2.2957265377044678, {'accuracy': 0.1009, 'data_size': 10000}, 84.12723667499085)
INFO flwr 2024-04-05 22:13:46,637 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:13:46,637 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:13:54,824 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:14:01,927 | server.py:125 | fit progress: (7, 2.2920925617218018, {'accuracy': 0.101, 'data_size': 10000}, 99.4175212400005)
INFO flwr 2024-04-05 22:14:01,927 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:14:01,928 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:14:10,188 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:14:18,513 | server.py:125 | fit progress: (8, 2.2881791591644287, {'accuracy': 0.1739, 'data_size': 10000}, 116.00340115498693)
INFO flwr 2024-04-05 22:14:18,513 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:14:18,514 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:14:26,961 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:14:36,332 | server.py:125 | fit progress: (9, 2.2858333587646484, {'accuracy': 0.1872, 'data_size': 10000}, 133.82252639999206)
INFO flwr 2024-04-05 22:14:36,332 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:14:36,333 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:14:44,418 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:14:54,899 | server.py:125 | fit progress: (10, 2.2702624797821045, {'accuracy': 0.1939, 'data_size': 10000}, 152.38950000499608)
INFO flwr 2024-04-05 22:14:54,899 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:14:54,899 | server.py:153 | FL finished in 152.38989776199742
INFO flwr 2024-04-05 22:14:54,900 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:14:54,900 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:14:54,900 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:14:54,900 | app.py:229 | app_fit: losses_centralized [(0, 2.3024063110351562), (1, 2.3020591735839844), (2, 2.3015575408935547), (3, 2.301058292388916), (4, 2.30033016204834), (5, 2.2992122173309326), (6, 2.2957265377044678), (7, 2.2920925617218018), (8, 2.2881791591644287), (9, 2.2858333587646484), (10, 2.2702624797821045)]
INFO flwr 2024-04-05 22:14:54,900 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1184), (1, 0.1107), (2, 0.0974), (3, 0.1658), (4, 0.1009), (5, 0.1009), (6, 0.1009), (7, 0.101), (8, 0.1739), (9, 0.1872), (10, 0.1939)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1939
wandb:     loss 2.27026
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_221203-ikfobycu
wandb: Find logs at: ./wandb/offline-run-20240405_221203-ikfobycu/logs
INFO flwr 2024-04-05 22:14:58,460 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:22:08,161 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=575191)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=575191)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 22:22:13,518	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:22:13,862	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:22:14,286	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_addb206c5e755229.zip' (7.88MiB) to Ray cluster...
2024-04-05 22:22:14,308	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_addb206c5e755229.zip'.
INFO flwr 2024-04-05 22:22:25,222 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 68457781248.0, 'node:__internal_head__': 1.0, 'memory': 149734822912.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-05 22:22:25,223 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:22:25,223 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:22:25,244 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:22:25,246 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:22:25,247 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:22:25,247 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 22:22:28,843 | server.py:94 | initial parameters (loss, other metrics): 2.3024022579193115, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-05 22:22:28,844 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:22:28,844 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=582740)[0m 2024-04-05 22:22:31.169096: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=582740)[0m 2024-04-05 22:22:31.262294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=582740)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=582740)[0m 2024-04-05 22:22:33.233006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=582741)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=582741)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=582741)[0m 2024-04-05 22:22:31.548869: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=582741)[0m 2024-04-05 22:22:31.645625: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=582741)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=582741)[0m 2024-04-05 22:22:33.709517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 22:22:45,864 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:22:46,353 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:22:47,387 | server.py:125 | fit progress: (1, 2.3023998737335205, {'accuracy': 0.101, 'data_size': 10000}, 18.542681104998337)
INFO flwr 2024-04-05 22:22:47,387 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:22:47,387 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:22:56,812 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:22:59,549 | server.py:125 | fit progress: (2, 2.3023979663848877, {'accuracy': 0.101, 'data_size': 10000}, 30.705188089996227)
INFO flwr 2024-04-05 22:22:59,550 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:22:59,550 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:23:08,739 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:23:11,959 | server.py:125 | fit progress: (3, 2.3023953437805176, {'accuracy': 0.101, 'data_size': 10000}, 43.11436992199742)
INFO flwr 2024-04-05 22:23:11,959 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:23:11,959 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:23:20,662 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:23:25,544 | server.py:125 | fit progress: (4, 2.3023929595947266, {'accuracy': 0.101, 'data_size': 10000}, 56.70012125300127)
INFO flwr 2024-04-05 22:23:25,545 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:23:25,545 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:23:33,536 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:23:38,695 | server.py:125 | fit progress: (5, 2.3023903369903564, {'accuracy': 0.101, 'data_size': 10000}, 69.85067018099653)
INFO flwr 2024-04-05 22:23:38,695 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:23:38,695 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:23:46,945 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:23:53,191 | server.py:125 | fit progress: (6, 2.3023881912231445, {'accuracy': 0.101, 'data_size': 10000}, 84.34638290500152)
INFO flwr 2024-04-05 22:23:53,191 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:23:53,191 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:24:01,519 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:24:09,399 | server.py:125 | fit progress: (7, 2.302385091781616, {'accuracy': 0.101, 'data_size': 10000}, 100.55439095599286)
INFO flwr 2024-04-05 22:24:09,399 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:24:09,399 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:24:17,383 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:24:24,934 | server.py:125 | fit progress: (8, 2.3023836612701416, {'accuracy': 0.101, 'data_size': 10000}, 116.09012817298935)
INFO flwr 2024-04-05 22:24:24,935 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:24:24,935 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:24:33,001 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:24:42,116 | server.py:125 | fit progress: (9, 2.3023815155029297, {'accuracy': 0.101, 'data_size': 10000}, 133.27174882299732)
INFO flwr 2024-04-05 22:24:42,116 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:24:42,116 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:24:49,685 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:24:59,649 | server.py:125 | fit progress: (10, 2.302379846572876, {'accuracy': 0.101, 'data_size': 10000}, 150.8046073049918)
INFO flwr 2024-04-05 22:24:59,649 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:24:59,649 | server.py:153 | FL finished in 150.80502574700222
INFO flwr 2024-04-05 22:24:59,649 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:24:59,650 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:24:59,650 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:24:59,650 | app.py:229 | app_fit: losses_centralized [(0, 2.3024022579193115), (1, 2.3023998737335205), (2, 2.3023979663848877), (3, 2.3023953437805176), (4, 2.3023929595947266), (5, 2.3023903369903564), (6, 2.3023881912231445), (7, 2.302385091781616), (8, 2.3023836612701416), (9, 2.3023815155029297), (10, 2.302379846572876)]
INFO flwr 2024-04-05 22:24:59,650 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.101), (2, 0.101), (3, 0.101), (4, 0.101), (5, 0.101), (6, 0.101), (7, 0.101), (8, 0.101), (9, 0.101), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.30238
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_222204-xtd4dxa6
wandb: Find logs at: ./wandb/offline-run-20240405_222204-xtd4dxa6/logs
INFO flwr 2024-04-05 22:25:03,188 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:32:09,966 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=582732)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=582732)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 22:32:14,924	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:32:15,265	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:32:16,983	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c08f83b3d5ab9d7e.zip' (7.90MiB) to Ray cluster...
2024-04-05 22:32:17,005	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c08f83b3d5ab9d7e.zip'.
INFO flwr 2024-04-05 22:32:28,188 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66808294195.0, 'memory': 145886019789.0}
INFO flwr 2024-04-05 22:32:28,188 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:32:28,189 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:32:28,206 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:32:28,206 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:32:28,207 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:32:28,207 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 22:32:30,470 | server.py:94 | initial parameters (loss, other metrics): 2.3025107383728027, {'accuracy': 0.0756, 'data_size': 10000}
INFO flwr 2024-04-05 22:32:30,470 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:32:30,470 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=590380)[0m 2024-04-05 22:32:34.003061: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=590380)[0m 2024-04-05 22:32:34.100827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=590380)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=590381)[0m 2024-04-05 22:32:36.479005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=590374)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=590374)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=590374)[0m 2024-04-05 22:32:34.848210: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=590374)[0m 2024-04-05 22:32:34.931403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=590374)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=590372)[0m 2024-04-05 22:32:36.933107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 22:32:49,135 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:32:49,720 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:32:50,751 | server.py:125 | fit progress: (1, 2.302417039871216, {'accuracy': 0.1127, 'data_size': 10000}, 20.28093648099457)
INFO flwr 2024-04-05 22:32:50,751 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:32:50,752 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:32:59,808 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:33:02,575 | server.py:125 | fit progress: (2, 2.3023064136505127, {'accuracy': 0.133, 'data_size': 10000}, 32.10442355799023)
INFO flwr 2024-04-05 22:33:02,575 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:33:02,575 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:33:10,075 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:33:13,700 | server.py:125 | fit progress: (3, 2.3022007942199707, {'accuracy': 0.1337, 'data_size': 10000}, 43.23006755299866)
INFO flwr 2024-04-05 22:33:13,700 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:33:13,701 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:33:21,445 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:33:27,390 | server.py:125 | fit progress: (4, 2.302067279815674, {'accuracy': 0.1064, 'data_size': 10000}, 56.9202841869992)
INFO flwr 2024-04-05 22:33:27,391 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:33:27,391 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:33:35,106 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:33:40,788 | server.py:125 | fit progress: (5, 2.301910400390625, {'accuracy': 0.1529, 'data_size': 10000}, 70.31800219298748)
INFO flwr 2024-04-05 22:33:40,788 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:33:40,789 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:33:48,537 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:33:55,001 | server.py:125 | fit progress: (6, 2.3017756938934326, {'accuracy': 0.1455, 'data_size': 10000}, 84.53094953099207)
INFO flwr 2024-04-05 22:33:55,001 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:33:55,002 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:34:02,925 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:34:10,195 | server.py:125 | fit progress: (7, 2.301666259765625, {'accuracy': 0.2692, 'data_size': 10000}, 99.72454556099547)
INFO flwr 2024-04-05 22:34:10,195 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:34:10,195 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:34:17,885 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:34:26,808 | server.py:125 | fit progress: (8, 2.3014771938323975, {'accuracy': 0.2491, 'data_size': 10000}, 116.33770672298851)
INFO flwr 2024-04-05 22:34:26,808 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:34:26,808 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:34:34,647 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:34:43,777 | server.py:125 | fit progress: (9, 2.301328182220459, {'accuracy': 0.2861, 'data_size': 10000}, 133.3068679729913)
INFO flwr 2024-04-05 22:34:43,777 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:34:43,778 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:34:51,720 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:35:01,881 | server.py:125 | fit progress: (10, 2.3011724948883057, {'accuracy': 0.1368, 'data_size': 10000}, 151.41034044999105)
INFO flwr 2024-04-05 22:35:01,881 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:35:01,881 | server.py:153 | FL finished in 151.41071612099768
INFO flwr 2024-04-05 22:35:01,881 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:35:01,881 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:35:01,881 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:35:01,881 | app.py:229 | app_fit: losses_centralized [(0, 2.3025107383728027), (1, 2.302417039871216), (2, 2.3023064136505127), (3, 2.3022007942199707), (4, 2.302067279815674), (5, 2.301910400390625), (6, 2.3017756938934326), (7, 2.301666259765625), (8, 2.3014771938323975), (9, 2.301328182220459), (10, 2.3011724948883057)]
INFO flwr 2024-04-05 22:35:01,881 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0756), (1, 0.1127), (2, 0.133), (3, 0.1337), (4, 0.1064), (5, 0.1529), (6, 0.1455), (7, 0.2692), (8, 0.2491), (9, 0.2861), (10, 0.1368)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1368
wandb:     loss 2.30117
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_223209-wrf92acv
wandb: Find logs at: ./wandb/offline-run-20240405_223209-wrf92acv/logs
INFO flwr 2024-04-05 22:35:05,425 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:42:11,546 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=590376)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=590376)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 22:42:17,258	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:42:17,565	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:42:17,853	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b2421776590cbe32.zip' (7.92MiB) to Ray cluster...
2024-04-05 22:42:17,872	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b2421776590cbe32.zip'.
INFO flwr 2024-04-05 22:42:28,795 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 154837469799.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 70644629913.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 22:42:28,795 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:42:28,795 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:42:28,809 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:42:28,811 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:42:28,811 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:42:28,815 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 22:42:31,290 | server.py:94 | initial parameters (loss, other metrics): 2.3026223182678223, {'accuracy': 0.1402, 'data_size': 10000}
INFO flwr 2024-04-05 22:42:31,291 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:42:31,291 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=594834)[0m 2024-04-05 22:42:35.135995: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=594834)[0m 2024-04-05 22:42:35.230473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=594834)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=594834)[0m 2024-04-05 22:42:38.064714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=594834)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=594834)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=594831)[0m 2024-04-05 22:42:35.351171: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=594831)[0m 2024-04-05 22:42:35.435370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=594831)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=594828)[0m 2024-04-05 22:42:38.249299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 22:42:50,157 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:42:50,662 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:42:51,666 | server.py:125 | fit progress: (1, 2.302438259124756, {'accuracy': 0.1309, 'data_size': 10000}, 20.375065829997766)
INFO flwr 2024-04-05 22:42:51,666 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:42:51,666 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:42:59,992 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:43:02,553 | server.py:125 | fit progress: (2, 2.302168369293213, {'accuracy': 0.1449, 'data_size': 10000}, 31.26162016300077)
INFO flwr 2024-04-05 22:43:02,553 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:43:02,553 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:43:10,110 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:43:13,342 | server.py:125 | fit progress: (3, 2.30191707611084, {'accuracy': 0.1858, 'data_size': 10000}, 42.05092330300249)
INFO flwr 2024-04-05 22:43:13,342 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:43:13,342 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:43:21,411 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:43:25,747 | server.py:125 | fit progress: (4, 2.301668882369995, {'accuracy': 0.2203, 'data_size': 10000}, 54.45554295000329)
INFO flwr 2024-04-05 22:43:25,747 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:43:25,747 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:43:33,567 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:43:38,770 | server.py:125 | fit progress: (5, 2.3013694286346436, {'accuracy': 0.2175, 'data_size': 10000}, 67.47868160999496)
INFO flwr 2024-04-05 22:43:38,770 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:43:38,770 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:43:46,997 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:43:52,818 | server.py:125 | fit progress: (6, 2.30108642578125, {'accuracy': 0.111, 'data_size': 10000}, 81.52733470000385)
INFO flwr 2024-04-05 22:43:52,819 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:43:52,819 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:44:00,752 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:44:07,841 | server.py:125 | fit progress: (7, 2.3007493019104004, {'accuracy': 0.1204, 'data_size': 10000}, 96.54997849400388)
INFO flwr 2024-04-05 22:44:07,841 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:44:07,841 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:44:15,740 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:44:23,652 | server.py:125 | fit progress: (8, 2.3003482818603516, {'accuracy': 0.1116, 'data_size': 10000}, 112.3610067240079)
INFO flwr 2024-04-05 22:44:23,652 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:44:23,652 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:44:31,732 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:44:40,675 | server.py:125 | fit progress: (9, 2.299637794494629, {'accuracy': 0.0982, 'data_size': 10000}, 129.38365173399507)
INFO flwr 2024-04-05 22:44:40,675 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:44:40,675 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:44:48,733 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:44:58,598 | server.py:125 | fit progress: (10, 2.2989959716796875, {'accuracy': 0.0982, 'data_size': 10000}, 147.30708394300018)
INFO flwr 2024-04-05 22:44:58,598 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:44:58,598 | server.py:153 | FL finished in 147.30744635200244
INFO flwr 2024-04-05 22:44:58,599 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:44:58,599 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:44:58,599 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:44:58,599 | app.py:229 | app_fit: losses_centralized [(0, 2.3026223182678223), (1, 2.302438259124756), (2, 2.302168369293213), (3, 2.30191707611084), (4, 2.301668882369995), (5, 2.3013694286346436), (6, 2.30108642578125), (7, 2.3007493019104004), (8, 2.3003482818603516), (9, 2.299637794494629), (10, 2.2989959716796875)]
INFO flwr 2024-04-05 22:44:58,599 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1402), (1, 0.1309), (2, 0.1449), (3, 0.1858), (4, 0.2203), (5, 0.2175), (6, 0.111), (7, 0.1204), (8, 0.1116), (9, 0.0982), (10, 0.0982)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0982
wandb:     loss 2.299
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_224210-qulb1770
wandb: Find logs at: ./wandb/offline-run-20240405_224210-qulb1770/logs
INFO flwr 2024-04-05 22:45:02,140 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:52:10,941 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=594828)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=594828)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 22:52:16,597	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:52:16,888	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:52:17,180	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7768fc2817d61135.zip' (7.94MiB) to Ray cluster...
2024-04-05 22:52:17,198	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7768fc2817d61135.zip'.
INFO flwr 2024-04-05 22:52:28,111 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 138411848704.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63605078016.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-05 22:52:28,112 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:52:28,112 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:52:28,126 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:52:28,128 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:52:28,128 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:52:28,128 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 22:52:30,730 | server.py:94 | initial parameters (loss, other metrics): 2.3026084899902344, {'accuracy': 0.1026, 'data_size': 10000}
INFO flwr 2024-04-05 22:52:30,730 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:52:30,731 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=602331)[0m 2024-04-05 22:52:35.194474: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=602331)[0m 2024-04-05 22:52:35.298868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=602331)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=602331)[0m 2024-04-05 22:52:37.743641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=602331)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=602331)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=602333)[0m 2024-04-05 22:52:35.274271: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=602333)[0m 2024-04-05 22:52:35.357882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=602333)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=602339)[0m 2024-04-05 22:52:37.765469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 22:53:05,767 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:53:06,300 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:53:07,544 | server.py:125 | fit progress: (1, 2.3023245334625244, {'accuracy': 0.101, 'data_size': 10000}, 36.81365325998922)
INFO flwr 2024-04-05 22:53:07,544 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:53:07,544 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:53:16,391 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:53:19,252 | server.py:125 | fit progress: (2, 2.302076578140259, {'accuracy': 0.1508, 'data_size': 10000}, 48.521465685989824)
INFO flwr 2024-04-05 22:53:19,252 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:53:19,252 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:53:27,565 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:53:31,559 | server.py:125 | fit progress: (3, 2.301800012588501, {'accuracy': 0.1012, 'data_size': 10000}, 60.82867055600218)
INFO flwr 2024-04-05 22:53:31,559 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:53:31,560 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:53:39,945 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:53:44,478 | server.py:125 | fit progress: (4, 2.3012499809265137, {'accuracy': 0.1011, 'data_size': 10000}, 73.74724630000128)
INFO flwr 2024-04-05 22:53:44,478 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:53:44,478 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:53:52,623 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:53:57,847 | server.py:125 | fit progress: (5, 2.3007607460021973, {'accuracy': 0.1031, 'data_size': 10000}, 87.11659237000276)
INFO flwr 2024-04-05 22:53:57,847 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:53:57,847 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:54:05,934 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:54:11,957 | server.py:125 | fit progress: (6, 2.2998106479644775, {'accuracy': 0.1041, 'data_size': 10000}, 101.22709208600281)
INFO flwr 2024-04-05 22:54:11,958 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:54:11,958 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:54:19,782 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:54:28,066 | server.py:125 | fit progress: (7, 2.298224687576294, {'accuracy': 0.0965, 'data_size': 10000}, 117.33605495899974)
INFO flwr 2024-04-05 22:54:28,067 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:54:28,067 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:54:36,234 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:54:43,837 | server.py:125 | fit progress: (8, 2.295269250869751, {'accuracy': 0.0958, 'data_size': 10000}, 133.1071166109905)
INFO flwr 2024-04-05 22:54:43,838 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:54:43,838 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:54:51,659 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:55:01,462 | server.py:125 | fit progress: (9, 2.2926394939422607, {'accuracy': 0.1223, 'data_size': 10000}, 150.73136491898913)
INFO flwr 2024-04-05 22:55:01,462 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:55:01,462 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:55:09,404 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:55:18,736 | server.py:125 | fit progress: (10, 2.2845449447631836, {'accuracy': 0.167, 'data_size': 10000}, 168.0058002549922)
INFO flwr 2024-04-05 22:55:18,736 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:55:18,737 | server.py:153 | FL finished in 168.00629457799369
INFO flwr 2024-04-05 22:55:18,749 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:55:18,750 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:55:18,750 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:55:18,750 | app.py:229 | app_fit: losses_centralized [(0, 2.3026084899902344), (1, 2.3023245334625244), (2, 2.302076578140259), (3, 2.301800012588501), (4, 2.3012499809265137), (5, 2.3007607460021973), (6, 2.2998106479644775), (7, 2.298224687576294), (8, 2.295269250869751), (9, 2.2926394939422607), (10, 2.2845449447631836)]
INFO flwr 2024-04-05 22:55:18,750 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1026), (1, 0.101), (2, 0.1508), (3, 0.1012), (4, 0.1011), (5, 0.1031), (6, 0.1041), (7, 0.0965), (8, 0.0958), (9, 0.1223), (10, 0.167)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.167
wandb:     loss 2.28454
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_225210-6f3xgz8t
wandb: Find logs at: ./wandb/offline-run-20240405_225210-6f3xgz8t/logs
INFO flwr 2024-04-05 22:55:22,352 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:02:30,693 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=602339)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=602339)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 23:02:35,197	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:02:35,480	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:02:35,788	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5823ff4c4542261f.zip' (7.96MiB) to Ray cluster...
2024-04-05 23:02:35,807	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5823ff4c4542261f.zip'.
INFO flwr 2024-04-05 23:02:46,848 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 153112611021.0, 'node:__internal_head__': 1.0, 'object_store_memory': 69905404723.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-05 23:02:46,848 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:02:46,849 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:02:46,865 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:02:46,867 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:02:46,868 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:02:46,868 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 23:02:49,830 | server.py:94 | initial parameters (loss, other metrics): 2.3024754524230957, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-05 23:02:49,830 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:02:49,831 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=610080)[0m 2024-04-05 23:02:52.258500: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=610080)[0m 2024-04-05 23:02:52.349470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=610080)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=610081)[0m 2024-04-05 23:02:54.328596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=610081)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=610081)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=610078)[0m 2024-04-05 23:02:53.150202: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=610078)[0m 2024-04-05 23:02:53.239906: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=610078)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=610084)[0m 2024-04-05 23:02:55.338074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 23:03:07,907 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:03:08,472 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:03:09,747 | server.py:125 | fit progress: (1, 2.302074432373047, {'accuracy': 0.098, 'data_size': 10000}, 19.91598777899344)
INFO flwr 2024-04-05 23:03:09,747 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:03:09,747 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:03:19,000 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:03:21,678 | server.py:125 | fit progress: (2, 2.301588773727417, {'accuracy': 0.098, 'data_size': 10000}, 31.847048737996374)
INFO flwr 2024-04-05 23:03:21,678 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:03:21,678 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:03:29,604 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:03:33,129 | server.py:125 | fit progress: (3, 2.300713062286377, {'accuracy': 0.1726, 'data_size': 10000}, 43.298430444003316)
INFO flwr 2024-04-05 23:03:33,130 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:03:33,130 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:03:41,313 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:03:45,992 | server.py:125 | fit progress: (4, 2.2995548248291016, {'accuracy': 0.169, 'data_size': 10000}, 56.161024066997925)
INFO flwr 2024-04-05 23:03:45,992 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:03:45,992 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:03:54,193 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:04:00,233 | server.py:125 | fit progress: (5, 2.2962899208068848, {'accuracy': 0.186, 'data_size': 10000}, 70.40205792299821)
INFO flwr 2024-04-05 23:04:00,233 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:04:00,233 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:04:08,471 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:04:14,753 | server.py:125 | fit progress: (6, 2.2903125286102295, {'accuracy': 0.1039, 'data_size': 10000}, 84.92185215899372)
INFO flwr 2024-04-05 23:04:14,753 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:04:14,753 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:04:23,309 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:04:31,042 | server.py:125 | fit progress: (7, 2.2822954654693604, {'accuracy': 0.1028, 'data_size': 10000}, 101.21136135900451)
INFO flwr 2024-04-05 23:04:31,042 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:04:31,043 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:04:38,729 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:04:47,849 | server.py:125 | fit progress: (8, 2.264542579650879, {'accuracy': 0.1151, 'data_size': 10000}, 118.01817072500126)
INFO flwr 2024-04-05 23:04:47,849 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:04:47,849 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:04:56,398 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:05:06,066 | server.py:125 | fit progress: (9, 2.2280280590057373, {'accuracy': 0.2895, 'data_size': 10000}, 136.23537278200092)
INFO flwr 2024-04-05 23:05:06,067 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:05:06,067 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:05:14,898 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:05:25,722 | server.py:125 | fit progress: (10, 2.187100887298584, {'accuracy': 0.335, 'data_size': 10000}, 155.89076651299547)
INFO flwr 2024-04-05 23:05:25,722 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:05:25,722 | server.py:153 | FL finished in 155.89121351500216
INFO flwr 2024-04-05 23:05:25,722 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:05:25,722 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:05:25,722 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:05:25,723 | app.py:229 | app_fit: losses_centralized [(0, 2.3024754524230957), (1, 2.302074432373047), (2, 2.301588773727417), (3, 2.300713062286377), (4, 2.2995548248291016), (5, 2.2962899208068848), (6, 2.2903125286102295), (7, 2.2822954654693604), (8, 2.264542579650879), (9, 2.2280280590057373), (10, 2.187100887298584)]
INFO flwr 2024-04-05 23:05:25,723 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.098), (2, 0.098), (3, 0.1726), (4, 0.169), (5, 0.186), (6, 0.1039), (7, 0.1028), (8, 0.1151), (9, 0.2895), (10, 0.335)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.335
wandb:     loss 2.1871
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_230230-vanb22ge
wandb: Find logs at: ./wandb/offline-run-20240405_230230-vanb22ge/logs
INFO flwr 2024-04-05 23:05:29,247 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:12:36,222 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=610077)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=610077)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 23:12:40,811	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:12:41,209	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:12:41,579	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d95b0a3e1b4e334b.zip' (7.98MiB) to Ray cluster...
2024-04-05 23:12:41,600	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d95b0a3e1b4e334b.zip'.
INFO flwr 2024-04-05 23:12:52,472 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 154075130061.0, 'object_store_memory': 70317912883.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-05 23:12:52,473 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:12:52,473 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:12:52,487 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:12:52,487 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:12:52,487 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:12:52,488 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 23:12:55,192 | server.py:94 | initial parameters (loss, other metrics): 2.302752733230591, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-05 23:12:55,192 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:12:55,192 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=614570)[0m 2024-04-05 23:12:58.253652: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=614570)[0m 2024-04-05 23:12:58.350437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=614570)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=614570)[0m 2024-04-05 23:13:00.339264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=614566)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=614566)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=614563)[0m 2024-04-05 23:12:58.721668: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=614563)[0m 2024-04-05 23:12:58.820811: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=614563)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=614563)[0m 2024-04-05 23:13:00.933250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 23:13:12,441 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:13:12,971 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:13:14,198 | server.py:125 | fit progress: (1, 2.3022358417510986, {'accuracy': 0.1016, 'data_size': 10000}, 19.005234498996288)
INFO flwr 2024-04-05 23:13:14,198 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:13:14,198 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:13:22,878 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:13:25,223 | server.py:125 | fit progress: (2, 2.3014495372772217, {'accuracy': 0.101, 'data_size': 10000}, 30.03080368200608)
INFO flwr 2024-04-05 23:13:25,223 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:13:25,224 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:13:32,980 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:13:36,423 | server.py:125 | fit progress: (3, 2.298814058303833, {'accuracy': 0.101, 'data_size': 10000}, 41.231041759994696)
INFO flwr 2024-04-05 23:13:36,424 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:13:36,424 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:13:44,325 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:13:48,429 | server.py:125 | fit progress: (4, 2.29280948638916, {'accuracy': 0.101, 'data_size': 10000}, 53.236405760995694)
INFO flwr 2024-04-05 23:13:48,429 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:13:48,429 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:13:56,210 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:14:01,433 | server.py:125 | fit progress: (5, 2.286651849746704, {'accuracy': 0.2487, 'data_size': 10000}, 66.24108381300175)
INFO flwr 2024-04-05 23:14:01,434 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:14:01,434 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:14:09,051 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:14:15,069 | server.py:125 | fit progress: (6, 2.265716791152954, {'accuracy': 0.1368, 'data_size': 10000}, 79.87696675999905)
INFO flwr 2024-04-05 23:14:15,070 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:14:15,070 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:14:22,850 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:14:29,500 | server.py:125 | fit progress: (7, 2.212664842605591, {'accuracy': 0.3212, 'data_size': 10000}, 94.3075029939937)
INFO flwr 2024-04-05 23:14:29,500 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:14:29,500 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:14:37,713 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:14:45,424 | server.py:125 | fit progress: (8, 2.1246817111968994, {'accuracy': 0.36, 'data_size': 10000}, 110.2313800349948)
INFO flwr 2024-04-05 23:14:45,424 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:14:45,424 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:14:53,415 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:15:01,771 | server.py:125 | fit progress: (9, 2.05102276802063, {'accuracy': 0.4774, 'data_size': 10000}, 126.57836227900407)
INFO flwr 2024-04-05 23:15:01,771 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:15:01,771 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:15:10,150 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:15:19,681 | server.py:125 | fit progress: (10, 1.9928122758865356, {'accuracy': 0.502, 'data_size': 10000}, 144.48881003800489)
INFO flwr 2024-04-05 23:15:19,681 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:15:19,682 | server.py:153 | FL finished in 144.48915821699484
INFO flwr 2024-04-05 23:15:19,682 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:15:19,682 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:15:19,682 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:15:19,682 | app.py:229 | app_fit: losses_centralized [(0, 2.302752733230591), (1, 2.3022358417510986), (2, 2.3014495372772217), (3, 2.298814058303833), (4, 2.29280948638916), (5, 2.286651849746704), (6, 2.265716791152954), (7, 2.212664842605591), (8, 2.1246817111968994), (9, 2.05102276802063), (10, 1.9928122758865356)]
INFO flwr 2024-04-05 23:15:19,682 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.1016), (2, 0.101), (3, 0.101), (4, 0.101), (5, 0.2487), (6, 0.1368), (7, 0.3212), (8, 0.36), (9, 0.4774), (10, 0.502)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.502
wandb:     loss 1.99281
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_231235-sgyt07it
wandb: Find logs at: ./wandb/offline-run-20240405_231235-sgyt07it/logs
INFO flwr 2024-04-05 23:15:23,187 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:22:29,938 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=614565)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=614565)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 23:22:34,984	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:22:35,409	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:22:35,821	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ee9d47e63b0b17a2.zip' (8.00MiB) to Ray cluster...
2024-04-05 23:22:35,840	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ee9d47e63b0b17a2.zip'.
INFO flwr 2024-04-05 23:22:47,210 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 142781665076.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 65477856460.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-05 23:22:47,211 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:22:47,212 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:22:47,244 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:22:47,246 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:22:47,246 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:22:47,246 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 23:22:51,319 | server.py:94 | initial parameters (loss, other metrics): 2.3026187419891357, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-05 23:22:51,319 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:22:51,320 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=622055)[0m 2024-04-05 23:22:53.379032: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=622055)[0m 2024-04-05 23:22:53.487660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=622055)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=622055)[0m 2024-04-05 23:22:55.550846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=622060)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=622060)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=622062)[0m 2024-04-05 23:22:53.514715: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=622060)[0m 2024-04-05 23:22:53.538900: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=622060)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=622062)[0m 2024-04-05 23:22:55.854460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 23:23:09,747 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:23:10,321 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:23:11,405 | server.py:125 | fit progress: (1, 2.3020339012145996, {'accuracy': 0.117, 'data_size': 10000}, 20.08487356400292)
INFO flwr 2024-04-05 23:23:11,405 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:23:11,405 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:23:21,204 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:23:24,215 | server.py:125 | fit progress: (2, 2.3005101680755615, {'accuracy': 0.3241, 'data_size': 10000}, 32.89471738800057)
INFO flwr 2024-04-05 23:23:24,215 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:23:24,215 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:23:33,563 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:23:37,147 | server.py:125 | fit progress: (3, 2.2972195148468018, {'accuracy': 0.0992, 'data_size': 10000}, 45.82677047701145)
INFO flwr 2024-04-05 23:23:37,147 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:23:37,147 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:23:46,728 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:23:51,318 | server.py:125 | fit progress: (4, 2.285655975341797, {'accuracy': 0.1531, 'data_size': 10000}, 59.99861865500861)
INFO flwr 2024-04-05 23:23:51,319 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:23:51,319 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:24:00,445 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:24:13,740 | server.py:125 | fit progress: (5, 2.2412772178649902, {'accuracy': 0.2931, 'data_size': 10000}, 82.41988092400425)
INFO flwr 2024-04-05 23:24:13,740 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:24:13,740 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:24:23,214 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:24:30,669 | server.py:125 | fit progress: (6, 2.155034303665161, {'accuracy': 0.4445, 'data_size': 10000}, 99.3494920200028)
INFO flwr 2024-04-05 23:24:30,670 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:24:30,670 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:24:39,172 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:24:47,643 | server.py:125 | fit progress: (7, 2.0643205642700195, {'accuracy': 0.4082, 'data_size': 10000}, 116.32294014500803)
INFO flwr 2024-04-05 23:24:47,643 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:24:47,643 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:24:56,631 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:25:04,551 | server.py:125 | fit progress: (8, 2.001890182495117, {'accuracy': 0.4607, 'data_size': 10000}, 133.23085549900134)
INFO flwr 2024-04-05 23:25:04,551 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:25:04,551 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:25:13,483 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:25:23,462 | server.py:125 | fit progress: (9, 1.8983511924743652, {'accuracy': 0.6153, 'data_size': 10000}, 152.1418107220088)
INFO flwr 2024-04-05 23:25:23,462 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:25:23,462 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:25:32,386 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:25:53,649 | server.py:125 | fit progress: (10, 1.8561495542526245, {'accuracy': 0.63, 'data_size': 10000}, 182.32892491899838)
INFO flwr 2024-04-05 23:25:53,649 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:25:53,649 | server.py:153 | FL finished in 182.32943110300403
INFO flwr 2024-04-05 23:25:53,649 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:25:53,649 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:25:53,650 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:25:53,650 | app.py:229 | app_fit: losses_centralized [(0, 2.3026187419891357), (1, 2.3020339012145996), (2, 2.3005101680755615), (3, 2.2972195148468018), (4, 2.285655975341797), (5, 2.2412772178649902), (6, 2.155034303665161), (7, 2.0643205642700195), (8, 2.001890182495117), (9, 1.8983511924743652), (10, 1.8561495542526245)]
INFO flwr 2024-04-05 23:25:53,650 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.117), (2, 0.3241), (3, 0.0992), (4, 0.1531), (5, 0.2931), (6, 0.4445), (7, 0.4082), (8, 0.4607), (9, 0.6153), (10, 0.63)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.63
wandb:     loss 1.85615
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_232229-colecgph
wandb: Find logs at: ./wandb/offline-run-20240405_232229-colecgph/logs
INFO flwr 2024-04-05 23:25:57,559 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:33:03,093 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=622058)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=622058)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 23:33:07,809	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:33:08,084	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:33:08,403	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2ae446359db57f15.zip' (8.01MiB) to Ray cluster...
2024-04-05 23:33:08,421	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2ae446359db57f15.zip'.
INFO flwr 2024-04-05 23:33:19,275 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 154033406567.0, 'object_store_memory': 70300031385.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 23:33:19,275 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:33:19,275 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:33:19,292 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:33:19,294 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:33:19,294 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:33:19,294 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 23:33:21,497 | server.py:94 | initial parameters (loss, other metrics): 2.302555799484253, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-05 23:33:21,498 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:33:21,498 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=626693)[0m 2024-04-05 23:33:25.119395: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=626693)[0m 2024-04-05 23:33:25.199723: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=626693)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=626696)[0m 2024-04-05 23:33:27.226757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=626701)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=626701)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=626699)[0m 2024-04-05 23:33:25.896397: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=626699)[0m 2024-04-05 23:33:25.981142: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=626699)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=626699)[0m 2024-04-05 23:33:28.011332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 23:33:42,622 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:33:43,161 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:33:44,378 | server.py:125 | fit progress: (1, 2.302502155303955, {'accuracy': 0.101, 'data_size': 10000}, 22.880287668987876)
INFO flwr 2024-04-05 23:33:44,379 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:33:44,379 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:33:54,307 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:33:56,972 | server.py:125 | fit progress: (2, 2.3024661540985107, {'accuracy': 0.1009, 'data_size': 10000}, 35.47384257399244)
INFO flwr 2024-04-05 23:33:56,972 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:33:56,972 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:34:05,989 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:34:09,283 | server.py:125 | fit progress: (3, 2.3024160861968994, {'accuracy': 0.101, 'data_size': 10000}, 47.785169771988876)
INFO flwr 2024-04-05 23:34:09,283 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:34:09,284 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:34:18,005 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:34:22,382 | server.py:125 | fit progress: (4, 2.3023629188537598, {'accuracy': 0.101, 'data_size': 10000}, 60.883862368995324)
INFO flwr 2024-04-05 23:34:22,382 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:34:22,382 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:34:31,562 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:34:36,554 | server.py:125 | fit progress: (5, 2.3023147583007812, {'accuracy': 0.101, 'data_size': 10000}, 75.05622052399849)
INFO flwr 2024-04-05 23:34:36,554 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:34:36,555 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:34:45,837 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:34:51,972 | server.py:125 | fit progress: (6, 2.3022685050964355, {'accuracy': 0.101, 'data_size': 10000}, 90.47432309899887)
INFO flwr 2024-04-05 23:34:51,973 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:34:51,973 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:35:01,271 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:35:08,050 | server.py:125 | fit progress: (7, 2.302215337753296, {'accuracy': 0.101, 'data_size': 10000}, 106.5518847679923)
INFO flwr 2024-04-05 23:35:08,050 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:35:08,050 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:35:17,163 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:35:25,039 | server.py:125 | fit progress: (8, 2.302170991897583, {'accuracy': 0.101, 'data_size': 10000}, 123.54135131200019)
INFO flwr 2024-04-05 23:35:25,040 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:35:25,040 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:35:34,429 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:35:43,166 | server.py:125 | fit progress: (9, 2.302119255065918, {'accuracy': 0.101, 'data_size': 10000}, 141.66789094099659)
INFO flwr 2024-04-05 23:35:43,166 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:35:43,166 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:35:53,029 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:36:02,467 | server.py:125 | fit progress: (10, 2.3020694255828857, {'accuracy': 0.101, 'data_size': 10000}, 160.96910746599315)
INFO flwr 2024-04-05 23:36:02,467 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:36:02,467 | server.py:153 | FL finished in 160.96947429499414
INFO flwr 2024-04-05 23:36:02,468 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:36:02,468 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:36:02,468 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:36:02,468 | app.py:229 | app_fit: losses_centralized [(0, 2.302555799484253), (1, 2.302502155303955), (2, 2.3024661540985107), (3, 2.3024160861968994), (4, 2.3023629188537598), (5, 2.3023147583007812), (6, 2.3022685050964355), (7, 2.302215337753296), (8, 2.302170991897583), (9, 2.302119255065918), (10, 2.3020694255828857)]
INFO flwr 2024-04-05 23:36:02,468 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.101), (2, 0.1009), (3, 0.101), (4, 0.101), (5, 0.101), (6, 0.101), (7, 0.101), (8, 0.101), (9, 0.101), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.30207
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_233302-blb4euti
wandb: Find logs at: ./wandb/offline-run-20240405_233302-blb4euti/logs
INFO flwr 2024-04-05 23:36:06,049 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:43:13,899 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=626693)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=626693)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 23:43:22,381	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:43:22,850	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:43:23,274	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_140de3c4af0d2cdc.zip' (8.04MiB) to Ray cluster...
2024-04-05 23:43:23,302	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_140de3c4af0d2cdc.zip'.
INFO flwr 2024-04-05 23:43:34,244 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64396479283.0, 'node:__internal_head__': 1.0, 'memory': 140258451661.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-05 23:43:34,245 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:43:34,245 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:43:34,262 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:43:34,263 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:43:34,263 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:43:34,263 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 23:43:36,338 | server.py:94 | initial parameters (loss, other metrics): 2.302591562271118, {'accuracy': 0.1049, 'data_size': 10000}
INFO flwr 2024-04-05 23:43:36,339 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:43:36,339 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=634264)[0m 2024-04-05 23:43:40.293652: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=634264)[0m 2024-04-05 23:43:40.384864: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=634264)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=634264)[0m 2024-04-05 23:43:42.644480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=634264)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=634264)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=634253)[0m 2024-04-05 23:43:40.584045: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=634253)[0m 2024-04-05 23:43:40.677262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=634253)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=634262)[0m 2024-04-05 23:43:42.713187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 23:43:56,144 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:43:56,698 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:43:57,966 | server.py:125 | fit progress: (1, 2.290485382080078, {'accuracy': 0.1028, 'data_size': 10000}, 21.62680671000271)
INFO flwr 2024-04-05 23:43:57,966 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:43:57,966 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:44:08,069 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:44:10,459 | server.py:125 | fit progress: (2, 2.175206422805786, {'accuracy': 0.3485, 'data_size': 10000}, 34.12037818599492)
INFO flwr 2024-04-05 23:44:10,460 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:44:10,460 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:44:20,026 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:44:23,530 | server.py:125 | fit progress: (3, 1.990100622177124, {'accuracy': 0.4791, 'data_size': 10000}, 47.191175982996356)
INFO flwr 2024-04-05 23:44:23,530 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:44:23,531 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:44:32,734 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:44:37,406 | server.py:125 | fit progress: (4, 1.8058935403823853, {'accuracy': 0.7252, 'data_size': 10000}, 61.06745339499321)
INFO flwr 2024-04-05 23:44:37,407 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:44:37,407 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:44:47,427 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:44:52,338 | server.py:125 | fit progress: (5, 1.7167863845825195, {'accuracy': 0.7678, 'data_size': 10000}, 75.99915752900415)
INFO flwr 2024-04-05 23:44:52,338 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:44:52,339 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:45:01,591 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:45:07,752 | server.py:125 | fit progress: (6, 1.6832906007766724, {'accuracy': 0.7851, 'data_size': 10000}, 91.41317384700233)
INFO flwr 2024-04-05 23:45:07,752 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:45:07,753 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:45:16,893 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:45:23,940 | server.py:125 | fit progress: (7, 1.6721543073654175, {'accuracy': 0.7949, 'data_size': 10000}, 107.60141870600637)
INFO flwr 2024-04-05 23:45:23,941 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:45:23,941 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:45:33,281 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:45:40,790 | server.py:125 | fit progress: (8, 1.6335976123809814, {'accuracy': 0.8439, 'data_size': 10000}, 124.45123014200362)
INFO flwr 2024-04-05 23:45:40,790 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:45:40,791 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:45:49,517 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:45:57,985 | server.py:125 | fit progress: (9, 1.6165353059768677, {'accuracy': 0.8516, 'data_size': 10000}, 141.6457778759941)
INFO flwr 2024-04-05 23:45:57,985 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:45:57,985 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:46:06,768 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:46:16,387 | server.py:125 | fit progress: (10, 1.5953162908554077, {'accuracy': 0.8726, 'data_size': 10000}, 160.04835815000115)
INFO flwr 2024-04-05 23:46:16,388 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:46:16,388 | server.py:153 | FL finished in 160.048742813
INFO flwr 2024-04-05 23:46:16,388 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:46:16,388 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:46:16,388 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:46:16,388 | app.py:229 | app_fit: losses_centralized [(0, 2.302591562271118), (1, 2.290485382080078), (2, 2.175206422805786), (3, 1.990100622177124), (4, 1.8058935403823853), (5, 1.7167863845825195), (6, 1.6832906007766724), (7, 1.6721543073654175), (8, 1.6335976123809814), (9, 1.6165353059768677), (10, 1.5953162908554077)]
INFO flwr 2024-04-05 23:46:16,388 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1049), (1, 0.1028), (2, 0.3485), (3, 0.4791), (4, 0.7252), (5, 0.7678), (6, 0.7851), (7, 0.7949), (8, 0.8439), (9, 0.8516), (10, 0.8726)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8726
wandb:     loss 1.59532
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_234313-h1girbuk
wandb: Find logs at: ./wandb/offline-run-20240405_234313-h1girbuk/logs
INFO flwr 2024-04-05 23:46:19,888 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:53:27,622 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=634265)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=634265)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 23:53:32,348	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:53:32,696	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:53:33,075	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fd03b5ddb5109534.zip' (8.06MiB) to Ray cluster...
2024-04-05 23:53:33,096	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fd03b5ddb5109534.zip'.
INFO flwr 2024-04-05 23:53:43,982 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 147746835456.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 67605786624.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-05 23:53:43,983 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:53:43,983 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:53:44,004 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:53:44,005 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:53:44,005 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:53:44,005 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-05 23:53:46,095 | server.py:94 | initial parameters (loss, other metrics): 2.302717924118042, {'accuracy': 0.0981, 'data_size': 10000}
INFO flwr 2024-04-05 23:53:46,095 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:53:46,095 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=641613)[0m 2024-04-05 23:53:50.101295: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=641611)[0m 2024-04-05 23:53:50.215279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=641611)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=641611)[0m 2024-04-05 23:53:52.252410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=641610)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=641610)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=641604)[0m 2024-04-05 23:53:50.599053: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=641604)[0m 2024-04-05 23:53:50.698512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=641604)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=641604)[0m 2024-04-05 23:53:53.034079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 23:54:07,395 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:54:07,956 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:54:09,214 | server.py:125 | fit progress: (1, 2.2681117057800293, {'accuracy': 0.2569, 'data_size': 10000}, 23.118977119011106)
INFO flwr 2024-04-05 23:54:09,214 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:54:09,215 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:54:19,457 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:54:22,166 | server.py:125 | fit progress: (2, 2.048001289367676, {'accuracy': 0.3974, 'data_size': 10000}, 36.070432261010865)
INFO flwr 2024-04-05 23:54:22,166 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:54:22,166 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:54:32,191 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:54:36,151 | server.py:125 | fit progress: (3, 1.8605473041534424, {'accuracy': 0.6231, 'data_size': 10000}, 50.05602829498821)
INFO flwr 2024-04-05 23:54:36,152 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:54:36,152 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:54:45,755 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:54:50,406 | server.py:125 | fit progress: (4, 1.7529706954956055, {'accuracy': 0.7149, 'data_size': 10000}, 64.31093398100347)
INFO flwr 2024-04-05 23:54:50,406 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:54:50,407 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:55:00,177 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:55:06,102 | server.py:125 | fit progress: (5, 1.7357969284057617, {'accuracy': 0.727, 'data_size': 10000}, 80.00657028600108)
INFO flwr 2024-04-05 23:55:06,102 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:55:06,102 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:55:15,853 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:55:22,476 | server.py:125 | fit progress: (6, 1.6797581911087036, {'accuracy': 0.7793, 'data_size': 10000}, 96.38115254801232)
INFO flwr 2024-04-05 23:55:22,477 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:55:22,477 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:55:32,592 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:55:40,304 | server.py:125 | fit progress: (7, 1.6288642883300781, {'accuracy': 0.8404, 'data_size': 10000}, 114.20875919400714)
INFO flwr 2024-04-05 23:55:40,304 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:55:40,304 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:55:49,279 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:55:58,849 | server.py:125 | fit progress: (8, 1.6281542778015137, {'accuracy': 0.8417, 'data_size': 10000}, 132.75402995600598)
INFO flwr 2024-04-05 23:55:58,849 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:55:58,850 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:56:08,281 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:56:18,258 | server.py:125 | fit progress: (9, 1.6742156744003296, {'accuracy': 0.7837, 'data_size': 10000}, 152.16246424798737)
INFO flwr 2024-04-05 23:56:18,258 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:56:18,258 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:56:27,102 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:56:38,917 | server.py:125 | fit progress: (10, 1.6949282884597778, {'accuracy': 0.7619, 'data_size': 10000}, 172.8221291330119)
INFO flwr 2024-04-05 23:56:38,918 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:56:38,918 | server.py:153 | FL finished in 172.82251107800403
INFO flwr 2024-04-05 23:56:38,918 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:56:38,918 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:56:38,918 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:56:38,918 | app.py:229 | app_fit: losses_centralized [(0, 2.302717924118042), (1, 2.2681117057800293), (2, 2.048001289367676), (3, 1.8605473041534424), (4, 1.7529706954956055), (5, 1.7357969284057617), (6, 1.6797581911087036), (7, 1.6288642883300781), (8, 1.6281542778015137), (9, 1.6742156744003296), (10, 1.6949282884597778)]
INFO flwr 2024-04-05 23:56:38,918 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0981), (1, 0.2569), (2, 0.3974), (3, 0.6231), (4, 0.7149), (5, 0.727), (6, 0.7793), (7, 0.8404), (8, 0.8417), (9, 0.7837), (10, 0.7619)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7619
wandb:     loss 1.69493
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_235327-rjtx9a16
wandb: Find logs at: ./wandb/offline-run-20240405_235327-rjtx9a16/logs
INFO flwr 2024-04-05 23:56:42,469 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:03:48,867 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=641602)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=641602)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 00:03:53,575	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:03:53,885	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:03:54,168	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9889a2388bab2e75.zip' (8.08MiB) to Ray cluster...
2024-04-06 00:03:54,193	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9889a2388bab2e75.zip'.
INFO flwr 2024-04-06 00:04:05,013 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 153863266919.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 70227114393.0}
INFO flwr 2024-04-06 00:04:05,014 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:04:05,014 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:04:05,030 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:04:05,031 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:04:05,031 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:04:05,032 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 00:04:07,082 | server.py:94 | initial parameters (loss, other metrics): 2.3025662899017334, {'accuracy': 0.1137, 'data_size': 10000}
INFO flwr 2024-04-06 00:04:07,082 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:04:07,083 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=646407)[0m 2024-04-06 00:04:11.224995: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=646407)[0m 2024-04-06 00:04:11.319097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=646407)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=646408)[0m 2024-04-06 00:04:13.485191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=646407)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=646407)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=646408)[0m 2024-04-06 00:04:11.379747: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=646408)[0m 2024-04-06 00:04:11.477446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=646408)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=646404)[0m 2024-04-06 00:04:13.504090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 00:04:28,760 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:04:29,317 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:04:30,591 | server.py:125 | fit progress: (1, 2.2482080459594727, {'accuracy': 0.3825, 'data_size': 10000}, 23.5081426280085)
INFO flwr 2024-04-06 00:04:30,591 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:04:30,591 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:04:40,522 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:04:43,156 | server.py:125 | fit progress: (2, 1.9955099821090698, {'accuracy': 0.4172, 'data_size': 10000}, 36.07381465801154)
INFO flwr 2024-04-06 00:04:43,157 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:04:43,157 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:04:51,866 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:04:55,151 | server.py:125 | fit progress: (3, 1.8615611791610718, {'accuracy': 0.59, 'data_size': 10000}, 48.06869308301248)
INFO flwr 2024-04-06 00:04:55,152 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:04:55,152 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:05:04,415 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:05:08,778 | server.py:125 | fit progress: (4, 1.8112744092941284, {'accuracy': 0.6492, 'data_size': 10000}, 61.69534663201193)
INFO flwr 2024-04-06 00:05:08,778 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:05:08,778 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:05:17,807 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:05:23,034 | server.py:125 | fit progress: (5, 1.7444466352462769, {'accuracy': 0.7235, 'data_size': 10000}, 75.95188036400941)
INFO flwr 2024-04-06 00:05:23,035 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:05:23,035 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:05:32,608 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:05:38,283 | server.py:125 | fit progress: (6, 1.7569154500961304, {'accuracy': 0.6894, 'data_size': 10000}, 91.20059617501101)
INFO flwr 2024-04-06 00:05:38,283 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:05:38,284 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:05:47,642 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:05:54,692 | server.py:125 | fit progress: (7, 1.7942421436309814, {'accuracy': 0.6768, 'data_size': 10000}, 107.609192739008)
INFO flwr 2024-04-06 00:05:54,692 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:05:54,692 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:06:03,947 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 00:06:11,299 | server.py:125 | fit progress: (8, 1.948652744293213, {'accuracy': 0.5119, 'data_size': 10000}, 124.21600373700494)
INFO flwr 2024-04-06 00:06:11,299 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 00:06:11,299 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:06:20,724 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 00:06:29,316 | server.py:125 | fit progress: (9, 1.7344517707824707, {'accuracy': 0.7242, 'data_size': 10000}, 142.23306365899043)
INFO flwr 2024-04-06 00:06:29,316 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 00:06:29,316 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:06:38,593 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:06:47,927 | server.py:125 | fit progress: (10, 1.7863365411758423, {'accuracy': 0.6719, 'data_size': 10000}, 160.84475453401683)
INFO flwr 2024-04-06 00:06:47,928 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:06:47,928 | server.py:153 | FL finished in 160.84513926200452
INFO flwr 2024-04-06 00:06:47,928 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:06:47,928 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:06:47,928 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:06:47,928 | app.py:229 | app_fit: losses_centralized [(0, 2.3025662899017334), (1, 2.2482080459594727), (2, 1.9955099821090698), (3, 1.8615611791610718), (4, 1.8112744092941284), (5, 1.7444466352462769), (6, 1.7569154500961304), (7, 1.7942421436309814), (8, 1.948652744293213), (9, 1.7344517707824707), (10, 1.7863365411758423)]
INFO flwr 2024-04-06 00:06:47,928 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1137), (1, 0.3825), (2, 0.4172), (3, 0.59), (4, 0.6492), (5, 0.7235), (6, 0.6894), (7, 0.6768), (8, 0.5119), (9, 0.7242), (10, 0.6719)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6719
wandb:     loss 1.78634
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_000348-wlvp48ct
wandb: Find logs at: ./wandb/offline-run-20240406_000348-wlvp48ct/logs
INFO flwr 2024-04-06 00:06:51,480 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:13:59,014 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=646403)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=646403)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 00:14:07,436	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:14:07,803	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:14:08,152	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f934343cb2c14639.zip' (8.10MiB) to Ray cluster...
2024-04-06 00:14:08,172	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f934343cb2c14639.zip'.
INFO flwr 2024-04-06 00:14:19,238 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 67750866124.0, 'memory': 148085354292.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 00:14:19,239 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:14:19,239 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:14:19,259 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:14:19,260 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:14:19,261 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:14:19,261 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 00:14:21,451 | server.py:94 | initial parameters (loss, other metrics): 2.3026654720306396, {'accuracy': 0.0782, 'data_size': 10000}
INFO flwr 2024-04-06 00:14:21,451 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:14:21,451 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=653894)[0m 2024-04-06 00:14:25.639768: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=653894)[0m 2024-04-06 00:14:25.705201: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=653894)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=653903)[0m 2024-04-06 00:14:27.926730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=653895)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=653895)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=653900)[0m 2024-04-06 00:14:27.154689: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=653900)[0m 2024-04-06 00:14:27.245404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=653900)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=653900)[0m 2024-04-06 00:14:29.776478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 00:14:43,343 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:14:43,960 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:14:45,242 | server.py:125 | fit progress: (1, 2.245786428451538, {'accuracy': 0.2321, 'data_size': 10000}, 23.790554734005127)
INFO flwr 2024-04-06 00:14:45,242 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:14:45,242 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:14:55,286 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:14:57,764 | server.py:125 | fit progress: (2, 2.0708119869232178, {'accuracy': 0.376, 'data_size': 10000}, 36.31298138399143)
INFO flwr 2024-04-06 00:14:57,765 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:14:57,765 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:15:07,144 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:15:10,692 | server.py:125 | fit progress: (3, 2.0379436016082764, {'accuracy': 0.4258, 'data_size': 10000}, 49.24100861500483)
INFO flwr 2024-04-06 00:15:10,693 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:15:10,693 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:15:20,455 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:15:25,119 | server.py:125 | fit progress: (4, 1.8890478610992432, {'accuracy': 0.5622, 'data_size': 10000}, 63.66758768298314)
INFO flwr 2024-04-06 00:15:25,119 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:15:25,119 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:15:34,634 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:15:40,010 | server.py:125 | fit progress: (5, 1.8193359375, {'accuracy': 0.6571, 'data_size': 10000}, 78.55873293598415)
INFO flwr 2024-04-06 00:15:40,010 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:15:40,010 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:15:49,169 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:15:56,284 | server.py:125 | fit progress: (6, 1.9749681949615479, {'accuracy': 0.4871, 'data_size': 10000}, 94.83324935598648)
INFO flwr 2024-04-06 00:15:56,285 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:15:56,285 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:16:05,464 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:16:14,876 | server.py:125 | fit progress: (7, 1.8440948724746704, {'accuracy': 0.6199, 'data_size': 10000}, 113.425148498005)
INFO flwr 2024-04-06 00:16:14,877 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:16:14,877 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:16:24,274 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 00:16:32,096 | server.py:125 | fit progress: (8, 1.8718152046203613, {'accuracy': 0.6028, 'data_size': 10000}, 130.64501853100955)
INFO flwr 2024-04-06 00:16:32,097 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 00:16:32,097 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:16:41,242 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 00:16:51,473 | server.py:125 | fit progress: (9, 1.8600642681121826, {'accuracy': 0.6129, 'data_size': 10000}, 150.02208448998863)
INFO flwr 2024-04-06 00:16:51,474 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 00:16:51,474 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:17:00,551 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:17:10,048 | server.py:125 | fit progress: (10, 1.9602121114730835, {'accuracy': 0.4998, 'data_size': 10000}, 168.59674610299408)
INFO flwr 2024-04-06 00:17:10,048 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:17:10,048 | server.py:153 | FL finished in 168.59717704899958
INFO flwr 2024-04-06 00:17:10,049 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:17:10,049 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:17:10,049 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:17:10,049 | app.py:229 | app_fit: losses_centralized [(0, 2.3026654720306396), (1, 2.245786428451538), (2, 2.0708119869232178), (3, 2.0379436016082764), (4, 1.8890478610992432), (5, 1.8193359375), (6, 1.9749681949615479), (7, 1.8440948724746704), (8, 1.8718152046203613), (9, 1.8600642681121826), (10, 1.9602121114730835)]
INFO flwr 2024-04-06 00:17:10,049 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0782), (1, 0.2321), (2, 0.376), (3, 0.4258), (4, 0.5622), (5, 0.6571), (6, 0.4871), (7, 0.6199), (8, 0.6028), (9, 0.6129), (10, 0.4998)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4998
wandb:     loss 1.96021
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_001358-cu1v4d4n
wandb: Find logs at: ./wandb/offline-run-20240406_001358-cu1v4d4n/logs
INFO flwr 2024-04-06 00:17:13,585 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:24:20,394 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=653900)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=653900)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 00:24:29,145	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:24:30,798	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:24:31,291	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cc9f957656f726cd.zip' (8.11MiB) to Ray cluster...
2024-04-06 00:24:31,313	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cc9f957656f726cd.zip'.
INFO flwr 2024-04-06 00:24:42,427 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 153962170983.0, 'object_store_memory': 70269501849.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 00:24:42,428 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:24:42,428 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:24:42,441 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:24:42,442 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:24:42,442 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:24:42,442 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 00:24:44,463 | server.py:94 | initial parameters (loss, other metrics): 2.3026978969573975, {'accuracy': 0.0947, 'data_size': 10000}
INFO flwr 2024-04-06 00:24:44,463 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:24:44,464 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=661034)[0m 2024-04-06 00:24:48.647915: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=661034)[0m 2024-04-06 00:24:48.750890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=661034)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=661037)[0m 2024-04-06 00:24:50.864385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=661037)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=661037)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=661032)[0m 2024-04-06 00:24:48.954091: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=661032)[0m 2024-04-06 00:24:49.048929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=661032)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=661032)[0m 2024-04-06 00:24:51.667960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 00:25:04,563 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:25:05,120 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:25:06,388 | server.py:125 | fit progress: (1, 2.252509355545044, {'accuracy': 0.2129, 'data_size': 10000}, 21.924522273999173)
INFO flwr 2024-04-06 00:25:06,388 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:25:06,389 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:25:15,845 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:25:18,226 | server.py:125 | fit progress: (2, 2.0730881690979004, {'accuracy': 0.4032, 'data_size': 10000}, 33.76236495698686)
INFO flwr 2024-04-06 00:25:18,226 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:25:18,226 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:25:27,206 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:25:30,745 | server.py:125 | fit progress: (3, 1.9955068826675415, {'accuracy': 0.4715, 'data_size': 10000}, 46.28111642799922)
INFO flwr 2024-04-06 00:25:30,745 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:25:30,745 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:25:40,322 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:25:44,920 | server.py:125 | fit progress: (4, 1.987104892730713, {'accuracy': 0.4643, 'data_size': 10000}, 60.45612876099767)
INFO flwr 2024-04-06 00:25:44,920 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:25:44,920 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:25:53,678 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:25:59,428 | server.py:125 | fit progress: (5, 1.9814704656600952, {'accuracy': 0.481, 'data_size': 10000}, 74.96449054600089)
INFO flwr 2024-04-06 00:25:59,428 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:25:59,428 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:26:08,239 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:26:14,975 | server.py:125 | fit progress: (6, 1.9387394189834595, {'accuracy': 0.524, 'data_size': 10000}, 90.51121325098211)
INFO flwr 2024-04-06 00:26:14,975 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:26:14,975 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:26:23,592 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:26:31,325 | server.py:125 | fit progress: (7, 1.9382388591766357, {'accuracy': 0.5217, 'data_size': 10000}, 106.86150789397652)
INFO flwr 2024-04-06 00:26:31,325 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:26:31,325 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:26:40,991 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 00:26:49,996 | server.py:125 | fit progress: (8, 2.1961042881011963, {'accuracy': 0.2644, 'data_size': 10000}, 125.53250368099543)
INFO flwr 2024-04-06 00:26:49,996 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 00:26:49,996 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:26:58,759 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 00:27:07,416 | server.py:125 | fit progress: (9, 2.2540624141693115, {'accuracy': 0.2064, 'data_size': 10000}, 142.95281568999053)
INFO flwr 2024-04-06 00:27:07,417 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 00:27:07,417 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:27:16,710 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:27:27,501 | server.py:125 | fit progress: (10, 2.1652302742004395, {'accuracy': 0.2941, 'data_size': 10000}, 163.03770457897917)
INFO flwr 2024-04-06 00:27:27,502 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:27:27,502 | server.py:153 | FL finished in 163.0381364269997
INFO flwr 2024-04-06 00:27:27,502 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:27:27,502 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:27:27,502 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:27:27,502 | app.py:229 | app_fit: losses_centralized [(0, 2.3026978969573975), (1, 2.252509355545044), (2, 2.0730881690979004), (3, 1.9955068826675415), (4, 1.987104892730713), (5, 1.9814704656600952), (6, 1.9387394189834595), (7, 1.9382388591766357), (8, 2.1961042881011963), (9, 2.2540624141693115), (10, 2.1652302742004395)]
INFO flwr 2024-04-06 00:27:27,502 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0947), (1, 0.2129), (2, 0.4032), (3, 0.4715), (4, 0.4643), (5, 0.481), (6, 0.524), (7, 0.5217), (8, 0.2644), (9, 0.2064), (10, 0.2941)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2941
wandb:     loss 2.16523
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_002420-mnio7uvh
wandb: Find logs at: ./wandb/offline-run-20240406_002420-mnio7uvh/logs
INFO flwr 2024-04-06 00:27:31,064 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:34:37,844 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=661032)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=661032)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 00:34:43,555	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:34:43,839	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:34:44,112	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4c3139a42d9b404f.zip' (8.14MiB) to Ray cluster...
2024-04-06 00:34:44,131	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4c3139a42d9b404f.zip'.
INFO flwr 2024-04-06 00:34:54,845 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 152967034676.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 69843014860.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 00:34:54,845 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:34:54,845 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:34:54,860 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:34:54,860 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:34:54,860 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:34:54,861 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 00:34:57,518 | server.py:94 | initial parameters (loss, other metrics): 2.3024990558624268, {'accuracy': 0.0922, 'data_size': 10000}
INFO flwr 2024-04-06 00:34:57,521 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:34:57,523 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=666089)[0m 2024-04-06 00:35:00.421976: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=666089)[0m 2024-04-06 00:35:00.519518: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=666089)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=666088)[0m 2024-04-06 00:35:02.631601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=666088)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=666088)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=666091)[0m 2024-04-06 00:35:01.513300: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=666091)[0m 2024-04-06 00:35:01.618616: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=666091)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=666091)[0m 2024-04-06 00:35:03.977431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 00:35:17,415 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:35:17,972 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:35:19,268 | server.py:125 | fit progress: (1, 2.249319553375244, {'accuracy': 0.3146, 'data_size': 10000}, 21.744573002011748)
INFO flwr 2024-04-06 00:35:19,268 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:35:19,268 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:35:28,833 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:35:31,227 | server.py:125 | fit progress: (2, 2.1491684913635254, {'accuracy': 0.3138, 'data_size': 10000}, 33.70427036500769)
INFO flwr 2024-04-06 00:35:31,228 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:35:31,228 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:35:41,036 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:35:44,613 | server.py:125 | fit progress: (3, 2.0173237323760986, {'accuracy': 0.4756, 'data_size': 10000}, 47.08975435298635)
INFO flwr 2024-04-06 00:35:44,613 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:35:44,613 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:35:53,940 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:35:58,023 | server.py:125 | fit progress: (4, 2.098867177963257, {'accuracy': 0.3712, 'data_size': 10000}, 60.50005269999383)
INFO flwr 2024-04-06 00:35:58,023 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:35:58,024 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:36:07,523 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:36:12,423 | server.py:125 | fit progress: (5, 2.0525269508361816, {'accuracy': 0.4071, 'data_size': 10000}, 74.8999464790104)
INFO flwr 2024-04-06 00:36:12,423 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:36:12,423 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:36:21,728 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:36:27,700 | server.py:125 | fit progress: (6, 2.0177364349365234, {'accuracy': 0.4413, 'data_size': 10000}, 90.17698487499729)
INFO flwr 2024-04-06 00:36:27,700 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:36:27,700 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:36:36,901 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:36:43,705 | server.py:125 | fit progress: (7, 2.0143637657165527, {'accuracy': 0.4414, 'data_size': 10000}, 106.18179853900801)
INFO flwr 2024-04-06 00:36:43,705 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:36:43,705 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:36:52,795 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 00:37:00,249 | server.py:125 | fit progress: (8, 1.9442609548568726, {'accuracy': 0.5177, 'data_size': 10000}, 122.72570862999419)
INFO flwr 2024-04-06 00:37:00,249 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 00:37:00,249 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:37:10,367 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 00:37:20,131 | server.py:125 | fit progress: (9, 2.0330309867858887, {'accuracy': 0.4288, 'data_size': 10000}, 142.60795164399315)
INFO flwr 2024-04-06 00:37:20,131 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 00:37:20,131 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:37:29,603 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:37:38,953 | server.py:125 | fit progress: (10, 2.047574520111084, {'accuracy': 0.4212, 'data_size': 10000}, 161.43015675598872)
INFO flwr 2024-04-06 00:37:38,953 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:37:38,954 | server.py:153 | FL finished in 161.43060548999347
INFO flwr 2024-04-06 00:37:38,954 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:37:38,954 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:37:38,954 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:37:38,954 | app.py:229 | app_fit: losses_centralized [(0, 2.3024990558624268), (1, 2.249319553375244), (2, 2.1491684913635254), (3, 2.0173237323760986), (4, 2.098867177963257), (5, 2.0525269508361816), (6, 2.0177364349365234), (7, 2.0143637657165527), (8, 1.9442609548568726), (9, 2.0330309867858887), (10, 2.047574520111084)]
INFO flwr 2024-04-06 00:37:38,954 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0922), (1, 0.3146), (2, 0.3138), (3, 0.4756), (4, 0.3712), (5, 0.4071), (6, 0.4413), (7, 0.4414), (8, 0.5177), (9, 0.4288), (10, 0.4212)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4212
wandb:     loss 2.04757
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_003437-gss13w3p
wandb: Find logs at: ./wandb/offline-run-20240406_003437-gss13w3p/logs
INFO flwr 2024-04-06 00:37:42,508 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:44:52,703 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=666077)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=666077)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 00:44:58,671	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:44:59,199	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:44:59,631	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9aeed465fa45c4da.zip' (8.16MiB) to Ray cluster...
2024-04-06 00:44:59,657	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9aeed465fa45c4da.zip'.
INFO flwr 2024-04-06 00:45:11,690 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 143263062221.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65684169523.0, 'CPU': 64.0}
INFO flwr 2024-04-06 00:45:11,690 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:45:11,691 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:45:11,703 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:45:11,705 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:45:11,705 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:45:11,705 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 00:45:14,108 | server.py:94 | initial parameters (loss, other metrics): 2.302473545074463, {'accuracy': 0.1771, 'data_size': 10000}
INFO flwr 2024-04-06 00:45:14,108 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:45:14,108 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=673514)[0m 2024-04-06 00:45:17.676173: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=673514)[0m 2024-04-06 00:45:17.776816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=673514)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=673514)[0m 2024-04-06 00:45:20.845888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=673522)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=673522)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=673522)[0m 2024-04-06 00:45:18.777103: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=673522)[0m 2024-04-06 00:45:18.839969: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=673522)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=673522)[0m 2024-04-06 00:45:21.588208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 00:45:47,517 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:45:48,374 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:45:49,749 | server.py:125 | fit progress: (1, 2.3022477626800537, {'accuracy': 0.1684, 'data_size': 10000}, 35.64106409999658)
INFO flwr 2024-04-06 00:45:49,749 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:45:49,750 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:46:05,559 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:46:08,210 | server.py:125 | fit progress: (2, 2.3020572662353516, {'accuracy': 0.2059, 'data_size': 10000}, 54.10218861699104)
INFO flwr 2024-04-06 00:46:08,211 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:46:08,211 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:46:24,887 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:46:28,578 | server.py:125 | fit progress: (3, 2.301825761795044, {'accuracy': 0.2101, 'data_size': 10000}, 74.46992783399764)
INFO flwr 2024-04-06 00:46:28,578 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:46:28,579 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:46:45,148 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:46:50,092 | server.py:125 | fit progress: (4, 2.301539659500122, {'accuracy': 0.1779, 'data_size': 10000}, 95.98354839900276)
INFO flwr 2024-04-06 00:46:50,092 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:46:50,092 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:47:07,884 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:47:12,876 | server.py:125 | fit progress: (5, 2.3012592792510986, {'accuracy': 0.0982, 'data_size': 10000}, 118.76761826698203)
INFO flwr 2024-04-06 00:47:12,876 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:47:12,876 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:47:33,437 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:47:40,379 | server.py:125 | fit progress: (6, 2.3007872104644775, {'accuracy': 0.098, 'data_size': 10000}, 146.27135042098234)
INFO flwr 2024-04-06 00:47:40,380 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:47:40,380 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:47:59,505 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:48:07,730 | server.py:125 | fit progress: (7, 2.300351858139038, {'accuracy': 0.098, 'data_size': 10000}, 173.62154059798922)
INFO flwr 2024-04-06 00:48:07,730 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:48:07,730 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:48:25,139 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 00:48:32,925 | server.py:125 | fit progress: (8, 2.2997193336486816, {'accuracy': 0.1363, 'data_size': 10000}, 198.81646645299043)
INFO flwr 2024-04-06 00:48:32,925 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 00:48:32,925 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:48:49,888 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 00:49:01,711 | server.py:125 | fit progress: (9, 2.2977941036224365, {'accuracy': 0.098, 'data_size': 10000}, 227.60303736198694)
INFO flwr 2024-04-06 00:49:01,711 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 00:49:01,712 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:49:18,266 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:49:29,002 | server.py:125 | fit progress: (10, 2.295180082321167, {'accuracy': 0.098, 'data_size': 10000}, 254.89350856898818)
INFO flwr 2024-04-06 00:49:29,002 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:49:29,002 | server.py:153 | FL finished in 254.89396887799376
INFO flwr 2024-04-06 00:49:29,002 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:49:29,002 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:49:29,003 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:49:29,003 | app.py:229 | app_fit: losses_centralized [(0, 2.302473545074463), (1, 2.3022477626800537), (2, 2.3020572662353516), (3, 2.301825761795044), (4, 2.301539659500122), (5, 2.3012592792510986), (6, 2.3007872104644775), (7, 2.300351858139038), (8, 2.2997193336486816), (9, 2.2977941036224365), (10, 2.295180082321167)]
INFO flwr 2024-04-06 00:49:29,003 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1771), (1, 0.1684), (2, 0.2059), (3, 0.2101), (4, 0.1779), (5, 0.0982), (6, 0.098), (7, 0.098), (8, 0.1363), (9, 0.098), (10, 0.098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.098
wandb:     loss 2.29518
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_004450-7jcjfn60
wandb: Find logs at: ./wandb/offline-run-20240406_004450-7jcjfn60/logs
INFO flwr 2024-04-06 00:49:32,992 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:56:39,738 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=673520)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=673520)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 00:56:44,976	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:56:45,438	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:56:45,808	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_23ef07eae6b3673d.zip' (8.17MiB) to Ray cluster...
2024-04-06 00:56:45,828	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_23ef07eae6b3673d.zip'.
INFO flwr 2024-04-06 00:56:56,527 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 152605965312.0, 'object_store_memory': 69688270848.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 00:56:56,527 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:56:56,527 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:56:56,545 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:56:56,545 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:56:56,545 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:56:56,546 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 00:56:59,255 | server.py:94 | initial parameters (loss, other metrics): 2.302797317504883, {'accuracy': 0.0684, 'data_size': 10000}
INFO flwr 2024-04-06 00:56:59,255 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:56:59,256 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=677922)[0m 2024-04-06 00:57:02.480383: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=677922)[0m 2024-04-06 00:57:02.585029: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=677922)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=677909)[0m 2024-04-06 00:57:04.641024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=677920)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=677920)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=677911)[0m 2024-04-06 00:57:02.767227: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=677911)[0m 2024-04-06 00:57:02.862435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=677911)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=677917)[0m 2024-04-06 00:57:04.944676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 00:57:29,069 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:57:29,599 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:57:30,898 | server.py:125 | fit progress: (1, 2.2042219638824463, {'accuracy': 0.4169, 'data_size': 10000}, 31.642043473024387)
INFO flwr 2024-04-06 00:57:30,898 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:57:30,898 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:57:48,035 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:57:50,668 | server.py:125 | fit progress: (2, 1.8985378742218018, {'accuracy': 0.589, 'data_size': 10000}, 51.412064959004056)
INFO flwr 2024-04-06 00:57:50,668 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:57:50,668 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:58:07,120 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:58:10,400 | server.py:125 | fit progress: (3, 1.7221131324768066, {'accuracy': 0.7451, 'data_size': 10000}, 71.14477161000832)
INFO flwr 2024-04-06 00:58:10,401 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:58:10,401 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:58:27,379 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:58:31,726 | server.py:125 | fit progress: (4, 1.6306021213531494, {'accuracy': 0.839, 'data_size': 10000}, 92.47039444601978)
INFO flwr 2024-04-06 00:58:31,726 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:58:31,726 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:58:49,530 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:58:55,057 | server.py:125 | fit progress: (5, 1.5604667663574219, {'accuracy': 0.9023, 'data_size': 10000}, 115.80180923599983)
INFO flwr 2024-04-06 00:58:55,058 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:58:55,058 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:59:13,540 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:59:19,680 | server.py:125 | fit progress: (6, 1.553434133529663, {'accuracy': 0.9072, 'data_size': 10000}, 140.42485401200247)
INFO flwr 2024-04-06 00:59:19,681 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:59:19,681 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:59:36,246 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:59:43,303 | server.py:125 | fit progress: (7, 1.5427359342575073, {'accuracy': 0.9192, 'data_size': 10000}, 164.04732247602078)
INFO flwr 2024-04-06 00:59:43,303 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:59:43,303 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:00:01,096 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:00:10,412 | server.py:125 | fit progress: (8, 1.5622692108154297, {'accuracy': 0.8985, 'data_size': 10000}, 191.156832201028)
INFO flwr 2024-04-06 01:00:10,413 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:00:10,413 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:00:27,868 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:00:37,437 | server.py:125 | fit progress: (9, 1.557251214981079, {'accuracy': 0.9037, 'data_size': 10000}, 218.18141680501867)
INFO flwr 2024-04-06 01:00:37,437 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:00:37,437 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:00:55,328 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:01:05,785 | server.py:125 | fit progress: (10, 1.5458252429962158, {'accuracy': 0.9156, 'data_size': 10000}, 246.5296106780006)
INFO flwr 2024-04-06 01:01:05,785 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:01:05,785 | server.py:153 | FL finished in 246.53000243302085
INFO flwr 2024-04-06 01:01:05,786 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:01:05,786 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:01:05,786 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:01:05,786 | app.py:229 | app_fit: losses_centralized [(0, 2.302797317504883), (1, 2.2042219638824463), (2, 1.8985378742218018), (3, 1.7221131324768066), (4, 1.6306021213531494), (5, 1.5604667663574219), (6, 1.553434133529663), (7, 1.5427359342575073), (8, 1.5622692108154297), (9, 1.557251214981079), (10, 1.5458252429962158)]
INFO flwr 2024-04-06 01:01:05,786 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0684), (1, 0.4169), (2, 0.589), (3, 0.7451), (4, 0.839), (5, 0.9023), (6, 0.9072), (7, 0.9192), (8, 0.8985), (9, 0.9037), (10, 0.9156)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9156
wandb:     loss 1.54583
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_005639-pmu5oa0h
wandb: Find logs at: ./wandb/offline-run-20240406_005639-pmu5oa0h/logs
INFO flwr 2024-04-06 01:01:09,372 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 01:08:27,271 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=677910)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=677910)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 01:08:51,323	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 01:08:59,552	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 01:08:59,870	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_661b874d5ee06672.zip' (8.20MiB) to Ray cluster...
2024-04-06 01:08:59,896	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_661b874d5ee06672.zip'.
INFO flwr 2024-04-06 01:09:10,793 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 153279246951.0, 'CPU': 64.0, 'object_store_memory': 69976820121.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 01:09:10,793 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 01:09:10,793 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 01:09:10,808 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 01:09:10,809 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 01:09:10,821 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 01:09:10,821 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 01:09:13,663 | server.py:94 | initial parameters (loss, other metrics): 2.302461862564087, {'accuracy': 0.1244, 'data_size': 10000}
INFO flwr 2024-04-06 01:09:13,664 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 01:09:13,664 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=686351)[0m 2024-04-06 01:09:23.871358: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=686351)[0m 2024-04-06 01:09:24.039725: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=686351)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=686351)[0m 2024-04-06 01:09:46.617662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=686347)[0m 2024-04-06 01:09:23.871990: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=686347)[0m 2024-04-06 01:09:24.001782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=686347)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=686351)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=686351)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=686347)[0m 2024-04-06 01:09:46.617780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 01:11:24,750 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 01:11:25,271 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 01:11:26,517 | server.py:125 | fit progress: (1, 2.2062582969665527, {'accuracy': 0.5884, 'data_size': 10000}, 132.85267147797276)
INFO flwr 2024-04-06 01:11:26,517 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 01:11:26,517 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:11:45,464 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 01:11:48,120 | server.py:125 | fit progress: (2, 1.7682979106903076, {'accuracy': 0.7166, 'data_size': 10000}, 154.45598532399163)
INFO flwr 2024-04-06 01:11:48,120 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 01:11:48,121 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:12:05,132 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 01:12:08,418 | server.py:125 | fit progress: (3, 1.706459403038025, {'accuracy': 0.7544, 'data_size': 10000}, 174.753903076984)
INFO flwr 2024-04-06 01:12:08,418 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 01:12:08,418 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:12:25,814 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 01:12:30,275 | server.py:125 | fit progress: (4, 1.8293371200561523, {'accuracy': 0.635, 'data_size': 10000}, 196.61124933199608)
INFO flwr 2024-04-06 01:12:30,276 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 01:12:30,276 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:12:46,555 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:12:51,619 | server.py:125 | fit progress: (5, 1.7539515495300293, {'accuracy': 0.7061, 'data_size': 10000}, 217.95541839199723)
INFO flwr 2024-04-06 01:12:51,620 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:12:51,620 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:13:08,965 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:13:14,876 | server.py:125 | fit progress: (6, 1.8055572509765625, {'accuracy': 0.6566, 'data_size': 10000}, 241.21144935599295)
INFO flwr 2024-04-06 01:13:14,876 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:13:14,876 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:13:31,622 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 01:13:46,915 | server.py:125 | fit progress: (7, 1.7979010343551636, {'accuracy': 0.6637, 'data_size': 10000}, 273.25081040899386)
INFO flwr 2024-04-06 01:13:46,915 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 01:13:46,915 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:14:03,932 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:14:15,270 | server.py:125 | fit progress: (8, 1.8407012224197388, {'accuracy': 0.6215, 'data_size': 10000}, 301.60593719399185)
INFO flwr 2024-04-06 01:14:15,270 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:14:15,270 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:14:34,664 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:14:48,324 | server.py:125 | fit progress: (9, 1.674942970275879, {'accuracy': 0.7859, 'data_size': 10000}, 334.65945003999514)
INFO flwr 2024-04-06 01:14:48,324 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:14:48,324 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:15:05,409 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:15:18,379 | server.py:125 | fit progress: (10, 1.7547701597213745, {'accuracy': 0.7061, 'data_size': 10000}, 364.7145325569727)
INFO flwr 2024-04-06 01:15:18,379 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:15:18,379 | server.py:153 | FL finished in 364.71496672398644
INFO flwr 2024-04-06 01:15:18,379 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:15:18,379 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:15:18,379 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:15:18,380 | app.py:229 | app_fit: losses_centralized [(0, 2.302461862564087), (1, 2.2062582969665527), (2, 1.7682979106903076), (3, 1.706459403038025), (4, 1.8293371200561523), (5, 1.7539515495300293), (6, 1.8055572509765625), (7, 1.7979010343551636), (8, 1.8407012224197388), (9, 1.674942970275879), (10, 1.7547701597213745)]
INFO flwr 2024-04-06 01:15:18,380 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1244), (1, 0.5884), (2, 0.7166), (3, 0.7544), (4, 0.635), (5, 0.7061), (6, 0.6566), (7, 0.6637), (8, 0.6215), (9, 0.7859), (10, 0.7061)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7061
wandb:     loss 1.75477
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_010822-onswi4el
wandb: Find logs at: ./wandb/offline-run-20240406_010822-onswi4el/logs
INFO flwr 2024-04-06 01:15:21,971 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 01:22:29,644 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=686347)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=686347)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 01:22:35,490	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 01:22:36,010	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 01:22:36,561	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7c8650c568085b93.zip' (8.22MiB) to Ray cluster...
2024-04-06 01:22:36,593	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7c8650c568085b93.zip'.
INFO flwr 2024-04-06 01:22:47,822 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65093002444.0, 'memory': 141883672372.0}
INFO flwr 2024-04-06 01:22:47,823 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 01:22:47,823 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 01:22:47,841 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 01:22:47,843 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 01:22:47,843 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 01:22:47,844 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 01:22:50,350 | server.py:94 | initial parameters (loss, other metrics): 2.3026673793792725, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-06 01:22:50,351 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 01:22:50,351 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=693926)[0m 2024-04-06 01:22:53.735776: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=693926)[0m 2024-04-06 01:22:53.842351: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=693926)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=693929)[0m 2024-04-06 01:22:56.109027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=693929)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=693929)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=693927)[0m 2024-04-06 01:22:54.437063: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=693927)[0m 2024-04-06 01:22:54.533950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=693927)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=693927)[0m 2024-04-06 01:22:56.626758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 01:23:19,063 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 01:23:19,585 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 01:23:20,895 | server.py:125 | fit progress: (1, 2.2413175106048584, {'accuracy': 0.3168, 'data_size': 10000}, 30.54347103802138)
INFO flwr 2024-04-06 01:23:20,895 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 01:23:20,895 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:23:37,866 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 01:23:40,522 | server.py:125 | fit progress: (2, 1.9375091791152954, {'accuracy': 0.5357, 'data_size': 10000}, 50.17134455800988)
INFO flwr 2024-04-06 01:23:40,523 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 01:23:40,523 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:23:57,552 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 01:24:00,821 | server.py:125 | fit progress: (3, 1.9295908212661743, {'accuracy': 0.5254, 'data_size': 10000}, 70.47017035100725)
INFO flwr 2024-04-06 01:24:00,822 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 01:24:00,822 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:24:19,444 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 01:24:23,851 | server.py:125 | fit progress: (4, 1.85660982131958, {'accuracy': 0.6022, 'data_size': 10000}, 93.49987360800151)
INFO flwr 2024-04-06 01:24:23,851 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 01:24:23,852 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:24:42,110 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:24:47,059 | server.py:125 | fit progress: (5, 1.8639127016067505, {'accuracy': 0.5941, 'data_size': 10000}, 116.70793561000028)
INFO flwr 2024-04-06 01:24:47,059 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:24:47,059 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:25:04,099 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:25:09,937 | server.py:125 | fit progress: (6, 1.8318737745285034, {'accuracy': 0.631, 'data_size': 10000}, 139.58610820400645)
INFO flwr 2024-04-06 01:25:09,937 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:25:09,938 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:25:28,739 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 01:25:35,636 | server.py:125 | fit progress: (7, 2.0788886547088623, {'accuracy': 0.3817, 'data_size': 10000}, 165.28484472702257)
INFO flwr 2024-04-06 01:25:35,636 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 01:25:35,636 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:25:52,769 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:26:00,522 | server.py:125 | fit progress: (8, 1.898785948753357, {'accuracy': 0.5612, 'data_size': 10000}, 190.1711260440061)
INFO flwr 2024-04-06 01:26:00,522 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:26:00,523 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:26:19,262 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:26:27,720 | server.py:125 | fit progress: (9, 1.9358148574829102, {'accuracy': 0.5226, 'data_size': 10000}, 217.36923496000236)
INFO flwr 2024-04-06 01:26:27,721 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:26:27,721 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:26:45,017 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:26:54,441 | server.py:125 | fit progress: (10, 1.9828271865844727, {'accuracy': 0.478, 'data_size': 10000}, 244.08957599601126)
INFO flwr 2024-04-06 01:26:54,441 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:26:54,441 | server.py:153 | FL finished in 244.08994485301082
INFO flwr 2024-04-06 01:26:54,441 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:26:54,441 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:26:54,441 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:26:54,442 | app.py:229 | app_fit: losses_centralized [(0, 2.3026673793792725), (1, 2.2413175106048584), (2, 1.9375091791152954), (3, 1.9295908212661743), (4, 1.85660982131958), (5, 1.8639127016067505), (6, 1.8318737745285034), (7, 2.0788886547088623), (8, 1.898785948753357), (9, 1.9358148574829102), (10, 1.9828271865844727)]
INFO flwr 2024-04-06 01:26:54,442 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.3168), (2, 0.5357), (3, 0.5254), (4, 0.6022), (5, 0.5941), (6, 0.631), (7, 0.3817), (8, 0.5612), (9, 0.5226), (10, 0.478)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.478
wandb:     loss 1.98283
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_012229-k3l5drg7
wandb: Find logs at: ./wandb/offline-run-20240406_012229-k3l5drg7/logs
INFO flwr 2024-04-06 01:26:58,097 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 01:34:05,663 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=693927)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=693927)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 01:34:11,335	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 01:34:11,685	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 01:34:12,025	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_119f07b58b755c27.zip' (8.24MiB) to Ray cluster...
2024-04-06 01:34:12,046	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_119f07b58b755c27.zip'.
INFO flwr 2024-04-06 01:34:23,203 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 140746308608.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64605560832.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 01:34:23,203 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 01:34:23,203 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 01:34:23,218 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 01:34:23,219 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 01:34:23,220 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 01:34:23,220 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 01:34:25,859 | server.py:94 | initial parameters (loss, other metrics): 2.302553415298462, {'accuracy': 0.109, 'data_size': 10000}
INFO flwr 2024-04-06 01:34:25,859 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 01:34:25,860 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=702292)[0m 2024-04-06 01:34:28.910626: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=702292)[0m 2024-04-06 01:34:29.008298: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=702292)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=702285)[0m 2024-04-06 01:34:33.726806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=702281)[0m 2024-04-06 01:34:29.575731: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=702279)[0m 2024-04-06 01:34:29.689327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=702279)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=702285)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=702285)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=702290)[0m 2024-04-06 01:34:35.612389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=702279)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=702279)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 01:34:58,267 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 01:34:58,782 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 01:35:00,076 | server.py:125 | fit progress: (1, 2.166688919067383, {'accuracy': 0.4017, 'data_size': 10000}, 34.216727766004624)
INFO flwr 2024-04-06 01:35:00,076 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 01:35:00,077 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:35:17,926 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 01:35:20,532 | server.py:125 | fit progress: (2, 2.2025318145751953, {'accuracy': 0.2593, 'data_size': 10000}, 54.67240778499399)
INFO flwr 2024-04-06 01:35:20,532 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 01:35:20,532 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:35:40,891 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 01:35:44,119 | server.py:125 | fit progress: (3, 2.1023497581481934, {'accuracy': 0.3547, 'data_size': 10000}, 78.25978326998302)
INFO flwr 2024-04-06 01:35:44,119 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 01:35:44,120 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:36:01,861 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 01:36:06,211 | server.py:125 | fit progress: (4, 2.0905086994171143, {'accuracy': 0.3675, 'data_size': 10000}, 100.3514720670064)
INFO flwr 2024-04-06 01:36:06,211 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 01:36:06,211 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:36:22,596 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:36:27,949 | server.py:125 | fit progress: (5, 2.2020413875579834, {'accuracy': 0.2597, 'data_size': 10000}, 122.08984876598697)
INFO flwr 2024-04-06 01:36:27,950 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:36:27,950 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:36:45,144 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:36:51,107 | server.py:125 | fit progress: (6, 2.0167484283447266, {'accuracy': 0.4439, 'data_size': 10000}, 145.24710277598933)
INFO flwr 2024-04-06 01:36:51,107 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:36:51,107 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:37:09,033 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 01:37:16,019 | server.py:125 | fit progress: (7, 2.043179512023926, {'accuracy': 0.4171, 'data_size': 10000}, 170.15988110599574)
INFO flwr 2024-04-06 01:37:16,020 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 01:37:16,020 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:37:33,576 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:37:42,767 | server.py:125 | fit progress: (8, 1.9506683349609375, {'accuracy': 0.5104, 'data_size': 10000}, 196.90755719700246)
INFO flwr 2024-04-06 01:37:42,767 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:37:42,767 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:37:59,881 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:38:08,463 | server.py:125 | fit progress: (9, 2.204742193222046, {'accuracy': 0.2564, 'data_size': 10000}, 222.60337345200242)
INFO flwr 2024-04-06 01:38:08,463 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:38:08,463 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:38:26,749 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:38:35,508 | server.py:125 | fit progress: (10, 2.069700241088867, {'accuracy': 0.3914, 'data_size': 10000}, 249.64885269699153)
INFO flwr 2024-04-06 01:38:35,509 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:38:35,509 | server.py:153 | FL finished in 249.6493147549918
INFO flwr 2024-04-06 01:38:35,509 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:38:35,509 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:38:35,509 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:38:35,509 | app.py:229 | app_fit: losses_centralized [(0, 2.302553415298462), (1, 2.166688919067383), (2, 2.2025318145751953), (3, 2.1023497581481934), (4, 2.0905086994171143), (5, 2.2020413875579834), (6, 2.0167484283447266), (7, 2.043179512023926), (8, 1.9506683349609375), (9, 2.204742193222046), (10, 2.069700241088867)]
INFO flwr 2024-04-06 01:38:35,509 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.109), (1, 0.4017), (2, 0.2593), (3, 0.3547), (4, 0.3675), (5, 0.2597), (6, 0.4439), (7, 0.4171), (8, 0.5104), (9, 0.2564), (10, 0.3914)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3914
wandb:     loss 2.0697
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_013405-izfib446
wandb: Find logs at: ./wandb/offline-run-20240406_013405-izfib446/logs
INFO flwr 2024-04-06 01:38:39,334 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 01:45:45,529 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 01:45:50,465	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 01:45:50,798	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 01:45:51,085	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_00696bee44c7493a.zip' (8.26MiB) to Ray cluster...
2024-04-06 01:45:51,106	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_00696bee44c7493a.zip'.
INFO flwr 2024-04-06 01:46:02,194 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151855771034.0, 'CPU': 64.0, 'object_store_memory': 69366759014.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 01:46:02,195 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 01:46:02,195 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 01:46:02,209 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 01:46:02,210 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 01:46:02,210 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 01:46:02,210 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 01:46:04,907 | server.py:94 | initial parameters (loss, other metrics): 2.3025448322296143, {'accuracy': 0.1005, 'data_size': 10000}
INFO flwr 2024-04-06 01:46:04,908 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 01:46:04,908 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=706694)[0m 2024-04-06 01:46:08.113072: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=706694)[0m 2024-04-06 01:46:08.212458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=706694)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=706694)[0m 2024-04-06 01:46:10.341892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=706728)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=706728)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=706732)[0m 2024-04-06 01:46:08.794220: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=706732)[0m 2024-04-06 01:46:08.891494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=706732)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=706732)[0m 2024-04-06 01:46:11.264822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 01:46:36,331 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 01:46:36,887 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 01:46:38,182 | server.py:125 | fit progress: (1, 2.322324514389038, {'accuracy': 0.1047, 'data_size': 10000}, 33.27362898501451)
INFO flwr 2024-04-06 01:46:38,182 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 01:46:38,182 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:46:57,150 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 01:46:59,797 | server.py:125 | fit progress: (2, 2.0705809593200684, {'accuracy': 0.3898, 'data_size': 10000}, 54.88880645102472)
INFO flwr 2024-04-06 01:46:59,797 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 01:46:59,797 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:47:16,699 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 01:47:19,986 | server.py:125 | fit progress: (3, 2.2017810344696045, {'accuracy': 0.2559, 'data_size': 10000}, 75.07798916500178)
INFO flwr 2024-04-06 01:47:19,986 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 01:47:19,986 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:47:38,382 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 01:47:42,688 | server.py:125 | fit progress: (4, 2.0887703895568848, {'accuracy': 0.3691, 'data_size': 10000}, 97.77958365500672)
INFO flwr 2024-04-06 01:47:42,688 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 01:47:42,688 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:47:59,499 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:48:04,411 | server.py:125 | fit progress: (5, 2.1944284439086914, {'accuracy': 0.2632, 'data_size': 10000}, 119.5026557870151)
INFO flwr 2024-04-06 01:48:04,411 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:48:04,411 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:48:21,689 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:48:27,873 | server.py:125 | fit progress: (6, 2.1282918453216553, {'accuracy': 0.3323, 'data_size': 10000}, 142.96457061200636)
INFO flwr 2024-04-06 01:48:27,873 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:48:27,873 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:48:46,440 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 01:48:53,580 | server.py:125 | fit progress: (7, 2.1806018352508545, {'accuracy': 0.2795, 'data_size': 10000}, 168.67209586000536)
INFO flwr 2024-04-06 01:48:53,580 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 01:48:53,581 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:49:11,578 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:49:19,791 | server.py:125 | fit progress: (8, 2.198892593383789, {'accuracy': 0.261, 'data_size': 10000}, 194.8830712500203)
INFO flwr 2024-04-06 01:49:19,791 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:49:19,792 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:49:36,268 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:49:45,158 | server.py:125 | fit progress: (9, 2.2860872745513916, {'accuracy': 0.1744, 'data_size': 10000}, 220.25008710002294)
INFO flwr 2024-04-06 01:49:45,158 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:49:45,159 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:50:03,439 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:50:12,576 | server.py:125 | fit progress: (10, 2.3300414085388184, {'accuracy': 0.1311, 'data_size': 10000}, 247.66846586501924)
INFO flwr 2024-04-06 01:50:12,577 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:50:12,577 | server.py:153 | FL finished in 247.66884628101252
INFO flwr 2024-04-06 01:50:12,577 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:50:12,577 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:50:12,577 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:50:12,577 | app.py:229 | app_fit: losses_centralized [(0, 2.3025448322296143), (1, 2.322324514389038), (2, 2.0705809593200684), (3, 2.2017810344696045), (4, 2.0887703895568848), (5, 2.1944284439086914), (6, 2.1282918453216553), (7, 2.1806018352508545), (8, 2.198892593383789), (9, 2.2860872745513916), (10, 2.3300414085388184)]
INFO flwr 2024-04-06 01:50:12,577 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1005), (1, 0.1047), (2, 0.3898), (3, 0.2559), (4, 0.3691), (5, 0.2632), (6, 0.3323), (7, 0.2795), (8, 0.261), (9, 0.1744), (10, 0.1311)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1311
wandb:     loss 2.33004
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_014545-ukug2hrj
wandb: Find logs at: ./wandb/offline-run-20240406_014545-ukug2hrj/logs
INFO flwr 2024-04-06 01:50:16,164 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 01:57:21,844 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=706691)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=706691)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 01:57:26,831	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 01:57:27,274	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 01:57:27,651	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0cf4153b4798a6d9.zip' (8.28MiB) to Ray cluster...
2024-04-06 01:57:27,672	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0cf4153b4798a6d9.zip'.
INFO flwr 2024-04-06 01:57:38,501 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151929968436.0, 'node:__internal_head__': 1.0, 'object_store_memory': 69398557900.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 01:57:38,501 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 01:57:38,501 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 01:57:38,516 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 01:57:38,517 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 01:57:38,517 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 01:57:38,517 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 01:57:40,979 | server.py:94 | initial parameters (loss, other metrics): 2.3026821613311768, {'accuracy': 0.089, 'data_size': 10000}
INFO flwr 2024-04-06 01:57:40,979 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 01:57:40,980 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=714218)[0m 2024-04-06 01:57:44.406841: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=714218)[0m 2024-04-06 01:57:44.503941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=714218)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=714219)[0m 2024-04-06 01:57:46.590978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=714219)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=714219)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=714221)[0m 2024-04-06 01:57:44.890739: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=714221)[0m 2024-04-06 01:57:45.033783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=714221)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=714221)[0m 2024-04-06 01:57:47.121803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 01:58:10,230 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 01:58:10,767 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 01:58:12,003 | server.py:125 | fit progress: (1, 2.2304883003234863, {'accuracy': 0.1683, 'data_size': 10000}, 31.02356698299991)
INFO flwr 2024-04-06 01:58:12,003 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 01:58:12,004 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:58:28,598 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 01:58:31,178 | server.py:125 | fit progress: (2, 2.2406270503997803, {'accuracy': 0.2202, 'data_size': 10000}, 50.19804402202135)
INFO flwr 2024-04-06 01:58:31,178 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 01:58:31,178 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:58:47,835 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 01:58:51,039 | server.py:125 | fit progress: (3, 2.0788323879241943, {'accuracy': 0.3974, 'data_size': 10000}, 70.0596066529979)
INFO flwr 2024-04-06 01:58:51,039 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 01:58:51,039 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:59:08,632 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 01:59:12,994 | server.py:125 | fit progress: (4, 2.068680763244629, {'accuracy': 0.384, 'data_size': 10000}, 92.01421963400207)
INFO flwr 2024-04-06 01:59:12,994 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 01:59:12,994 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:59:30,452 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:59:35,419 | server.py:125 | fit progress: (5, 2.212613344192505, {'accuracy': 0.252, 'data_size': 10000}, 114.43989019299624)
INFO flwr 2024-04-06 01:59:35,420 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:59:35,420 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:59:52,842 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:59:59,001 | server.py:125 | fit progress: (6, 2.2496392726898193, {'accuracy': 0.2113, 'data_size': 10000}, 138.02155969801242)
INFO flwr 2024-04-06 01:59:59,001 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:59:59,001 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:00:17,188 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:00:23,974 | server.py:125 | fit progress: (7, 2.1143176555633545, {'accuracy': 0.3458, 'data_size': 10000}, 162.99449490802363)
INFO flwr 2024-04-06 02:00:23,974 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:00:23,974 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:00:40,758 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:00:48,580 | server.py:125 | fit progress: (8, 2.1656877994537354, {'accuracy': 0.295, 'data_size': 10000}, 187.6000508230063)
INFO flwr 2024-04-06 02:00:48,580 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:00:48,580 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:01:05,177 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:01:13,570 | server.py:125 | fit progress: (9, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 212.59087057600846)
INFO flwr 2024-04-06 02:01:13,571 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:01:13,571 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:01:31,411 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:01:40,910 | server.py:125 | fit progress: (10, 2.365341901779175, {'accuracy': 0.0958, 'data_size': 10000}, 239.93032005502027)
INFO flwr 2024-04-06 02:01:40,910 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:01:40,910 | server.py:153 | FL finished in 239.93078823201358
INFO flwr 2024-04-06 02:01:40,910 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:01:40,911 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:01:40,911 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:01:40,911 | app.py:229 | app_fit: losses_centralized [(0, 2.3026821613311768), (1, 2.2304883003234863), (2, 2.2406270503997803), (3, 2.0788323879241943), (4, 2.068680763244629), (5, 2.212613344192505), (6, 2.2496392726898193), (7, 2.1143176555633545), (8, 2.1656877994537354), (9, 2.365341901779175), (10, 2.365341901779175)]
INFO flwr 2024-04-06 02:01:40,911 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.089), (1, 0.1683), (2, 0.2202), (3, 0.3974), (4, 0.384), (5, 0.252), (6, 0.2113), (7, 0.3458), (8, 0.295), (9, 0.0958), (10, 0.0958)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0958
wandb:     loss 2.36534
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_015721-k1dnscud
wandb: Find logs at: ./wandb/offline-run-20240406_015721-k1dnscud/logs
INFO flwr 2024-04-06 02:01:44,396 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 02:08:52,180 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=714217)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=714217)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 02:08:59,280	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 02:08:59,732	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 02:09:00,197	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c61b1c93d1b3eb0c.zip' (8.31MiB) to Ray cluster...
2024-04-06 02:09:00,226	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c61b1c93d1b3eb0c.zip'.
INFO flwr 2024-04-06 02:09:11,646 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 143126471680.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65625630720.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 02:09:11,647 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 02:09:11,647 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 02:09:11,666 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 02:09:11,667 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 02:09:11,667 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 02:09:11,667 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 02:09:14,286 | server.py:94 | initial parameters (loss, other metrics): 2.302520275115967, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-06 02:09:14,286 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 02:09:14,286 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=722602)[0m 2024-04-06 02:09:18.242238: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=722602)[0m 2024-04-06 02:09:18.340917: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=722602)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=722598)[0m 2024-04-06 02:09:20.705214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=722598)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=722598)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=722593)[0m 2024-04-06 02:09:18.585557: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=722593)[0m 2024-04-06 02:09:18.678496: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=722593)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=722601)[0m 2024-04-06 02:09:21.412174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=722606)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=722606)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 02:09:58,180 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 02:09:58,859 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 02:10:00,322 | server.py:125 | fit progress: (1, 2.301856756210327, {'accuracy': 0.1009, 'data_size': 10000}, 46.035380431014346)
INFO flwr 2024-04-06 02:10:00,322 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 02:10:00,322 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:10:34,826 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 02:10:37,576 | server.py:125 | fit progress: (2, 2.3009819984436035, {'accuracy': 0.1009, 'data_size': 10000}, 83.28958399000112)
INFO flwr 2024-04-06 02:10:37,576 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 02:10:37,576 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:11:10,104 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 02:11:13,361 | server.py:125 | fit progress: (3, 2.2977213859558105, {'accuracy': 0.1009, 'data_size': 10000}, 119.07444590100204)
INFO flwr 2024-04-06 02:11:13,361 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 02:11:13,361 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:11:46,316 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 02:11:50,855 | server.py:125 | fit progress: (4, 2.2915282249450684, {'accuracy': 0.1009, 'data_size': 10000}, 156.56920864601852)
INFO flwr 2024-04-06 02:11:50,856 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 02:11:50,856 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:12:19,243 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 02:12:25,172 | server.py:125 | fit progress: (5, 2.286682605743408, {'accuracy': 0.1927, 'data_size': 10000}, 190.88556851199246)
INFO flwr 2024-04-06 02:12:25,172 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 02:12:25,172 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:12:59,080 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 02:13:05,370 | server.py:125 | fit progress: (6, 2.2680983543395996, {'accuracy': 0.187, 'data_size': 10000}, 231.08332264001365)
INFO flwr 2024-04-06 02:13:05,370 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 02:13:05,370 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:13:34,807 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:13:41,880 | server.py:125 | fit progress: (7, 2.233865261077881, {'accuracy': 0.1922, 'data_size': 10000}, 267.5939717250003)
INFO flwr 2024-04-06 02:13:41,880 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:13:41,881 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:14:14,651 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:14:22,166 | server.py:125 | fit progress: (8, 2.1906089782714844, {'accuracy': 0.3189, 'data_size': 10000}, 307.87988408401725)
INFO flwr 2024-04-06 02:14:22,166 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:14:22,167 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:14:55,894 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:15:04,038 | server.py:125 | fit progress: (9, 2.1292881965637207, {'accuracy': 0.3419, 'data_size': 10000}, 349.7519351289957)
INFO flwr 2024-04-06 02:15:04,038 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:15:04,039 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:15:35,360 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:15:45,895 | server.py:125 | fit progress: (10, 2.0816071033477783, {'accuracy': 0.3697, 'data_size': 10000}, 391.60923105699476)
INFO flwr 2024-04-06 02:15:45,896 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:15:45,896 | server.py:153 | FL finished in 391.6096531340154
INFO flwr 2024-04-06 02:15:45,896 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:15:45,896 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:15:45,896 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:15:45,896 | app.py:229 | app_fit: losses_centralized [(0, 2.302520275115967), (1, 2.301856756210327), (2, 2.3009819984436035), (3, 2.2977213859558105), (4, 2.2915282249450684), (5, 2.286682605743408), (6, 2.2680983543395996), (7, 2.233865261077881), (8, 2.1906089782714844), (9, 2.1292881965637207), (10, 2.0816071033477783)]
INFO flwr 2024-04-06 02:15:45,896 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.1009), (2, 0.1009), (3, 0.1009), (4, 0.1009), (5, 0.1927), (6, 0.187), (7, 0.1922), (8, 0.3189), (9, 0.3419), (10, 0.3697)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3697
wandb:     loss 2.08161
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_020851-crs1nnc8
wandb: Find logs at: ./wandb/offline-run-20240406_020851-crs1nnc8/logs
INFO flwr 2024-04-06 02:15:49,397 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 02:22:56,627 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=722593)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=722593)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 02:23:01,837	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 02:23:02,199	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 02:23:02,484	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_05564f158f47ae92.zip' (8.33MiB) to Ray cluster...
2024-04-06 02:23:02,511	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_05564f158f47ae92.zip'.
INFO flwr 2024-04-06 02:23:13,949 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 133630571930.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61555959398.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 02:23:13,950 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 02:23:13,950 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 02:23:13,972 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 02:23:13,974 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 02:23:13,974 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 02:23:13,974 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 02:23:16,427 | server.py:94 | initial parameters (loss, other metrics): 2.302640914916992, {'accuracy': 0.1017, 'data_size': 10000}
INFO flwr 2024-04-06 02:23:16,436 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 02:23:16,436 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=730132)[0m 2024-04-06 02:23:20.173442: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=730130)[0m 2024-04-06 02:23:20.248665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=730130)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=730132)[0m 2024-04-06 02:23:24.404726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=730129)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=730129)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=730127)[0m 2024-04-06 02:23:20.677952: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=730127)[0m 2024-04-06 02:23:20.772799: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=730127)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=730127)[0m 2024-04-06 02:23:25.163297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=730131)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=730131)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
DEBUG flwr 2024-04-06 02:24:06,151 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 02:24:06,671 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 02:24:07,963 | server.py:125 | fit progress: (1, 2.1577975749969482, {'accuracy': 0.4597, 'data_size': 10000}, 51.5273314260121)
INFO flwr 2024-04-06 02:24:07,964 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 02:24:07,964 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:24:39,077 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 02:24:41,698 | server.py:125 | fit progress: (2, 1.6641947031021118, {'accuracy': 0.8238, 'data_size': 10000}, 85.26162329601357)
INFO flwr 2024-04-06 02:24:41,698 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 02:24:41,698 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:25:12,926 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 02:25:16,213 | server.py:125 | fit progress: (3, 1.5572750568389893, {'accuracy': 0.9053, 'data_size': 10000}, 119.77644060002058)
INFO flwr 2024-04-06 02:25:16,213 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 02:25:16,213 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:25:46,681 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 02:25:51,179 | server.py:125 | fit progress: (4, 1.5655580759048462, {'accuracy': 0.8973, 'data_size': 10000}, 154.7430670070171)
INFO flwr 2024-04-06 02:25:51,179 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 02:25:51,180 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:26:27,060 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 02:26:32,066 | server.py:125 | fit progress: (5, 1.5443625450134277, {'accuracy': 0.9158, 'data_size': 10000}, 195.63015780202113)
INFO flwr 2024-04-06 02:26:32,067 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 02:26:32,067 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:27:05,801 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 02:27:11,620 | server.py:125 | fit progress: (6, 1.561834692955017, {'accuracy': 0.8989, 'data_size': 10000}, 235.18427388899727)
INFO flwr 2024-04-06 02:27:11,621 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 02:27:11,621 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:27:41,804 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:27:48,544 | server.py:125 | fit progress: (7, 1.5439423322677612, {'accuracy': 0.9173, 'data_size': 10000}, 272.1074284000206)
INFO flwr 2024-04-06 02:27:48,544 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:27:48,544 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:28:16,344 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:28:23,780 | server.py:125 | fit progress: (8, 1.5560413599014282, {'accuracy': 0.9045, 'data_size': 10000}, 307.34361882600933)
INFO flwr 2024-04-06 02:28:23,780 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:28:23,780 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:28:55,049 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:29:03,140 | server.py:125 | fit progress: (9, 1.546636700630188, {'accuracy': 0.9141, 'data_size': 10000}, 346.7034836030216)
INFO flwr 2024-04-06 02:29:03,140 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:29:03,140 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:29:30,276 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:29:39,139 | server.py:125 | fit progress: (10, 1.5513687133789062, {'accuracy': 0.9096, 'data_size': 10000}, 382.7033151460055)
INFO flwr 2024-04-06 02:29:39,140 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:29:39,140 | server.py:153 | FL finished in 382.7037157790037
INFO flwr 2024-04-06 02:29:39,140 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:29:39,140 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:29:39,140 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:29:39,140 | app.py:229 | app_fit: losses_centralized [(0, 2.302640914916992), (1, 2.1577975749969482), (2, 1.6641947031021118), (3, 1.5572750568389893), (4, 1.5655580759048462), (5, 1.5443625450134277), (6, 1.561834692955017), (7, 1.5439423322677612), (8, 1.5560413599014282), (9, 1.546636700630188), (10, 1.5513687133789062)]
INFO flwr 2024-04-06 02:29:39,140 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1017), (1, 0.4597), (2, 0.8238), (3, 0.9053), (4, 0.8973), (5, 0.9158), (6, 0.8989), (7, 0.9173), (8, 0.9045), (9, 0.9141), (10, 0.9096)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9096
wandb:     loss 1.55137
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_022256-nw6yb972
wandb: Find logs at: ./wandb/offline-run-20240406_022256-nw6yb972/logs
INFO flwr 2024-04-06 02:29:42,729 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 02:36:49,881 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=730128)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=730128)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
2024-04-06 02:36:55,977	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 02:36:56,317	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 02:36:56,604	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_312d9d97dd639e9a.zip' (8.35MiB) to Ray cluster...
2024-04-06 02:36:56,638	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_312d9d97dd639e9a.zip'.
INFO flwr 2024-04-06 02:37:07,719 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 64214887219.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 139834736845.0}
INFO flwr 2024-04-06 02:37:07,719 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 02:37:07,719 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 02:37:07,732 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 02:37:07,733 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 02:37:07,733 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 02:37:07,733 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 02:37:10,743 | server.py:94 | initial parameters (loss, other metrics): 2.302516222000122, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-06 02:37:10,748 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 02:37:10,749 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=738577)[0m 2024-04-06 02:37:15.870718: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=738574)[0m 2024-04-06 02:37:15.982879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=738574)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=738569)[0m 2024-04-06 02:37:18.302560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=738572)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=738572)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=738575)[0m 2024-04-06 02:37:16.097757: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=738575)[0m 2024-04-06 02:37:16.198310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=738575)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=738571)[0m 2024-04-06 02:37:18.963802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 02:37:55,267 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 02:37:55,766 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 02:37:57,105 | server.py:125 | fit progress: (1, 2.121737003326416, {'accuracy': 0.4785, 'data_size': 10000}, 46.355998472019564)
INFO flwr 2024-04-06 02:37:57,105 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 02:37:57,106 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:38:28,140 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 02:38:30,840 | server.py:125 | fit progress: (2, 1.9011269807815552, {'accuracy': 0.5548, 'data_size': 10000}, 80.09108947301866)
INFO flwr 2024-04-06 02:38:30,840 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 02:38:30,840 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:39:06,339 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 02:39:09,575 | server.py:125 | fit progress: (3, 1.713822841644287, {'accuracy': 0.7534, 'data_size': 10000}, 118.8259770910081)
INFO flwr 2024-04-06 02:39:09,575 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 02:39:09,575 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:39:40,101 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 02:39:44,437 | server.py:125 | fit progress: (4, 1.814808964729309, {'accuracy': 0.6475, 'data_size': 10000}, 153.6879599460226)
INFO flwr 2024-04-06 02:39:44,437 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 02:39:44,437 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:40:17,967 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 02:40:22,952 | server.py:125 | fit progress: (5, 1.7453978061676025, {'accuracy': 0.7154, 'data_size': 10000}, 192.20283648901386)
INFO flwr 2024-04-06 02:40:22,952 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 02:40:22,952 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:40:55,555 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 02:41:01,657 | server.py:125 | fit progress: (6, 1.7360941171646118, {'accuracy': 0.7258, 'data_size': 10000}, 230.90768047000165)
INFO flwr 2024-04-06 02:41:01,657 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 02:41:01,657 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:41:33,517 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:41:41,908 | server.py:125 | fit progress: (7, 1.869168996810913, {'accuracy': 0.592, 'data_size': 10000}, 271.1593150140252)
INFO flwr 2024-04-06 02:41:41,908 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:41:41,909 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:42:11,193 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:42:18,925 | server.py:125 | fit progress: (8, 1.9762566089630127, {'accuracy': 0.4847, 'data_size': 10000}, 308.1763295350247)
INFO flwr 2024-04-06 02:42:18,925 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:42:18,926 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:42:54,962 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:43:02,815 | server.py:125 | fit progress: (9, 1.8832398653030396, {'accuracy': 0.577, 'data_size': 10000}, 352.06602776702493)
INFO flwr 2024-04-06 02:43:02,815 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:43:02,815 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:43:35,631 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:43:44,925 | server.py:125 | fit progress: (10, 1.8195831775665283, {'accuracy': 0.6412, 'data_size': 10000}, 394.17647376901004)
INFO flwr 2024-04-06 02:43:44,926 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:43:44,926 | server.py:153 | FL finished in 394.1770440550172
INFO flwr 2024-04-06 02:43:44,926 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:43:44,926 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:43:44,926 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:43:44,926 | app.py:229 | app_fit: losses_centralized [(0, 2.302516222000122), (1, 2.121737003326416), (2, 1.9011269807815552), (3, 1.713822841644287), (4, 1.814808964729309), (5, 1.7453978061676025), (6, 1.7360941171646118), (7, 1.869168996810913), (8, 1.9762566089630127), (9, 1.8832398653030396), (10, 1.8195831775665283)]
INFO flwr 2024-04-06 02:43:44,926 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.4785), (2, 0.5548), (3, 0.7534), (4, 0.6475), (5, 0.7154), (6, 0.7258), (7, 0.592), (8, 0.4847), (9, 0.577), (10, 0.6412)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6412
wandb:     loss 1.81958
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_023649-wdmxj6mc
wandb: Find logs at: ./wandb/offline-run-20240406_023649-wdmxj6mc/logs
INFO flwr 2024-04-06 02:43:48,490 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 02:50:55,093 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=738570)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=738570)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 02:50:59,666	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 02:51:00,064	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 02:51:00,416	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c0ac7c9abdec809f.zip' (8.38MiB) to Ray cluster...
2024-04-06 02:51:00,435	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c0ac7c9abdec809f.zip'.
INFO flwr 2024-04-06 02:51:11,438 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 66863423078.0, 'node:10.20.240.18': 1.0, 'memory': 146014653850.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 02:51:11,438 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 02:51:11,438 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 02:51:11,452 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 02:51:11,453 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 02:51:11,453 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 02:51:11,453 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 02:51:14,017 | server.py:94 | initial parameters (loss, other metrics): 2.302703619003296, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-06 02:51:14,025 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 02:51:14,027 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=746670)[0m 2024-04-06 02:51:17.196161: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=746670)[0m 2024-04-06 02:51:17.262632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=746670)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=746670)[0m 2024-04-06 02:51:21.313839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=746663)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=746663)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=746673)[0m 2024-04-06 02:51:18.932934: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=746667)[0m 2024-04-06 02:51:18.946428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=746667)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=746664)[0m 2024-04-06 02:51:21.734368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 02:51:56,169 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 02:51:56,702 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 02:51:57,931 | server.py:125 | fit progress: (1, 2.127751111984253, {'accuracy': 0.3668, 'data_size': 10000}, 43.904772337991744)
INFO flwr 2024-04-06 02:51:57,932 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 02:51:57,932 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:52:30,139 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 02:52:32,773 | server.py:125 | fit progress: (2, 1.9260281324386597, {'accuracy': 0.5337, 'data_size': 10000}, 78.7460577939928)
INFO flwr 2024-04-06 02:52:32,773 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 02:52:32,773 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:53:02,330 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 02:53:05,527 | server.py:125 | fit progress: (3, 2.3406026363372803, {'accuracy': 0.1169, 'data_size': 10000}, 111.50011255699792)
INFO flwr 2024-04-06 02:53:05,527 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 02:53:05,527 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:53:36,531 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 02:53:40,887 | server.py:125 | fit progress: (4, 1.9884748458862305, {'accuracy': 0.4708, 'data_size': 10000}, 146.8601058909844)
INFO flwr 2024-04-06 02:53:40,887 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 02:53:40,887 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:54:09,127 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 02:54:14,325 | server.py:125 | fit progress: (5, 1.9835624694824219, {'accuracy': 0.4787, 'data_size': 10000}, 180.29861124398303)
INFO flwr 2024-04-06 02:54:14,325 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 02:54:14,326 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:54:51,926 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 02:54:58,834 | server.py:125 | fit progress: (6, 1.9274660348892212, {'accuracy': 0.532, 'data_size': 10000}, 224.80763925198698)
INFO flwr 2024-04-06 02:54:58,834 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 02:54:58,835 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:55:31,005 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:55:38,392 | server.py:125 | fit progress: (7, 2.0482730865478516, {'accuracy': 0.4118, 'data_size': 10000}, 264.36533263899037)
INFO flwr 2024-04-06 02:55:38,392 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:55:38,393 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:56:07,685 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:56:15,335 | server.py:125 | fit progress: (8, 1.8806647062301636, {'accuracy': 0.5804, 'data_size': 10000}, 301.3085761560069)
INFO flwr 2024-04-06 02:56:15,335 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:56:15,336 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:56:47,179 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:56:56,865 | server.py:125 | fit progress: (9, 1.9636365175247192, {'accuracy': 0.4969, 'data_size': 10000}, 342.83895863298676)
INFO flwr 2024-04-06 02:56:56,866 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:56:56,866 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:57:28,080 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:57:37,211 | server.py:125 | fit progress: (10, 2.061368942260742, {'accuracy': 0.3997, 'data_size': 10000}, 383.1843841809896)
INFO flwr 2024-04-06 02:57:37,211 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:57:37,211 | server.py:153 | FL finished in 383.1847690749855
INFO flwr 2024-04-06 02:57:37,211 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:57:37,212 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:57:37,212 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:57:37,212 | app.py:229 | app_fit: losses_centralized [(0, 2.302703619003296), (1, 2.127751111984253), (2, 1.9260281324386597), (3, 2.3406026363372803), (4, 1.9884748458862305), (5, 1.9835624694824219), (6, 1.9274660348892212), (7, 2.0482730865478516), (8, 1.8806647062301636), (9, 1.9636365175247192), (10, 2.061368942260742)]
INFO flwr 2024-04-06 02:57:37,212 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.3668), (2, 0.5337), (3, 0.1169), (4, 0.4708), (5, 0.4787), (6, 0.532), (7, 0.4118), (8, 0.5804), (9, 0.4969), (10, 0.3997)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3997
wandb:     loss 2.06137
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_025054-qvfinezt
wandb: Find logs at: ./wandb/offline-run-20240406_025054-qvfinezt/logs
INFO flwr 2024-04-06 02:57:40,781 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 03:04:48,012 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=746665)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=746665)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 03:04:56,125	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 03:04:56,556	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 03:04:56,972	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8428d39a8e9dca62.zip' (8.40MiB) to Ray cluster...
2024-04-06 03:04:56,992	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8428d39a8e9dca62.zip'.
INFO flwr 2024-04-06 03:05:07,773 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 68891123712.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 150745955328.0}
INFO flwr 2024-04-06 03:05:07,773 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 03:05:07,773 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 03:05:07,793 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 03:05:07,794 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 03:05:07,795 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 03:05:07,795 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 03:05:10,424 | server.py:94 | initial parameters (loss, other metrics): 2.3026130199432373, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-06 03:05:10,431 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 03:05:10,434 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=752024)[0m 2024-04-06 03:05:14.831029: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=752024)[0m 2024-04-06 03:05:14.936519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=752024)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=752029)[0m 2024-04-06 03:05:18.445758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=752028)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=752028)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=752027)[0m 2024-04-06 03:05:15.035151: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=752027)[0m 2024-04-06 03:05:15.113365: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=752027)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=752032)[0m 2024-04-06 03:05:18.449031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 03:06:11,366 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 03:06:11,900 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 03:06:13,164 | server.py:125 | fit progress: (1, 2.1847469806671143, {'accuracy': 0.2936, 'data_size': 10000}, 62.730538062984124)
INFO flwr 2024-04-06 03:06:13,164 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 03:06:13,164 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:06:42,608 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 03:06:45,365 | server.py:125 | fit progress: (2, 2.078094244003296, {'accuracy': 0.3631, 'data_size': 10000}, 94.93206842298969)
INFO flwr 2024-04-06 03:06:45,366 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 03:06:45,366 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:07:19,269 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 03:07:22,868 | server.py:125 | fit progress: (3, 2.0618245601654053, {'accuracy': 0.3999, 'data_size': 10000}, 132.43447087399545)
INFO flwr 2024-04-06 03:07:22,868 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 03:07:22,868 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:07:54,981 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 03:07:59,917 | server.py:125 | fit progress: (4, 2.2056033611297607, {'accuracy': 0.2554, 'data_size': 10000}, 169.48374892398715)
INFO flwr 2024-04-06 03:07:59,917 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 03:07:59,918 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:08:32,324 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 03:08:37,683 | server.py:125 | fit progress: (5, 2.0998334884643555, {'accuracy': 0.3607, 'data_size': 10000}, 207.24937757800217)
INFO flwr 2024-04-06 03:08:37,683 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 03:08:37,683 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:09:12,107 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 03:09:19,037 | server.py:125 | fit progress: (6, 2.160079002380371, {'accuracy': 0.3029, 'data_size': 10000}, 248.6035217519966)
INFO flwr 2024-04-06 03:09:19,037 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 03:09:19,037 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:09:50,515 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 03:09:57,870 | server.py:125 | fit progress: (7, 2.0746965408325195, {'accuracy': 0.3865, 'data_size': 10000}, 287.4364815369772)
INFO flwr 2024-04-06 03:09:57,870 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 03:09:57,870 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:10:26,933 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 03:10:34,916 | server.py:125 | fit progress: (8, 2.02469801902771, {'accuracy': 0.4398, 'data_size': 10000}, 324.482386468997)
INFO flwr 2024-04-06 03:10:34,916 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 03:10:34,916 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:11:05,767 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 03:11:15,403 | server.py:125 | fit progress: (9, 2.1661055088043213, {'accuracy': 0.295, 'data_size': 10000}, 364.96975954898517)
INFO flwr 2024-04-06 03:11:15,403 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 03:11:15,404 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:11:48,688 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 03:11:59,012 | server.py:125 | fit progress: (10, 2.250843048095703, {'accuracy': 0.2103, 'data_size': 10000}, 408.5781773029885)
INFO flwr 2024-04-06 03:11:59,012 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 03:11:59,012 | server.py:153 | FL finished in 408.5785886599915
INFO flwr 2024-04-06 03:11:59,012 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 03:11:59,012 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 03:11:59,012 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 03:11:59,012 | app.py:229 | app_fit: losses_centralized [(0, 2.3026130199432373), (1, 2.1847469806671143), (2, 2.078094244003296), (3, 2.0618245601654053), (4, 2.2056033611297607), (5, 2.0998334884643555), (6, 2.160079002380371), (7, 2.0746965408325195), (8, 2.02469801902771), (9, 2.1661055088043213), (10, 2.250843048095703)]
INFO flwr 2024-04-06 03:11:59,013 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.2936), (2, 0.3631), (3, 0.3999), (4, 0.2554), (5, 0.3607), (6, 0.3029), (7, 0.3865), (8, 0.4398), (9, 0.295), (10, 0.2103)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2103
wandb:     loss 2.25084
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_030447-acz1gpn3
wandb: Find logs at: ./wandb/offline-run-20240406_030447-acz1gpn3/logs
INFO flwr 2024-04-06 03:12:02,633 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 03:19:08,996 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=752029)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=752029)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 03:19:14,691	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 03:19:15,588	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 03:19:15,862	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b9b3afca6a66071a.zip' (8.42MiB) to Ray cluster...
2024-04-06 03:19:15,897	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b9b3afca6a66071a.zip'.
INFO flwr 2024-04-06 03:19:26,767 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 68769572044.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 150462334772.0}
INFO flwr 2024-04-06 03:19:26,768 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 03:19:26,768 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 03:19:26,783 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 03:19:26,784 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 03:19:26,784 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 03:19:26,785 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 03:19:29,562 | server.py:94 | initial parameters (loss, other metrics): 2.3025851249694824, {'accuracy': 0.1031, 'data_size': 10000}
INFO flwr 2024-04-06 03:19:29,562 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 03:19:29,563 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=760184)[0m 2024-04-06 03:19:32.348801: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=760184)[0m 2024-04-06 03:19:32.450009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=760184)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=760186)[0m 2024-04-06 03:19:34.658669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=760190)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=760190)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=760180)[0m 2024-04-06 03:19:33.359614: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=760180)[0m 2024-04-06 03:19:33.454182: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=760180)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=760180)[0m 2024-04-06 03:19:35.460775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 03:20:12,955 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 03:20:13,461 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 03:20:14,692 | server.py:125 | fit progress: (1, 2.365015745162964, {'accuracy': 0.0958, 'data_size': 10000}, 45.12983493000502)
INFO flwr 2024-04-06 03:20:14,693 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 03:20:14,693 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:20:45,859 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 03:20:48,590 | server.py:125 | fit progress: (2, 2.081523895263672, {'accuracy': 0.3702, 'data_size': 10000}, 79.02786120798555)
INFO flwr 2024-04-06 03:20:48,591 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 03:20:48,591 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:21:18,605 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 03:21:21,844 | server.py:125 | fit progress: (3, 1.9812185764312744, {'accuracy': 0.4741, 'data_size': 10000}, 112.2818745179975)
INFO flwr 2024-04-06 03:21:21,845 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 03:21:21,845 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:21:50,951 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 03:21:55,210 | server.py:125 | fit progress: (4, 2.1285006999969482, {'accuracy': 0.3239, 'data_size': 10000}, 145.64740564499516)
INFO flwr 2024-04-06 03:21:55,210 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 03:21:55,210 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:22:26,733 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 03:22:31,726 | server.py:125 | fit progress: (5, 2.1684505939483643, {'accuracy': 0.2916, 'data_size': 10000}, 182.16369009698974)
INFO flwr 2024-04-06 03:22:31,727 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 03:22:31,727 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:23:04,604 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 03:23:11,212 | server.py:125 | fit progress: (6, 2.13686203956604, {'accuracy': 0.3234, 'data_size': 10000}, 221.64926902099978)
INFO flwr 2024-04-06 03:23:11,212 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 03:23:11,212 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:23:44,321 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 03:23:51,620 | server.py:125 | fit progress: (7, 2.082026481628418, {'accuracy': 0.3787, 'data_size': 10000}, 262.05732698299107)
INFO flwr 2024-04-06 03:23:51,620 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 03:23:51,620 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:24:24,012 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 03:24:32,686 | server.py:125 | fit progress: (8, 2.316777229309082, {'accuracy': 0.1444, 'data_size': 10000}, 303.12306331298896)
INFO flwr 2024-04-06 03:24:32,686 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 03:24:32,686 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:25:05,215 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 03:25:14,619 | server.py:125 | fit progress: (9, 2.2408926486968994, {'accuracy': 0.2202, 'data_size': 10000}, 345.0566325089894)
INFO flwr 2024-04-06 03:25:14,619 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 03:25:14,620 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:25:49,147 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 03:25:59,765 | server.py:125 | fit progress: (10, 2.093153953552246, {'accuracy': 0.3678, 'data_size': 10000}, 390.202199516003)
INFO flwr 2024-04-06 03:25:59,765 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 03:25:59,765 | server.py:153 | FL finished in 390.20258416299475
INFO flwr 2024-04-06 03:25:59,765 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 03:25:59,765 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 03:25:59,765 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 03:25:59,766 | app.py:229 | app_fit: losses_centralized [(0, 2.3025851249694824), (1, 2.365015745162964), (2, 2.081523895263672), (3, 1.9812185764312744), (4, 2.1285006999969482), (5, 2.1684505939483643), (6, 2.13686203956604), (7, 2.082026481628418), (8, 2.316777229309082), (9, 2.2408926486968994), (10, 2.093153953552246)]
INFO flwr 2024-04-06 03:25:59,766 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1031), (1, 0.0958), (2, 0.3702), (3, 0.4741), (4, 0.3239), (5, 0.2916), (6, 0.3234), (7, 0.3787), (8, 0.1444), (9, 0.2202), (10, 0.3678)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3678
wandb:     loss 2.09315
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_031908-qup5juro
wandb: Find logs at: ./wandb/offline-run-20240406_031908-qup5juro/logs
INFO flwr 2024-04-06 03:26:03,303 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 03:33:08,901 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=760182)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=760182)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 03:33:13,721	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 03:33:14,119	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 03:33:14,555	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fb4c07dea3b226d5.zip' (8.44MiB) to Ray cluster...
2024-04-06 03:33:14,576	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fb4c07dea3b226d5.zip'.
INFO flwr 2024-04-06 03:33:25,397 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 150512527975.0, 'node:__internal_head__': 1.0, 'object_store_memory': 68791083417.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 03:33:25,397 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 03:33:25,397 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 03:33:25,413 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 03:33:25,414 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 03:33:25,415 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 03:33:25,415 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 03:33:27,669 | server.py:94 | initial parameters (loss, other metrics): 2.3024520874023438, {'accuracy': 0.1035, 'data_size': 10000}
INFO flwr 2024-04-06 03:33:27,669 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 03:33:27,670 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=768018)[0m 2024-04-06 03:33:30.356363: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=768018)[0m 2024-04-06 03:33:30.453081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=768018)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=768011)[0m 2024-04-06 03:33:32.982893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=768019)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=768019)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=768014)[0m 2024-04-06 03:33:32.406538: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=768014)[0m 2024-04-06 03:33:32.499872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=768014)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=768014)[0m 2024-04-06 03:33:34.839287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=768009)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=768009)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 03:34:11,213 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 03:34:11,776 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 03:34:13,287 | server.py:125 | fit progress: (1, 2.3626368045806885, {'accuracy': 0.0982, 'data_size': 10000}, 45.61675299197668)
INFO flwr 2024-04-06 03:34:13,287 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 03:34:13,288 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:34:45,748 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 03:34:48,545 | server.py:125 | fit progress: (2, 2.360146999359131, {'accuracy': 0.1009, 'data_size': 10000}, 80.87553437700262)
INFO flwr 2024-04-06 03:34:48,546 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 03:34:48,546 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:35:18,129 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 03:35:21,403 | server.py:125 | fit progress: (3, 2.133687973022461, {'accuracy': 0.348, 'data_size': 10000}, 113.73286351998104)
INFO flwr 2024-04-06 03:35:21,403 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 03:35:21,403 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:35:50,896 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 03:35:55,134 | server.py:125 | fit progress: (4, 2.1379237174987793, {'accuracy': 0.3082, 'data_size': 10000}, 147.46451694198186)
INFO flwr 2024-04-06 03:35:55,135 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 03:35:55,135 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:36:28,545 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 03:36:33,568 | server.py:125 | fit progress: (5, 2.067772150039673, {'accuracy': 0.3851, 'data_size': 10000}, 185.89760311099235)
INFO flwr 2024-04-06 03:36:33,568 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 03:36:33,568 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:37:04,309 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 03:37:10,380 | server.py:125 | fit progress: (6, 2.1522278785705566, {'accuracy': 0.3064, 'data_size': 10000}, 222.7104188700032)
INFO flwr 2024-04-06 03:37:10,381 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 03:37:10,381 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:37:40,027 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 03:37:46,980 | server.py:125 | fit progress: (7, 2.0416502952575684, {'accuracy': 0.413, 'data_size': 10000}, 259.30979153199587)
INFO flwr 2024-04-06 03:37:46,980 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 03:37:46,980 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:38:20,825 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 03:38:28,508 | server.py:125 | fit progress: (8, 2.1496598720550537, {'accuracy': 0.3106, 'data_size': 10000}, 300.8382134620042)
INFO flwr 2024-04-06 03:38:28,508 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 03:38:28,509 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:38:59,177 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 03:39:08,918 | server.py:125 | fit progress: (9, 2.2388875484466553, {'accuracy': 0.2221, 'data_size': 10000}, 341.2484443419962)
INFO flwr 2024-04-06 03:39:08,919 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 03:39:08,919 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:39:40,226 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 03:39:51,240 | server.py:125 | fit progress: (10, 2.151240587234497, {'accuracy': 0.3093, 'data_size': 10000}, 383.5702303650032)
INFO flwr 2024-04-06 03:39:51,240 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 03:39:51,241 | server.py:153 | FL finished in 383.5706952579785
INFO flwr 2024-04-06 03:39:51,241 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 03:39:51,241 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 03:39:51,241 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 03:39:51,241 | app.py:229 | app_fit: losses_centralized [(0, 2.3024520874023438), (1, 2.3626368045806885), (2, 2.360146999359131), (3, 2.133687973022461), (4, 2.1379237174987793), (5, 2.067772150039673), (6, 2.1522278785705566), (7, 2.0416502952575684), (8, 2.1496598720550537), (9, 2.2388875484466553), (10, 2.151240587234497)]
INFO flwr 2024-04-06 03:39:51,241 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1035), (1, 0.0982), (2, 0.1009), (3, 0.348), (4, 0.3082), (5, 0.3851), (6, 0.3064), (7, 0.413), (8, 0.3106), (9, 0.2221), (10, 0.3093)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3093
wandb:     loss 2.15124
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_033308-r6tmkb7u
wandb: Find logs at: ./wandb/offline-run-20240406_033308-r6tmkb7u/logs
INFO flwr 2024-04-06 03:39:54,868 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 03:47:01,702 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 03:47:08,174	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 03:47:08,505	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 03:47:08,812	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d7ff33c329e938f8.zip' (8.47MiB) to Ray cluster...
2024-04-06 03:47:08,833	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d7ff33c329e938f8.zip'.
INFO flwr 2024-04-06 03:47:20,561 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62834966937.0, 'memory': 136614922855.0, 'CPU': 64.0}
INFO flwr 2024-04-06 03:47:20,562 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 03:47:20,563 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 03:47:20,593 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 03:47:20,594 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 03:47:20,594 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 03:47:20,595 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 03:47:22,476 | server.py:94 | initial parameters (loss, other metrics): 2.302743673324585, {'accuracy': 0.0496, 'data_size': 10000}
INFO flwr 2024-04-06 03:47:22,476 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 03:47:22,477 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=776335)[0m 2024-04-06 03:47:27.019342: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=776335)[0m 2024-04-06 03:47:27.120097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=776335)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=776335)[0m 2024-04-06 03:47:29.112494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=776335)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=776335)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=776332)[0m 2024-04-06 03:47:27.230368: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=776332)[0m 2024-04-06 03:47:27.323999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=776332)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=776337)[0m 2024-04-06 03:47:29.734506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 03:48:38,023 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 03:48:38,554 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 03:48:39,784 | server.py:125 | fit progress: (1, 2.3012125492095947, {'accuracy': 0.1786, 'data_size': 10000}, 77.30693626499851)
INFO flwr 2024-04-06 03:48:39,784 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 03:48:39,784 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:49:38,636 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 03:49:41,713 | server.py:125 | fit progress: (2, 2.2908496856689453, {'accuracy': 0.1429, 'data_size': 10000}, 139.2367930469918)
INFO flwr 2024-04-06 03:49:41,714 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 03:49:41,714 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:50:46,384 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 03:50:49,790 | server.py:125 | fit progress: (3, 2.247769355773926, {'accuracy': 0.2474, 'data_size': 10000}, 207.3137863049924)
INFO flwr 2024-04-06 03:50:49,791 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 03:50:49,791 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:51:43,706 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 03:51:48,665 | server.py:125 | fit progress: (4, 2.1333935260772705, {'accuracy': 0.3611, 'data_size': 10000}, 266.18809276499087)
INFO flwr 2024-04-06 03:51:48,665 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 03:51:48,665 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:52:51,951 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 03:52:57,640 | server.py:125 | fit progress: (5, 2.0429060459136963, {'accuracy': 0.4174, 'data_size': 10000}, 335.16305968599045)
INFO flwr 2024-04-06 03:52:57,640 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 03:52:57,640 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:53:56,872 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 03:54:03,850 | server.py:125 | fit progress: (6, 1.9483712911605835, {'accuracy': 0.5068, 'data_size': 10000}, 401.3733880009968)
INFO flwr 2024-04-06 03:54:03,850 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 03:54:03,851 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:54:54,416 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 03:55:02,209 | server.py:125 | fit progress: (7, 1.8617254495620728, {'accuracy': 0.6116, 'data_size': 10000}, 459.7327716180007)
INFO flwr 2024-04-06 03:55:02,210 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 03:55:02,210 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:56:11,170 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 03:56:21,426 | server.py:125 | fit progress: (8, 1.7811654806137085, {'accuracy': 0.6946, 'data_size': 10000}, 538.9494006419845)
INFO flwr 2024-04-06 03:56:21,426 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 03:56:21,427 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:57:25,510 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 03:57:35,383 | server.py:125 | fit progress: (9, 1.7454971075057983, {'accuracy': 0.7301, 'data_size': 10000}, 612.9064505969873)
INFO flwr 2024-04-06 03:57:35,383 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 03:57:35,384 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:58:37,725 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 03:58:48,664 | server.py:125 | fit progress: (10, 1.7089693546295166, {'accuracy': 0.7781, 'data_size': 10000}, 686.1872146239912)
INFO flwr 2024-04-06 03:58:48,664 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 03:58:48,664 | server.py:153 | FL finished in 686.1876913399901
INFO flwr 2024-04-06 03:58:48,665 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 03:58:48,665 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 03:58:48,665 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 03:58:48,665 | app.py:229 | app_fit: losses_centralized [(0, 2.302743673324585), (1, 2.3012125492095947), (2, 2.2908496856689453), (3, 2.247769355773926), (4, 2.1333935260772705), (5, 2.0429060459136963), (6, 1.9483712911605835), (7, 1.8617254495620728), (8, 1.7811654806137085), (9, 1.7454971075057983), (10, 1.7089693546295166)]
INFO flwr 2024-04-06 03:58:48,665 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0496), (1, 0.1786), (2, 0.1429), (3, 0.2474), (4, 0.3611), (5, 0.4174), (6, 0.5068), (7, 0.6116), (8, 0.6946), (9, 0.7301), (10, 0.7781)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7781
wandb:     loss 1.70897
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_034701-h05vy4ou
wandb: Find logs at: ./wandb/offline-run-20240406_034701-h05vy4ou/logs
INFO flwr 2024-04-06 03:58:52,304 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 04:05:58,802 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=776334)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=776334)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 04:06:04,581	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 04:06:08,187	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 04:06:08,463	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_adc2cceead8c97a7.zip' (8.49MiB) to Ray cluster...
2024-04-06 04:06:08,495	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_adc2cceead8c97a7.zip'.
INFO flwr 2024-04-06 04:06:19,250 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 149941278516.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 68546262220.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 04:06:19,250 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 04:06:19,250 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 04:06:19,268 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 04:06:19,270 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 04:06:19,270 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 04:06:19,270 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 04:06:22,646 | server.py:94 | initial parameters (loss, other metrics): 2.3024911880493164, {'accuracy': 0.1065, 'data_size': 10000}
INFO flwr 2024-04-06 04:06:22,646 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 04:06:22,646 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=785358)[0m 2024-04-06 04:06:25.403331: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=785358)[0m 2024-04-06 04:06:25.469045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=785358)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=785359)[0m 2024-04-06 04:06:28.302413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=785358)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=785358)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=785367)[0m 2024-04-06 04:06:25.659707: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=785367)[0m 2024-04-06 04:06:25.730297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=785367)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=785361)[0m 2024-04-06 04:06:28.336447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 04:07:39,240 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 04:07:39,797 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 04:07:40,830 | server.py:125 | fit progress: (1, 2.0651729106903076, {'accuracy': 0.4854, 'data_size': 10000}, 78.18356309999945)
INFO flwr 2024-04-06 04:07:40,830 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 04:07:40,830 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:08:36,732 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 04:08:39,562 | server.py:125 | fit progress: (2, 1.617739200592041, {'accuracy': 0.868, 'data_size': 10000}, 136.91594945598627)
INFO flwr 2024-04-06 04:08:39,562 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 04:08:39,563 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:09:36,201 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 04:09:39,800 | server.py:125 | fit progress: (3, 1.5892916917800903, {'accuracy': 0.8729, 'data_size': 10000}, 197.15339390499867)
INFO flwr 2024-04-06 04:09:39,800 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 04:09:39,800 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:10:38,013 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 04:10:42,932 | server.py:125 | fit progress: (4, 1.5595024824142456, {'accuracy': 0.9018, 'data_size': 10000}, 260.28547030399204)
INFO flwr 2024-04-06 04:10:42,932 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 04:10:42,932 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:11:44,598 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 04:11:50,444 | server.py:125 | fit progress: (5, 1.5534625053405762, {'accuracy': 0.9079, 'data_size': 10000}, 327.7978260499949)
INFO flwr 2024-04-06 04:11:50,444 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 04:11:50,444 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:12:47,021 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 04:12:54,058 | server.py:125 | fit progress: (6, 1.5596615076065063, {'accuracy': 0.9012, 'data_size': 10000}, 391.4114701479848)
INFO flwr 2024-04-06 04:12:54,058 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 04:12:54,058 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:13:58,711 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 04:14:06,642 | server.py:125 | fit progress: (7, 1.8077741861343384, {'accuracy': 0.6533, 'data_size': 10000}, 463.9956688309903)
INFO flwr 2024-04-06 04:14:06,642 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 04:14:06,642 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:15:11,602 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 04:15:20,312 | server.py:125 | fit progress: (8, 1.6512185335159302, {'accuracy': 0.8093, 'data_size': 10000}, 537.6663427169842)
INFO flwr 2024-04-06 04:15:20,313 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 04:15:20,313 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:16:15,207 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 04:16:25,142 | server.py:125 | fit progress: (9, 1.5959234237670898, {'accuracy': 0.8653, 'data_size': 10000}, 602.4955880699854)
INFO flwr 2024-04-06 04:16:25,142 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 04:16:25,142 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:17:22,967 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 04:17:34,126 | server.py:125 | fit progress: (10, 1.616590976715088, {'accuracy': 0.8448, 'data_size': 10000}, 671.4798919839959)
INFO flwr 2024-04-06 04:17:34,126 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 04:17:34,126 | server.py:153 | FL finished in 671.480339753005
INFO flwr 2024-04-06 04:17:34,127 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 04:17:34,127 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 04:17:34,127 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 04:17:34,127 | app.py:229 | app_fit: losses_centralized [(0, 2.3024911880493164), (1, 2.0651729106903076), (2, 1.617739200592041), (3, 1.5892916917800903), (4, 1.5595024824142456), (5, 1.5534625053405762), (6, 1.5596615076065063), (7, 1.8077741861343384), (8, 1.6512185335159302), (9, 1.5959234237670898), (10, 1.616590976715088)]
INFO flwr 2024-04-06 04:17:34,127 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1065), (1, 0.4854), (2, 0.868), (3, 0.8729), (4, 0.9018), (5, 0.9079), (6, 0.9012), (7, 0.6533), (8, 0.8093), (9, 0.8653), (10, 0.8448)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8448
wandb:     loss 1.61659
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_040557-3cvqqn2z
wandb: Find logs at: ./wandb/offline-run-20240406_040557-3cvqqn2z/logs
INFO flwr 2024-04-06 04:17:37,701 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 04:24:44,193 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=785368)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=785368)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 04:24:54,654	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 04:25:04,213	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 04:25:04,498	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b9bfe08ebea94471.zip' (8.52MiB) to Ray cluster...
2024-04-06 04:25:04,526	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b9bfe08ebea94471.zip'.
INFO flwr 2024-04-06 04:25:15,327 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 144417993319.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66179139993.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 04:25:15,327 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 04:25:15,327 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 04:25:15,346 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 04:25:15,347 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 04:25:15,347 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 04:25:15,348 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 04:25:18,905 | server.py:94 | initial parameters (loss, other metrics): 2.302623748779297, {'accuracy': 0.082, 'data_size': 10000}
INFO flwr 2024-04-06 04:25:18,906 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 04:25:18,906 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=794176)[0m 2024-04-06 04:25:23.811292: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=794176)[0m 2024-04-06 04:25:23.924744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=794176)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=794176)[0m 2024-04-06 04:25:28.653778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=794176)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=794176)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=794167)[0m 2024-04-06 04:25:24.007349: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=794167)[0m 2024-04-06 04:25:24.107046: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=794167)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=794175)[0m 2024-04-06 04:25:28.661987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 04:26:48,058 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 04:26:48,637 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 04:26:49,650 | server.py:125 | fit progress: (1, 2.2586357593536377, {'accuracy': 0.1652, 'data_size': 10000}, 90.74390147300437)
INFO flwr 2024-04-06 04:26:49,650 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 04:26:49,650 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:27:49,984 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 04:27:52,682 | server.py:125 | fit progress: (2, 1.900695562362671, {'accuracy': 0.5583, 'data_size': 10000}, 153.77629177400377)
INFO flwr 2024-04-06 04:27:52,682 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 04:27:52,683 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:28:56,799 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 04:29:00,330 | server.py:125 | fit progress: (3, 1.7712907791137695, {'accuracy': 0.6891, 'data_size': 10000}, 221.42358899698593)
INFO flwr 2024-04-06 04:29:00,330 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 04:29:00,330 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:29:57,963 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 04:30:02,797 | server.py:125 | fit progress: (4, 2.206946849822998, {'accuracy': 0.254, 'data_size': 10000}, 283.8908370069985)
INFO flwr 2024-04-06 04:30:02,797 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 04:30:02,797 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:31:03,532 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 04:31:09,188 | server.py:125 | fit progress: (5, 1.8975183963775635, {'accuracy': 0.5626, 'data_size': 10000}, 350.2818021879939)
INFO flwr 2024-04-06 04:31:09,188 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 04:31:09,188 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:32:02,777 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 04:32:09,709 | server.py:125 | fit progress: (6, 1.8177460432052612, {'accuracy': 0.6423, 'data_size': 10000}, 410.8025524109835)
INFO flwr 2024-04-06 04:32:09,709 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 04:32:09,709 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:33:11,264 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 04:33:20,288 | server.py:125 | fit progress: (7, 1.7741906642913818, {'accuracy': 0.6866, 'data_size': 10000}, 481.38183287900756)
INFO flwr 2024-04-06 04:33:20,288 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 04:33:20,288 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:34:18,006 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 04:34:27,038 | server.py:125 | fit progress: (8, 1.9752718210220337, {'accuracy': 0.4862, 'data_size': 10000}, 548.1320498499845)
INFO flwr 2024-04-06 04:34:27,038 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 04:34:27,038 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:35:28,353 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 04:35:38,076 | server.py:125 | fit progress: (9, 1.9009679555892944, {'accuracy': 0.56, 'data_size': 10000}, 619.1695860539912)
INFO flwr 2024-04-06 04:35:38,076 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 04:35:38,076 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:36:26,619 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 04:36:37,543 | server.py:125 | fit progress: (10, 1.9110937118530273, {'accuracy': 0.55, 'data_size': 10000}, 678.6369536939892)
INFO flwr 2024-04-06 04:36:37,543 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 04:36:37,543 | server.py:153 | FL finished in 678.6374407779949
INFO flwr 2024-04-06 04:36:37,544 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 04:36:37,544 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 04:36:37,544 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 04:36:37,544 | app.py:229 | app_fit: losses_centralized [(0, 2.302623748779297), (1, 2.2586357593536377), (2, 1.900695562362671), (3, 1.7712907791137695), (4, 2.206946849822998), (5, 1.8975183963775635), (6, 1.8177460432052612), (7, 1.7741906642913818), (8, 1.9752718210220337), (9, 1.9009679555892944), (10, 1.9110937118530273)]
INFO flwr 2024-04-06 04:36:37,544 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.082), (1, 0.1652), (2, 0.5583), (3, 0.6891), (4, 0.254), (5, 0.5626), (6, 0.6423), (7, 0.6866), (8, 0.4862), (9, 0.56), (10, 0.55)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.55
wandb:     loss 1.91109
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_042443-nabkydw5
wandb: Find logs at: ./wandb/offline-run-20240406_042443-nabkydw5/logs
INFO flwr 2024-04-06 04:36:41,103 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 04:43:46,801 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=794175)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=794175)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 04:43:52,607	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 04:43:52,938	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 04:43:53,212	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e03b08ebddad142c.zip' (8.54MiB) to Ray cluster...
2024-04-06 04:43:53,241	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e03b08ebddad142c.zip'.
INFO flwr 2024-04-06 04:44:04,004 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 144378325607.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66162139545.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 04:44:04,004 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 04:44:04,004 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 04:44:04,021 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 04:44:04,023 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 04:44:04,023 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 04:44:04,023 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 04:44:07,400 | server.py:94 | initial parameters (loss, other metrics): 2.302476167678833, {'accuracy': 0.1219, 'data_size': 10000}
INFO flwr 2024-04-06 04:44:07,400 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 04:44:07,401 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=802751)[0m 2024-04-06 04:44:09.787561: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=802751)[0m 2024-04-06 04:44:09.884030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=802751)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=802751)[0m 2024-04-06 04:44:12.074340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=802751)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=802751)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=802753)[0m 2024-04-06 04:44:10.288686: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=802753)[0m 2024-04-06 04:44:10.381982: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=802753)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=802753)[0m 2024-04-06 04:44:12.649191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 04:45:11,065 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 04:45:11,654 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 04:45:12,689 | server.py:125 | fit progress: (1, 2.25406551361084, {'accuracy': 0.2023, 'data_size': 10000}, 65.28846612400957)
INFO flwr 2024-04-06 04:45:12,689 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 04:45:12,689 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:46:10,255 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 04:46:13,028 | server.py:125 | fit progress: (2, 2.1617214679718018, {'accuracy': 0.2947, 'data_size': 10000}, 125.62797281800886)
INFO flwr 2024-04-06 04:46:13,029 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 04:46:13,029 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:47:11,773 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 04:47:15,300 | server.py:125 | fit progress: (3, 2.0805678367614746, {'accuracy': 0.3802, 'data_size': 10000}, 187.8991153560055)
INFO flwr 2024-04-06 04:47:15,300 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 04:47:15,300 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:48:19,821 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 04:48:24,610 | server.py:125 | fit progress: (4, 2.0665767192840576, {'accuracy': 0.3929, 'data_size': 10000}, 257.20919142101775)
INFO flwr 2024-04-06 04:48:24,610 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 04:48:24,610 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:49:20,716 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 04:49:26,858 | server.py:125 | fit progress: (5, 2.185396194458008, {'accuracy': 0.2756, 'data_size': 10000}, 319.45795707401703)
INFO flwr 2024-04-06 04:49:26,859 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 04:49:26,859 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:50:26,520 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 04:50:33,106 | server.py:125 | fit progress: (6, 1.915917992591858, {'accuracy': 0.5441, 'data_size': 10000}, 385.7056251670001)
INFO flwr 2024-04-06 04:50:33,106 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 04:50:33,106 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:51:25,058 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 04:51:32,870 | server.py:125 | fit progress: (7, 2.2115283012390137, {'accuracy': 0.2495, 'data_size': 10000}, 445.46921646001283)
INFO flwr 2024-04-06 04:51:32,870 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 04:51:32,870 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:52:30,309 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 04:52:38,986 | server.py:125 | fit progress: (8, 2.0867459774017334, {'accuracy': 0.374, 'data_size': 10000}, 511.58573887700913)
INFO flwr 2024-04-06 04:52:38,987 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 04:52:38,987 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:53:40,942 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 04:53:50,778 | server.py:125 | fit progress: (9, 2.086585521697998, {'accuracy': 0.374, 'data_size': 10000}, 583.3779031510057)
INFO flwr 2024-04-06 04:53:50,779 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 04:53:50,779 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:54:49,582 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 04:55:00,057 | server.py:125 | fit progress: (10, 2.253141164779663, {'accuracy': 0.208, 'data_size': 10000}, 652.6567623199953)
INFO flwr 2024-04-06 04:55:00,057 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 04:55:00,058 | server.py:153 | FL finished in 652.6571427989984
INFO flwr 2024-04-06 04:55:00,058 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 04:55:00,058 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 04:55:00,058 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 04:55:00,058 | app.py:229 | app_fit: losses_centralized [(0, 2.302476167678833), (1, 2.25406551361084), (2, 2.1617214679718018), (3, 2.0805678367614746), (4, 2.0665767192840576), (5, 2.185396194458008), (6, 1.915917992591858), (7, 2.2115283012390137), (8, 2.0867459774017334), (9, 2.086585521697998), (10, 2.253141164779663)]
INFO flwr 2024-04-06 04:55:00,058 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1219), (1, 0.2023), (2, 0.2947), (3, 0.3802), (4, 0.3929), (5, 0.2756), (6, 0.5441), (7, 0.2495), (8, 0.374), (9, 0.374), (10, 0.208)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.208
wandb:     loss 2.25314
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_044346-p00zojcd
wandb: Find logs at: ./wandb/offline-run-20240406_044346-p00zojcd/logs
INFO flwr 2024-04-06 04:55:03,628 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 05:02:09,107 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=802745)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=802745)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 05:02:15,276	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 05:02:15,645	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 05:02:16,049	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_108719de5c700a29.zip' (8.57MiB) to Ray cluster...
2024-04-06 05:02:16,071	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_108719de5c700a29.zip'.
INFO flwr 2024-04-06 05:02:26,879 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 66046848614.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 144109313434.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 05:02:26,879 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 05:02:26,880 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 05:02:26,903 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 05:02:26,904 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 05:02:26,905 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 05:02:26,905 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 05:02:30,650 | server.py:94 | initial parameters (loss, other metrics): 2.302431106567383, {'accuracy': 0.1156, 'data_size': 10000}
INFO flwr 2024-04-06 05:02:30,651 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 05:02:30,651 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=811825)[0m 2024-04-06 05:02:34.932371: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=811825)[0m 2024-04-06 05:02:35.089835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=811825)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=811825)[0m 2024-04-06 05:02:38.240364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=811828)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=811828)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=811822)[0m 2024-04-06 05:02:35.003730: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=811822)[0m 2024-04-06 05:02:35.156238: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=811822)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=811814)[0m 2024-04-06 05:02:38.248189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 05:03:44,364 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 05:03:44,885 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 05:03:45,894 | server.py:125 | fit progress: (1, 2.2746853828430176, {'accuracy': 0.1813, 'data_size': 10000}, 75.24340327500249)
INFO flwr 2024-04-06 05:03:45,895 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 05:03:45,895 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:04:33,980 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 05:04:36,369 | server.py:125 | fit progress: (2, 2.047934055328369, {'accuracy': 0.3988, 'data_size': 10000}, 125.7184386520239)
INFO flwr 2024-04-06 05:04:36,370 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 05:04:36,370 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:05:34,559 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 05:05:38,086 | server.py:125 | fit progress: (3, 2.096252918243408, {'accuracy': 0.3656, 'data_size': 10000}, 187.43489655200392)
INFO flwr 2024-04-06 05:05:38,086 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 05:05:38,086 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:06:36,959 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 05:06:41,269 | server.py:125 | fit progress: (4, 1.9996833801269531, {'accuracy': 0.4599, 'data_size': 10000}, 250.61853794602212)
INFO flwr 2024-04-06 05:06:41,270 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 05:06:41,270 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:07:38,648 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 05:07:43,584 | server.py:125 | fit progress: (5, 2.2472054958343506, {'accuracy': 0.2137, 'data_size': 10000}, 312.9330383440247)
INFO flwr 2024-04-06 05:07:43,584 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 05:07:43,584 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:08:37,834 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 05:08:45,163 | server.py:125 | fit progress: (6, 2.0096819400787354, {'accuracy': 0.45, 'data_size': 10000}, 374.5119770950114)
INFO flwr 2024-04-06 05:08:45,163 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 05:08:45,163 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:09:40,999 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 05:09:48,584 | server.py:125 | fit progress: (7, 2.121232032775879, {'accuracy': 0.3392, 'data_size': 10000}, 437.9332635780156)
INFO flwr 2024-04-06 05:09:48,584 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 05:09:48,585 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:10:45,346 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 05:10:53,605 | server.py:125 | fit progress: (8, 2.0808348655700684, {'accuracy': 0.3803, 'data_size': 10000}, 502.9543954220135)
INFO flwr 2024-04-06 05:10:53,606 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 05:10:53,606 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:11:58,410 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 05:12:08,552 | server.py:125 | fit progress: (9, 2.2838134765625, {'accuracy': 0.1773, 'data_size': 10000}, 577.9009692170075)
INFO flwr 2024-04-06 05:12:08,552 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 05:12:08,552 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:13:02,893 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 05:13:14,277 | server.py:125 | fit progress: (10, 2.2416934967041016, {'accuracy': 0.2193, 'data_size': 10000}, 643.6262305100099)
INFO flwr 2024-04-06 05:13:14,277 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 05:13:14,278 | server.py:153 | FL finished in 643.6265877360129
INFO flwr 2024-04-06 05:13:14,278 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 05:13:14,278 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 05:13:14,278 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 05:13:14,278 | app.py:229 | app_fit: losses_centralized [(0, 2.302431106567383), (1, 2.2746853828430176), (2, 2.047934055328369), (3, 2.096252918243408), (4, 1.9996833801269531), (5, 2.2472054958343506), (6, 2.0096819400787354), (7, 2.121232032775879), (8, 2.0808348655700684), (9, 2.2838134765625), (10, 2.2416934967041016)]
INFO flwr 2024-04-06 05:13:14,278 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1156), (1, 0.1813), (2, 0.3988), (3, 0.3656), (4, 0.4599), (5, 0.2137), (6, 0.45), (7, 0.3392), (8, 0.3803), (9, 0.1773), (10, 0.2193)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2193
wandb:     loss 2.24169
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_050208-um14o4nn
wandb: Find logs at: ./wandb/offline-run-20240406_050208-um14o4nn/logs
INFO flwr 2024-04-06 05:13:17,868 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 05:20:24,228 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=811829)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=811829)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 05:20:29,055	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 05:20:29,469	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 05:20:29,761	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1f66eecb7d188667.zip' (8.59MiB) to Ray cluster...
2024-04-06 05:20:29,790	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1f66eecb7d188667.zip'.
INFO flwr 2024-04-06 05:20:40,767 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 143547577344.0, 'object_store_memory': 65806104576.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 05:20:40,768 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 05:20:40,768 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 05:20:40,782 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 05:20:40,783 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 05:20:40,783 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 05:20:40,783 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 05:20:44,289 | server.py:94 | initial parameters (loss, other metrics): 2.302560806274414, {'accuracy': 0.0956, 'data_size': 10000}
INFO flwr 2024-04-06 05:20:44,290 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 05:20:44,291 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=820605)[0m 2024-04-06 05:20:46.814635: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=820605)[0m 2024-04-06 05:20:46.936583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=820605)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=820599)[0m 2024-04-06 05:20:49.139262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=820605)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=820605)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=820601)[0m 2024-04-06 05:20:47.036356: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=820601)[0m 2024-04-06 05:20:47.138375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=820601)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=820605)[0m 2024-04-06 05:20:49.223349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 05:21:45,840 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 05:21:46,356 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 05:21:47,361 | server.py:125 | fit progress: (1, 2.1667110919952393, {'accuracy': 0.2916, 'data_size': 10000}, 63.07066861598287)
INFO flwr 2024-04-06 05:21:47,361 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 05:21:47,362 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:22:44,555 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 05:22:47,183 | server.py:125 | fit progress: (2, 2.184105396270752, {'accuracy': 0.3017, 'data_size': 10000}, 122.89239606898627)
INFO flwr 2024-04-06 05:22:47,183 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 05:22:47,183 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:23:46,668 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 05:23:50,153 | server.py:125 | fit progress: (3, 2.0408520698547363, {'accuracy': 0.4189, 'data_size': 10000}, 185.8623387699772)
INFO flwr 2024-04-06 05:23:50,153 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 05:23:50,153 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:24:43,706 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 05:24:48,001 | server.py:125 | fit progress: (4, 2.183690071105957, {'accuracy': 0.2769, 'data_size': 10000}, 243.71022154198727)
INFO flwr 2024-04-06 05:24:48,001 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 05:24:48,001 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:25:46,617 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 05:25:52,087 | server.py:125 | fit progress: (5, 2.2594034671783447, {'accuracy': 0.2016, 'data_size': 10000}, 307.79674984898884)
INFO flwr 2024-04-06 05:25:52,088 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 05:25:52,088 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:26:51,575 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 05:26:58,608 | server.py:125 | fit progress: (6, 2.215485095977783, {'accuracy': 0.2453, 'data_size': 10000}, 374.3177662699891)
INFO flwr 2024-04-06 05:26:58,609 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 05:26:58,609 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:27:55,972 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 05:28:03,610 | server.py:125 | fit progress: (7, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 439.31936099298764)
INFO flwr 2024-04-06 05:28:03,610 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 05:28:03,610 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:28:58,551 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 05:29:07,300 | server.py:125 | fit progress: (8, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 503.0091554649989)
INFO flwr 2024-04-06 05:29:07,300 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 05:29:07,300 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:30:04,987 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 05:30:14,342 | server.py:125 | fit progress: (9, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 570.0517033269862)
INFO flwr 2024-04-06 05:30:14,343 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 05:30:14,343 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:31:13,278 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 05:31:23,672 | server.py:125 | fit progress: (10, 2.358342170715332, {'accuracy': 0.1028, 'data_size': 10000}, 639.3817511759989)
INFO flwr 2024-04-06 05:31:23,673 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 05:31:23,673 | server.py:153 | FL finished in 639.3825175229867
INFO flwr 2024-04-06 05:31:23,673 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 05:31:23,673 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 05:31:23,674 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 05:31:23,674 | app.py:229 | app_fit: losses_centralized [(0, 2.302560806274414), (1, 2.1667110919952393), (2, 2.184105396270752), (3, 2.0408520698547363), (4, 2.183690071105957), (5, 2.2594034671783447), (6, 2.215485095977783), (7, 2.358342170715332), (8, 2.358342170715332), (9, 2.358342170715332), (10, 2.358342170715332)]
INFO flwr 2024-04-06 05:31:23,674 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0956), (1, 0.2916), (2, 0.3017), (3, 0.4189), (4, 0.2769), (5, 0.2016), (6, 0.2453), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.35834
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_052023-d63iblxz
wandb: Find logs at: ./wandb/offline-run-20240406_052023-d63iblxz/logs
INFO flwr 2024-04-06 05:31:27,247 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 05:38:40,957 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=820594)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=820594)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 05:38:51,238	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 05:38:53,321	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 05:38:53,596	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8187b44a01f906c8.zip' (8.62MiB) to Ray cluster...
2024-04-06 05:38:53,624	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8187b44a01f906c8.zip'.
INFO flwr 2024-04-06 05:39:04,333 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65908660224.0, 'memory': 143786873856.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 05:39:04,333 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 05:39:04,334 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 05:39:04,348 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 05:39:04,349 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 05:39:04,349 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 05:39:04,349 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 05:39:06,361 | server.py:94 | initial parameters (loss, other metrics): 2.302462339401245, {'accuracy': 0.0879, 'data_size': 10000}
INFO flwr 2024-04-06 05:39:06,361 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 05:39:06,362 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=829099)[0m 2024-04-06 05:39:12.238349: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=829099)[0m 2024-04-06 05:39:12.327920: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=829099)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=829093)[0m 2024-04-06 05:39:15.953625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=829105)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=829105)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=829100)[0m 2024-04-06 05:39:12.427996: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=829100)[0m 2024-04-06 05:39:12.530428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=829100)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=829100)[0m 2024-04-06 05:39:15.955428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 05:40:35,002 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 05:40:35,536 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 05:40:36,926 | server.py:125 | fit progress: (1, 2.254625082015991, {'accuracy': 0.2007, 'data_size': 10000}, 90.56456585502019)
INFO flwr 2024-04-06 05:40:36,926 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 05:40:36,927 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:41:33,370 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 05:41:35,876 | server.py:125 | fit progress: (2, 2.1393864154815674, {'accuracy': 0.3138, 'data_size': 10000}, 149.51449078001315)
INFO flwr 2024-04-06 05:41:35,876 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 05:41:35,877 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:42:40,475 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 05:42:44,356 | server.py:125 | fit progress: (3, 2.2219510078430176, {'accuracy': 0.2444, 'data_size': 10000}, 217.9944443970162)
INFO flwr 2024-04-06 05:42:44,356 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 05:42:44,357 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:43:41,167 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 05:43:45,771 | server.py:125 | fit progress: (4, 2.197638750076294, {'accuracy': 0.2621, 'data_size': 10000}, 279.4095372490119)
INFO flwr 2024-04-06 05:43:45,771 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 05:43:45,772 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:44:40,462 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 05:44:46,213 | server.py:125 | fit progress: (5, 2.1205344200134277, {'accuracy': 0.3387, 'data_size': 10000}, 339.85144388102344)
INFO flwr 2024-04-06 05:44:46,213 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 05:44:46,214 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:45:45,482 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 05:45:52,116 | server.py:125 | fit progress: (6, 2.2318191528320312, {'accuracy': 0.2293, 'data_size': 10000}, 405.7542286560056)
INFO flwr 2024-04-06 05:45:52,116 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 05:45:52,116 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:46:59,876 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 05:47:07,226 | server.py:125 | fit progress: (7, 2.183748722076416, {'accuracy': 0.2774, 'data_size': 10000}, 480.86436848400626)
INFO flwr 2024-04-06 05:47:07,226 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 05:47:07,227 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:48:06,916 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 05:48:15,429 | server.py:125 | fit progress: (8, 2.186213970184326, {'accuracy': 0.2749, 'data_size': 10000}, 549.0678388100059)
INFO flwr 2024-04-06 05:48:15,430 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 05:48:15,430 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:49:23,002 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 05:49:32,560 | server.py:125 | fit progress: (9, 2.1614491939544678, {'accuracy': 0.2997, 'data_size': 10000}, 626.198878138006)
INFO flwr 2024-04-06 05:49:32,561 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 05:49:32,561 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:50:37,695 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 05:50:47,376 | server.py:125 | fit progress: (10, 2.1634833812713623, {'accuracy': 0.2961, 'data_size': 10000}, 701.013951120025)
INFO flwr 2024-04-06 05:50:47,376 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 05:50:47,376 | server.py:153 | FL finished in 701.0144896550046
INFO flwr 2024-04-06 05:50:47,376 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 05:50:47,376 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 05:50:47,376 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 05:50:47,377 | app.py:229 | app_fit: losses_centralized [(0, 2.302462339401245), (1, 2.254625082015991), (2, 2.1393864154815674), (3, 2.2219510078430176), (4, 2.197638750076294), (5, 2.1205344200134277), (6, 2.2318191528320312), (7, 2.183748722076416), (8, 2.186213970184326), (9, 2.1614491939544678), (10, 2.1634833812713623)]
INFO flwr 2024-04-06 05:50:47,377 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0879), (1, 0.2007), (2, 0.3138), (3, 0.2444), (4, 0.2621), (5, 0.3387), (6, 0.2293), (7, 0.2774), (8, 0.2749), (9, 0.2997), (10, 0.2961)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2961
wandb:     loss 2.16348
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_053836-vj354jnn
wandb: Find logs at: ./wandb/offline-run-20240406_053836-vj354jnn/logs
INFO flwr 2024-04-06 05:50:50,981 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 05:58:00,878 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=829100)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=829100)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 05:58:06,406	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 05:58:11,700	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 05:58:12,036	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_43558312d75e5d9a.zip' (8.64MiB) to Ray cluster...
2024-04-06 05:58:12,066	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_43558312d75e5d9a.zip'.
INFO flwr 2024-04-06 05:58:22,789 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 143762353562.0, 'object_store_memory': 65898151526.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 05:58:22,790 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 05:58:22,790 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 05:58:22,806 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 05:58:22,809 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 05:58:22,810 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 05:58:22,810 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 05:58:25,507 | server.py:94 | initial parameters (loss, other metrics): 2.3027961254119873, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-06 05:58:25,508 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 05:58:25,508 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=837889)[0m 2024-04-06 05:58:28.779748: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=837889)[0m 2024-04-06 05:58:28.869265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=837889)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=837883)[0m 2024-04-06 05:58:31.014196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=837881)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=837881)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=837886)[0m 2024-04-06 05:58:29.062222: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=837886)[0m 2024-04-06 05:58:29.155981: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=837886)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=837886)[0m 2024-04-06 05:58:31.130009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 05:58:44,863 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 05:58:45,415 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 05:58:46,541 | server.py:125 | fit progress: (1, 2.3027894496917725, {'accuracy': 0.101, 'data_size': 10000}, 21.03345642000204)
INFO flwr 2024-04-06 05:58:46,542 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 05:58:46,542 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:58:55,324 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 05:58:58,155 | server.py:125 | fit progress: (2, 2.3027846813201904, {'accuracy': 0.101, 'data_size': 10000}, 32.64735544199357)
INFO flwr 2024-04-06 05:58:58,156 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 05:58:58,156 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:59:07,083 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 05:59:10,872 | server.py:125 | fit progress: (3, 2.302778720855713, {'accuracy': 0.101, 'data_size': 10000}, 45.36406365400762)
INFO flwr 2024-04-06 05:59:10,872 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 05:59:10,873 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:59:19,618 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 05:59:24,565 | server.py:125 | fit progress: (4, 2.3027706146240234, {'accuracy': 0.101, 'data_size': 10000}, 59.05736303300364)
INFO flwr 2024-04-06 05:59:24,566 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 05:59:24,566 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:59:32,744 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 05:59:38,351 | server.py:125 | fit progress: (5, 2.3027660846710205, {'accuracy': 0.101, 'data_size': 10000}, 72.84341024799505)
INFO flwr 2024-04-06 05:59:38,352 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 05:59:38,352 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:59:47,055 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 05:59:53,013 | server.py:125 | fit progress: (6, 2.3027596473693848, {'accuracy': 0.101, 'data_size': 10000}, 87.50529304600786)
INFO flwr 2024-04-06 05:59:53,014 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 05:59:53,014 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:00:02,477 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:00:09,554 | server.py:125 | fit progress: (7, 2.302755355834961, {'accuracy': 0.101, 'data_size': 10000}, 104.04645958601031)
INFO flwr 2024-04-06 06:00:09,555 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:00:09,555 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:00:17,658 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:00:25,615 | server.py:125 | fit progress: (8, 2.302748918533325, {'accuracy': 0.101, 'data_size': 10000}, 120.10725625700434)
INFO flwr 2024-04-06 06:00:25,616 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:00:25,616 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:00:34,445 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:00:43,742 | server.py:125 | fit progress: (9, 2.3027429580688477, {'accuracy': 0.101, 'data_size': 10000}, 138.2343600440072)
INFO flwr 2024-04-06 06:00:43,743 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:00:43,743 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:00:52,322 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:01:02,387 | server.py:125 | fit progress: (10, 2.3027384281158447, {'accuracy': 0.101, 'data_size': 10000}, 156.87947715001064)
INFO flwr 2024-04-06 06:01:02,388 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:01:02,388 | server.py:153 | FL finished in 156.8798617410066
INFO flwr 2024-04-06 06:01:02,388 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:01:02,388 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:01:02,388 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:01:02,388 | app.py:229 | app_fit: losses_centralized [(0, 2.3027961254119873), (1, 2.3027894496917725), (2, 2.3027846813201904), (3, 2.302778720855713), (4, 2.3027706146240234), (5, 2.3027660846710205), (6, 2.3027596473693848), (7, 2.302755355834961), (8, 2.302748918533325), (9, 2.3027429580688477), (10, 2.3027384281158447)]
INFO flwr 2024-04-06 06:01:02,388 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.101), (2, 0.101), (3, 0.101), (4, 0.101), (5, 0.101), (6, 0.101), (7, 0.101), (8, 0.101), (9, 0.101), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.30274
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_055800-uyp1ag91
wandb: Find logs at: ./wandb/offline-run-20240406_055800-uyp1ag91/logs
INFO flwr 2024-04-06 06:01:05,943 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:08:16,422 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=837880)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=837880)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 06:08:22,518	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:08:22,839	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:08:23,147	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6ed61829b6679d8e.zip' (8.66MiB) to Ray cluster...
2024-04-06 06:08:23,169	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6ed61829b6679d8e.zip'.
INFO flwr 2024-04-06 06:08:34,239 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64401395712.0, 'memory': 140269923328.0}
INFO flwr 2024-04-06 06:08:34,240 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:08:34,240 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:08:34,259 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:08:34,260 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:08:34,260 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:08:34,260 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 06:08:36,262 | server.py:94 | initial parameters (loss, other metrics): 2.3023369312286377, {'accuracy': 0.1023, 'data_size': 10000}
INFO flwr 2024-04-06 06:08:36,263 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:08:36,264 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=845582)[0m 2024-04-06 06:08:40.515908: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=845582)[0m 2024-04-06 06:08:40.613025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=845582)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=845582)[0m 2024-04-06 06:08:43.338726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=845581)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=845581)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=845586)[0m 2024-04-06 06:08:40.771647: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=845586)[0m 2024-04-06 06:08:40.840874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=845586)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=845585)[0m 2024-04-06 06:08:43.436178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 06:08:55,633 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 06:08:56,203 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 06:08:57,550 | server.py:125 | fit progress: (1, 2.3020431995391846, {'accuracy': 0.1622, 'data_size': 10000}, 21.286906649998855)
INFO flwr 2024-04-06 06:08:57,551 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 06:08:57,551 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:09:06,545 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 06:09:09,076 | server.py:125 | fit progress: (2, 2.3016576766967773, {'accuracy': 0.1032, 'data_size': 10000}, 32.812504211004125)
INFO flwr 2024-04-06 06:09:09,076 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 06:09:09,076 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:09:18,109 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 06:09:22,110 | server.py:125 | fit progress: (3, 2.301206588745117, {'accuracy': 0.1032, 'data_size': 10000}, 45.84697054300341)
INFO flwr 2024-04-06 06:09:22,111 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 06:09:22,111 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:09:30,654 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 06:09:35,286 | server.py:125 | fit progress: (4, 2.300780773162842, {'accuracy': 0.1032, 'data_size': 10000}, 59.02222086398979)
INFO flwr 2024-04-06 06:09:35,286 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 06:09:35,286 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:09:44,924 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 06:09:50,737 | server.py:125 | fit progress: (5, 2.3002827167510986, {'accuracy': 0.1032, 'data_size': 10000}, 74.47357342700707)
INFO flwr 2024-04-06 06:09:50,738 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 06:09:50,738 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:09:59,806 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 06:10:06,788 | server.py:125 | fit progress: (6, 2.2996809482574463, {'accuracy': 0.1567, 'data_size': 10000}, 90.52420668001287)
INFO flwr 2024-04-06 06:10:06,788 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 06:10:06,788 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:10:15,984 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:10:23,898 | server.py:125 | fit progress: (7, 2.298739194869995, {'accuracy': 0.1032, 'data_size': 10000}, 107.63470933400095)
INFO flwr 2024-04-06 06:10:23,898 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:10:23,899 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:10:32,787 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:10:43,476 | server.py:125 | fit progress: (8, 2.2971789836883545, {'accuracy': 0.1032, 'data_size': 10000}, 127.21273635499529)
INFO flwr 2024-04-06 06:10:43,477 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:10:43,477 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:10:52,618 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:11:02,986 | server.py:125 | fit progress: (9, 2.295581340789795, {'accuracy': 0.2059, 'data_size': 10000}, 146.7226915390056)
INFO flwr 2024-04-06 06:11:02,987 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:11:02,987 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:11:11,184 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:11:21,698 | server.py:125 | fit progress: (10, 2.292351245880127, {'accuracy': 0.1067, 'data_size': 10000}, 165.43492441700073)
INFO flwr 2024-04-06 06:11:21,699 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:11:21,699 | server.py:153 | FL finished in 165.43533375501283
INFO flwr 2024-04-06 06:11:21,699 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:11:21,699 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:11:21,699 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:11:21,699 | app.py:229 | app_fit: losses_centralized [(0, 2.3023369312286377), (1, 2.3020431995391846), (2, 2.3016576766967773), (3, 2.301206588745117), (4, 2.300780773162842), (5, 2.3002827167510986), (6, 2.2996809482574463), (7, 2.298739194869995), (8, 2.2971789836883545), (9, 2.295581340789795), (10, 2.292351245880127)]
INFO flwr 2024-04-06 06:11:21,699 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1023), (1, 0.1622), (2, 0.1032), (3, 0.1032), (4, 0.1032), (5, 0.1032), (6, 0.1567), (7, 0.1032), (8, 0.1032), (9, 0.2059), (10, 0.1067)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1067
wandb:     loss 2.29235
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_060815-tsp4gjlz
wandb: Find logs at: ./wandb/offline-run-20240406_060815-tsp4gjlz/logs
INFO flwr 2024-04-06 06:11:25,250 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:18:35,722 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=845582)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=845582)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 06:18:41,710	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:18:43,375	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:18:43,673	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0ef81a085642d11b.zip' (8.68MiB) to Ray cluster...
2024-04-06 06:18:43,702	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0ef81a085642d11b.zip'.
INFO flwr 2024-04-06 06:18:54,412 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 143457530061.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65767512883.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 06:18:54,412 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:18:54,413 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:18:54,430 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:18:54,431 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:18:54,431 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:18:54,435 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 06:18:57,737 | server.py:94 | initial parameters (loss, other metrics): 2.302738904953003, {'accuracy': 0.0958, 'data_size': 10000}
INFO flwr 2024-04-06 06:18:57,738 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:18:57,739 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=850155)[0m 2024-04-06 06:19:00.280307: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=850155)[0m 2024-04-06 06:19:00.375780: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=850155)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=850155)[0m 2024-04-06 06:19:02.532429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=850154)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=850154)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=850146)[0m 2024-04-06 06:19:00.948788: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=850146)[0m 2024-04-06 06:19:01.034049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=850146)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=850146)[0m 2024-04-06 06:19:03.077629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 06:19:18,657 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 06:19:19,187 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 06:19:20,689 | server.py:125 | fit progress: (1, 2.30236554145813, {'accuracy': 0.1028, 'data_size': 10000}, 22.95027758300421)
INFO flwr 2024-04-06 06:19:20,689 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 06:19:20,689 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:19:30,179 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 06:19:32,762 | server.py:125 | fit progress: (2, 2.3015551567077637, {'accuracy': 0.0958, 'data_size': 10000}, 35.023642624000786)
INFO flwr 2024-04-06 06:19:32,762 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 06:19:32,763 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:19:41,625 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 06:19:45,537 | server.py:125 | fit progress: (3, 2.2985780239105225, {'accuracy': 0.0958, 'data_size': 10000}, 47.79824313800782)
INFO flwr 2024-04-06 06:19:45,537 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 06:19:45,537 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:19:54,020 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 06:19:58,300 | server.py:125 | fit progress: (4, 2.2955849170684814, {'accuracy': 0.1934, 'data_size': 10000}, 60.56189598800847)
INFO flwr 2024-04-06 06:19:58,301 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 06:19:58,301 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:20:06,723 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 06:20:12,098 | server.py:125 | fit progress: (5, 2.288658857345581, {'accuracy': 0.18, 'data_size': 10000}, 74.3594981690112)
INFO flwr 2024-04-06 06:20:12,098 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 06:20:12,099 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:20:21,451 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 06:20:28,167 | server.py:125 | fit progress: (6, 2.2675373554229736, {'accuracy': 0.2711, 'data_size': 10000}, 90.42800659200293)
INFO flwr 2024-04-06 06:20:28,167 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 06:20:28,167 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:20:37,879 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:20:45,486 | server.py:125 | fit progress: (7, 2.2125062942504883, {'accuracy': 0.2349, 'data_size': 10000}, 107.74716037898907)
INFO flwr 2024-04-06 06:20:45,486 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:20:45,486 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:20:54,393 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:21:02,561 | server.py:125 | fit progress: (8, 2.09778094291687, {'accuracy': 0.366, 'data_size': 10000}, 124.82247837699833)
INFO flwr 2024-04-06 06:21:02,561 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:21:02,561 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:21:11,842 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:21:21,201 | server.py:125 | fit progress: (9, 1.988189458847046, {'accuracy': 0.5349, 'data_size': 10000}, 143.46230645800824)
INFO flwr 2024-04-06 06:21:21,201 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:21:21,201 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:21:30,430 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:21:40,940 | server.py:125 | fit progress: (10, 1.9309675693511963, {'accuracy': 0.5175, 'data_size': 10000}, 163.2019244900148)
INFO flwr 2024-04-06 06:21:40,941 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:21:40,941 | server.py:153 | FL finished in 163.2024195730046
INFO flwr 2024-04-06 06:21:40,941 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:21:40,941 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:21:40,941 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:21:40,941 | app.py:229 | app_fit: losses_centralized [(0, 2.302738904953003), (1, 2.30236554145813), (2, 2.3015551567077637), (3, 2.2985780239105225), (4, 2.2955849170684814), (5, 2.288658857345581), (6, 2.2675373554229736), (7, 2.2125062942504883), (8, 2.09778094291687), (9, 1.988189458847046), (10, 1.9309675693511963)]
INFO flwr 2024-04-06 06:21:40,942 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0958), (1, 0.1028), (2, 0.0958), (3, 0.0958), (4, 0.1934), (5, 0.18), (6, 0.2711), (7, 0.2349), (8, 0.366), (9, 0.5349), (10, 0.5175)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5175
wandb:     loss 1.93097
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_061835-du45fzk0
wandb: Find logs at: ./wandb/offline-run-20240406_061835-du45fzk0/logs
INFO flwr 2024-04-06 06:21:44,479 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:28:53,351 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=850152)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=850152)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 06:28:59,246	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:28:59,569	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:28:59,855	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_252331559094d518.zip' (8.70MiB) to Ray cluster...
2024-04-06 06:28:59,886	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_252331559094d518.zip'.
INFO flwr 2024-04-06 06:29:10,943 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 143283772007.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 65693045145.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 06:29:10,943 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:29:10,943 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:29:10,957 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:29:10,958 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:29:10,958 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:29:10,958 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 06:29:13,040 | server.py:94 | initial parameters (loss, other metrics): 2.302691698074341, {'accuracy': 0.0762, 'data_size': 10000}
INFO flwr 2024-04-06 06:29:13,041 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:29:13,041 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=857902)[0m 2024-04-06 06:29:17.290638: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=857902)[0m 2024-04-06 06:29:17.388305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=857902)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=857902)[0m 2024-04-06 06:29:19.760066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=857913)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=857913)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=857905)[0m 2024-04-06 06:29:17.662292: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=857905)[0m 2024-04-06 06:29:17.755791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=857905)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=857905)[0m 2024-04-06 06:29:19.904468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 06:29:32,465 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 06:29:33,013 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 06:29:34,373 | server.py:125 | fit progress: (1, 2.3013250827789307, {'accuracy': 0.1444, 'data_size': 10000}, 21.332406408007955)
INFO flwr 2024-04-06 06:29:34,374 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 06:29:34,374 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:29:43,830 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 06:29:46,353 | server.py:125 | fit progress: (2, 2.2939014434814453, {'accuracy': 0.1009, 'data_size': 10000}, 33.31194175200653)
INFO flwr 2024-04-06 06:29:46,353 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 06:29:46,354 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:29:56,207 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 06:30:00,109 | server.py:125 | fit progress: (3, 2.271909475326538, {'accuracy': 0.1888, 'data_size': 10000}, 47.068031646020245)
INFO flwr 2024-04-06 06:30:00,109 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 06:30:00,110 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:30:09,577 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 06:30:14,341 | server.py:125 | fit progress: (4, 2.2354207038879395, {'accuracy': 0.2882, 'data_size': 10000}, 61.300346535019344)
INFO flwr 2024-04-06 06:30:14,342 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 06:30:14,342 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:30:23,716 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 06:30:29,216 | server.py:125 | fit progress: (5, 2.1569840908050537, {'accuracy': 0.3541, 'data_size': 10000}, 76.17472119699232)
INFO flwr 2024-04-06 06:30:29,216 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 06:30:29,217 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:30:37,851 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 06:30:44,778 | server.py:125 | fit progress: (6, 2.0196330547332764, {'accuracy': 0.4939, 'data_size': 10000}, 91.73707721699611)
INFO flwr 2024-04-06 06:30:44,778 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 06:30:44,779 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:30:53,669 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:31:02,224 | server.py:125 | fit progress: (7, 1.914003849029541, {'accuracy': 0.5916, 'data_size': 10000}, 109.1825429740129)
INFO flwr 2024-04-06 06:31:02,224 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:31:02,224 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:31:10,953 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:31:19,258 | server.py:125 | fit progress: (8, 1.829696536064148, {'accuracy': 0.6725, 'data_size': 10000}, 126.21656884200638)
INFO flwr 2024-04-06 06:31:19,258 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:31:19,258 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:31:28,140 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:31:37,662 | server.py:125 | fit progress: (9, 1.7873985767364502, {'accuracy': 0.6854, 'data_size': 10000}, 144.6211382219917)
INFO flwr 2024-04-06 06:31:37,663 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:31:37,663 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:31:47,064 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:31:58,532 | server.py:125 | fit progress: (10, 1.7395142316818237, {'accuracy': 0.7353, 'data_size': 10000}, 165.49058556900127)
INFO flwr 2024-04-06 06:31:58,532 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:31:58,532 | server.py:153 | FL finished in 165.49109115000465
INFO flwr 2024-04-06 06:31:58,532 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:31:58,532 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:31:58,533 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:31:58,533 | app.py:229 | app_fit: losses_centralized [(0, 2.302691698074341), (1, 2.3013250827789307), (2, 2.2939014434814453), (3, 2.271909475326538), (4, 2.2354207038879395), (5, 2.1569840908050537), (6, 2.0196330547332764), (7, 1.914003849029541), (8, 1.829696536064148), (9, 1.7873985767364502), (10, 1.7395142316818237)]
INFO flwr 2024-04-06 06:31:58,533 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0762), (1, 0.1444), (2, 0.1009), (3, 0.1888), (4, 0.2882), (5, 0.3541), (6, 0.4939), (7, 0.5916), (8, 0.6725), (9, 0.6854), (10, 0.7353)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7353
wandb:     loss 1.73951
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_062852-0mi49csb
wandb: Find logs at: ./wandb/offline-run-20240406_062852-0mi49csb/logs
INFO flwr 2024-04-06 06:32:02,439 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:39:09,392 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=857905)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=857905)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 06:39:16,344	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:39:16,706	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:39:17,134	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cbd02a988e102699.zip' (8.71MiB) to Ray cluster...
2024-04-06 06:39:17,159	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cbd02a988e102699.zip'.
INFO flwr 2024-04-06 06:39:28,564 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 143455852749.0, 'object_store_memory': 65766794035.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 06:39:28,564 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:39:28,564 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:39:28,578 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:39:28,579 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:39:28,579 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:39:28,579 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 06:39:31,506 | server.py:94 | initial parameters (loss, other metrics): 2.302668809890747, {'accuracy': 0.0989, 'data_size': 10000}
INFO flwr 2024-04-06 06:39:31,506 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:39:31,507 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=862308)[0m 2024-04-06 06:39:34.687954: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=862308)[0m 2024-04-06 06:39:34.793808: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=862308)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=862308)[0m 2024-04-06 06:39:36.895009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=862308)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=862308)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=862305)[0m 2024-04-06 06:39:34.824220: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=862307)[0m 2024-04-06 06:39:34.890522: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=862307)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=862305)[0m 2024-04-06 06:39:37.082200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 06:39:55,075 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 06:39:55,587 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 06:39:56,832 | server.py:125 | fit progress: (1, 2.3009984493255615, {'accuracy': 0.101, 'data_size': 10000}, 25.325883529003477)
INFO flwr 2024-04-06 06:39:56,833 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 06:39:56,833 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:40:05,552 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 06:40:07,927 | server.py:125 | fit progress: (2, 2.286714553833008, {'accuracy': 0.1019, 'data_size': 10000}, 36.420888043008745)
INFO flwr 2024-04-06 06:40:07,928 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 06:40:07,928 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:40:16,085 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 06:40:19,538 | server.py:125 | fit progress: (3, 2.2067503929138184, {'accuracy': 0.2632, 'data_size': 10000}, 48.0316875790013)
INFO flwr 2024-04-06 06:40:19,538 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 06:40:19,539 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:40:27,331 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 06:40:31,465 | server.py:125 | fit progress: (4, 2.049795150756836, {'accuracy': 0.4195, 'data_size': 10000}, 59.958799574000295)
INFO flwr 2024-04-06 06:40:31,466 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 06:40:31,466 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:40:40,385 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 06:40:45,604 | server.py:125 | fit progress: (5, 1.9396984577178955, {'accuracy': 0.5477, 'data_size': 10000}, 74.09774088501581)
INFO flwr 2024-04-06 06:40:45,605 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 06:40:45,605 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:40:53,361 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 06:40:59,119 | server.py:125 | fit progress: (6, 1.8764361143112183, {'accuracy': 0.5827, 'data_size': 10000}, 87.61237984002219)
INFO flwr 2024-04-06 06:40:59,119 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 06:40:59,119 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:41:07,285 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:41:13,864 | server.py:125 | fit progress: (7, 1.8535211086273193, {'accuracy': 0.6078, 'data_size': 10000}, 102.35779696400277)
INFO flwr 2024-04-06 06:41:13,865 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:41:13,865 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:41:21,836 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:41:29,596 | server.py:125 | fit progress: (8, 1.825913906097412, {'accuracy': 0.6282, 'data_size': 10000}, 118.08928662101971)
INFO flwr 2024-04-06 06:41:29,596 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:41:29,596 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:41:37,846 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:41:46,121 | server.py:125 | fit progress: (9, 1.7972830533981323, {'accuracy': 0.6721, 'data_size': 10000}, 134.61422431000392)
INFO flwr 2024-04-06 06:41:46,121 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:41:46,121 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:41:54,302 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:42:03,641 | server.py:125 | fit progress: (10, 1.8105952739715576, {'accuracy': 0.6496, 'data_size': 10000}, 152.13416568300454)
INFO flwr 2024-04-06 06:42:03,641 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:42:03,641 | server.py:153 | FL finished in 152.1346074580215
INFO flwr 2024-04-06 06:42:03,641 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:42:03,641 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:42:03,642 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:42:03,642 | app.py:229 | app_fit: losses_centralized [(0, 2.302668809890747), (1, 2.3009984493255615), (2, 2.286714553833008), (3, 2.2067503929138184), (4, 2.049795150756836), (5, 1.9396984577178955), (6, 1.8764361143112183), (7, 1.8535211086273193), (8, 1.825913906097412), (9, 1.7972830533981323), (10, 1.8105952739715576)]
INFO flwr 2024-04-06 06:42:03,642 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0989), (1, 0.101), (2, 0.1019), (3, 0.2632), (4, 0.4195), (5, 0.5477), (6, 0.5827), (7, 0.6078), (8, 0.6282), (9, 0.6721), (10, 0.6496)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6496
wandb:     loss 1.8106
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_063908-owckzck8
wandb: Find logs at: ./wandb/offline-run-20240406_063908-owckzck8/logs
INFO flwr 2024-04-06 06:42:07,165 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:49:14,696 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=862299)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=862299)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 06:49:19,471	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:49:19,809	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:49:20,201	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d07122266dbc60fe.zip' (8.73MiB) to Ray cluster...
2024-04-06 06:49:20,223	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d07122266dbc60fe.zip'.
INFO flwr 2024-04-06 06:49:31,430 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 143016052941.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65578308403.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 06:49:31,430 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:49:31,430 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:49:31,449 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:49:31,450 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:49:31,450 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:49:31,450 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 06:49:33,702 | server.py:94 | initial parameters (loss, other metrics): 2.3025295734405518, {'accuracy': 0.0959, 'data_size': 10000}
INFO flwr 2024-04-06 06:49:33,702 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:49:33,703 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=869734)[0m 2024-04-06 06:49:37.441036: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=869734)[0m 2024-04-06 06:49:37.540419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=869734)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=869735)[0m 2024-04-06 06:49:39.692941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=869736)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=869736)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=869745)[0m 2024-04-06 06:49:37.819275: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=869745)[0m 2024-04-06 06:49:37.904653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=869745)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=869730)[0m 2024-04-06 06:49:39.933209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 06:49:52,046 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 06:49:52,576 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 06:49:53,827 | server.py:125 | fit progress: (1, 2.2990312576293945, {'accuracy': 0.1268, 'data_size': 10000}, 20.12422678800067)
INFO flwr 2024-04-06 06:49:53,827 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 06:49:53,827 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:50:02,592 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 06:50:05,340 | server.py:125 | fit progress: (2, 2.2744836807250977, {'accuracy': 0.3499, 'data_size': 10000}, 31.637094019009965)
INFO flwr 2024-04-06 06:50:05,340 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 06:50:05,340 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:50:13,321 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 06:50:16,696 | server.py:125 | fit progress: (3, 2.162890911102295, {'accuracy': 0.3733, 'data_size': 10000}, 42.99309845000971)
INFO flwr 2024-04-06 06:50:16,696 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 06:50:16,696 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:50:26,249 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 06:50:30,626 | server.py:125 | fit progress: (4, 2.0449154376983643, {'accuracy': 0.4168, 'data_size': 10000}, 56.92296514098416)
INFO flwr 2024-04-06 06:50:30,626 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 06:50:30,626 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:50:38,503 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 06:50:47,966 | server.py:125 | fit progress: (5, 1.9826910495758057, {'accuracy': 0.4768, 'data_size': 10000}, 74.26326726799016)
INFO flwr 2024-04-06 06:50:47,966 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 06:50:47,966 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:50:56,605 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 06:51:03,080 | server.py:125 | fit progress: (6, 1.8928608894348145, {'accuracy': 0.5508, 'data_size': 10000}, 89.37688145801076)
INFO flwr 2024-04-06 06:51:03,080 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 06:51:03,080 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:51:11,716 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:51:21,055 | server.py:125 | fit progress: (7, 1.8190945386886597, {'accuracy': 0.6474, 'data_size': 10000}, 107.35276153200539)
INFO flwr 2024-04-06 06:51:21,056 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:51:21,056 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:51:28,974 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:51:36,479 | server.py:125 | fit progress: (8, 1.7463855743408203, {'accuracy': 0.7313, 'data_size': 10000}, 122.77611884399084)
INFO flwr 2024-04-06 06:51:36,479 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:51:36,479 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:51:44,726 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:51:53,889 | server.py:125 | fit progress: (9, 1.6850306987762451, {'accuracy': 0.7857, 'data_size': 10000}, 140.18655320099788)
INFO flwr 2024-04-06 06:51:53,889 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:51:53,890 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:52:02,155 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:52:12,114 | server.py:125 | fit progress: (10, 1.7062290906906128, {'accuracy': 0.7577, 'data_size': 10000}, 158.4113316599978)
INFO flwr 2024-04-06 06:52:12,114 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:52:12,114 | server.py:153 | FL finished in 158.41169433199684
INFO flwr 2024-04-06 06:52:12,114 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:52:12,115 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:52:12,115 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:52:12,115 | app.py:229 | app_fit: losses_centralized [(0, 2.3025295734405518), (1, 2.2990312576293945), (2, 2.2744836807250977), (3, 2.162890911102295), (4, 2.0449154376983643), (5, 1.9826910495758057), (6, 1.8928608894348145), (7, 1.8190945386886597), (8, 1.7463855743408203), (9, 1.6850306987762451), (10, 1.7062290906906128)]
INFO flwr 2024-04-06 06:52:12,115 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0959), (1, 0.1268), (2, 0.3499), (3, 0.3733), (4, 0.4168), (5, 0.4768), (6, 0.5508), (7, 0.6474), (8, 0.7313), (9, 0.7857), (10, 0.7577)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7577
wandb:     loss 1.70623
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_064913-hcw3tcss
wandb: Find logs at: ./wandb/offline-run-20240406_064913-hcw3tcss/logs
INFO flwr 2024-04-06 06:52:15,724 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:59:22,407 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=869730)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=869730)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 06:59:28,652	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:59:30,112	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:59:30,412	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_01d0b6ac4509b3df.zip' (8.75MiB) to Ray cluster...
2024-04-06 06:59:30,442	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_01d0b6ac4509b3df.zip'.
INFO flwr 2024-04-06 06:59:41,208 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 143168114893.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65643477811.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 06:59:41,208 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:59:41,208 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:59:41,222 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:59:41,223 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:59:41,224 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:59:41,224 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 06:59:45,109 | server.py:94 | initial parameters (loss, other metrics): 2.302673816680908, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-06 06:59:45,110 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:59:45,110 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=874477)[0m 2024-04-06 06:59:47.118535: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=874478)[0m 2024-04-06 06:59:47.172692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=874478)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=874484)[0m 2024-04-06 06:59:49.361004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=874479)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=874479)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=874475)[0m 2024-04-06 06:59:47.241392: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=874475)[0m 2024-04-06 06:59:47.346543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=874475)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=874481)[0m 2024-04-06 06:59:49.768767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 07:00:10,525 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:00:11,035 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:00:12,271 | server.py:125 | fit progress: (1, 2.297966241836548, {'accuracy': 0.1398, 'data_size': 10000}, 27.16172547399765)
INFO flwr 2024-04-06 07:00:12,272 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:00:12,272 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:00:20,939 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:00:23,356 | server.py:125 | fit progress: (2, 2.2648541927337646, {'accuracy': 0.1916, 'data_size': 10000}, 38.24590051898849)
INFO flwr 2024-04-06 07:00:23,356 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:00:23,356 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:00:31,477 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:00:35,538 | server.py:125 | fit progress: (3, 2.1609928607940674, {'accuracy': 0.2638, 'data_size': 10000}, 50.42825965298107)
INFO flwr 2024-04-06 07:00:35,538 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:00:35,538 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:00:45,190 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:00:49,737 | server.py:125 | fit progress: (4, 2.0204813480377197, {'accuracy': 0.4573, 'data_size': 10000}, 64.62698940598057)
INFO flwr 2024-04-06 07:00:49,737 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:00:49,737 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:00:59,509 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:01:04,811 | server.py:125 | fit progress: (5, 1.885941743850708, {'accuracy': 0.6118, 'data_size': 10000}, 79.70121207498596)
INFO flwr 2024-04-06 07:01:04,811 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:01:04,811 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:01:13,297 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:01:19,141 | server.py:125 | fit progress: (6, 1.724879503250122, {'accuracy': 0.7868, 'data_size': 10000}, 94.03102454400505)
INFO flwr 2024-04-06 07:01:19,141 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:01:19,141 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:01:27,658 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:01:34,490 | server.py:125 | fit progress: (7, 1.6696596145629883, {'accuracy': 0.8279, 'data_size': 10000}, 109.38074621799751)
INFO flwr 2024-04-06 07:01:34,491 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:01:34,491 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:01:42,595 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 07:01:50,111 | server.py:125 | fit progress: (8, 1.6267896890640259, {'accuracy': 0.8616, 'data_size': 10000}, 125.00102686500759)
INFO flwr 2024-04-06 07:01:50,111 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 07:01:50,111 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:01:58,233 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 07:02:06,691 | server.py:125 | fit progress: (9, 1.6710177659988403, {'accuracy': 0.7973, 'data_size': 10000}, 141.58139172699885)
INFO flwr 2024-04-06 07:02:06,691 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 07:02:06,692 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:02:15,009 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 07:02:24,310 | server.py:125 | fit progress: (10, 1.611284852027893, {'accuracy': 0.8574, 'data_size': 10000}, 159.20014395698672)
INFO flwr 2024-04-06 07:02:24,310 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 07:02:24,310 | server.py:153 | FL finished in 159.2006525140023
INFO flwr 2024-04-06 07:02:24,311 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 07:02:24,311 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 07:02:24,311 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 07:02:24,311 | app.py:229 | app_fit: losses_centralized [(0, 2.302673816680908), (1, 2.297966241836548), (2, 2.2648541927337646), (3, 2.1609928607940674), (4, 2.0204813480377197), (5, 1.885941743850708), (6, 1.724879503250122), (7, 1.6696596145629883), (8, 1.6267896890640259), (9, 1.6710177659988403), (10, 1.611284852027893)]
INFO flwr 2024-04-06 07:02:24,311 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.1398), (2, 0.1916), (3, 0.2638), (4, 0.4573), (5, 0.6118), (6, 0.7868), (7, 0.8279), (8, 0.8616), (9, 0.7973), (10, 0.8574)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8574
wandb:     loss 1.61128
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_065921-v7gay0r6
wandb: Find logs at: ./wandb/offline-run-20240406_065921-v7gay0r6/logs
INFO flwr 2024-04-06 07:02:27,929 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 07:09:35,294 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=874475)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=874475)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 07:09:39,823	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 07:09:40,157	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 07:09:40,525	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2720123673b8113e.zip' (8.77MiB) to Ray cluster...
2024-04-06 07:09:40,548	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2720123673b8113e.zip'.
INFO flwr 2024-04-06 07:09:51,776 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 142961011303.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'object_store_memory': 65554719129.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 07:09:51,776 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 07:09:51,776 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 07:09:51,793 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 07:09:51,794 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 07:09:51,794 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 07:09:51,795 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 07:09:54,565 | server.py:94 | initial parameters (loss, other metrics): 2.302583932876587, {'accuracy': 0.085, 'data_size': 10000}
INFO flwr 2024-04-06 07:09:54,565 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 07:09:54,565 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=881883)[0m 2024-04-06 07:09:57.873425: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=881883)[0m 2024-04-06 07:09:57.975634: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=881883)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=881883)[0m 2024-04-06 07:10:00.009369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=881896)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=881896)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=881897)[0m 2024-04-06 07:09:58.217664: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=881897)[0m 2024-04-06 07:09:58.312224: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=881897)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=881898)[0m 2024-04-06 07:10:00.171403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 07:10:13,337 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:10:13,889 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:10:15,158 | server.py:125 | fit progress: (1, 2.302562713623047, {'accuracy': 0.0877, 'data_size': 10000}, 20.593097980017774)
INFO flwr 2024-04-06 07:10:15,159 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:10:15,159 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:10:24,478 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:10:26,951 | server.py:125 | fit progress: (2, 2.3025319576263428, {'accuracy': 0.0893, 'data_size': 10000}, 32.385701146005886)
INFO flwr 2024-04-06 07:10:26,952 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:10:26,952 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:10:35,823 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:10:39,667 | server.py:125 | fit progress: (3, 2.3025009632110596, {'accuracy': 0.0912, 'data_size': 10000}, 45.101704262022395)
INFO flwr 2024-04-06 07:10:39,667 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:10:39,667 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:10:48,354 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:10:53,087 | server.py:125 | fit progress: (4, 2.302473545074463, {'accuracy': 0.0927, 'data_size': 10000}, 58.52188832400134)
INFO flwr 2024-04-06 07:10:53,087 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:10:53,088 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:11:01,919 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:11:07,465 | server.py:125 | fit progress: (5, 2.302448272705078, {'accuracy': 0.0945, 'data_size': 10000}, 72.89989377101301)
INFO flwr 2024-04-06 07:11:07,465 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:11:07,466 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:11:16,569 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:11:23,281 | server.py:125 | fit progress: (6, 2.3024306297302246, {'accuracy': 0.1038, 'data_size': 10000}, 88.71619747401564)
INFO flwr 2024-04-06 07:11:23,282 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:11:23,282 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:11:32,097 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:11:40,183 | server.py:125 | fit progress: (7, 2.3024039268493652, {'accuracy': 0.105, 'data_size': 10000}, 105.61814730902552)
INFO flwr 2024-04-06 07:11:40,184 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:11:40,184 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:11:49,274 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 07:11:58,509 | server.py:125 | fit progress: (8, 2.302377462387085, {'accuracy': 0.103, 'data_size': 10000}, 123.94359770402662)
INFO flwr 2024-04-06 07:11:58,509 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 07:11:58,509 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:12:07,925 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 07:12:18,026 | server.py:125 | fit progress: (9, 2.3023428916931152, {'accuracy': 0.1051, 'data_size': 10000}, 143.46030082000652)
INFO flwr 2024-04-06 07:12:18,026 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 07:12:18,026 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:12:27,273 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 07:12:38,007 | server.py:125 | fit progress: (10, 2.3023178577423096, {'accuracy': 0.1135, 'data_size': 10000}, 163.4418553080177)
INFO flwr 2024-04-06 07:12:38,007 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 07:12:38,008 | server.py:153 | FL finished in 163.44228859301074
INFO flwr 2024-04-06 07:12:38,008 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 07:12:38,008 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 07:12:38,008 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 07:12:38,008 | app.py:229 | app_fit: losses_centralized [(0, 2.302583932876587), (1, 2.302562713623047), (2, 2.3025319576263428), (3, 2.3025009632110596), (4, 2.302473545074463), (5, 2.302448272705078), (6, 2.3024306297302246), (7, 2.3024039268493652), (8, 2.302377462387085), (9, 2.3023428916931152), (10, 2.3023178577423096)]
INFO flwr 2024-04-06 07:12:38,008 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.085), (1, 0.0877), (2, 0.0893), (3, 0.0912), (4, 0.0927), (5, 0.0945), (6, 0.1038), (7, 0.105), (8, 0.103), (9, 0.1051), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.30232
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_070934-0l3588et
wandb: Find logs at: ./wandb/offline-run-20240406_070934-0l3588et/logs
INFO flwr 2024-04-06 07:12:41,604 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 07:19:53,208 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=881891)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=881891)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 07:20:02,286	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 07:20:10,074	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 07:20:10,384	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0fdfb09903ee58fe.zip' (8.78MiB) to Ray cluster...
2024-04-06 07:20:10,417	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0fdfb09903ee58fe.zip'.
INFO flwr 2024-04-06 07:20:21,353 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 61825742438.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 134260065690.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 07:20:21,353 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 07:20:21,353 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 07:20:21,366 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 07:20:21,367 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 07:20:21,367 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 07:20:21,367 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 07:20:23,688 | server.py:94 | initial parameters (loss, other metrics): 2.3025217056274414, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-06 07:20:23,689 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 07:20:23,689 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=886371)[0m 2024-04-06 07:20:40.878316: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=886371)[0m 2024-04-06 07:20:40.974151: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=886371)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=886371)[0m 2024-04-06 07:20:44.480041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=886371)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=886371)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=886377)[0m 2024-04-06 07:20:40.918356: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=886374)[0m 2024-04-06 07:20:41.028596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=886374)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=886370)[0m 2024-04-06 07:20:44.492542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 07:21:01,917 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:21:03,188 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:21:04,448 | server.py:125 | fit progress: (1, 2.2956955432891846, {'accuracy': 0.1172, 'data_size': 10000}, 40.75887062199763)
INFO flwr 2024-04-06 07:21:04,448 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:21:04,448 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:21:13,543 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:21:15,947 | server.py:125 | fit progress: (2, 2.2771196365356445, {'accuracy': 0.1428, 'data_size': 10000}, 52.25822308199713)
INFO flwr 2024-04-06 07:21:15,947 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:21:15,948 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:21:24,840 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:21:28,368 | server.py:125 | fit progress: (3, 2.1089425086975098, {'accuracy': 0.3646, 'data_size': 10000}, 64.67859739199048)
INFO flwr 2024-04-06 07:21:28,368 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:21:28,368 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:21:37,040 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:21:41,755 | server.py:125 | fit progress: (4, 1.9696708917617798, {'accuracy': 0.4999, 'data_size': 10000}, 78.06610750599066)
INFO flwr 2024-04-06 07:21:41,755 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:21:41,756 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:21:50,631 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:21:55,833 | server.py:125 | fit progress: (5, 1.859223484992981, {'accuracy': 0.6023, 'data_size': 10000}, 92.14357572901645)
INFO flwr 2024-04-06 07:21:55,833 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:21:55,833 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:22:04,538 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:22:10,878 | server.py:125 | fit progress: (6, 1.7741622924804688, {'accuracy': 0.706, 'data_size': 10000}, 107.18927033001091)
INFO flwr 2024-04-06 07:22:10,879 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:22:10,879 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:22:18,952 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:22:26,230 | server.py:125 | fit progress: (7, 1.7432074546813965, {'accuracy': 0.7286, 'data_size': 10000}, 122.54127893099212)
INFO flwr 2024-04-06 07:22:26,231 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:22:26,231 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:22:35,379 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 07:22:43,791 | server.py:125 | fit progress: (8, 1.7251585721969604, {'accuracy': 0.7369, 'data_size': 10000}, 140.10233771399362)
INFO flwr 2024-04-06 07:22:43,792 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 07:22:43,792 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:22:52,203 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 07:23:01,472 | server.py:125 | fit progress: (9, 1.676939845085144, {'accuracy': 0.7929, 'data_size': 10000}, 157.78313605301082)
INFO flwr 2024-04-06 07:23:01,472 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 07:23:01,472 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:23:10,529 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 07:23:23,800 | server.py:125 | fit progress: (10, 1.6661547422409058, {'accuracy': 0.8015, 'data_size': 10000}, 180.1108152169909)
INFO flwr 2024-04-06 07:23:23,800 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 07:23:23,800 | server.py:153 | FL finished in 180.11121316600475
INFO flwr 2024-04-06 07:23:23,800 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 07:23:23,800 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 07:23:23,801 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 07:23:23,801 | app.py:229 | app_fit: losses_centralized [(0, 2.3025217056274414), (1, 2.2956955432891846), (2, 2.2771196365356445), (3, 2.1089425086975098), (4, 1.9696708917617798), (5, 1.859223484992981), (6, 1.7741622924804688), (7, 1.7432074546813965), (8, 1.7251585721969604), (9, 1.676939845085144), (10, 1.6661547422409058)]
INFO flwr 2024-04-06 07:23:23,801 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1172), (2, 0.1428), (3, 0.3646), (4, 0.4999), (5, 0.6023), (6, 0.706), (7, 0.7286), (8, 0.7369), (9, 0.7929), (10, 0.8015)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8015
wandb:     loss 1.66615
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_071949-4dvfss2w
wandb: Find logs at: ./wandb/offline-run-20240406_071949-4dvfss2w/logs
INFO flwr 2024-04-06 07:23:27,397 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 07:30:35,225 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=886370)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=886370)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 07:30:41,218	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 07:30:41,518	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 07:30:41,806	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a60fb83f939cba20.zip' (8.81MiB) to Ray cluster...
2024-04-06 07:30:41,840	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a60fb83f939cba20.zip'.
INFO flwr 2024-04-06 07:30:53,138 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 142805313741.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65487991603.0}
INFO flwr 2024-04-06 07:30:53,138 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 07:30:53,138 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 07:30:53,156 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 07:30:53,158 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 07:30:53,158 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 07:30:53,159 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 07:30:55,131 | server.py:94 | initial parameters (loss, other metrics): 2.3025450706481934, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-06 07:30:55,132 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 07:30:55,132 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=894664)[0m 2024-04-06 07:30:59.200854: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=894664)[0m 2024-04-06 07:30:59.296649: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=894664)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=894571)[0m 2024-04-06 07:31:01.493474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=894663)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=894663)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=894575)[0m 2024-04-06 07:30:59.645763: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=894575)[0m 2024-04-06 07:30:59.739252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=894575)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=894662)[0m 2024-04-06 07:31:01.805578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 07:31:14,730 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:31:15,264 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:31:16,523 | server.py:125 | fit progress: (1, 2.278585433959961, {'accuracy': 0.1944, 'data_size': 10000}, 21.391252778004855)
INFO flwr 2024-04-06 07:31:16,524 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:31:16,524 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:31:26,761 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:31:29,800 | server.py:125 | fit progress: (2, 2.179271936416626, {'accuracy': 0.4135, 'data_size': 10000}, 34.667565998010105)
INFO flwr 2024-04-06 07:31:29,800 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:31:29,800 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:31:38,261 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:31:41,862 | server.py:125 | fit progress: (3, 1.9142277240753174, {'accuracy': 0.5497, 'data_size': 10000}, 46.72958704599296)
INFO flwr 2024-04-06 07:31:41,862 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:31:41,862 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:31:50,962 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:31:55,885 | server.py:125 | fit progress: (4, 1.7490423917770386, {'accuracy': 0.7179, 'data_size': 10000}, 60.752462067001034)
INFO flwr 2024-04-06 07:31:55,885 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:31:55,885 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:32:04,652 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:32:10,006 | server.py:125 | fit progress: (5, 1.6562113761901855, {'accuracy': 0.8315, 'data_size': 10000}, 74.87377848199685)
INFO flwr 2024-04-06 07:32:10,006 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:32:10,007 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:32:18,790 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:32:24,888 | server.py:125 | fit progress: (6, 1.6570271253585815, {'accuracy': 0.8046, 'data_size': 10000}, 89.75547667601495)
INFO flwr 2024-04-06 07:32:24,888 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:32:24,888 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:32:34,077 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:32:42,816 | server.py:125 | fit progress: (7, 1.5946649312973022, {'accuracy': 0.8767, 'data_size': 10000}, 107.68354143001488)
INFO flwr 2024-04-06 07:32:42,816 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:32:42,816 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:32:51,691 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 07:33:00,283 | server.py:125 | fit progress: (8, 1.5748732089996338, {'accuracy': 0.8947, 'data_size': 10000}, 125.15117014100542)
INFO flwr 2024-04-06 07:33:00,284 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 07:33:00,284 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:33:09,143 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 07:33:18,944 | server.py:125 | fit progress: (9, 1.5604969263076782, {'accuracy': 0.9052, 'data_size': 10000}, 143.81198263299302)
INFO flwr 2024-04-06 07:33:18,944 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 07:33:18,945 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:33:28,084 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 07:33:39,728 | server.py:125 | fit progress: (10, 1.5560202598571777, {'accuracy': 0.9115, 'data_size': 10000}, 164.59550875800778)
INFO flwr 2024-04-06 07:33:39,728 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 07:33:39,728 | server.py:153 | FL finished in 164.59587396800634
INFO flwr 2024-04-06 07:33:39,728 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 07:33:39,728 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 07:33:39,728 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 07:33:39,729 | app.py:229 | app_fit: losses_centralized [(0, 2.3025450706481934), (1, 2.278585433959961), (2, 2.179271936416626), (3, 1.9142277240753174), (4, 1.7490423917770386), (5, 1.6562113761901855), (6, 1.6570271253585815), (7, 1.5946649312973022), (8, 1.5748732089996338), (9, 1.5604969263076782), (10, 1.5560202598571777)]
INFO flwr 2024-04-06 07:33:39,729 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.1944), (2, 0.4135), (3, 0.5497), (4, 0.7179), (5, 0.8315), (6, 0.8046), (7, 0.8767), (8, 0.8947), (9, 0.9052), (10, 0.9115)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9115
wandb:     loss 1.55602
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_073033-rv1v76kd
wandb: Find logs at: ./wandb/offline-run-20240406_073033-rv1v76kd/logs
INFO flwr 2024-04-06 07:33:43,356 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 07:40:51,004 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=894662)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=894662)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 07:40:56,077	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 07:40:56,568	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 07:40:56,960	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6e25944b403f22c3.zip' (8.82MiB) to Ray cluster...
2024-04-06 07:40:56,991	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6e25944b403f22c3.zip'.
INFO flwr 2024-04-06 07:41:08,077 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 65204056473.0, 'node:10.20.240.18': 1.0, 'memory': 142142798439.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 07:41:08,077 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 07:41:08,077 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 07:41:08,093 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 07:41:08,095 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 07:41:08,096 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 07:41:08,096 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 07:41:09,982 | server.py:94 | initial parameters (loss, other metrics): 2.302626132965088, {'accuracy': 0.1091, 'data_size': 10000}
INFO flwr 2024-04-06 07:41:09,982 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 07:41:09,982 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=899099)[0m 2024-04-06 07:41:14.532631: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=899099)[0m 2024-04-06 07:41:14.633434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=899099)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=899099)[0m 2024-04-06 07:41:16.942553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=899110)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=899110)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=899110)[0m 2024-04-06 07:41:14.908939: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=899110)[0m 2024-04-06 07:41:15.008044: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=899110)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=899103)[0m 2024-04-06 07:41:17.136496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 07:41:31,593 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:41:32,135 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:41:33,533 | server.py:125 | fit progress: (1, 2.2495079040527344, {'accuracy': 0.2984, 'data_size': 10000}, 23.550319906993536)
INFO flwr 2024-04-06 07:41:33,533 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:41:33,533 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:41:43,119 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:41:45,663 | server.py:125 | fit progress: (2, 2.080268621444702, {'accuracy': 0.3604, 'data_size': 10000}, 35.68119985901285)
INFO flwr 2024-04-06 07:41:45,664 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:41:45,664 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:41:54,121 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:41:58,014 | server.py:125 | fit progress: (3, 1.7515649795532227, {'accuracy': 0.7506, 'data_size': 10000}, 48.03148359499755)
INFO flwr 2024-04-06 07:41:58,014 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:41:58,014 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:42:06,514 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:42:10,602 | server.py:125 | fit progress: (4, 1.6672604084014893, {'accuracy': 0.7957, 'data_size': 10000}, 60.619809927011374)
INFO flwr 2024-04-06 07:42:10,602 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:42:10,603 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:42:19,711 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:42:26,133 | server.py:125 | fit progress: (5, 1.63655686378479, {'accuracy': 0.8315, 'data_size': 10000}, 76.15063840299263)
INFO flwr 2024-04-06 07:42:26,133 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:42:26,133 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:42:35,525 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:42:41,540 | server.py:125 | fit progress: (6, 1.574357032775879, {'accuracy': 0.8932, 'data_size': 10000}, 91.55762175199925)
INFO flwr 2024-04-06 07:42:41,540 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:42:41,540 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:42:49,680 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:42:56,507 | server.py:125 | fit progress: (7, 1.5647542476654053, {'accuracy': 0.8991, 'data_size': 10000}, 106.5245421360014)
INFO flwr 2024-04-06 07:42:56,507 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:42:56,507 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:43:05,018 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 07:43:12,466 | server.py:125 | fit progress: (8, 1.5539299249649048, {'accuracy': 0.9113, 'data_size': 10000}, 122.48323445100687)
INFO flwr 2024-04-06 07:43:12,466 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 07:43:12,466 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:43:21,201 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 07:43:31,072 | server.py:125 | fit progress: (9, 1.5482661724090576, {'accuracy': 0.9166, 'data_size': 10000}, 141.0894701530051)
INFO flwr 2024-04-06 07:43:31,072 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 07:43:31,072 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:43:39,560 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 07:43:48,921 | server.py:125 | fit progress: (10, 1.5386979579925537, {'accuracy': 0.9247, 'data_size': 10000}, 158.9388806920033)
INFO flwr 2024-04-06 07:43:48,921 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 07:43:48,921 | server.py:153 | FL finished in 158.9392401459918
INFO flwr 2024-04-06 07:43:48,922 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 07:43:48,922 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 07:43:48,922 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 07:43:48,922 | app.py:229 | app_fit: losses_centralized [(0, 2.302626132965088), (1, 2.2495079040527344), (2, 2.080268621444702), (3, 1.7515649795532227), (4, 1.6672604084014893), (5, 1.63655686378479), (6, 1.574357032775879), (7, 1.5647542476654053), (8, 1.5539299249649048), (9, 1.5482661724090576), (10, 1.5386979579925537)]
INFO flwr 2024-04-06 07:43:48,922 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1091), (1, 0.2984), (2, 0.3604), (3, 0.7506), (4, 0.7957), (5, 0.8315), (6, 0.8932), (7, 0.8991), (8, 0.9113), (9, 0.9166), (10, 0.9247)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9247
wandb:     loss 1.5387
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_074050-qdtaykuo
wandb: Find logs at: ./wandb/offline-run-20240406_074050-qdtaykuo/logs
INFO flwr 2024-04-06 07:43:52,505 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 07:51:00,486 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=899102)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=899102)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 07:51:05,112	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 07:51:05,512	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 07:51:05,941	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d222b6171d270943.zip' (8.84MiB) to Ray cluster...
2024-04-06 07:51:05,977	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d222b6171d270943.zip'.
INFO flwr 2024-04-06 07:51:17,158 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 143931234509.0, 'object_store_memory': 65970529075.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 07:51:17,158 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 07:51:17,159 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 07:51:17,183 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 07:51:17,184 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 07:51:17,184 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 07:51:17,184 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 07:51:20,361 | server.py:94 | initial parameters (loss, other metrics): 2.302656888961792, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-06 07:51:20,361 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 07:51:20,362 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=906432)[0m 2024-04-06 07:51:23.178993: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=906432)[0m 2024-04-06 07:51:23.275207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=906432)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=906431)[0m 2024-04-06 07:51:25.258927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=906431)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=906431)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=906429)[0m 2024-04-06 07:51:23.626048: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=906429)[0m 2024-04-06 07:51:23.719313: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=906429)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=906429)[0m 2024-04-06 07:51:25.917847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 07:51:38,457 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:51:38,968 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:51:40,214 | server.py:125 | fit progress: (1, 2.2501060962677, {'accuracy': 0.3821, 'data_size': 10000}, 19.85232156701386)
INFO flwr 2024-04-06 07:51:40,214 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:51:40,214 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:51:49,144 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:51:51,509 | server.py:125 | fit progress: (2, 1.9263867139816284, {'accuracy': 0.5506, 'data_size': 10000}, 31.14790971399634)
INFO flwr 2024-04-06 07:51:51,510 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:51:51,510 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:52:00,328 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:52:03,752 | server.py:125 | fit progress: (3, 1.7615712881088257, {'accuracy': 0.6934, 'data_size': 10000}, 43.39075962701463)
INFO flwr 2024-04-06 07:52:03,752 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:52:03,753 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:52:12,411 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:52:16,972 | server.py:125 | fit progress: (4, 1.613835096359253, {'accuracy': 0.8554, 'data_size': 10000}, 56.61066105199279)
INFO flwr 2024-04-06 07:52:16,972 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:52:16,973 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:52:25,777 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:52:30,703 | server.py:125 | fit progress: (5, 1.5754759311676025, {'accuracy': 0.8891, 'data_size': 10000}, 70.34181335000903)
INFO flwr 2024-04-06 07:52:30,704 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:52:30,704 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:52:39,599 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:52:45,754 | server.py:125 | fit progress: (6, 1.5557819604873657, {'accuracy': 0.9083, 'data_size': 10000}, 85.39282877399819)
INFO flwr 2024-04-06 07:52:45,755 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:52:45,755 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:52:54,869 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:53:02,731 | server.py:125 | fit progress: (7, 1.5554752349853516, {'accuracy': 0.9068, 'data_size': 10000}, 102.36915156198665)
INFO flwr 2024-04-06 07:53:02,731 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:53:02,731 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:53:11,302 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 07:53:19,650 | server.py:125 | fit progress: (8, 1.547835111618042, {'accuracy': 0.9139, 'data_size': 10000}, 119.28872517301352)
INFO flwr 2024-04-06 07:53:19,650 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 07:53:19,651 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:53:28,716 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 07:53:37,830 | server.py:125 | fit progress: (9, 1.5488483905792236, {'accuracy': 0.9121, 'data_size': 10000}, 137.4686162660073)
INFO flwr 2024-04-06 07:53:37,830 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 07:53:37,831 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:53:47,034 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 07:53:57,423 | server.py:125 | fit progress: (10, 1.5423986911773682, {'accuracy': 0.9204, 'data_size': 10000}, 157.06108352699084)
INFO flwr 2024-04-06 07:53:57,423 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 07:53:57,423 | server.py:153 | FL finished in 157.06147705198964
INFO flwr 2024-04-06 07:53:57,423 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 07:53:57,423 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 07:53:57,423 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 07:53:57,423 | app.py:229 | app_fit: losses_centralized [(0, 2.302656888961792), (1, 2.2501060962677), (2, 1.9263867139816284), (3, 1.7615712881088257), (4, 1.613835096359253), (5, 1.5754759311676025), (6, 1.5557819604873657), (7, 1.5554752349853516), (8, 1.547835111618042), (9, 1.5488483905792236), (10, 1.5423986911773682)]
INFO flwr 2024-04-06 07:53:57,424 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.3821), (2, 0.5506), (3, 0.6934), (4, 0.8554), (5, 0.8891), (6, 0.9083), (7, 0.9068), (8, 0.9139), (9, 0.9121), (10, 0.9204)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9204
wandb:     loss 1.5424
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_075100-dtss1gqc
wandb: Find logs at: ./wandb/offline-run-20240406_075100-dtss1gqc/logs
INFO flwr 2024-04-06 07:54:01,006 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:01:09,042 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=906422)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=906422)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 08:01:14,964	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:01:15,332	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:01:15,627	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b7075de0e433ff33.zip' (8.86MiB) to Ray cluster...
2024-04-06 08:01:15,652	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b7075de0e433ff33.zip'.
INFO flwr 2024-04-06 08:01:26,650 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 65088857702.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 141874001306.0}
INFO flwr 2024-04-06 08:01:26,651 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:01:26,651 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:01:26,666 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:01:26,667 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:01:26,667 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:01:26,667 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 08:01:29,031 | server.py:94 | initial parameters (loss, other metrics): 2.3026978969573975, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-06 08:01:29,031 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:01:29,032 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=911204)[0m 2024-04-06 08:01:33.281622: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=911204)[0m 2024-04-06 08:01:33.383034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=911204)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=911204)[0m 2024-04-06 08:01:35.855624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=911204)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=911204)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=911198)[0m 2024-04-06 08:01:33.688318: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=911198)[0m 2024-04-06 08:01:33.803589: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=911198)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=911198)[0m 2024-04-06 08:01:36.153545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 08:01:49,890 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:01:50,491 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:01:51,868 | server.py:125 | fit progress: (1, 2.2322795391082764, {'accuracy': 0.1788, 'data_size': 10000}, 22.836483122984646)
INFO flwr 2024-04-06 08:01:51,868 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:01:51,869 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:02:01,785 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:02:04,650 | server.py:125 | fit progress: (2, 1.917125940322876, {'accuracy': 0.6014, 'data_size': 10000}, 35.61890173298889)
INFO flwr 2024-04-06 08:02:04,651 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:02:04,651 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:02:13,724 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 08:02:17,284 | server.py:125 | fit progress: (3, 1.709920883178711, {'accuracy': 0.7813, 'data_size': 10000}, 48.25202506399364)
INFO flwr 2024-04-06 08:02:17,284 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 08:02:17,284 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:02:26,254 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 08:02:31,163 | server.py:125 | fit progress: (4, 1.587534785270691, {'accuracy': 0.8855, 'data_size': 10000}, 62.131814616004704)
INFO flwr 2024-04-06 08:02:31,164 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 08:02:31,164 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:02:41,445 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 08:02:47,238 | server.py:125 | fit progress: (5, 1.5886527299880981, {'accuracy': 0.8768, 'data_size': 10000}, 78.20627921298728)
INFO flwr 2024-04-06 08:02:47,239 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 08:02:47,239 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:02:57,365 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 08:03:03,702 | server.py:125 | fit progress: (6, 1.5545741319656372, {'accuracy': 0.9083, 'data_size': 10000}, 94.6706122510077)
INFO flwr 2024-04-06 08:03:03,703 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 08:03:03,703 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:03:13,941 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 08:03:22,674 | server.py:125 | fit progress: (7, 1.5450879335403442, {'accuracy': 0.9184, 'data_size': 10000}, 113.64202090699109)
INFO flwr 2024-04-06 08:03:22,674 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 08:03:22,674 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:03:32,978 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:03:41,482 | server.py:125 | fit progress: (8, 1.548055648803711, {'accuracy': 0.9148, 'data_size': 10000}, 132.45039648198872)
INFO flwr 2024-04-06 08:03:41,482 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:03:41,483 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:03:51,461 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:04:06,319 | server.py:125 | fit progress: (9, 1.5339689254760742, {'accuracy': 0.9278, 'data_size': 10000}, 157.28756139599136)
INFO flwr 2024-04-06 08:04:06,320 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:04:06,320 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:04:16,424 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:04:27,053 | server.py:125 | fit progress: (10, 1.5313823223114014, {'accuracy': 0.9306, 'data_size': 10000}, 178.02182099499623)
INFO flwr 2024-04-06 08:04:27,054 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:04:27,054 | server.py:153 | FL finished in 178.0223116480047
INFO flwr 2024-04-06 08:04:27,054 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:04:27,054 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:04:27,054 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:04:27,055 | app.py:229 | app_fit: losses_centralized [(0, 2.3026978969573975), (1, 2.2322795391082764), (2, 1.917125940322876), (3, 1.709920883178711), (4, 1.587534785270691), (5, 1.5886527299880981), (6, 1.5545741319656372), (7, 1.5450879335403442), (8, 1.548055648803711), (9, 1.5339689254760742), (10, 1.5313823223114014)]
INFO flwr 2024-04-06 08:04:27,055 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1788), (2, 0.6014), (3, 0.7813), (4, 0.8855), (5, 0.8768), (6, 0.9083), (7, 0.9184), (8, 0.9148), (9, 0.9278), (10, 0.9306)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9306
wandb:     loss 1.53138
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_080108-rae6ee4n
wandb: Find logs at: ./wandb/offline-run-20240406_080108-rae6ee4n/logs
INFO flwr 2024-04-06 08:04:30,677 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:11:41,382 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=911206)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=911206)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 08:11:46,845	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:11:47,230	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:11:47,591	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_abd5a77e38ca9b95.zip' (8.87MiB) to Ray cluster...
2024-04-06 08:11:47,612	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_abd5a77e38ca9b95.zip'.
INFO flwr 2024-04-06 08:11:58,353 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 67340222054.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 147127184794.0}
INFO flwr 2024-04-06 08:11:58,353 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:11:58,354 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:11:58,373 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:11:58,374 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:11:58,374 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:11:58,375 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 08:12:00,308 | server.py:94 | initial parameters (loss, other metrics): 2.302424430847168, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-06 08:12:00,308 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:12:00,308 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=915552)[0m 2024-04-06 08:12:04.406644: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=915545)[0m 2024-04-06 08:12:04.415437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=915545)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=915557)[0m 2024-04-06 08:12:06.653330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=915547)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=915547)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=915558)[0m 2024-04-06 08:12:04.759992: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=915556)[0m 2024-04-06 08:12:04.758953: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=915556)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=915558)[0m 2024-04-06 08:12:06.819332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 08:12:27,074 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:12:27,610 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:12:29,095 | server.py:125 | fit progress: (1, 2.1724390983581543, {'accuracy': 0.4354, 'data_size': 10000}, 28.7870473319781)
INFO flwr 2024-04-06 08:12:29,096 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:12:29,096 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:12:39,730 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:12:42,171 | server.py:125 | fit progress: (2, 1.8482836484909058, {'accuracy': 0.6289, 'data_size': 10000}, 41.86285148898605)
INFO flwr 2024-04-06 08:12:42,171 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:12:42,172 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:12:51,902 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 08:12:55,572 | server.py:125 | fit progress: (3, 1.702883243560791, {'accuracy': 0.7628, 'data_size': 10000}, 55.263698522991035)
INFO flwr 2024-04-06 08:12:55,572 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 08:12:55,572 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:13:05,492 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 08:13:10,226 | server.py:125 | fit progress: (4, 1.6364907026290894, {'accuracy': 0.8285, 'data_size': 10000}, 69.91750736598624)
INFO flwr 2024-04-06 08:13:10,227 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 08:13:10,227 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:13:20,548 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 08:13:25,543 | server.py:125 | fit progress: (5, 1.5523898601531982, {'accuracy': 0.9111, 'data_size': 10000}, 85.23504258098546)
INFO flwr 2024-04-06 08:13:25,543 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 08:13:25,544 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:13:35,260 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 08:13:41,404 | server.py:125 | fit progress: (6, 1.5457738637924194, {'accuracy': 0.9165, 'data_size': 10000}, 101.09623690499575)
INFO flwr 2024-04-06 08:13:41,405 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 08:13:41,405 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:13:50,873 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 08:13:57,874 | server.py:125 | fit progress: (7, 1.5468169450759888, {'accuracy': 0.9151, 'data_size': 10000}, 117.56612815498374)
INFO flwr 2024-04-06 08:13:57,875 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 08:13:57,875 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:14:07,593 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:14:15,294 | server.py:125 | fit progress: (8, 1.549705982208252, {'accuracy': 0.9125, 'data_size': 10000}, 134.98590319100185)
INFO flwr 2024-04-06 08:14:15,294 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:14:15,295 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:14:25,178 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:14:34,435 | server.py:125 | fit progress: (9, 1.5378882884979248, {'accuracy': 0.9238, 'data_size': 10000}, 154.1270443139947)
INFO flwr 2024-04-06 08:14:34,436 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:14:34,436 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:14:44,590 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:14:54,885 | server.py:125 | fit progress: (10, 1.5316205024719238, {'accuracy': 0.9294, 'data_size': 10000}, 174.5766999979969)
INFO flwr 2024-04-06 08:14:54,885 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:14:54,885 | server.py:153 | FL finished in 174.5771298680047
INFO flwr 2024-04-06 08:14:54,885 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:14:54,886 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:14:54,886 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:14:54,886 | app.py:229 | app_fit: losses_centralized [(0, 2.302424430847168), (1, 2.1724390983581543), (2, 1.8482836484909058), (3, 1.702883243560791), (4, 1.6364907026290894), (5, 1.5523898601531982), (6, 1.5457738637924194), (7, 1.5468169450759888), (8, 1.549705982208252), (9, 1.5378882884979248), (10, 1.5316205024719238)]
INFO flwr 2024-04-06 08:14:54,886 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.4354), (2, 0.6289), (3, 0.7628), (4, 0.8285), (5, 0.9111), (6, 0.9165), (7, 0.9151), (8, 0.9125), (9, 0.9238), (10, 0.9294)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9294
wandb:     loss 1.53162
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_081140-ngfg5yyf
wandb: Find logs at: ./wandb/offline-run-20240406_081140-ngfg5yyf/logs
INFO flwr 2024-04-06 08:14:58,477 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:22:06,343 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=915552)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=915552)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 08:22:12,580	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:22:13,259	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:22:13,594	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2da50bb6c0732a02.zip' (8.89MiB) to Ray cluster...
2024-04-06 08:22:13,621	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2da50bb6c0732a02.zip'.
INFO flwr 2024-04-06 08:22:24,808 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 135252492493.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62251068211.0}
INFO flwr 2024-04-06 08:22:24,808 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:22:24,808 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:22:24,823 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:22:24,825 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:22:24,825 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:22:24,825 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 08:22:27,318 | server.py:94 | initial parameters (loss, other metrics): 2.3027634620666504, {'accuracy': 0.0579, 'data_size': 10000}
INFO flwr 2024-04-06 08:22:27,318 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:22:27,318 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=923054)[0m 2024-04-06 08:22:30.996844: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=923054)[0m 2024-04-06 08:22:31.094359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=923054)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=923054)[0m 2024-04-06 08:22:33.265779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=923056)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=923056)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=923055)[0m 2024-04-06 08:22:31.192136: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=923055)[0m 2024-04-06 08:22:31.284441: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=923055)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=923052)[0m 2024-04-06 08:22:33.481421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 08:22:49,009 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:22:49,556 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:22:50,828 | server.py:125 | fit progress: (1, 2.302705764770508, {'accuracy': 0.06, 'data_size': 10000}, 23.509975934022805)
INFO flwr 2024-04-06 08:22:50,829 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:22:50,829 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:23:01,099 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:23:03,686 | server.py:125 | fit progress: (2, 2.302654504776001, {'accuracy': 0.0577, 'data_size': 10000}, 36.36745491201873)
INFO flwr 2024-04-06 08:23:03,686 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:23:03,686 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:23:13,133 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 08:23:16,943 | server.py:125 | fit progress: (3, 2.3026182651519775, {'accuracy': 0.0787, 'data_size': 10000}, 49.62437474401668)
INFO flwr 2024-04-06 08:23:16,943 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 08:23:16,943 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:23:26,953 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 08:23:31,773 | server.py:125 | fit progress: (4, 2.302574634552002, {'accuracy': 0.1013, 'data_size': 10000}, 64.45433597802185)
INFO flwr 2024-04-06 08:23:31,773 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 08:23:31,773 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:23:41,715 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 08:23:47,529 | server.py:125 | fit progress: (5, 2.3025176525115967, {'accuracy': 0.0826, 'data_size': 10000}, 80.2110271640122)
INFO flwr 2024-04-06 08:23:47,530 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 08:23:47,530 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:23:57,385 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 08:24:03,214 | server.py:125 | fit progress: (6, 2.302471399307251, {'accuracy': 0.0898, 'data_size': 10000}, 95.89578164499835)
INFO flwr 2024-04-06 08:24:03,214 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 08:24:03,215 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:24:12,870 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 08:24:21,423 | server.py:125 | fit progress: (7, 2.302426815032959, {'accuracy': 0.1117, 'data_size': 10000}, 114.10467306600185)
INFO flwr 2024-04-06 08:24:21,423 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 08:24:21,423 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:24:30,710 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:24:38,392 | server.py:125 | fit progress: (8, 2.302382230758667, {'accuracy': 0.1148, 'data_size': 10000}, 131.07403408299433)
INFO flwr 2024-04-06 08:24:38,393 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:24:38,393 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:24:47,601 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:24:55,927 | server.py:125 | fit progress: (9, 2.302337408065796, {'accuracy': 0.1176, 'data_size': 10000}, 148.60867115901783)
INFO flwr 2024-04-06 08:24:55,927 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:24:55,927 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:25:05,420 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:25:14,815 | server.py:125 | fit progress: (10, 2.302292823791504, {'accuracy': 0.1136, 'data_size': 10000}, 167.49704387201928)
INFO flwr 2024-04-06 08:25:14,816 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:25:14,816 | server.py:153 | FL finished in 167.49749897100264
INFO flwr 2024-04-06 08:25:14,816 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:25:14,816 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:25:14,816 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:25:14,816 | app.py:229 | app_fit: losses_centralized [(0, 2.3027634620666504), (1, 2.302705764770508), (2, 2.302654504776001), (3, 2.3026182651519775), (4, 2.302574634552002), (5, 2.3025176525115967), (6, 2.302471399307251), (7, 2.302426815032959), (8, 2.302382230758667), (9, 2.302337408065796), (10, 2.302292823791504)]
INFO flwr 2024-04-06 08:25:14,816 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0579), (1, 0.06), (2, 0.0577), (3, 0.0787), (4, 0.1013), (5, 0.0826), (6, 0.0898), (7, 0.1117), (8, 0.1148), (9, 0.1176), (10, 0.1136)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1136
wandb:     loss 2.30229
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_082205-bb0skjte
wandb: Find logs at: ./wandb/offline-run-20240406_082205-bb0skjte/logs
INFO flwr 2024-04-06 08:25:18,401 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:32:24,738 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=923051)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=923051)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 08:32:29,984	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:32:30,348	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:32:30,777	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b15b17f650b8d373.zip' (8.91MiB) to Ray cluster...
2024-04-06 08:32:30,799	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b15b17f650b8d373.zip'.
INFO flwr 2024-04-06 08:32:41,533 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 67272645427.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 146969505997.0}
INFO flwr 2024-04-06 08:32:41,533 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:32:41,534 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:32:41,547 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:32:41,548 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:32:41,548 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:32:41,548 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 08:32:44,323 | server.py:94 | initial parameters (loss, other metrics): 2.302609443664551, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-06 08:32:44,324 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:32:44,324 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=927735)[0m 2024-04-06 08:32:47.127938: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=927735)[0m 2024-04-06 08:32:47.223958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=927735)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=927735)[0m 2024-04-06 08:32:49.349279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=927738)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=927738)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=927733)[0m 2024-04-06 08:32:48.026298: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=927733)[0m 2024-04-06 08:32:48.121611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=927733)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=927733)[0m 2024-04-06 08:32:50.371684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 08:33:03,847 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:33:04,368 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:33:05,606 | server.py:125 | fit progress: (1, 2.289590835571289, {'accuracy': 0.2508, 'data_size': 10000}, 21.28271444499842)
INFO flwr 2024-04-06 08:33:05,607 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:33:05,607 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:33:15,850 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:33:18,281 | server.py:125 | fit progress: (2, 2.130155086517334, {'accuracy': 0.4129, 'data_size': 10000}, 33.95699228401645)
INFO flwr 2024-04-06 08:33:18,281 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:33:18,281 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:33:27,928 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 08:33:31,347 | server.py:125 | fit progress: (3, 1.9325765371322632, {'accuracy': 0.5204, 'data_size': 10000}, 47.02286158001516)
INFO flwr 2024-04-06 08:33:31,347 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 08:33:31,347 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:33:40,878 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 08:33:45,073 | server.py:125 | fit progress: (4, 1.7188555002212524, {'accuracy': 0.753, 'data_size': 10000}, 60.74899123699288)
INFO flwr 2024-04-06 08:33:45,073 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 08:33:45,073 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:33:55,208 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 08:34:00,324 | server.py:125 | fit progress: (5, 1.6327725648880005, {'accuracy': 0.8465, 'data_size': 10000}, 75.99983306901413)
INFO flwr 2024-04-06 08:34:00,324 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 08:34:00,325 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:34:09,991 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 08:34:16,098 | server.py:125 | fit progress: (6, 1.6159557104110718, {'accuracy': 0.8528, 'data_size': 10000}, 91.77474753200659)
INFO flwr 2024-04-06 08:34:16,099 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 08:34:16,099 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:34:25,411 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 08:34:32,226 | server.py:125 | fit progress: (7, 1.5885497331619263, {'accuracy': 0.8824, 'data_size': 10000}, 107.90249771100935)
INFO flwr 2024-04-06 08:34:32,227 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 08:34:32,227 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:34:42,105 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:34:49,669 | server.py:125 | fit progress: (8, 1.5913636684417725, {'accuracy': 0.8777, 'data_size': 10000}, 125.34522088200902)
INFO flwr 2024-04-06 08:34:49,669 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:34:49,669 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:34:59,416 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:35:08,993 | server.py:125 | fit progress: (9, 1.5761234760284424, {'accuracy': 0.8873, 'data_size': 10000}, 144.66877636901336)
INFO flwr 2024-04-06 08:35:08,993 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:35:08,993 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:35:18,559 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:35:29,154 | server.py:125 | fit progress: (10, 1.5711100101470947, {'accuracy': 0.8961, 'data_size': 10000}, 164.83074631702038)
INFO flwr 2024-04-06 08:35:29,155 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:35:29,155 | server.py:153 | FL finished in 164.83111198901315
INFO flwr 2024-04-06 08:35:29,155 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:35:29,155 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:35:29,155 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:35:29,155 | app.py:229 | app_fit: losses_centralized [(0, 2.302609443664551), (1, 2.289590835571289), (2, 2.130155086517334), (3, 1.9325765371322632), (4, 1.7188555002212524), (5, 1.6327725648880005), (6, 1.6159557104110718), (7, 1.5885497331619263), (8, 1.5913636684417725), (9, 1.5761234760284424), (10, 1.5711100101470947)]
INFO flwr 2024-04-06 08:35:29,155 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.2508), (2, 0.4129), (3, 0.5204), (4, 0.753), (5, 0.8465), (6, 0.8528), (7, 0.8824), (8, 0.8777), (9, 0.8873), (10, 0.8961)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8961
wandb:     loss 1.57111
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_083224-hh3dw9p2
wandb: Find logs at: ./wandb/offline-run-20240406_083224-hh3dw9p2/logs
INFO flwr 2024-04-06 08:35:32,744 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:42:39,305 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=927735)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=927735)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 08:42:43,892	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:42:44,236	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:42:44,645	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b857c104cb3b498c.zip' (8.93MiB) to Ray cluster...
2024-04-06 08:42:44,667	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b857c104cb3b498c.zip'.
INFO flwr 2024-04-06 08:42:55,653 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 141601491149.0, 'object_store_memory': 64972067635.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 08:42:55,653 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:42:55,653 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:42:55,671 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:42:55,672 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:42:55,672 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:42:55,672 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 08:42:58,318 | server.py:94 | initial parameters (loss, other metrics): 2.302713632583618, {'accuracy': 0.0954, 'data_size': 10000}
INFO flwr 2024-04-06 08:42:58,319 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:42:58,319 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=935249)[0m 2024-04-06 08:43:02.104103: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=935247)[0m 2024-04-06 08:43:02.261769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=935247)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=935249)[0m 2024-04-06 08:43:04.398889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=935249)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=935249)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=935248)[0m 2024-04-06 08:43:02.319112: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=935248)[0m 2024-04-06 08:43:02.413692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=935248)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=935248)[0m 2024-04-06 08:43:04.998238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 08:43:20,060 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:43:20,611 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:43:21,916 | server.py:125 | fit progress: (1, 2.2620091438293457, {'accuracy': 0.1245, 'data_size': 10000}, 23.59722600801615)
INFO flwr 2024-04-06 08:43:21,916 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:43:21,917 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:43:32,578 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:43:34,977 | server.py:125 | fit progress: (2, 2.026205062866211, {'accuracy': 0.4075, 'data_size': 10000}, 36.65836446802132)
INFO flwr 2024-04-06 08:43:34,978 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:43:34,978 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:43:45,297 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 08:43:48,939 | server.py:125 | fit progress: (3, 1.7250455617904663, {'accuracy': 0.7393, 'data_size': 10000}, 50.62019346101442)
INFO flwr 2024-04-06 08:43:48,939 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 08:43:48,940 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:43:58,885 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 08:44:03,080 | server.py:125 | fit progress: (4, 1.5982258319854736, {'accuracy': 0.8737, 'data_size': 10000}, 64.7607816280215)
INFO flwr 2024-04-06 08:44:03,080 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 08:44:03,080 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:44:12,548 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 08:44:18,279 | server.py:125 | fit progress: (5, 1.635732889175415, {'accuracy': 0.8281, 'data_size': 10000}, 79.96005822200095)
INFO flwr 2024-04-06 08:44:18,279 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 08:44:18,279 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:44:27,762 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 08:44:33,855 | server.py:125 | fit progress: (6, 1.575538992881775, {'accuracy': 0.8896, 'data_size': 10000}, 95.53617818999919)
INFO flwr 2024-04-06 08:44:33,855 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 08:44:33,856 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:44:43,341 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 08:44:50,303 | server.py:125 | fit progress: (7, 1.5626848936080933, {'accuracy': 0.9015, 'data_size': 10000}, 111.98443280000356)
INFO flwr 2024-04-06 08:44:50,304 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 08:44:50,304 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:45:00,069 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:45:08,577 | server.py:125 | fit progress: (8, 1.5543599128723145, {'accuracy': 0.9097, 'data_size': 10000}, 130.25809563300572)
INFO flwr 2024-04-06 08:45:08,577 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:45:08,577 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:45:18,291 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:45:26,916 | server.py:125 | fit progress: (9, 1.5585670471191406, {'accuracy': 0.9051, 'data_size': 10000}, 148.59746716701193)
INFO flwr 2024-04-06 08:45:26,917 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:45:26,917 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:45:35,932 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:45:52,769 | server.py:125 | fit progress: (10, 1.5442079305648804, {'accuracy': 0.9186, 'data_size': 10000}, 174.44998509800644)
INFO flwr 2024-04-06 08:45:52,769 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:45:52,769 | server.py:153 | FL finished in 174.45037590101128
INFO flwr 2024-04-06 08:45:52,769 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:45:52,769 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:45:52,770 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:45:52,770 | app.py:229 | app_fit: losses_centralized [(0, 2.302713632583618), (1, 2.2620091438293457), (2, 2.026205062866211), (3, 1.7250455617904663), (4, 1.5982258319854736), (5, 1.635732889175415), (6, 1.575538992881775), (7, 1.5626848936080933), (8, 1.5543599128723145), (9, 1.5585670471191406), (10, 1.5442079305648804)]
INFO flwr 2024-04-06 08:45:52,770 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0954), (1, 0.1245), (2, 0.4075), (3, 0.7393), (4, 0.8737), (5, 0.8281), (6, 0.8896), (7, 0.9015), (8, 0.9097), (9, 0.9051), (10, 0.9186)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9186
wandb:     loss 1.54421
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_084238-45k8ryye
wandb: Find logs at: ./wandb/offline-run-20240406_084238-45k8ryye/logs
INFO flwr 2024-04-06 08:45:56,412 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:53:03,176 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=935248)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=935248)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 08:53:08,165	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:53:08,636	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:53:09,059	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2781f95376f3d672.zip' (8.94MiB) to Ray cluster...
2024-04-06 08:53:09,082	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2781f95376f3d672.zip'.
INFO flwr 2024-04-06 08:53:19,864 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 141460470784.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 64911630336.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 08:53:19,865 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:53:19,865 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:53:19,880 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:53:19,880 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:53:19,880 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:53:19,881 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 08:53:22,376 | server.py:94 | initial parameters (loss, other metrics): 2.3025598526000977, {'accuracy': 0.1317, 'data_size': 10000}
INFO flwr 2024-04-06 08:53:22,386 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:53:22,386 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=939623)[0m 2024-04-06 08:53:25.750442: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=939623)[0m 2024-04-06 08:53:25.851565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=939623)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=939623)[0m 2024-04-06 08:53:27.950231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=939633)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=939633)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=939628)[0m 2024-04-06 08:53:26.180937: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=939628)[0m 2024-04-06 08:53:26.275249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=939628)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=939628)[0m 2024-04-06 08:53:28.730585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 08:53:42,211 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:53:42,710 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:53:43,991 | server.py:125 | fit progress: (1, 2.191697120666504, {'accuracy': 0.4283, 'data_size': 10000}, 21.60526831698371)
INFO flwr 2024-04-06 08:53:43,991 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:53:43,992 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:53:54,392 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:53:56,768 | server.py:125 | fit progress: (2, 1.78340482711792, {'accuracy': 0.7467, 'data_size': 10000}, 34.38195363199338)
INFO flwr 2024-04-06 08:53:56,768 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:53:56,768 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:54:06,805 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 08:54:10,238 | server.py:125 | fit progress: (3, 1.5960193872451782, {'accuracy': 0.8773, 'data_size': 10000}, 47.852132323983824)
INFO flwr 2024-04-06 08:54:10,238 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 08:54:10,239 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:54:19,774 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 08:54:23,862 | server.py:125 | fit progress: (4, 1.5680458545684814, {'accuracy': 0.8987, 'data_size': 10000}, 61.476069783006096)
INFO flwr 2024-04-06 08:54:23,862 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 08:54:23,863 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:54:33,166 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 08:54:38,282 | server.py:125 | fit progress: (5, 1.5684101581573486, {'accuracy': 0.8956, 'data_size': 10000}, 75.89619828900322)
INFO flwr 2024-04-06 08:54:38,282 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 08:54:38,283 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:54:48,324 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 08:54:54,073 | server.py:125 | fit progress: (6, 1.5447384119033813, {'accuracy': 0.9205, 'data_size': 10000}, 91.68658214298193)
INFO flwr 2024-04-06 08:54:54,073 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 08:54:54,073 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:55:03,793 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 08:55:10,606 | server.py:125 | fit progress: (7, 1.5418896675109863, {'accuracy': 0.9198, 'data_size': 10000}, 108.21963173500262)
INFO flwr 2024-04-06 08:55:10,606 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 08:55:10,606 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:55:20,071 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:55:27,727 | server.py:125 | fit progress: (8, 1.5346016883850098, {'accuracy': 0.9279, 'data_size': 10000}, 125.3407217549975)
INFO flwr 2024-04-06 08:55:27,727 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:55:27,727 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:55:37,075 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:55:45,761 | server.py:125 | fit progress: (9, 1.5300184488296509, {'accuracy': 0.931, 'data_size': 10000}, 143.37455430999398)
INFO flwr 2024-04-06 08:55:45,761 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:55:45,761 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:55:56,140 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:56:06,596 | server.py:125 | fit progress: (10, 1.5278996229171753, {'accuracy': 0.9331, 'data_size': 10000}, 164.20979591298965)
INFO flwr 2024-04-06 08:56:06,597 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:56:06,597 | server.py:153 | FL finished in 164.2107657059969
INFO flwr 2024-04-06 08:56:06,597 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:56:06,597 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:56:06,597 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:56:06,597 | app.py:229 | app_fit: losses_centralized [(0, 2.3025598526000977), (1, 2.191697120666504), (2, 1.78340482711792), (3, 1.5960193872451782), (4, 1.5680458545684814), (5, 1.5684101581573486), (6, 1.5447384119033813), (7, 1.5418896675109863), (8, 1.5346016883850098), (9, 1.5300184488296509), (10, 1.5278996229171753)]
INFO flwr 2024-04-06 08:56:06,598 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1317), (1, 0.4283), (2, 0.7467), (3, 0.8773), (4, 0.8987), (5, 0.8956), (6, 0.9205), (7, 0.9198), (8, 0.9279), (9, 0.931), (10, 0.9331)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9331
wandb:     loss 1.5279
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_085302-87rwy5qv
wandb: Find logs at: ./wandb/offline-run-20240406_085302-87rwy5qv/logs
INFO flwr 2024-04-06 08:56:10,054 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 09:03:16,901 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=939623)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=939623)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 09:03:21,608	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 09:03:21,904	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 09:03:22,279	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9369434f3ed0a870.zip' (8.96MiB) to Ray cluster...
2024-04-06 09:03:22,301	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9369434f3ed0a870.zip'.
INFO flwr 2024-04-06 09:03:33,446 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 141382847079.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 64878363033.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 09:03:33,446 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 09:03:33,447 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 09:03:33,463 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 09:03:33,465 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 09:03:33,465 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 09:03:33,465 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 09:03:35,609 | server.py:94 | initial parameters (loss, other metrics): 2.3027164936065674, {'accuracy': 0.1057, 'data_size': 10000}
INFO flwr 2024-04-06 09:03:35,609 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 09:03:35,610 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=947382)[0m 2024-04-06 09:03:39.128902: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=947382)[0m 2024-04-06 09:03:39.224009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=947382)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=947379)[0m 2024-04-06 09:03:41.565952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=947379)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=947379)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=947377)[0m 2024-04-06 09:03:40.295494: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=947377)[0m 2024-04-06 09:03:40.386311: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=947377)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=947377)[0m 2024-04-06 09:03:42.281459: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 09:03:56,717 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 09:03:57,244 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 09:03:58,661 | server.py:125 | fit progress: (1, 2.1366469860076904, {'accuracy': 0.3804, 'data_size': 10000}, 23.051548510004068)
INFO flwr 2024-04-06 09:03:58,661 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 09:03:58,662 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:04:09,873 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 09:04:12,262 | server.py:125 | fit progress: (2, 1.7801306247711182, {'accuracy': 0.7139, 'data_size': 10000}, 36.65247755101882)
INFO flwr 2024-04-06 09:04:12,262 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 09:04:12,263 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:04:22,478 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:04:26,101 | server.py:125 | fit progress: (3, 1.6172358989715576, {'accuracy': 0.8587, 'data_size': 10000}, 50.4913800439972)
INFO flwr 2024-04-06 09:04:26,101 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:04:26,101 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:04:36,168 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:04:40,408 | server.py:125 | fit progress: (4, 1.562514305114746, {'accuracy': 0.9037, 'data_size': 10000}, 64.79801680301898)
INFO flwr 2024-04-06 09:04:40,408 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:04:40,408 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:04:51,202 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 09:04:56,677 | server.py:125 | fit progress: (5, 1.5464037656784058, {'accuracy': 0.9177, 'data_size': 10000}, 81.06760873200255)
INFO flwr 2024-04-06 09:04:56,677 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 09:04:56,678 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:05:07,571 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 09:05:13,519 | server.py:125 | fit progress: (6, 1.540412187576294, {'accuracy': 0.9236, 'data_size': 10000}, 97.90961930900812)
INFO flwr 2024-04-06 09:05:13,519 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 09:05:13,520 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:05:23,751 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 09:05:30,721 | server.py:125 | fit progress: (7, 1.5320065021514893, {'accuracy': 0.9306, 'data_size': 10000}, 115.11123485499411)
INFO flwr 2024-04-06 09:05:30,721 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 09:05:30,721 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:05:40,403 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 09:05:48,150 | server.py:125 | fit progress: (8, 1.5309022665023804, {'accuracy': 0.9318, 'data_size': 10000}, 132.54013564399793)
INFO flwr 2024-04-06 09:05:48,150 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 09:05:48,150 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:05:58,009 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 09:06:06,417 | server.py:125 | fit progress: (9, 1.5260088443756104, {'accuracy': 0.9353, 'data_size': 10000}, 150.80737857200438)
INFO flwr 2024-04-06 09:06:06,417 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 09:06:06,417 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:06:16,578 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 09:06:26,198 | server.py:125 | fit progress: (10, 1.5268968343734741, {'accuracy': 0.9346, 'data_size': 10000}, 170.58848235401092)
INFO flwr 2024-04-06 09:06:26,198 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 09:06:26,198 | server.py:153 | FL finished in 170.58886434399756
INFO flwr 2024-04-06 09:06:26,199 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 09:06:26,199 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 09:06:26,199 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 09:06:26,199 | app.py:229 | app_fit: losses_centralized [(0, 2.3027164936065674), (1, 2.1366469860076904), (2, 1.7801306247711182), (3, 1.6172358989715576), (4, 1.562514305114746), (5, 1.5464037656784058), (6, 1.540412187576294), (7, 1.5320065021514893), (8, 1.5309022665023804), (9, 1.5260088443756104), (10, 1.5268968343734741)]
INFO flwr 2024-04-06 09:06:26,199 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1057), (1, 0.3804), (2, 0.7139), (3, 0.8587), (4, 0.9037), (5, 0.9177), (6, 0.9236), (7, 0.9306), (8, 0.9318), (9, 0.9353), (10, 0.9346)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9346
wandb:     loss 1.5269
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_090316-q2sajc6j
wandb: Find logs at: ./wandb/offline-run-20240406_090316-q2sajc6j/logs
INFO flwr 2024-04-06 09:06:29,729 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 09:13:36,292 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=947374)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=947374)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 09:13:49,115	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 09:13:49,541	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 09:13:49,966	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a4cc6f30c80495c0.zip' (8.98MiB) to Ray cluster...
2024-04-06 09:13:50,002	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a4cc6f30c80495c0.zip'.
INFO flwr 2024-04-06 09:14:01,114 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 141239914292.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64817106124.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 09:14:01,114 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 09:14:01,114 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 09:14:01,129 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 09:14:01,130 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 09:14:01,130 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 09:14:01,130 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 09:14:03,570 | server.py:94 | initial parameters (loss, other metrics): 2.302563190460205, {'accuracy': 0.0933, 'data_size': 10000}
INFO flwr 2024-04-06 09:14:03,571 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 09:14:03,571 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=951849)[0m 2024-04-06 09:14:07.379239: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=951841)[0m 2024-04-06 09:14:07.355152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=951841)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=951849)[0m 2024-04-06 09:14:09.713913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=951849)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=951849)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=951843)[0m 2024-04-06 09:14:07.642005: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=951843)[0m 2024-04-06 09:14:07.743043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=951843)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=951842)[0m 2024-04-06 09:14:09.751173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 09:14:25,538 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 09:14:26,121 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 09:14:27,374 | server.py:125 | fit progress: (1, 2.104541063308716, {'accuracy': 0.5704, 'data_size': 10000}, 23.80340729400632)
INFO flwr 2024-04-06 09:14:27,375 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 09:14:27,375 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:14:37,516 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 09:14:40,129 | server.py:125 | fit progress: (2, 1.6873167753219604, {'accuracy': 0.7822, 'data_size': 10000}, 36.55796897399705)
INFO flwr 2024-04-06 09:14:40,129 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 09:14:40,129 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:14:49,975 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:14:53,446 | server.py:125 | fit progress: (3, 1.623524785041809, {'accuracy': 0.8401, 'data_size': 10000}, 49.8749642779876)
INFO flwr 2024-04-06 09:14:53,446 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:14:53,446 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:15:03,198 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:15:07,290 | server.py:125 | fit progress: (4, 1.5530683994293213, {'accuracy': 0.9101, 'data_size': 10000}, 63.71941498699016)
INFO flwr 2024-04-06 09:15:07,291 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:15:07,291 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:15:16,821 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 09:15:22,029 | server.py:125 | fit progress: (5, 1.5434632301330566, {'accuracy': 0.9184, 'data_size': 10000}, 78.45848542798194)
INFO flwr 2024-04-06 09:15:22,030 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 09:15:22,030 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:15:31,650 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 09:15:37,734 | server.py:125 | fit progress: (6, 1.537093162536621, {'accuracy': 0.9245, 'data_size': 10000}, 94.16354159000912)
INFO flwr 2024-04-06 09:15:37,735 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 09:15:37,735 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:15:47,381 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 09:15:55,707 | server.py:125 | fit progress: (7, 1.5265740156173706, {'accuracy': 0.9343, 'data_size': 10000}, 112.13612339799874)
INFO flwr 2024-04-06 09:15:55,707 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 09:15:55,707 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:16:05,419 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 09:16:17,110 | server.py:125 | fit progress: (8, 1.5197865962982178, {'accuracy': 0.942, 'data_size': 10000}, 133.53946322898264)
INFO flwr 2024-04-06 09:16:17,111 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 09:16:17,111 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:16:27,560 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 09:16:36,059 | server.py:125 | fit progress: (9, 1.520416021347046, {'accuracy': 0.9413, 'data_size': 10000}, 152.48858073100564)
INFO flwr 2024-04-06 09:16:36,060 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 09:16:36,060 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:16:46,321 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 09:16:55,287 | server.py:125 | fit progress: (10, 1.5278950929641724, {'accuracy': 0.9329, 'data_size': 10000}, 171.7157154539891)
INFO flwr 2024-04-06 09:16:55,287 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 09:16:55,287 | server.py:153 | FL finished in 171.71614540199516
INFO flwr 2024-04-06 09:16:55,287 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 09:16:55,287 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 09:16:55,287 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 09:16:55,288 | app.py:229 | app_fit: losses_centralized [(0, 2.302563190460205), (1, 2.104541063308716), (2, 1.6873167753219604), (3, 1.623524785041809), (4, 1.5530683994293213), (5, 1.5434632301330566), (6, 1.537093162536621), (7, 1.5265740156173706), (8, 1.5197865962982178), (9, 1.520416021347046), (10, 1.5278950929641724)]
INFO flwr 2024-04-06 09:16:55,288 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0933), (1, 0.5704), (2, 0.7822), (3, 0.8401), (4, 0.9101), (5, 0.9184), (6, 0.9245), (7, 0.9343), (8, 0.942), (9, 0.9413), (10, 0.9329)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9329
wandb:     loss 1.5279
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_091335-vszig3qu
wandb: Find logs at: ./wandb/offline-run-20240406_091335-vszig3qu/logs
INFO flwr 2024-04-06 09:16:58,857 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 09:24:06,687 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=951838)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=951838)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 09:24:11,657	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 09:24:12,051	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 09:24:12,447	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4a904a258a93ee51.zip' (9.00MiB) to Ray cluster...
2024-04-06 09:24:12,470	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4a904a258a93ee51.zip'.
INFO flwr 2024-04-06 09:24:23,653 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 146243003392.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66961287168.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 09:24:23,654 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 09:24:23,654 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 09:24:23,668 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 09:24:23,669 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 09:24:23,669 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 09:24:23,669 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 09:24:26,711 | server.py:94 | initial parameters (loss, other metrics): 2.3027360439300537, {'accuracy': 0.117, 'data_size': 10000}
INFO flwr 2024-04-06 09:24:26,711 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 09:24:26,712 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=959031)[0m 2024-04-06 09:24:29.818033: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=959031)[0m 2024-04-06 09:24:29.932070: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=959031)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=959031)[0m 2024-04-06 09:24:31.978758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=959031)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=959031)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=959029)[0m 2024-04-06 09:24:30.090206: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=959032)[0m 2024-04-06 09:24:30.118478: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=959032)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=959022)[0m 2024-04-06 09:24:32.348851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 09:24:46,493 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 09:24:47,006 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 09:24:48,247 | server.py:125 | fit progress: (1, 2.0869762897491455, {'accuracy': 0.4397, 'data_size': 10000}, 21.535445161978714)
INFO flwr 2024-04-06 09:24:48,247 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 09:24:48,248 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:24:58,819 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 09:25:01,320 | server.py:125 | fit progress: (2, 1.7515456676483154, {'accuracy': 0.7352, 'data_size': 10000}, 34.60831446899101)
INFO flwr 2024-04-06 09:25:01,320 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 09:25:01,320 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:25:10,901 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:25:14,367 | server.py:125 | fit progress: (3, 1.5619356632232666, {'accuracy': 0.9027, 'data_size': 10000}, 47.6552372709848)
INFO flwr 2024-04-06 09:25:14,367 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:25:14,367 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:25:23,663 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:25:27,738 | server.py:125 | fit progress: (4, 1.563569188117981, {'accuracy': 0.9001, 'data_size': 10000}, 61.02643805000116)
INFO flwr 2024-04-06 09:25:27,738 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:25:27,739 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:25:37,571 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 09:25:42,704 | server.py:125 | fit progress: (5, 1.5532164573669434, {'accuracy': 0.9079, 'data_size': 10000}, 75.99224743500235)
INFO flwr 2024-04-06 09:25:42,704 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 09:25:42,704 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:25:51,943 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 09:25:57,770 | server.py:125 | fit progress: (6, 1.5496811866760254, {'accuracy': 0.9117, 'data_size': 10000}, 91.05848514300305)
INFO flwr 2024-04-06 09:25:57,771 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 09:25:57,771 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:26:07,382 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 09:26:14,209 | server.py:125 | fit progress: (7, 1.5368001461029053, {'accuracy': 0.9252, 'data_size': 10000}, 107.49692360998597)
INFO flwr 2024-04-06 09:26:14,209 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 09:26:14,209 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:26:24,030 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 09:26:31,693 | server.py:125 | fit progress: (8, 1.5314215421676636, {'accuracy': 0.9301, 'data_size': 10000}, 124.9817652629863)
INFO flwr 2024-04-06 09:26:31,694 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 09:26:31,694 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:26:42,184 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 09:26:50,367 | server.py:125 | fit progress: (9, 1.5370142459869385, {'accuracy': 0.9241, 'data_size': 10000}, 143.65559754899004)
INFO flwr 2024-04-06 09:26:50,368 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 09:26:50,368 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:27:00,164 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 09:27:09,474 | server.py:125 | fit progress: (10, 1.577061414718628, {'accuracy': 0.8844, 'data_size': 10000}, 162.7627727490035)
INFO flwr 2024-04-06 09:27:09,475 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 09:27:09,475 | server.py:153 | FL finished in 162.7631706649845
INFO flwr 2024-04-06 09:27:09,475 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 09:27:09,475 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 09:27:09,475 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 09:27:09,475 | app.py:229 | app_fit: losses_centralized [(0, 2.3027360439300537), (1, 2.0869762897491455), (2, 1.7515456676483154), (3, 1.5619356632232666), (4, 1.563569188117981), (5, 1.5532164573669434), (6, 1.5496811866760254), (7, 1.5368001461029053), (8, 1.5314215421676636), (9, 1.5370142459869385), (10, 1.577061414718628)]
INFO flwr 2024-04-06 09:27:09,476 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.117), (1, 0.4397), (2, 0.7352), (3, 0.9027), (4, 0.9001), (5, 0.9079), (6, 0.9117), (7, 0.9252), (8, 0.9301), (9, 0.9241), (10, 0.8844)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8844
wandb:     loss 1.57706
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_092405-7m67gmyk
wandb: Find logs at: ./wandb/offline-run-20240406_092405-7m67gmyk/logs
INFO flwr 2024-04-06 09:27:13,085 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 09:34:20,663 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=959019)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=959019)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 09:34:25,217	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 09:34:25,504	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 09:34:25,898	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2423433d013ea189.zip' (9.01MiB) to Ray cluster...
2024-04-06 09:34:25,922	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2423433d013ea189.zip'.
INFO flwr 2024-04-06 09:34:37,178 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62728035532.0, 'memory': 136365416244.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 09:34:37,178 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 09:34:37,178 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 09:34:37,194 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 09:34:37,195 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 09:34:37,196 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 09:34:37,196 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 09:34:39,944 | server.py:94 | initial parameters (loss, other metrics): 2.3024535179138184, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-06 09:34:39,944 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 09:34:39,944 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=963992)[0m 2024-04-06 09:34:43.285522: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=963992)[0m 2024-04-06 09:34:43.426886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=963992)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=963991)[0m 2024-04-06 09:34:45.537292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=963991)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=963991)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=963984)[0m 2024-04-06 09:34:43.647455: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=963989)[0m 2024-04-06 09:34:43.657267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=963989)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=963990)[0m 2024-04-06 09:34:45.991904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 09:35:04,669 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 09:35:05,160 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 09:35:06,409 | server.py:125 | fit progress: (1, 2.3023805618286133, {'accuracy': 0.1009, 'data_size': 10000}, 26.464203532988904)
INFO flwr 2024-04-06 09:35:06,409 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 09:35:06,409 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:35:18,906 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 09:35:21,330 | server.py:125 | fit progress: (2, 2.3022959232330322, {'accuracy': 0.1168, 'data_size': 10000}, 41.38526435699896)
INFO flwr 2024-04-06 09:35:21,330 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 09:35:21,330 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:35:33,644 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:35:37,133 | server.py:125 | fit progress: (3, 2.3022232055664062, {'accuracy': 0.1228, 'data_size': 10000}, 57.18826812200132)
INFO flwr 2024-04-06 09:35:37,133 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:35:37,133 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:35:50,164 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:35:54,327 | server.py:125 | fit progress: (4, 2.302119493484497, {'accuracy': 0.0974, 'data_size': 10000}, 74.38273207499878)
INFO flwr 2024-04-06 09:35:54,327 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:35:54,328 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:36:06,430 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 09:36:11,370 | server.py:125 | fit progress: (5, 2.3020284175872803, {'accuracy': 0.0974, 'data_size': 10000}, 91.42539978699642)
INFO flwr 2024-04-06 09:36:11,370 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 09:36:11,370 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:36:24,593 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 09:36:30,557 | server.py:125 | fit progress: (6, 2.301933526992798, {'accuracy': 0.0985, 'data_size': 10000}, 110.61301908799214)
INFO flwr 2024-04-06 09:36:30,558 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 09:36:30,558 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:36:42,254 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 09:36:50,986 | server.py:125 | fit progress: (7, 2.301859140396118, {'accuracy': 0.0991, 'data_size': 10000}, 131.04200462499284)
INFO flwr 2024-04-06 09:36:50,987 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 09:36:50,987 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:37:02,980 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 09:37:10,476 | server.py:125 | fit progress: (8, 2.301772356033325, {'accuracy': 0.0974, 'data_size': 10000}, 150.5312834320066)
INFO flwr 2024-04-06 09:37:10,476 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 09:37:10,476 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:37:23,096 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 09:37:31,879 | server.py:125 | fit progress: (9, 2.3016445636749268, {'accuracy': 0.0976, 'data_size': 10000}, 171.93454000900965)
INFO flwr 2024-04-06 09:37:31,879 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 09:37:31,879 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:37:43,624 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 09:37:53,541 | server.py:125 | fit progress: (10, 2.301548480987549, {'accuracy': 0.1112, 'data_size': 10000}, 193.59642764599994)
INFO flwr 2024-04-06 09:37:53,541 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 09:37:53,541 | server.py:153 | FL finished in 193.59687606498483
INFO flwr 2024-04-06 09:37:53,541 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 09:37:53,541 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 09:37:53,542 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 09:37:53,542 | app.py:229 | app_fit: losses_centralized [(0, 2.3024535179138184), (1, 2.3023805618286133), (2, 2.3022959232330322), (3, 2.3022232055664062), (4, 2.302119493484497), (5, 2.3020284175872803), (6, 2.301933526992798), (7, 2.301859140396118), (8, 2.301772356033325), (9, 2.3016445636749268), (10, 2.301548480987549)]
INFO flwr 2024-04-06 09:37:53,542 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.1009), (2, 0.1168), (3, 0.1228), (4, 0.0974), (5, 0.0974), (6, 0.0985), (7, 0.0991), (8, 0.0974), (9, 0.0976), (10, 0.1112)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1112
wandb:     loss 2.30155
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_093420-3wqqynxp
wandb: Find logs at: ./wandb/offline-run-20240406_093420-3wqqynxp/logs
INFO flwr 2024-04-06 09:37:57,152 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 09:45:02,680 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=963984)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=963984)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 09:45:08,067	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 09:45:08,437	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 09:45:08,784	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4b8f19f6ece92e17.zip' (9.03MiB) to Ray cluster...
2024-04-06 09:45:08,806	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4b8f19f6ece92e17.zip'.
INFO flwr 2024-04-06 09:45:19,647 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 140948474880.0, 'object_store_memory': 64692203520.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 09:45:19,647 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 09:45:19,648 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 09:45:19,668 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 09:45:19,669 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 09:45:19,669 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 09:45:19,670 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 09:45:22,636 | server.py:94 | initial parameters (loss, other metrics): 2.302640199661255, {'accuracy': 0.0552, 'data_size': 10000}
INFO flwr 2024-04-06 09:45:22,641 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 09:45:22,645 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=968516)[0m 2024-04-06 09:45:25.605452: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=968516)[0m 2024-04-06 09:45:25.706421: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=968516)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=968510)[0m 2024-04-06 09:45:28.010578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=968511)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=968511)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=968508)[0m 2024-04-06 09:45:25.787583: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=968508)[0m 2024-04-06 09:45:25.877887: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=968508)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=968513)[0m 2024-04-06 09:45:28.027148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 09:45:46,322 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 09:45:46,899 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 09:45:48,146 | server.py:125 | fit progress: (1, 2.2082180976867676, {'accuracy': 0.4282, 'data_size': 10000}, 25.501313012995524)
INFO flwr 2024-04-06 09:45:48,146 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 09:45:48,146 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:46:01,385 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 09:46:04,003 | server.py:125 | fit progress: (2, 1.990264654159546, {'accuracy': 0.4502, 'data_size': 10000}, 41.35865318699507)
INFO flwr 2024-04-06 09:46:04,004 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 09:46:04,004 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:46:15,103 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:46:18,342 | server.py:125 | fit progress: (3, 1.737257957458496, {'accuracy': 0.7278, 'data_size': 10000}, 55.69766247100779)
INFO flwr 2024-04-06 09:46:18,343 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:46:18,343 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:46:29,884 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:46:34,142 | server.py:125 | fit progress: (4, 1.6738715171813965, {'accuracy': 0.7928, 'data_size': 10000}, 71.49786462599877)
INFO flwr 2024-04-06 09:46:34,143 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:46:34,143 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:46:46,351 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 09:46:51,318 | server.py:125 | fit progress: (5, 1.6863652467727661, {'accuracy': 0.7727, 'data_size': 10000}, 88.67369589101872)
INFO flwr 2024-04-06 09:46:51,319 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 09:46:51,319 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:47:04,011 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 09:47:10,214 | server.py:125 | fit progress: (6, 1.6226398944854736, {'accuracy': 0.8416, 'data_size': 10000}, 107.56977479701163)
INFO flwr 2024-04-06 09:47:10,215 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 09:47:10,215 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:47:21,831 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 09:47:29,230 | server.py:125 | fit progress: (7, 1.5781192779541016, {'accuracy': 0.8895, 'data_size': 10000}, 126.58500713299145)
INFO flwr 2024-04-06 09:47:29,230 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 09:47:29,230 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:47:41,323 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 09:47:49,393 | server.py:125 | fit progress: (8, 1.572810411453247, {'accuracy': 0.8928, 'data_size': 10000}, 146.748547338997)
INFO flwr 2024-04-06 09:47:49,393 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 09:47:49,394 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:48:01,417 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 09:48:10,562 | server.py:125 | fit progress: (9, 1.5776937007904053, {'accuracy': 0.8852, 'data_size': 10000}, 167.9174559749954)
INFO flwr 2024-04-06 09:48:10,562 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 09:48:10,563 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:48:21,836 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 09:48:31,745 | server.py:125 | fit progress: (10, 1.5562447309494019, {'accuracy': 0.9073, 'data_size': 10000}, 189.1001755729958)
INFO flwr 2024-04-06 09:48:31,745 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 09:48:31,745 | server.py:153 | FL finished in 189.10066102500423
INFO flwr 2024-04-06 09:48:31,745 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 09:48:31,746 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 09:48:31,746 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 09:48:31,746 | app.py:229 | app_fit: losses_centralized [(0, 2.302640199661255), (1, 2.2082180976867676), (2, 1.990264654159546), (3, 1.737257957458496), (4, 1.6738715171813965), (5, 1.6863652467727661), (6, 1.6226398944854736), (7, 1.5781192779541016), (8, 1.572810411453247), (9, 1.5776937007904053), (10, 1.5562447309494019)]
INFO flwr 2024-04-06 09:48:31,746 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0552), (1, 0.4282), (2, 0.4502), (3, 0.7278), (4, 0.7928), (5, 0.7727), (6, 0.8416), (7, 0.8895), (8, 0.8928), (9, 0.8852), (10, 0.9073)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9073
wandb:     loss 1.55624
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_094502-jdmi6lvs
wandb: Find logs at: ./wandb/offline-run-20240406_094502-jdmi6lvs/logs
INFO flwr 2024-04-06 09:48:35,408 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 09:55:41,087 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=968508)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=968508)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 09:55:46,660	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 09:55:46,965	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 09:55:47,264	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9cf54c256b8aba75.zip' (9.05MiB) to Ray cluster...
2024-04-06 09:55:47,285	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9cf54c256b8aba75.zip'.
INFO flwr 2024-04-06 09:55:58,365 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 141001457869.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64714910515.0}
INFO flwr 2024-04-06 09:55:58,365 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 09:55:58,365 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 09:55:58,383 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 09:55:58,384 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 09:55:58,384 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 09:55:58,384 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 09:56:01,140 | server.py:94 | initial parameters (loss, other metrics): 2.3026628494262695, {'accuracy': 0.088, 'data_size': 10000}
INFO flwr 2024-04-06 09:56:01,140 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 09:56:01,141 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=976336)[0m 2024-04-06 09:56:03.803178: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=976336)[0m 2024-04-06 09:56:03.903625: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=976336)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=976328)[0m 2024-04-06 09:56:06.293133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=976333)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=976333)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=976324)[0m 2024-04-06 09:56:04.904867: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=976324)[0m 2024-04-06 09:56:04.998234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=976324)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=976324)[0m 2024-04-06 09:56:07.347948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 09:56:25,436 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 09:56:25,935 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 09:56:27,175 | server.py:125 | fit progress: (1, 2.207367420196533, {'accuracy': 0.185, 'data_size': 10000}, 26.03470678499434)
INFO flwr 2024-04-06 09:56:27,176 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 09:56:27,176 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:56:39,346 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 09:56:42,141 | server.py:125 | fit progress: (2, 1.8327606916427612, {'accuracy': 0.6601, 'data_size': 10000}, 41.00061270900187)
INFO flwr 2024-04-06 09:56:42,141 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 09:56:42,142 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:56:55,272 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:56:58,704 | server.py:125 | fit progress: (3, 1.6537119150161743, {'accuracy': 0.8101, 'data_size': 10000}, 57.56344424901181)
INFO flwr 2024-04-06 09:56:58,704 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:56:58,704 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:57:11,140 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:57:16,279 | server.py:125 | fit progress: (4, 1.5656719207763672, {'accuracy': 0.8993, 'data_size': 10000}, 75.138036911987)
INFO flwr 2024-04-06 09:57:16,279 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:57:16,279 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:57:28,892 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 09:57:34,403 | server.py:125 | fit progress: (5, 1.5658806562423706, {'accuracy': 0.8992, 'data_size': 10000}, 93.262191427988)
INFO flwr 2024-04-06 09:57:34,403 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 09:57:34,404 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:57:47,454 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 09:57:54,360 | server.py:125 | fit progress: (6, 1.5443456172943115, {'accuracy': 0.9188, 'data_size': 10000}, 113.21983311101212)
INFO flwr 2024-04-06 09:57:54,361 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 09:57:54,361 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:58:05,891 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 09:58:13,431 | server.py:125 | fit progress: (7, 1.5381351709365845, {'accuracy': 0.925, 'data_size': 10000}, 132.29087424100726)
INFO flwr 2024-04-06 09:58:13,432 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 09:58:13,432 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:58:25,194 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 09:58:33,928 | server.py:125 | fit progress: (8, 1.5408861637115479, {'accuracy': 0.9226, 'data_size': 10000}, 152.78733621101128)
INFO flwr 2024-04-06 09:58:33,928 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 09:58:33,928 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:58:45,667 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 09:58:55,138 | server.py:125 | fit progress: (9, 1.5305469036102295, {'accuracy': 0.9328, 'data_size': 10000}, 173.99735941601102)
INFO flwr 2024-04-06 09:58:55,138 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 09:58:55,138 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:59:08,231 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 09:59:18,943 | server.py:125 | fit progress: (10, 1.5331077575683594, {'accuracy': 0.9292, 'data_size': 10000}, 197.8023598100117)
INFO flwr 2024-04-06 09:59:18,943 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 09:59:18,943 | server.py:153 | FL finished in 197.80276337399846
INFO flwr 2024-04-06 09:59:18,944 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 09:59:18,944 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 09:59:18,944 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 09:59:18,944 | app.py:229 | app_fit: losses_centralized [(0, 2.3026628494262695), (1, 2.207367420196533), (2, 1.8327606916427612), (3, 1.6537119150161743), (4, 1.5656719207763672), (5, 1.5658806562423706), (6, 1.5443456172943115), (7, 1.5381351709365845), (8, 1.5408861637115479), (9, 1.5305469036102295), (10, 1.5331077575683594)]
INFO flwr 2024-04-06 09:59:18,944 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.088), (1, 0.185), (2, 0.6601), (3, 0.8101), (4, 0.8993), (5, 0.8992), (6, 0.9188), (7, 0.925), (8, 0.9226), (9, 0.9328), (10, 0.9292)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9292
wandb:     loss 1.53311
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_095540-6qsaz65d
wandb: Find logs at: ./wandb/offline-run-20240406_095540-6qsaz65d/logs
INFO flwr 2024-04-06 09:59:22,516 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 10:06:28,423 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=976324)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=976324)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 10:06:34,074	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 10:06:34,501	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 10:06:34,828	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8ebc3a8e8b181743.zip' (9.06MiB) to Ray cluster...
2024-04-06 10:06:34,849	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8ebc3a8e8b181743.zip'.
INFO flwr 2024-04-06 10:06:45,894 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64455161856.0, 'memory': 140395377664.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 10:06:45,894 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 10:06:45,895 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 10:06:45,908 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 10:06:45,909 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 10:06:45,910 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 10:06:45,910 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 10:06:47,926 | server.py:94 | initial parameters (loss, other metrics): 2.302501678466797, {'accuracy': 0.0961, 'data_size': 10000}
INFO flwr 2024-04-06 10:06:47,927 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 10:06:47,927 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=981669)[0m 2024-04-06 10:06:52.119763: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=981669)[0m 2024-04-06 10:06:52.215125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=981669)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=981669)[0m 2024-04-06 10:06:54.305277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=981670)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=981670)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=981666)[0m 2024-04-06 10:06:52.278593: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=981666)[0m 2024-04-06 10:06:52.415957: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=981666)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=981666)[0m 2024-04-06 10:06:54.500129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 10:07:13,125 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 10:07:13,653 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 10:07:14,897 | server.py:125 | fit progress: (1, 2.0861475467681885, {'accuracy': 0.4246, 'data_size': 10000}, 26.9694231050089)
INFO flwr 2024-04-06 10:07:14,897 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 10:07:14,897 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:07:27,234 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 10:07:30,021 | server.py:125 | fit progress: (2, 1.671724796295166, {'accuracy': 0.8063, 'data_size': 10000}, 42.09337625899934)
INFO flwr 2024-04-06 10:07:30,021 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 10:07:30,021 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:07:42,889 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 10:07:46,435 | server.py:125 | fit progress: (3, 1.5796761512756348, {'accuracy': 0.886, 'data_size': 10000}, 58.50739104798413)
INFO flwr 2024-04-06 10:07:46,435 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 10:07:46,435 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:07:57,599 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 10:08:02,730 | server.py:125 | fit progress: (4, 1.5741628408432007, {'accuracy': 0.8898, 'data_size': 10000}, 74.80265558799147)
INFO flwr 2024-04-06 10:08:02,730 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 10:08:02,730 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:08:15,383 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 10:08:20,994 | server.py:125 | fit progress: (5, 1.5406298637390137, {'accuracy': 0.9222, 'data_size': 10000}, 93.06701304399758)
INFO flwr 2024-04-06 10:08:20,995 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 10:08:20,995 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:08:32,373 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 10:08:39,328 | server.py:125 | fit progress: (6, 1.5425503253936768, {'accuracy': 0.9196, 'data_size': 10000}, 111.40043080601026)
INFO flwr 2024-04-06 10:08:39,328 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 10:08:39,328 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:08:51,494 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 10:08:59,257 | server.py:125 | fit progress: (7, 1.5383327007293701, {'accuracy': 0.925, 'data_size': 10000}, 131.3293350150052)
INFO flwr 2024-04-06 10:08:59,257 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 10:08:59,257 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:09:12,275 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 10:09:22,656 | server.py:125 | fit progress: (8, 1.5285392999649048, {'accuracy': 0.9327, 'data_size': 10000}, 154.72840766201261)
INFO flwr 2024-04-06 10:09:22,656 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 10:09:22,656 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:09:35,283 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 10:09:44,866 | server.py:125 | fit progress: (9, 1.5329052209854126, {'accuracy': 0.9294, 'data_size': 10000}, 176.93898423400242)
INFO flwr 2024-04-06 10:09:44,877 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 10:09:44,878 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:09:57,373 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 10:10:07,189 | server.py:125 | fit progress: (10, 1.5202217102050781, {'accuracy': 0.9418, 'data_size': 10000}, 199.26132095599314)
INFO flwr 2024-04-06 10:10:07,189 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 10:10:07,189 | server.py:153 | FL finished in 199.26171682198765
INFO flwr 2024-04-06 10:10:07,189 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 10:10:07,189 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 10:10:07,189 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 10:10:07,189 | app.py:229 | app_fit: losses_centralized [(0, 2.302501678466797), (1, 2.0861475467681885), (2, 1.671724796295166), (3, 1.5796761512756348), (4, 1.5741628408432007), (5, 1.5406298637390137), (6, 1.5425503253936768), (7, 1.5383327007293701), (8, 1.5285392999649048), (9, 1.5329052209854126), (10, 1.5202217102050781)]
INFO flwr 2024-04-06 10:10:07,189 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0961), (1, 0.4246), (2, 0.8063), (3, 0.886), (4, 0.8898), (5, 0.9222), (6, 0.9196), (7, 0.925), (8, 0.9327), (9, 0.9294), (10, 0.9418)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9418
wandb:     loss 1.52022
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_100628-7bjxjm8d
wandb: Find logs at: ./wandb/offline-run-20240406_100628-7bjxjm8d/logs
INFO flwr 2024-04-06 10:10:10,808 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 10:17:16,853 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=981664)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=981664)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 10:17:23,270	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 10:17:23,570	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 10:17:23,850	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_02b65aa69eef6103.zip' (9.08MiB) to Ray cluster...
2024-04-06 10:17:23,871	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_02b65aa69eef6103.zip'.
INFO flwr 2024-04-06 10:17:34,680 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 140449834394.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64478500454.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 10:17:34,681 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 10:17:34,681 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 10:17:34,700 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 10:17:34,701 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 10:17:34,702 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 10:17:34,702 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 10:17:37,947 | server.py:94 | initial parameters (loss, other metrics): 2.3026211261749268, {'accuracy': 0.0815, 'data_size': 10000}
INFO flwr 2024-04-06 10:17:37,948 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 10:17:37,948 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=986059)[0m 2024-04-06 10:17:40.344766: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=986059)[0m 2024-04-06 10:17:40.437502: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=986059)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=986059)[0m 2024-04-06 10:17:42.920395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=986059)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=986059)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=986148)[0m 2024-04-06 10:17:40.933987: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=986148)[0m 2024-04-06 10:17:41.035190: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=986148)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=986166)[0m 2024-04-06 10:17:42.919655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 10:18:01,275 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 10:18:01,764 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 10:18:02,988 | server.py:125 | fit progress: (1, 2.053107976913452, {'accuracy': 0.5465, 'data_size': 10000}, 25.039611287007574)
INFO flwr 2024-04-06 10:18:02,988 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 10:18:02,988 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:18:15,884 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 10:18:18,483 | server.py:125 | fit progress: (2, 1.7951650619506836, {'accuracy': 0.6623, 'data_size': 10000}, 40.53472897899337)
INFO flwr 2024-04-06 10:18:18,483 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 10:18:18,483 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:18:29,946 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 10:18:33,171 | server.py:125 | fit progress: (3, 1.707257866859436, {'accuracy': 0.7556, 'data_size': 10000}, 55.22289089701371)
INFO flwr 2024-04-06 10:18:33,171 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 10:18:33,171 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:18:44,049 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 10:18:48,399 | server.py:125 | fit progress: (4, 1.552897334098816, {'accuracy': 0.9115, 'data_size': 10000}, 70.45097704400541)
INFO flwr 2024-04-06 10:18:48,399 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 10:18:48,400 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:19:00,170 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 10:19:05,294 | server.py:125 | fit progress: (5, 1.5311957597732544, {'accuracy': 0.9326, 'data_size': 10000}, 87.34569838500465)
INFO flwr 2024-04-06 10:19:05,294 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 10:19:05,294 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:19:17,314 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 10:19:23,942 | server.py:125 | fit progress: (6, 1.5294110774993896, {'accuracy': 0.9331, 'data_size': 10000}, 105.99365876201773)
INFO flwr 2024-04-06 10:19:23,942 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 10:19:23,942 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:19:35,682 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 10:19:42,832 | server.py:125 | fit progress: (7, 1.525148868560791, {'accuracy': 0.9369, 'data_size': 10000}, 124.88402244600002)
INFO flwr 2024-04-06 10:19:42,832 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 10:19:42,832 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:19:54,809 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 10:20:02,951 | server.py:125 | fit progress: (8, 1.520402431488037, {'accuracy': 0.9423, 'data_size': 10000}, 145.0032507290016)
INFO flwr 2024-04-06 10:20:02,952 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 10:20:02,952 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:20:14,563 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 10:20:23,725 | server.py:125 | fit progress: (9, 1.5181740522384644, {'accuracy': 0.9441, 'data_size': 10000}, 165.7771529710153)
INFO flwr 2024-04-06 10:20:23,725 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 10:20:23,726 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:20:36,353 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 10:20:46,684 | server.py:125 | fit progress: (10, 1.5171725749969482, {'accuracy': 0.9436, 'data_size': 10000}, 188.7359762749984)
INFO flwr 2024-04-06 10:20:46,684 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 10:20:46,684 | server.py:153 | FL finished in 188.7364306980162
INFO flwr 2024-04-06 10:20:46,684 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 10:20:46,685 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 10:20:46,685 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 10:20:46,685 | app.py:229 | app_fit: losses_centralized [(0, 2.3026211261749268), (1, 2.053107976913452), (2, 1.7951650619506836), (3, 1.707257866859436), (4, 1.552897334098816), (5, 1.5311957597732544), (6, 1.5294110774993896), (7, 1.525148868560791), (8, 1.520402431488037), (9, 1.5181740522384644), (10, 1.5171725749969482)]
INFO flwr 2024-04-06 10:20:46,685 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0815), (1, 0.5465), (2, 0.6623), (3, 0.7556), (4, 0.9115), (5, 0.9326), (6, 0.9331), (7, 0.9369), (8, 0.9423), (9, 0.9441), (10, 0.9436)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9436
wandb:     loss 1.51717
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_101716-p8v87xro
wandb: Find logs at: ./wandb/offline-run-20240406_101716-p8v87xro/logs
INFO flwr 2024-04-06 10:20:50,245 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 10:27:56,155 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=986057)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=986057)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 10:28:01,654	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 10:28:02,005	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 10:28:02,373	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_39590d16a8d46a47.zip' (9.10MiB) to Ray cluster...
2024-04-06 10:28:02,410	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_39590d16a8d46a47.zip'.
INFO flwr 2024-04-06 10:28:13,315 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 140311451853.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64419193651.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 10:28:13,315 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 10:28:13,315 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 10:28:13,329 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 10:28:13,330 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 10:28:13,330 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 10:28:13,330 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 10:28:15,863 | server.py:94 | initial parameters (loss, other metrics): 2.302757740020752, {'accuracy': 0.0888, 'data_size': 10000}
INFO flwr 2024-04-06 10:28:15,866 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 10:28:15,867 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=993477)[0m 2024-04-06 10:28:19.141371: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=993477)[0m 2024-04-06 10:28:19.234916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=993477)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=993477)[0m 2024-04-06 10:28:21.345550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=993477)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=993477)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=993486)[0m 2024-04-06 10:28:19.847022: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=993486)[0m 2024-04-06 10:28:19.946846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=993486)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=993486)[0m 2024-04-06 10:28:22.308839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 10:28:39,079 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 10:28:39,610 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 10:28:40,849 | server.py:125 | fit progress: (1, 2.0767734050750732, {'accuracy': 0.4113, 'data_size': 10000}, 24.98192654098966)
INFO flwr 2024-04-06 10:28:40,849 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 10:28:40,850 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:28:53,555 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 10:28:56,135 | server.py:125 | fit progress: (2, 1.6248483657836914, {'accuracy': 0.8478, 'data_size': 10000}, 40.26766871899599)
INFO flwr 2024-04-06 10:28:56,135 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 10:28:56,135 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:29:07,829 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 10:29:11,053 | server.py:125 | fit progress: (3, 1.5543115139007568, {'accuracy': 0.9086, 'data_size': 10000}, 55.186062747990945)
INFO flwr 2024-04-06 10:29:11,053 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 10:29:11,054 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:29:22,647 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 10:29:27,304 | server.py:125 | fit progress: (4, 1.5415995121002197, {'accuracy': 0.9203, 'data_size': 10000}, 71.4365123109892)
INFO flwr 2024-04-06 10:29:27,304 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 10:29:27,304 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:29:39,465 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 10:29:44,879 | server.py:125 | fit progress: (5, 1.534563422203064, {'accuracy': 0.9268, 'data_size': 10000}, 89.01207412499934)
INFO flwr 2024-04-06 10:29:44,879 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 10:29:44,880 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:29:56,252 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 10:30:02,613 | server.py:125 | fit progress: (6, 1.5271711349487305, {'accuracy': 0.9348, 'data_size': 10000}, 106.74553736200323)
INFO flwr 2024-04-06 10:30:02,613 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 10:30:02,613 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:30:13,991 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 10:30:21,772 | server.py:125 | fit progress: (7, 1.569764256477356, {'accuracy': 0.8921, 'data_size': 10000}, 125.90515548200347)
INFO flwr 2024-04-06 10:30:21,773 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 10:30:21,773 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:30:33,479 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 10:30:42,645 | server.py:125 | fit progress: (8, 1.5261565446853638, {'accuracy': 0.9354, 'data_size': 10000}, 146.7776378550043)
INFO flwr 2024-04-06 10:30:42,645 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 10:30:42,645 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:30:54,604 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 10:31:03,979 | server.py:125 | fit progress: (9, 1.52497398853302, {'accuracy': 0.9363, 'data_size': 10000}, 168.11167151801055)
INFO flwr 2024-04-06 10:31:03,979 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 10:31:03,979 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:31:15,987 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 10:31:26,512 | server.py:125 | fit progress: (10, 1.5240143537521362, {'accuracy': 0.9377, 'data_size': 10000}, 190.6446771909832)
INFO flwr 2024-04-06 10:31:26,512 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 10:31:26,512 | server.py:153 | FL finished in 190.64504817899433
INFO flwr 2024-04-06 10:31:26,512 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 10:31:26,512 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 10:31:26,512 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 10:31:26,513 | app.py:229 | app_fit: losses_centralized [(0, 2.302757740020752), (1, 2.0767734050750732), (2, 1.6248483657836914), (3, 1.5543115139007568), (4, 1.5415995121002197), (5, 1.534563422203064), (6, 1.5271711349487305), (7, 1.569764256477356), (8, 1.5261565446853638), (9, 1.52497398853302), (10, 1.5240143537521362)]
INFO flwr 2024-04-06 10:31:26,513 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0888), (1, 0.4113), (2, 0.8478), (3, 0.9086), (4, 0.9203), (5, 0.9268), (6, 0.9348), (7, 0.8921), (8, 0.9354), (9, 0.9363), (10, 0.9377)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9377
wandb:     loss 1.52401
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_102755-7xrc5g9s
wandb: Find logs at: ./wandb/offline-run-20240406_102755-7xrc5g9s/logs
INFO flwr 2024-04-06 10:31:30,148 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 10:38:36,122 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=993475)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=993475)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 10:38:40,984	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 10:38:41,393	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 10:38:41,764	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d8430a8e513e5eaa.zip' (9.11MiB) to Ray cluster...
2024-04-06 10:38:41,793	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d8430a8e513e5eaa.zip'.
INFO flwr 2024-04-06 10:38:52,821 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 140155831706.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 64352499302.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 10:38:52,822 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 10:38:52,822 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 10:38:52,838 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 10:38:52,839 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 10:38:52,839 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 10:38:52,839 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 10:38:56,032 | server.py:94 | initial parameters (loss, other metrics): 2.3024888038635254, {'accuracy': 0.0985, 'data_size': 10000}
INFO flwr 2024-04-06 10:38:56,032 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 10:38:56,032 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=998221)[0m 2024-04-06 10:38:58.753544: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=998221)[0m 2024-04-06 10:38:58.852916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=998221)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=998221)[0m 2024-04-06 10:39:01.069081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=998224)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=998224)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=998227)[0m 2024-04-06 10:38:59.292615: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=998227)[0m 2024-04-06 10:38:59.402899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=998227)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=998227)[0m 2024-04-06 10:39:01.245078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 10:39:20,331 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 10:39:20,881 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 10:39:22,125 | server.py:125 | fit progress: (1, 2.070486545562744, {'accuracy': 0.5486, 'data_size': 10000}, 26.093072229006793)
INFO flwr 2024-04-06 10:39:22,126 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 10:39:22,126 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:39:34,988 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 10:39:37,745 | server.py:125 | fit progress: (2, 1.6143893003463745, {'accuracy': 0.851, 'data_size': 10000}, 41.712417492002714)
INFO flwr 2024-04-06 10:39:37,745 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 10:39:37,745 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:39:48,876 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 10:39:52,279 | server.py:125 | fit progress: (3, 1.5467817783355713, {'accuracy': 0.9181, 'data_size': 10000}, 56.24635676998878)
INFO flwr 2024-04-06 10:39:52,279 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 10:39:52,279 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:40:04,520 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 10:40:08,852 | server.py:125 | fit progress: (4, 1.5362292528152466, {'accuracy': 0.9267, 'data_size': 10000}, 72.81960425400757)
INFO flwr 2024-04-06 10:40:08,852 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 10:40:08,852 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:40:19,977 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 10:40:26,657 | server.py:125 | fit progress: (5, 1.5331339836120605, {'accuracy': 0.9273, 'data_size': 10000}, 90.62519255900406)
INFO flwr 2024-04-06 10:40:26,658 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 10:40:26,658 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:40:38,580 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 10:40:45,234 | server.py:125 | fit progress: (6, 1.525965929031372, {'accuracy': 0.9345, 'data_size': 10000}, 109.20214819398825)
INFO flwr 2024-04-06 10:40:45,235 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 10:40:45,235 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:40:57,548 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 10:41:04,744 | server.py:125 | fit progress: (7, 1.5233726501464844, {'accuracy': 0.938, 'data_size': 10000}, 128.71188386398717)
INFO flwr 2024-04-06 10:41:04,744 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 10:41:04,745 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:41:17,018 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 10:41:25,460 | server.py:125 | fit progress: (8, 1.5269156694412231, {'accuracy': 0.9339, 'data_size': 10000}, 149.42782255698694)
INFO flwr 2024-04-06 10:41:25,460 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 10:41:25,461 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:41:36,753 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 10:41:46,520 | server.py:125 | fit progress: (9, 1.5307880640029907, {'accuracy': 0.9299, 'data_size': 10000}, 170.48735227200086)
INFO flwr 2024-04-06 10:41:46,520 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 10:41:46,520 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:41:58,667 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 10:42:09,518 | server.py:125 | fit progress: (10, 1.5331153869628906, {'accuracy': 0.9275, 'data_size': 10000}, 193.4853459649894)
INFO flwr 2024-04-06 10:42:09,518 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 10:42:09,518 | server.py:153 | FL finished in 193.4859646279947
INFO flwr 2024-04-06 10:42:09,518 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 10:42:09,519 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 10:42:09,519 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 10:42:09,519 | app.py:229 | app_fit: losses_centralized [(0, 2.3024888038635254), (1, 2.070486545562744), (2, 1.6143893003463745), (3, 1.5467817783355713), (4, 1.5362292528152466), (5, 1.5331339836120605), (6, 1.525965929031372), (7, 1.5233726501464844), (8, 1.5269156694412231), (9, 1.5307880640029907), (10, 1.5331153869628906)]
INFO flwr 2024-04-06 10:42:09,519 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0985), (1, 0.5486), (2, 0.851), (3, 0.9181), (4, 0.9267), (5, 0.9273), (6, 0.9345), (7, 0.938), (8, 0.9339), (9, 0.9299), (10, 0.9275)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9275
wandb:     loss 1.53312
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_103835-8smtc33c
wandb: Find logs at: ./wandb/offline-run-20240406_103835-8smtc33c/logs
INFO flwr 2024-04-06 10:42:13,222 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 10:49:19,362 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=998219)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=998219)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 10:49:25,132	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 10:49:25,418	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 10:49:25,743	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cdc3e1e946327ce1.zip' (9.13MiB) to Ray cluster...
2024-04-06 10:49:25,774	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cdc3e1e946327ce1.zip'.
INFO flwr 2024-04-06 10:49:36,828 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 142140361319.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 65203011993.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 10:49:36,828 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 10:49:36,829 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 10:49:36,844 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 10:49:36,845 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 10:49:36,845 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 10:49:36,845 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 10:49:39,584 | server.py:94 | initial parameters (loss, other metrics): 2.302672863006592, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-06 10:49:39,590 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 10:49:39,591 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1005723)[0m 2024-04-06 10:49:42.822509: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1005720)[0m 2024-04-06 10:49:42.939030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1005720)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1005720)[0m 2024-04-06 10:49:44.976201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1005723)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1005723)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1005724)[0m 2024-04-06 10:49:43.182264: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1005724)[0m 2024-04-06 10:49:43.314905: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1005724)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1005724)[0m 2024-04-06 10:49:45.555562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 10:49:57,445 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 10:49:57,960 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 10:49:59,198 | server.py:125 | fit progress: (1, 2.3026702404022217, {'accuracy': 0.089, 'data_size': 10000}, 19.606547511997633)
INFO flwr 2024-04-06 10:49:59,198 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 10:49:59,198 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:50:08,250 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 10:50:10,885 | server.py:125 | fit progress: (2, 2.3026680946350098, {'accuracy': 0.0893, 'data_size': 10000}, 31.294422601989936)
INFO flwr 2024-04-06 10:50:10,886 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 10:50:10,886 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:50:19,007 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 10:50:22,345 | server.py:125 | fit progress: (3, 2.302666187286377, {'accuracy': 0.0896, 'data_size': 10000}, 42.754406082007335)
INFO flwr 2024-04-06 10:50:22,346 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 10:50:22,346 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:50:30,742 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 10:50:35,462 | server.py:125 | fit progress: (4, 2.3026630878448486, {'accuracy': 0.0893, 'data_size': 10000}, 55.87106416700408)
INFO flwr 2024-04-06 10:50:35,462 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 10:50:35,463 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:50:43,492 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 10:50:48,875 | server.py:125 | fit progress: (5, 2.3026609420776367, {'accuracy': 0.0895, 'data_size': 10000}, 69.28406792000169)
INFO flwr 2024-04-06 10:50:48,875 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 10:50:48,876 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:50:57,259 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 10:51:03,683 | server.py:125 | fit progress: (6, 2.3026583194732666, {'accuracy': 0.09, 'data_size': 10000}, 84.0915209389932)
INFO flwr 2024-04-06 10:51:03,683 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 10:51:03,683 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:51:11,433 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 10:51:18,976 | server.py:125 | fit progress: (7, 2.3026554584503174, {'accuracy': 0.0898, 'data_size': 10000}, 99.38543922698591)
INFO flwr 2024-04-06 10:51:18,977 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 10:51:18,977 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:51:26,872 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 10:51:35,783 | server.py:125 | fit progress: (8, 2.3026535511016846, {'accuracy': 0.0903, 'data_size': 10000}, 116.19231183599913)
INFO flwr 2024-04-06 10:51:35,784 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 10:51:35,784 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:51:43,951 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 10:51:53,519 | server.py:125 | fit progress: (9, 2.3026506900787354, {'accuracy': 0.09, 'data_size': 10000}, 133.92824779701186)
INFO flwr 2024-04-06 10:51:53,520 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 10:51:53,520 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:52:01,736 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 10:52:12,488 | server.py:125 | fit progress: (10, 2.3026485443115234, {'accuracy': 0.09, 'data_size': 10000}, 152.8965054200089)
INFO flwr 2024-04-06 10:52:12,488 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 10:52:12,488 | server.py:153 | FL finished in 152.89687074100948
INFO flwr 2024-04-06 10:52:12,488 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 10:52:12,488 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 10:52:12,488 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 10:52:12,488 | app.py:229 | app_fit: losses_centralized [(0, 2.302672863006592), (1, 2.3026702404022217), (2, 2.3026680946350098), (3, 2.302666187286377), (4, 2.3026630878448486), (5, 2.3026609420776367), (6, 2.3026583194732666), (7, 2.3026554584503174), (8, 2.3026535511016846), (9, 2.3026506900787354), (10, 2.3026485443115234)]
INFO flwr 2024-04-06 10:52:12,488 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.089), (2, 0.0893), (3, 0.0896), (4, 0.0893), (5, 0.0895), (6, 0.09), (7, 0.0898), (8, 0.0903), (9, 0.09), (10, 0.09)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.09
wandb:     loss 2.30265
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_104919-e3ck07ed
wandb: Find logs at: ./wandb/offline-run-20240406_104919-e3ck07ed/logs
INFO flwr 2024-04-06 10:52:16,104 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 10:59:22,122 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1005719)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1005719)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 10:59:26,897	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 10:59:27,268	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 10:59:27,662	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1a860ba88b28272d.zip' (9.15MiB) to Ray cluster...
2024-04-06 10:59:27,683	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1a860ba88b28272d.zip'.
INFO flwr 2024-04-06 10:59:38,851 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64235732582.0, 'CPU': 64.0, 'memory': 139883376026.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 10:59:38,852 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 10:59:38,852 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 10:59:38,871 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 10:59:38,872 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 10:59:38,872 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 10:59:38,872 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 10:59:42,779 | server.py:94 | initial parameters (loss, other metrics): 2.3027122020721436, {'accuracy': 0.0959, 'data_size': 10000}
INFO flwr 2024-04-06 10:59:42,779 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 10:59:42,780 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1010509)[0m 2024-04-06 10:59:44.922580: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1010509)[0m 2024-04-06 10:59:45.012055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1010509)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1010510)[0m 2024-04-06 10:59:47.149481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1010514)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1010514)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1010515)[0m 2024-04-06 10:59:45.382124: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1010515)[0m 2024-04-06 10:59:45.476320: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1010515)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1010511)[0m 2024-04-06 10:59:47.603558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 11:00:00,001 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 11:00:00,549 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 11:00:01,853 | server.py:125 | fit progress: (1, 2.302598714828491, {'accuracy': 0.1039, 'data_size': 10000}, 19.073132766003255)
INFO flwr 2024-04-06 11:00:01,853 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 11:00:01,853 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:00:10,499 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 11:00:13,018 | server.py:125 | fit progress: (2, 2.302466630935669, {'accuracy': 0.1085, 'data_size': 10000}, 30.23799213100574)
INFO flwr 2024-04-06 11:00:13,018 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 11:00:13,018 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:00:21,283 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 11:00:25,059 | server.py:125 | fit progress: (3, 2.3023738861083984, {'accuracy': 0.097, 'data_size': 10000}, 42.27919791798922)
INFO flwr 2024-04-06 11:00:25,059 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 11:00:25,059 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:00:33,181 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 11:00:37,670 | server.py:125 | fit progress: (4, 2.302276372909546, {'accuracy': 0.1151, 'data_size': 10000}, 54.890669652988436)
INFO flwr 2024-04-06 11:00:37,670 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 11:00:37,671 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:00:45,622 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 11:00:51,446 | server.py:125 | fit progress: (5, 2.3021395206451416, {'accuracy': 0.1188, 'data_size': 10000}, 68.666900901997)
INFO flwr 2024-04-06 11:00:51,447 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 11:00:51,447 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:01:00,015 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 11:01:07,550 | server.py:125 | fit progress: (6, 2.302044153213501, {'accuracy': 0.1233, 'data_size': 10000}, 84.77032317500561)
INFO flwr 2024-04-06 11:01:07,550 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 11:01:07,550 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:01:15,182 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:01:23,443 | server.py:125 | fit progress: (7, 2.301915168762207, {'accuracy': 0.1048, 'data_size': 10000}, 100.66305384898442)
INFO flwr 2024-04-06 11:01:23,443 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:01:23,443 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:01:32,065 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:01:40,298 | server.py:125 | fit progress: (8, 2.3017797470092773, {'accuracy': 0.1345, 'data_size': 10000}, 117.51806599000702)
INFO flwr 2024-04-06 11:01:40,298 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:01:40,298 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:01:48,273 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 11:01:57,963 | server.py:125 | fit progress: (9, 2.3016257286071777, {'accuracy': 0.1299, 'data_size': 10000}, 135.18298157598474)
INFO flwr 2024-04-06 11:01:57,963 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 11:01:57,963 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:02:06,071 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 11:02:16,141 | server.py:125 | fit progress: (10, 2.3014745712280273, {'accuracy': 0.2116, 'data_size': 10000}, 153.3614422910032)
INFO flwr 2024-04-06 11:02:16,141 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 11:02:16,142 | server.py:153 | FL finished in 153.36203485398437
INFO flwr 2024-04-06 11:02:16,142 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 11:02:16,142 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 11:02:16,142 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 11:02:16,142 | app.py:229 | app_fit: losses_centralized [(0, 2.3027122020721436), (1, 2.302598714828491), (2, 2.302466630935669), (3, 2.3023738861083984), (4, 2.302276372909546), (5, 2.3021395206451416), (6, 2.302044153213501), (7, 2.301915168762207), (8, 2.3017797470092773), (9, 2.3016257286071777), (10, 2.3014745712280273)]
INFO flwr 2024-04-06 11:02:16,142 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0959), (1, 0.1039), (2, 0.1085), (3, 0.097), (4, 0.1151), (5, 0.1188), (6, 0.1233), (7, 0.1048), (8, 0.1345), (9, 0.1299), (10, 0.2116)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2116
wandb:     loss 2.30147
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_105921-pg7s3wui
wandb: Find logs at: ./wandb/offline-run-20240406_105921-pg7s3wui/logs
INFO flwr 2024-04-06 11:02:19,743 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 11:09:26,213 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1010509)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1010509)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 11:09:32,141	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 11:09:32,559	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 11:09:32,956	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f32132a65f3acafe.zip' (9.16MiB) to Ray cluster...
2024-04-06 11:09:32,988	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f32132a65f3acafe.zip'.
INFO flwr 2024-04-06 11:09:43,762 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 139707255399.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64160252313.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 11:09:43,763 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 11:09:43,763 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 11:09:43,780 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 11:09:43,781 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 11:09:43,781 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 11:09:43,781 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 11:09:46,198 | server.py:94 | initial parameters (loss, other metrics): 2.3025059700012207, {'accuracy': 0.1114, 'data_size': 10000}
INFO flwr 2024-04-06 11:09:46,198 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 11:09:46,198 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1014921)[0m 2024-04-06 11:09:49.647293: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1014921)[0m 2024-04-06 11:09:49.748577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1014921)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1014915)[0m 2024-04-06 11:09:51.892264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1014915)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1014915)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1014910)[0m 2024-04-06 11:09:50.180714: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1014910)[0m 2024-04-06 11:09:50.310392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1014910)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1014910)[0m 2024-04-06 11:09:52.360847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 11:10:04,030 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 11:10:04,547 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 11:10:05,588 | server.py:125 | fit progress: (1, 2.302272319793701, {'accuracy': 0.1028, 'data_size': 10000}, 19.389949356991565)
INFO flwr 2024-04-06 11:10:05,588 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 11:10:05,589 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:10:14,868 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 11:10:17,254 | server.py:125 | fit progress: (2, 2.3020522594451904, {'accuracy': 0.1028, 'data_size': 10000}, 31.055537719017593)
INFO flwr 2024-04-06 11:10:17,254 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 11:10:17,254 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:10:25,344 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 11:10:28,836 | server.py:125 | fit progress: (3, 2.3018102645874023, {'accuracy': 0.1028, 'data_size': 10000}, 42.63827549101552)
INFO flwr 2024-04-06 11:10:28,837 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 11:10:28,837 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:10:37,076 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 11:10:42,036 | server.py:125 | fit progress: (4, 2.301515817642212, {'accuracy': 0.1028, 'data_size': 10000}, 55.837793373997556)
INFO flwr 2024-04-06 11:10:42,036 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 11:10:42,036 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:10:50,745 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 11:10:55,814 | server.py:125 | fit progress: (5, 2.300750255584717, {'accuracy': 0.0974, 'data_size': 10000}, 69.61584794800729)
INFO flwr 2024-04-06 11:10:55,814 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 11:10:55,815 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:11:03,888 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 11:11:10,058 | server.py:125 | fit progress: (6, 2.300309419631958, {'accuracy': 0.0974, 'data_size': 10000}, 83.85944381100126)
INFO flwr 2024-04-06 11:11:10,058 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 11:11:10,058 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:11:17,993 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:11:25,196 | server.py:125 | fit progress: (7, 2.299699306488037, {'accuracy': 0.1107, 'data_size': 10000}, 98.99804521899205)
INFO flwr 2024-04-06 11:11:25,197 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:11:25,197 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:11:33,130 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:11:41,167 | server.py:125 | fit progress: (8, 2.298734664916992, {'accuracy': 0.1274, 'data_size': 10000}, 114.96906288500759)
INFO flwr 2024-04-06 11:11:41,168 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:11:41,168 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:11:49,038 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 11:11:58,249 | server.py:125 | fit progress: (9, 2.2945961952209473, {'accuracy': 0.0974, 'data_size': 10000}, 132.05035053400206)
INFO flwr 2024-04-06 11:11:58,249 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 11:11:58,249 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:12:06,822 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 11:12:17,190 | server.py:125 | fit progress: (10, 2.291752338409424, {'accuracy': 0.0974, 'data_size': 10000}, 150.9913127720065)
INFO flwr 2024-04-06 11:12:17,190 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 11:12:17,190 | server.py:153 | FL finished in 150.9917286550044
INFO flwr 2024-04-06 11:12:17,190 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 11:12:17,190 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 11:12:17,190 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 11:12:17,190 | app.py:229 | app_fit: losses_centralized [(0, 2.3025059700012207), (1, 2.302272319793701), (2, 2.3020522594451904), (3, 2.3018102645874023), (4, 2.301515817642212), (5, 2.300750255584717), (6, 2.300309419631958), (7, 2.299699306488037), (8, 2.298734664916992), (9, 2.2945961952209473), (10, 2.291752338409424)]
INFO flwr 2024-04-06 11:12:17,191 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1114), (1, 0.1028), (2, 0.1028), (3, 0.1028), (4, 0.1028), (5, 0.0974), (6, 0.0974), (7, 0.1107), (8, 0.1274), (9, 0.0974), (10, 0.0974)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0974
wandb:     loss 2.29175
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_110925-7a1ejip3
wandb: Find logs at: ./wandb/offline-run-20240406_110925-7a1ejip3/logs
INFO flwr 2024-04-06 11:12:20,801 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 11:19:26,705 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1014910)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1014910)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 11:19:31,418	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 11:19:31,743	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 11:19:32,057	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d356dc6c91ebc3e8.zip' (9.18MiB) to Ray cluster...
2024-04-06 11:19:32,088	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d356dc6c91ebc3e8.zip'.
INFO flwr 2024-04-06 11:19:43,038 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 139956392141.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 64267025203.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 11:19:43,038 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 11:19:43,039 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 11:19:43,060 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 11:19:43,061 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 11:19:43,061 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 11:19:43,062 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 11:19:45,908 | server.py:94 | initial parameters (loss, other metrics): 2.3025736808776855, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-06 11:19:45,908 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 11:19:45,908 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1022299)[0m 2024-04-06 11:19:48.965856: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1022299)[0m 2024-04-06 11:19:49.064462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1022299)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1022294)[0m 2024-04-06 11:19:51.319119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1022294)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1022294)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1022295)[0m 2024-04-06 11:19:49.523301: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1022295)[0m 2024-04-06 11:19:49.614650: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1022295)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1022295)[0m 2024-04-06 11:19:52.070912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 11:20:05,054 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 11:20:05,647 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 11:20:06,945 | server.py:125 | fit progress: (1, 2.3021304607391357, {'accuracy': 0.1135, 'data_size': 10000}, 21.0370595830027)
INFO flwr 2024-04-06 11:20:06,946 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 11:20:06,946 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:20:16,176 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 11:20:18,853 | server.py:125 | fit progress: (2, 2.3015241622924805, {'accuracy': 0.1348, 'data_size': 10000}, 32.94489812199026)
INFO flwr 2024-04-06 11:20:18,853 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 11:20:18,854 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:20:27,099 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 11:20:31,010 | server.py:125 | fit progress: (3, 2.300748109817505, {'accuracy': 0.1665, 'data_size': 10000}, 45.102020138991065)
INFO flwr 2024-04-06 11:20:31,010 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 11:20:31,011 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:20:38,875 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 11:20:43,500 | server.py:125 | fit progress: (4, 2.29982328414917, {'accuracy': 0.1742, 'data_size': 10000}, 57.59140116898925)
INFO flwr 2024-04-06 11:20:43,500 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 11:20:43,500 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:20:51,594 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 11:20:57,440 | server.py:125 | fit progress: (5, 2.2966508865356445, {'accuracy': 0.1591, 'data_size': 10000}, 71.53143991099205)
INFO flwr 2024-04-06 11:20:57,440 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 11:20:57,440 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:21:05,794 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 11:21:12,334 | server.py:125 | fit progress: (6, 2.275674343109131, {'accuracy': 0.0982, 'data_size': 10000}, 86.42582341001253)
INFO flwr 2024-04-06 11:21:12,334 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 11:21:12,335 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:21:20,675 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:21:28,581 | server.py:125 | fit progress: (7, 2.251575231552124, {'accuracy': 0.1343, 'data_size': 10000}, 102.67299392999848)
INFO flwr 2024-04-06 11:21:28,581 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:21:28,582 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:21:36,574 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:21:44,988 | server.py:125 | fit progress: (8, 2.2292640209198, {'accuracy': 0.2833, 'data_size': 10000}, 119.08013040400692)
INFO flwr 2024-04-06 11:21:44,989 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:21:44,989 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:21:53,029 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 11:22:02,769 | server.py:125 | fit progress: (9, 2.189573287963867, {'accuracy': 0.3014, 'data_size': 10000}, 136.86073426099028)
INFO flwr 2024-04-06 11:22:02,769 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 11:22:02,769 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:22:11,027 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 11:22:21,687 | server.py:125 | fit progress: (10, 2.1315360069274902, {'accuracy': 0.3009, 'data_size': 10000}, 155.77933840299374)
INFO flwr 2024-04-06 11:22:21,688 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 11:22:21,688 | server.py:153 | FL finished in 155.7797981960175
INFO flwr 2024-04-06 11:22:21,688 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 11:22:21,688 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 11:22:21,688 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 11:22:21,688 | app.py:229 | app_fit: losses_centralized [(0, 2.3025736808776855), (1, 2.3021304607391357), (2, 2.3015241622924805), (3, 2.300748109817505), (4, 2.29982328414917), (5, 2.2966508865356445), (6, 2.275674343109131), (7, 2.251575231552124), (8, 2.2292640209198), (9, 2.189573287963867), (10, 2.1315360069274902)]
INFO flwr 2024-04-06 11:22:21,689 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.1135), (2, 0.1348), (3, 0.1665), (4, 0.1742), (5, 0.1591), (6, 0.0982), (7, 0.1343), (8, 0.2833), (9, 0.3014), (10, 0.3009)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3009
wandb:     loss 2.13154
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_111926-j0ht5xtx
wandb: Find logs at: ./wandb/offline-run-20240406_111926-j0ht5xtx/logs
INFO flwr 2024-04-06 11:22:25,322 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 11:29:32,719 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1022299)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1022299)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 11:29:37,842	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 11:29:38,202	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 11:29:38,616	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3262e57f1e7a6c8e.zip' (9.20MiB) to Ray cluster...
2024-04-06 11:29:38,641	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3262e57f1e7a6c8e.zip'.
INFO flwr 2024-04-06 11:29:49,774 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 139521498112.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64080642048.0, 'CPU': 64.0}
INFO flwr 2024-04-06 11:29:49,775 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 11:29:49,775 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 11:29:49,791 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 11:29:49,792 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 11:29:49,792 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 11:29:49,792 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 11:29:53,504 | server.py:94 | initial parameters (loss, other metrics): 2.3022618293762207, {'accuracy': 0.152, 'data_size': 10000}
INFO flwr 2024-04-06 11:29:53,505 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 11:29:53,505 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1027022)[0m 2024-04-06 11:29:55.551495: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1027022)[0m 2024-04-06 11:29:55.655215: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1027022)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1027017)[0m 2024-04-06 11:29:57.878167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1027017)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1027017)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1027020)[0m 2024-04-06 11:29:56.507350: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1027020)[0m 2024-04-06 11:29:56.593611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1027020)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1027020)[0m 2024-04-06 11:29:58.496319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 11:30:10,768 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 11:30:11,279 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 11:30:12,520 | server.py:125 | fit progress: (1, 2.3011274337768555, {'accuracy': 0.2097, 'data_size': 10000}, 19.014848318998702)
INFO flwr 2024-04-06 11:30:12,520 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 11:30:12,520 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:30:21,022 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 11:30:23,425 | server.py:125 | fit progress: (2, 2.2991151809692383, {'accuracy': 0.1415, 'data_size': 10000}, 29.91994507200434)
INFO flwr 2024-04-06 11:30:23,425 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 11:30:23,425 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:30:31,164 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 11:30:34,960 | server.py:125 | fit progress: (3, 2.2935280799865723, {'accuracy': 0.0974, 'data_size': 10000}, 41.45512972600409)
INFO flwr 2024-04-06 11:30:34,960 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 11:30:34,960 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:30:42,860 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 11:30:47,445 | server.py:125 | fit progress: (4, 2.281770706176758, {'accuracy': 0.1016, 'data_size': 10000}, 53.93980221200036)
INFO flwr 2024-04-06 11:30:47,445 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 11:30:47,445 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:30:55,258 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 11:31:00,971 | server.py:125 | fit progress: (5, 2.247703790664673, {'accuracy': 0.228, 'data_size': 10000}, 67.46598686199286)
INFO flwr 2024-04-06 11:31:00,971 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 11:31:00,971 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:31:09,112 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 11:31:15,462 | server.py:125 | fit progress: (6, 2.1619205474853516, {'accuracy': 0.2711, 'data_size': 10000}, 81.95743103398127)
INFO flwr 2024-04-06 11:31:15,463 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 11:31:15,463 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:31:23,820 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:31:31,474 | server.py:125 | fit progress: (7, 2.095851182937622, {'accuracy': 0.37, 'data_size': 10000}, 97.96904574599466)
INFO flwr 2024-04-06 11:31:31,474 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:31:31,474 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:31:39,796 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:31:48,695 | server.py:125 | fit progress: (8, 2.018878221511841, {'accuracy': 0.4251, 'data_size': 10000}, 115.18970925398753)
INFO flwr 2024-04-06 11:31:48,695 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:31:48,695 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:31:56,773 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 11:32:06,657 | server.py:125 | fit progress: (9, 1.9820244312286377, {'accuracy': 0.4739, 'data_size': 10000}, 133.15185880998615)
INFO flwr 2024-04-06 11:32:06,657 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 11:32:06,657 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:32:14,868 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 11:32:25,446 | server.py:125 | fit progress: (10, 1.9028549194335938, {'accuracy': 0.5664, 'data_size': 10000}, 151.9410435629834)
INFO flwr 2024-04-06 11:32:25,446 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 11:32:25,446 | server.py:153 | FL finished in 151.941614499985
INFO flwr 2024-04-06 11:32:25,447 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 11:32:25,447 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 11:32:25,447 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 11:32:25,447 | app.py:229 | app_fit: losses_centralized [(0, 2.3022618293762207), (1, 2.3011274337768555), (2, 2.2991151809692383), (3, 2.2935280799865723), (4, 2.281770706176758), (5, 2.247703790664673), (6, 2.1619205474853516), (7, 2.095851182937622), (8, 2.018878221511841), (9, 1.9820244312286377), (10, 1.9028549194335938)]
INFO flwr 2024-04-06 11:32:25,447 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.152), (1, 0.2097), (2, 0.1415), (3, 0.0974), (4, 0.1016), (5, 0.228), (6, 0.2711), (7, 0.37), (8, 0.4251), (9, 0.4739), (10, 0.5664)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5664
wandb:     loss 1.90285
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_112932-go8mrxmh
wandb: Find logs at: ./wandb/offline-run-20240406_112932-go8mrxmh/logs
INFO flwr 2024-04-06 11:32:28,944 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 11:39:35,025 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1027015)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1027015)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 11:39:41,278	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 11:39:41,634	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 11:39:41,996	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7f07a0a7f2425e27.zip' (9.21MiB) to Ray cluster...
2024-04-06 11:39:42,017	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7f07a0a7f2425e27.zip'.
INFO flwr 2024-04-06 11:39:52,830 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 139318566298.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63993671270.0}
INFO flwr 2024-04-06 11:39:52,830 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 11:39:52,830 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 11:39:52,851 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 11:39:52,853 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 11:39:52,853 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 11:39:52,853 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 11:39:55,830 | server.py:94 | initial parameters (loss, other metrics): 2.3026068210601807, {'accuracy': 0.1025, 'data_size': 10000}
INFO flwr 2024-04-06 11:39:55,830 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 11:39:55,830 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1031417)[0m 2024-04-06 11:39:58.785408: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1031417)[0m 2024-04-06 11:39:58.912667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1031417)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1031422)[0m 2024-04-06 11:40:00.953201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1031422)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1031422)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1031423)[0m 2024-04-06 11:39:59.223564: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1031423)[0m 2024-04-06 11:39:59.320023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1031423)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1031423)[0m 2024-04-06 11:40:01.417699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1031427)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1031427)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(DefaultActor pid=1031429)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1031429)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
DEBUG flwr 2024-04-06 11:40:16,703 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 11:40:17,214 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 11:40:18,259 | server.py:125 | fit progress: (1, 2.301485776901245, {'accuracy': 0.1007, 'data_size': 10000}, 22.42909921100363)
INFO flwr 2024-04-06 11:40:18,260 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 11:40:18,260 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:40:28,263 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 11:40:31,094 | server.py:125 | fit progress: (2, 2.299506664276123, {'accuracy': 0.0961, 'data_size': 10000}, 35.263540820014896)
INFO flwr 2024-04-06 11:40:31,094 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 11:40:31,094 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:40:40,385 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 11:40:43,920 | server.py:125 | fit progress: (3, 2.2924389839172363, {'accuracy': 0.2667, 'data_size': 10000}, 48.09015099200769)
INFO flwr 2024-04-06 11:40:43,921 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 11:40:43,921 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:40:53,201 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 11:40:57,714 | server.py:125 | fit progress: (4, 2.284240484237671, {'accuracy': 0.1894, 'data_size': 10000}, 61.883601363020716)
INFO flwr 2024-04-06 11:40:57,714 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 11:40:57,714 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:41:06,824 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 11:41:12,643 | server.py:125 | fit progress: (5, 2.20270037651062, {'accuracy': 0.3862, 'data_size': 10000}, 76.81243783800164)
INFO flwr 2024-04-06 11:41:12,643 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 11:41:12,643 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:41:21,420 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 11:41:28,296 | server.py:125 | fit progress: (6, 2.068915367126465, {'accuracy': 0.4256, 'data_size': 10000}, 92.46594615202048)
INFO flwr 2024-04-06 11:41:28,297 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 11:41:28,297 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:41:37,478 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:41:44,934 | server.py:125 | fit progress: (7, 2.0038113594055176, {'accuracy': 0.46, 'data_size': 10000}, 109.10405571601586)
INFO flwr 2024-04-06 11:41:44,935 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:41:44,935 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:41:54,154 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:42:03,012 | server.py:125 | fit progress: (8, 1.9801585674285889, {'accuracy': 0.4904, 'data_size': 10000}, 127.18196935101878)
INFO flwr 2024-04-06 11:42:03,013 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:42:03,013 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:42:12,184 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 11:42:21,533 | server.py:125 | fit progress: (9, 1.9475972652435303, {'accuracy': 0.5268, 'data_size': 10000}, 145.703070707008)
INFO flwr 2024-04-06 11:42:21,534 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 11:42:21,534 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:42:30,934 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 11:42:41,432 | server.py:125 | fit progress: (10, 1.9427655935287476, {'accuracy': 0.5198, 'data_size': 10000}, 165.60180019101244)
INFO flwr 2024-04-06 11:42:41,433 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 11:42:41,433 | server.py:153 | FL finished in 165.60231662500883
INFO flwr 2024-04-06 11:42:41,433 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 11:42:41,433 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 11:42:41,433 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 11:42:41,433 | app.py:229 | app_fit: losses_centralized [(0, 2.3026068210601807), (1, 2.301485776901245), (2, 2.299506664276123), (3, 2.2924389839172363), (4, 2.284240484237671), (5, 2.20270037651062), (6, 2.068915367126465), (7, 2.0038113594055176), (8, 1.9801585674285889), (9, 1.9475972652435303), (10, 1.9427655935287476)]
INFO flwr 2024-04-06 11:42:41,433 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1025), (1, 0.1007), (2, 0.0961), (3, 0.2667), (4, 0.1894), (5, 0.3862), (6, 0.4256), (7, 0.46), (8, 0.4904), (9, 0.5268), (10, 0.5198)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5198
wandb:     loss 1.94277
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_113934-co2jpz7z
wandb: Find logs at: ./wandb/offline-run-20240406_113934-co2jpz7z/logs
INFO flwr 2024-04-06 11:42:45,029 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 11:49:52,273 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1031421)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1031421)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
2024-04-06 11:49:58,324	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 11:49:58,712	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 11:49:59,005	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7e2ef271be4848ca.zip' (9.23MiB) to Ray cluster...
2024-04-06 11:49:59,027	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7e2ef271be4848ca.zip'.
INFO flwr 2024-04-06 11:50:09,951 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 139580169626.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64105786982.0}
INFO flwr 2024-04-06 11:50:09,951 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 11:50:09,951 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 11:50:09,969 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 11:50:09,971 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 11:50:09,971 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 11:50:09,971 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 11:50:12,788 | server.py:94 | initial parameters (loss, other metrics): 2.302543878555298, {'accuracy': 0.1025, 'data_size': 10000}
INFO flwr 2024-04-06 11:50:12,788 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 11:50:12,788 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1038833)[0m 2024-04-06 11:50:15.585122: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1038836)[0m 2024-04-06 11:50:15.791847: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1038836)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1038836)[0m 2024-04-06 11:50:17.904556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1038834)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1038834)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1038821)[0m 2024-04-06 11:50:16.384107: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1038821)[0m 2024-04-06 11:50:16.475896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1038821)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1038838)[0m 2024-04-06 11:50:18.510642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 11:50:30,320 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 11:50:30,826 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 11:50:32,131 | server.py:125 | fit progress: (1, 2.299747943878174, {'accuracy': 0.0974, 'data_size': 10000}, 19.342362466995837)
INFO flwr 2024-04-06 11:50:32,131 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 11:50:32,131 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:50:41,615 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 11:50:44,148 | server.py:125 | fit progress: (2, 2.2941646575927734, {'accuracy': 0.0974, 'data_size': 10000}, 31.359555959003046)
INFO flwr 2024-04-06 11:50:44,148 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 11:50:44,148 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:50:52,273 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 11:50:56,023 | server.py:125 | fit progress: (3, 2.2864019870758057, {'accuracy': 0.1758, 'data_size': 10000}, 43.23533763200976)
INFO flwr 2024-04-06 11:50:56,024 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 11:50:56,024 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:51:04,187 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 11:51:09,124 | server.py:125 | fit progress: (4, 2.26739501953125, {'accuracy': 0.3677, 'data_size': 10000}, 56.335483908012975)
INFO flwr 2024-04-06 11:51:09,124 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 11:51:09,124 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:51:17,216 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 11:51:22,825 | server.py:125 | fit progress: (5, 2.17095947265625, {'accuracy': 0.3594, 'data_size': 10000}, 70.03690834200825)
INFO flwr 2024-04-06 11:51:22,825 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 11:51:22,826 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:51:31,026 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 11:51:37,898 | server.py:125 | fit progress: (6, 2.034602165222168, {'accuracy': 0.517, 'data_size': 10000}, 85.11008700501407)
INFO flwr 2024-04-06 11:51:37,898 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 11:51:37,899 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:51:45,610 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:51:53,156 | server.py:125 | fit progress: (7, 1.9302306175231934, {'accuracy': 0.5518, 'data_size': 10000}, 100.36788489401806)
INFO flwr 2024-04-06 11:51:53,156 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:51:53,157 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:52:01,368 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:52:10,334 | server.py:125 | fit progress: (8, 1.833550214767456, {'accuracy': 0.6534, 'data_size': 10000}, 117.54610663600033)
INFO flwr 2024-04-06 11:52:10,335 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:52:10,335 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:52:18,766 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 11:52:28,758 | server.py:125 | fit progress: (9, 1.7773464918136597, {'accuracy': 0.7205, 'data_size': 10000}, 135.9701534699998)
INFO flwr 2024-04-06 11:52:28,759 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 11:52:28,759 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:52:36,576 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 11:52:47,238 | server.py:125 | fit progress: (10, 1.7353802919387817, {'accuracy': 0.7501, 'data_size': 10000}, 154.45034202199895)
INFO flwr 2024-04-06 11:52:47,239 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 11:52:47,239 | server.py:153 | FL finished in 154.4507635810005
INFO flwr 2024-04-06 11:52:47,239 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 11:52:47,239 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 11:52:47,239 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 11:52:47,239 | app.py:229 | app_fit: losses_centralized [(0, 2.302543878555298), (1, 2.299747943878174), (2, 2.2941646575927734), (3, 2.2864019870758057), (4, 2.26739501953125), (5, 2.17095947265625), (6, 2.034602165222168), (7, 1.9302306175231934), (8, 1.833550214767456), (9, 1.7773464918136597), (10, 1.7353802919387817)]
INFO flwr 2024-04-06 11:52:47,239 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1025), (1, 0.0974), (2, 0.0974), (3, 0.1758), (4, 0.3677), (5, 0.3594), (6, 0.517), (7, 0.5518), (8, 0.6534), (9, 0.7205), (10, 0.7501)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7501
wandb:     loss 1.73538
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_114951-k8a6xvyb
wandb: Find logs at: ./wandb/offline-run-20240406_114951-k8a6xvyb/logs
INFO flwr 2024-04-06 11:52:50,878 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 11:59:57,149 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1038821)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1038821)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 12:00:03,980	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:00:04,332	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:00:04,686	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3c75fd1ffe4bcda2.zip' (9.25MiB) to Ray cluster...
2024-04-06 12:00:04,715	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3c75fd1ffe4bcda2.zip'.
INFO flwr 2024-04-06 12:00:15,802 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 139043481396.0, 'node:__internal_head__': 1.0, 'object_store_memory': 63875777740.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 12:00:15,802 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:00:15,802 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:00:15,819 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:00:15,820 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:00:15,820 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:00:15,820 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 12:00:19,287 | server.py:94 | initial parameters (loss, other metrics): 2.3027327060699463, {'accuracy': 0.0684, 'data_size': 10000}
INFO flwr 2024-04-06 12:00:19,288 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:00:19,288 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1043611)[0m 2024-04-06 12:00:22.146263: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1043611)[0m 2024-04-06 12:00:22.259027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1043611)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1043607)[0m 2024-04-06 12:00:24.274648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1043607)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1043607)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1043605)[0m 2024-04-06 12:00:22.277529: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1043615)[0m 2024-04-06 12:00:22.307042: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1043615)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1043608)[0m 2024-04-06 12:00:24.647223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 12:00:39,479 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:00:40,013 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:00:41,071 | server.py:125 | fit progress: (1, 2.302718162536621, {'accuracy': 0.0694, 'data_size': 10000}, 21.782805035007186)
INFO flwr 2024-04-06 12:00:41,071 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:00:41,072 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:00:50,293 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:00:52,962 | server.py:125 | fit progress: (2, 2.302701473236084, {'accuracy': 0.0698, 'data_size': 10000}, 33.67368366298615)
INFO flwr 2024-04-06 12:00:52,962 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:00:52,962 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:01:00,904 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:01:04,994 | server.py:125 | fit progress: (3, 2.302687406539917, {'accuracy': 0.0734, 'data_size': 10000}, 45.70552387900534)
INFO flwr 2024-04-06 12:01:04,994 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:01:04,994 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:01:12,732 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:01:18,122 | server.py:125 | fit progress: (4, 2.3026766777038574, {'accuracy': 0.077, 'data_size': 10000}, 58.8334681050037)
INFO flwr 2024-04-06 12:01:18,122 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:01:18,122 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:01:26,401 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:01:33,930 | server.py:125 | fit progress: (5, 2.302661657333374, {'accuracy': 0.0803, 'data_size': 10000}, 74.64222166201216)
INFO flwr 2024-04-06 12:01:33,931 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:01:33,931 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:01:42,006 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:01:48,338 | server.py:125 | fit progress: (6, 2.302644729614258, {'accuracy': 0.0848, 'data_size': 10000}, 89.04973479200271)
INFO flwr 2024-04-06 12:01:48,338 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:01:48,338 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:01:56,734 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:02:06,735 | server.py:125 | fit progress: (7, 2.302628755569458, {'accuracy': 0.0888, 'data_size': 10000}, 107.44642088998808)
INFO flwr 2024-04-06 12:02:06,735 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:02:06,735 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:02:14,889 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:02:21,915 | server.py:125 | fit progress: (8, 2.3026163578033447, {'accuracy': 0.0856, 'data_size': 10000}, 122.6273585849849)
INFO flwr 2024-04-06 12:02:21,916 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:02:21,916 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:02:30,325 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:02:39,236 | server.py:125 | fit progress: (9, 2.3026037216186523, {'accuracy': 0.0858, 'data_size': 10000}, 139.94784187100595)
INFO flwr 2024-04-06 12:02:39,236 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:02:39,236 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:02:48,010 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:02:58,248 | server.py:125 | fit progress: (10, 2.3025882244110107, {'accuracy': 0.0866, 'data_size': 10000}, 158.95994049799629)
INFO flwr 2024-04-06 12:02:58,248 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:02:58,248 | server.py:153 | FL finished in 158.9603546859871
INFO flwr 2024-04-06 12:02:58,249 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:02:58,249 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:02:58,249 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:02:58,249 | app.py:229 | app_fit: losses_centralized [(0, 2.3027327060699463), (1, 2.302718162536621), (2, 2.302701473236084), (3, 2.302687406539917), (4, 2.3026766777038574), (5, 2.302661657333374), (6, 2.302644729614258), (7, 2.302628755569458), (8, 2.3026163578033447), (9, 2.3026037216186523), (10, 2.3025882244110107)]
INFO flwr 2024-04-06 12:02:58,249 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0684), (1, 0.0694), (2, 0.0698), (3, 0.0734), (4, 0.077), (5, 0.0803), (6, 0.0848), (7, 0.0888), (8, 0.0856), (9, 0.0858), (10, 0.0866)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0866
wandb:     loss 2.30259
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_115956-lo7hehuu
wandb: Find logs at: ./wandb/offline-run-20240406_115956-lo7hehuu/logs
INFO flwr 2024-04-06 12:03:01,832 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 12:10:08,340 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1043615)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1043615)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 12:10:13,883	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:10:14,360	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:10:14,680	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_748419ffdcf7fa3b.zip' (9.26MiB) to Ray cluster...
2024-04-06 12:10:14,702	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_748419ffdcf7fa3b.zip'.
INFO flwr 2024-04-06 12:10:25,516 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 63918460108.0, 'node:10.20.240.18': 1.0, 'memory': 139143073588.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 12:10:25,516 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:10:25,516 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:10:25,535 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:10:25,536 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:10:25,537 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:10:25,537 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 12:10:28,004 | server.py:94 | initial parameters (loss, other metrics): 2.3025317192077637, {'accuracy': 0.1135, 'data_size': 10000}
INFO flwr 2024-04-06 12:10:28,004 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:10:28,005 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1047969)[0m 2024-04-06 12:10:31.036484: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1047969)[0m 2024-04-06 12:10:31.136780: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1047969)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1047969)[0m 2024-04-06 12:10:33.256211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1047969)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1047969)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1047973)[0m 2024-04-06 12:10:31.957506: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1047973)[0m 2024-04-06 12:10:32.045681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1047973)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1047967)[0m 2024-04-06 12:10:34.046354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 12:10:45,829 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:10:46,330 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:10:47,610 | server.py:125 | fit progress: (1, 2.3016855716705322, {'accuracy': 0.1135, 'data_size': 10000}, 19.605384960974334)
INFO flwr 2024-04-06 12:10:47,610 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:10:47,611 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:10:56,570 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:10:58,925 | server.py:125 | fit progress: (2, 2.2998063564300537, {'accuracy': 0.1135, 'data_size': 10000}, 30.9201884479844)
INFO flwr 2024-04-06 12:10:58,925 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:10:58,925 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:11:06,458 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:11:09,883 | server.py:125 | fit progress: (3, 2.2926409244537354, {'accuracy': 0.2776, 'data_size': 10000}, 41.87897351899301)
INFO flwr 2024-04-06 12:11:09,884 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:11:09,884 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:11:18,247 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:11:22,530 | server.py:125 | fit progress: (4, 2.2449698448181152, {'accuracy': 0.2926, 'data_size': 10000}, 54.52572148499894)
INFO flwr 2024-04-06 12:11:22,530 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:11:22,531 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:11:30,332 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:11:35,511 | server.py:125 | fit progress: (5, 2.1347455978393555, {'accuracy': 0.2983, 'data_size': 10000}, 67.50631533798878)
INFO flwr 2024-04-06 12:11:35,511 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:11:35,511 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:11:43,921 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:11:50,413 | server.py:125 | fit progress: (6, 2.054960250854492, {'accuracy': 0.3858, 'data_size': 10000}, 82.40855288598686)
INFO flwr 2024-04-06 12:11:50,413 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:11:50,414 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:11:57,986 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:12:05,152 | server.py:125 | fit progress: (7, 1.95455801486969, {'accuracy': 0.5436, 'data_size': 10000}, 97.14734717400279)
INFO flwr 2024-04-06 12:12:05,152 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:12:05,152 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:12:14,188 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:12:23,509 | server.py:125 | fit progress: (8, 1.8982023000717163, {'accuracy': 0.577, 'data_size': 10000}, 115.50470123600098)
INFO flwr 2024-04-06 12:12:23,509 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:12:23,510 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:12:32,271 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:12:45,001 | server.py:125 | fit progress: (9, 1.8234707117080688, {'accuracy': 0.6695, 'data_size': 10000}, 136.99619545397582)
INFO flwr 2024-04-06 12:12:45,001 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:12:45,001 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:12:54,079 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:13:04,368 | server.py:125 | fit progress: (10, 1.768172025680542, {'accuracy': 0.7314, 'data_size': 10000}, 156.3632151229831)
INFO flwr 2024-04-06 12:13:04,368 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:13:04,368 | server.py:153 | FL finished in 156.36361208799644
INFO flwr 2024-04-06 12:13:04,368 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:13:04,368 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:13:04,368 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:13:04,368 | app.py:229 | app_fit: losses_centralized [(0, 2.3025317192077637), (1, 2.3016855716705322), (2, 2.2998063564300537), (3, 2.2926409244537354), (4, 2.2449698448181152), (5, 2.1347455978393555), (6, 2.054960250854492), (7, 1.95455801486969), (8, 1.8982023000717163), (9, 1.8234707117080688), (10, 1.768172025680542)]
INFO flwr 2024-04-06 12:13:04,369 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1135), (1, 0.1135), (2, 0.1135), (3, 0.2776), (4, 0.2926), (5, 0.2983), (6, 0.3858), (7, 0.5436), (8, 0.577), (9, 0.6695), (10, 0.7314)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7314
wandb:     loss 1.76817
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_121008-3qfxxhre
wandb: Find logs at: ./wandb/offline-run-20240406_121008-3qfxxhre/logs
INFO flwr 2024-04-06 12:13:07,960 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 12:20:15,752 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1047963)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1047963)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 12:20:20,864	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:20:21,438	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:20:21,881	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a4bc9f49a13fcb90.zip' (9.28MiB) to Ray cluster...
2024-04-06 12:20:21,923	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a4bc9f49a13fcb90.zip'.
INFO flwr 2024-04-06 12:20:32,824 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 131112072192.0, 'CPU': 64.0, 'object_store_memory': 60476602368.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 12:20:32,825 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:20:32,825 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:20:32,841 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:20:32,842 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:20:32,842 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:20:32,842 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 12:20:35,122 | server.py:94 | initial parameters (loss, other metrics): 2.3026363849639893, {'accuracy': 0.1281, 'data_size': 10000}
INFO flwr 2024-04-06 12:20:35,122 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:20:35,123 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1055479)[0m 2024-04-06 12:20:38.858728: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1055479)[0m 2024-04-06 12:20:38.956877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1055479)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1055479)[0m 2024-04-06 12:20:41.091323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1055485)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1055485)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1055485)[0m 2024-04-06 12:20:38.951567: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1055478)[0m 2024-04-06 12:20:39.016812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1055478)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1055477)[0m 2024-04-06 12:20:41.271708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 12:20:53,930 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:20:54,448 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:20:55,773 | server.py:125 | fit progress: (1, 2.2986385822296143, {'accuracy': 0.1017, 'data_size': 10000}, 20.650344174995553)
INFO flwr 2024-04-06 12:20:55,773 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:20:55,774 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:21:05,263 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:21:07,629 | server.py:125 | fit progress: (2, 2.2772278785705566, {'accuracy': 0.2777, 'data_size': 10000}, 32.50648107298184)
INFO flwr 2024-04-06 12:21:07,629 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:21:07,630 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:21:15,372 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:21:18,811 | server.py:125 | fit progress: (3, 2.153937578201294, {'accuracy': 0.2804, 'data_size': 10000}, 43.68859635098488)
INFO flwr 2024-04-06 12:21:18,812 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:21:18,812 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:21:27,230 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:21:31,478 | server.py:125 | fit progress: (4, 1.9760609865188599, {'accuracy': 0.5006, 'data_size': 10000}, 56.35576107999077)
INFO flwr 2024-04-06 12:21:31,479 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:21:31,479 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:21:40,047 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:21:45,355 | server.py:125 | fit progress: (5, 1.8746788501739502, {'accuracy': 0.5789, 'data_size': 10000}, 70.23235172498971)
INFO flwr 2024-04-06 12:21:45,355 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:21:45,355 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:21:53,735 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:22:00,313 | server.py:125 | fit progress: (6, 1.8071691989898682, {'accuracy': 0.6628, 'data_size': 10000}, 85.1902280219947)
INFO flwr 2024-04-06 12:22:00,313 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:22:00,313 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:22:09,280 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:22:17,016 | server.py:125 | fit progress: (7, 1.708348035812378, {'accuracy': 0.7723, 'data_size': 10000}, 101.89368154600379)
INFO flwr 2024-04-06 12:22:17,017 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:22:17,017 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:22:26,125 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:22:34,073 | server.py:125 | fit progress: (8, 1.6786776781082153, {'accuracy': 0.8032, 'data_size': 10000}, 118.95083722099662)
INFO flwr 2024-04-06 12:22:34,074 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:22:34,074 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:22:42,213 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:22:50,806 | server.py:125 | fit progress: (9, 1.649348258972168, {'accuracy': 0.8207, 'data_size': 10000}, 135.68314642598853)
INFO flwr 2024-04-06 12:22:50,806 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:22:50,806 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:22:59,395 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:23:08,940 | server.py:125 | fit progress: (10, 1.6280790567398071, {'accuracy': 0.8379, 'data_size': 10000}, 153.8173985540052)
INFO flwr 2024-04-06 12:23:08,940 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:23:08,940 | server.py:153 | FL finished in 153.81778228099574
INFO flwr 2024-04-06 12:23:08,941 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:23:08,941 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:23:08,941 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:23:08,941 | app.py:229 | app_fit: losses_centralized [(0, 2.3026363849639893), (1, 2.2986385822296143), (2, 2.2772278785705566), (3, 2.153937578201294), (4, 1.9760609865188599), (5, 1.8746788501739502), (6, 1.8071691989898682), (7, 1.708348035812378), (8, 1.6786776781082153), (9, 1.649348258972168), (10, 1.6280790567398071)]
INFO flwr 2024-04-06 12:23:08,941 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1281), (1, 0.1017), (2, 0.2777), (3, 0.2804), (4, 0.5006), (5, 0.5789), (6, 0.6628), (7, 0.7723), (8, 0.8032), (9, 0.8207), (10, 0.8379)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8379
wandb:     loss 1.62808
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_122015-y26n1yby
wandb: Find logs at: ./wandb/offline-run-20240406_122015-y26n1yby/logs
INFO flwr 2024-04-06 12:23:12,551 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 12:30:18,513 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1055476)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1055476)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 12:30:23,440	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:30:23,858	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:30:24,299	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fdcd118acbe51d9a.zip' (9.31MiB) to Ray cluster...
2024-04-06 12:30:24,324	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fdcd118acbe51d9a.zip'.
INFO flwr 2024-04-06 12:30:35,306 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 138898272052.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63813545164.0, 'CPU': 64.0}
INFO flwr 2024-04-06 12:30:35,306 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:30:35,306 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:30:35,319 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:30:35,320 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:30:35,320 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:30:35,320 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 12:30:38,270 | server.py:94 | initial parameters (loss, other metrics): 2.302483320236206, {'accuracy': 0.0893, 'data_size': 10000}
INFO flwr 2024-04-06 12:30:38,272 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:30:38,278 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1063129)[0m 2024-04-06 12:30:41.033696: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1063129)[0m 2024-04-06 12:30:41.133716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1063129)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1063129)[0m 2024-04-06 12:30:43.450724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1063126)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1063126)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1063127)[0m 2024-04-06 12:30:41.898797: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1063127)[0m 2024-04-06 12:30:41.993283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1063127)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1063127)[0m 2024-04-06 12:30:44.148305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 12:30:57,738 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:30:58,261 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:30:59,519 | server.py:125 | fit progress: (1, 2.2947895526885986, {'accuracy': 0.1032, 'data_size': 10000}, 21.241639473999385)
INFO flwr 2024-04-06 12:30:59,519 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:30:59,520 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:31:08,638 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:31:11,148 | server.py:125 | fit progress: (2, 2.162935256958008, {'accuracy': 0.3001, 'data_size': 10000}, 32.87001680099638)
INFO flwr 2024-04-06 12:31:11,148 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:31:11,148 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:31:19,596 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:31:23,299 | server.py:125 | fit progress: (3, 1.9522594213485718, {'accuracy': 0.5321, 'data_size': 10000}, 45.021663065010216)
INFO flwr 2024-04-06 12:31:23,299 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:31:23,300 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:31:31,369 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:31:35,910 | server.py:125 | fit progress: (4, 1.8572970628738403, {'accuracy': 0.6073, 'data_size': 10000}, 57.632820845989045)
INFO flwr 2024-04-06 12:31:35,911 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:31:35,911 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:31:44,523 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:31:49,986 | server.py:125 | fit progress: (5, 1.8007582426071167, {'accuracy': 0.6485, 'data_size': 10000}, 71.70869382101228)
INFO flwr 2024-04-06 12:31:49,987 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:31:49,987 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:31:58,376 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:32:06,785 | server.py:125 | fit progress: (6, 1.708724856376648, {'accuracy': 0.7656, 'data_size': 10000}, 88.50786926600267)
INFO flwr 2024-04-06 12:32:06,786 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:32:06,786 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:32:14,571 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:32:23,250 | server.py:125 | fit progress: (7, 1.6584527492523193, {'accuracy': 0.8109, 'data_size': 10000}, 104.97258241500822)
INFO flwr 2024-04-06 12:32:23,250 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:32:23,251 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:32:31,569 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:32:40,240 | server.py:125 | fit progress: (8, 1.6471601724624634, {'accuracy': 0.8202, 'data_size': 10000}, 121.96189562600921)
INFO flwr 2024-04-06 12:32:40,240 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:32:40,240 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:32:48,152 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:33:01,459 | server.py:125 | fit progress: (9, 1.6426275968551636, {'accuracy': 0.8223, 'data_size': 10000}, 143.18183533698902)
INFO flwr 2024-04-06 12:33:01,460 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:33:01,460 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:33:09,737 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:33:22,213 | server.py:125 | fit progress: (10, 1.6369054317474365, {'accuracy': 0.8271, 'data_size': 10000}, 163.93591071199626)
INFO flwr 2024-04-06 12:33:22,214 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:33:22,214 | server.py:153 | FL finished in 163.93626444099937
INFO flwr 2024-04-06 12:33:22,214 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:33:22,214 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:33:22,214 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:33:22,214 | app.py:229 | app_fit: losses_centralized [(0, 2.302483320236206), (1, 2.2947895526885986), (2, 2.162935256958008), (3, 1.9522594213485718), (4, 1.8572970628738403), (5, 1.8007582426071167), (6, 1.708724856376648), (7, 1.6584527492523193), (8, 1.6471601724624634), (9, 1.6426275968551636), (10, 1.6369054317474365)]
INFO flwr 2024-04-06 12:33:22,214 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0893), (1, 0.1032), (2, 0.3001), (3, 0.5321), (4, 0.6073), (5, 0.6485), (6, 0.7656), (7, 0.8109), (8, 0.8202), (9, 0.8223), (10, 0.8271)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8271
wandb:     loss 1.63691
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_123018-sad10e3l
wandb: Find logs at: ./wandb/offline-run-20240406_123018-sad10e3l/logs
INFO flwr 2024-04-06 12:33:25,485 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 12:40:31,821 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1063122)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1063122)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 12:40:37,747	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:40:38,093	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:40:38,382	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fa86df1d0759cc6c.zip' (9.32MiB) to Ray cluster...
2024-04-06 12:40:38,404	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fa86df1d0759cc6c.zip'.
INFO flwr 2024-04-06 12:40:49,206 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65960008089.0, 'memory': 143906685543.0}
INFO flwr 2024-04-06 12:40:49,206 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:40:49,206 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:40:49,223 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:40:49,224 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:40:49,224 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:40:49,224 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 12:40:52,258 | server.py:94 | initial parameters (loss, other metrics): 2.302715539932251, {'accuracy': 0.1056, 'data_size': 10000}
INFO flwr 2024-04-06 12:40:52,258 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:40:52,259 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1067598)[0m 2024-04-06 12:40:55.023841: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1067598)[0m 2024-04-06 12:40:55.121676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1067598)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1067599)[0m 2024-04-06 12:40:59.453626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1067599)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1067599)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1067600)[0m 2024-04-06 12:40:55.565812: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1067600)[0m 2024-04-06 12:40:55.659561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1067600)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1067606)[0m 2024-04-06 12:40:59.660362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 12:41:13,888 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:41:14,439 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:41:15,675 | server.py:125 | fit progress: (1, 2.268587350845337, {'accuracy': 0.2132, 'data_size': 10000}, 23.416709006007295)
INFO flwr 2024-04-06 12:41:15,676 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:41:15,676 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:41:24,232 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:41:26,570 | server.py:125 | fit progress: (2, 2.0939817428588867, {'accuracy': 0.4211, 'data_size': 10000}, 34.31103144999361)
INFO flwr 2024-04-06 12:41:26,570 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:41:26,570 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:41:35,278 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:41:38,878 | server.py:125 | fit progress: (3, 1.8176655769348145, {'accuracy': 0.6542, 'data_size': 10000}, 46.619649344007485)
INFO flwr 2024-04-06 12:41:38,879 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:41:38,879 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:41:47,002 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:41:51,107 | server.py:125 | fit progress: (4, 1.7239696979522705, {'accuracy': 0.7507, 'data_size': 10000}, 58.84793294701376)
INFO flwr 2024-04-06 12:41:51,107 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:41:51,107 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:41:59,533 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:42:04,475 | server.py:125 | fit progress: (5, 1.6322556734085083, {'accuracy': 0.8432, 'data_size': 10000}, 72.21601180601283)
INFO flwr 2024-04-06 12:42:04,475 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:42:04,475 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:42:13,206 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:42:19,427 | server.py:125 | fit progress: (6, 1.6150239706039429, {'accuracy': 0.8544, 'data_size': 10000}, 87.16805302799912)
INFO flwr 2024-04-06 12:42:19,427 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:42:19,427 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:42:28,009 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:42:35,868 | server.py:125 | fit progress: (7, 1.605691909790039, {'accuracy': 0.86, 'data_size': 10000}, 103.60964628000511)
INFO flwr 2024-04-06 12:42:35,868 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:42:35,869 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:42:43,770 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:42:51,189 | server.py:125 | fit progress: (8, 1.589038372039795, {'accuracy': 0.8772, 'data_size': 10000}, 118.92989834299078)
INFO flwr 2024-04-06 12:42:51,189 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:42:51,189 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:42:59,679 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:43:08,164 | server.py:125 | fit progress: (9, 1.5735435485839844, {'accuracy': 0.8911, 'data_size': 10000}, 135.9053698949865)
INFO flwr 2024-04-06 12:43:08,164 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:43:08,164 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:43:16,601 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:43:25,827 | server.py:125 | fit progress: (10, 1.600089192390442, {'accuracy': 0.8665, 'data_size': 10000}, 153.56851888200617)
INFO flwr 2024-04-06 12:43:25,827 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:43:25,828 | server.py:153 | FL finished in 153.5690243600111
INFO flwr 2024-04-06 12:43:25,828 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:43:25,828 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:43:25,828 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:43:25,828 | app.py:229 | app_fit: losses_centralized [(0, 2.302715539932251), (1, 2.268587350845337), (2, 2.0939817428588867), (3, 1.8176655769348145), (4, 1.7239696979522705), (5, 1.6322556734085083), (6, 1.6150239706039429), (7, 1.605691909790039), (8, 1.589038372039795), (9, 1.5735435485839844), (10, 1.600089192390442)]
INFO flwr 2024-04-06 12:43:25,828 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1056), (1, 0.2132), (2, 0.4211), (3, 0.6542), (4, 0.7507), (5, 0.8432), (6, 0.8544), (7, 0.86), (8, 0.8772), (9, 0.8911), (10, 0.8665)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8665
wandb:     loss 1.60009
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_124031-jpmmgmb7
wandb: Find logs at: ./wandb/offline-run-20240406_124031-jpmmgmb7/logs
INFO flwr 2024-04-06 12:43:29,381 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 12:50:35,620 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1067609)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1067609)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 12:50:40,472	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:50:40,891	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:50:41,424	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dbac65060420cd0d.zip' (9.34MiB) to Ray cluster...
2024-04-06 12:50:41,447	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dbac65060420cd0d.zip'.
INFO flwr 2024-04-06 12:50:52,611 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 131697987380.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60727708876.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 12:50:52,612 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:50:52,612 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:50:52,633 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:50:52,634 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:50:52,634 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:50:52,634 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 12:50:55,382 | server.py:94 | initial parameters (loss, other metrics): 2.302417039871216, {'accuracy': 0.1395, 'data_size': 10000}
INFO flwr 2024-04-06 12:50:55,383 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:50:55,383 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1075077)[0m 2024-04-06 12:50:58.584688: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1075077)[0m 2024-04-06 12:50:58.681469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1075077)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1075083)[0m 2024-04-06 12:51:00.916059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1075080)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1075080)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1075081)[0m 2024-04-06 12:50:59.017171: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1075081)[0m 2024-04-06 12:50:59.079589: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1075081)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1075081)[0m 2024-04-06 12:51:01.174526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 12:51:14,634 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:51:15,175 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:51:16,567 | server.py:125 | fit progress: (1, 2.2418441772460938, {'accuracy': 0.2269, 'data_size': 10000}, 21.18414598199888)
INFO flwr 2024-04-06 12:51:16,567 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:51:16,568 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:51:26,718 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:51:29,133 | server.py:125 | fit progress: (2, 1.9704996347427368, {'accuracy': 0.509, 'data_size': 10000}, 33.74956033201306)
INFO flwr 2024-04-06 12:51:29,133 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:51:29,133 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:51:38,538 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:51:42,151 | server.py:125 | fit progress: (3, 1.8512234687805176, {'accuracy': 0.6169, 'data_size': 10000}, 46.767565837013535)
INFO flwr 2024-04-06 12:51:42,151 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:51:42,151 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:51:51,511 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:51:56,480 | server.py:125 | fit progress: (4, 1.732327938079834, {'accuracy': 0.7304, 'data_size': 10000}, 61.09745307901176)
INFO flwr 2024-04-06 12:51:56,481 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:51:56,481 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:52:05,748 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:52:12,427 | server.py:125 | fit progress: (5, 1.6764194965362549, {'accuracy': 0.7895, 'data_size': 10000}, 77.04376445201342)
INFO flwr 2024-04-06 12:52:12,427 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:52:12,427 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:52:21,887 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:52:28,552 | server.py:125 | fit progress: (6, 1.5929715633392334, {'accuracy': 0.8792, 'data_size': 10000}, 93.16854717300157)
INFO flwr 2024-04-06 12:52:28,552 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:52:28,552 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:52:38,485 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:52:45,623 | server.py:125 | fit progress: (7, 1.6239773035049438, {'accuracy': 0.8413, 'data_size': 10000}, 110.24040778400376)
INFO flwr 2024-04-06 12:52:45,624 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:52:45,624 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:52:54,926 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:53:06,079 | server.py:125 | fit progress: (8, 1.5653157234191895, {'accuracy': 0.9009, 'data_size': 10000}, 130.69564850901952)
INFO flwr 2024-04-06 12:53:06,079 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:53:06,079 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:53:15,674 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:53:25,041 | server.py:125 | fit progress: (9, 1.5669541358947754, {'accuracy': 0.897, 'data_size': 10000}, 149.65760443601175)
INFO flwr 2024-04-06 12:53:25,041 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:53:25,041 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:53:34,425 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:53:48,066 | server.py:125 | fit progress: (10, 1.5670061111450195, {'accuracy': 0.8976, 'data_size': 10000}, 172.68338175301324)
INFO flwr 2024-04-06 12:53:48,067 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:53:48,067 | server.py:153 | FL finished in 172.6838372189959
INFO flwr 2024-04-06 12:53:48,067 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:53:48,067 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:53:48,067 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:53:48,067 | app.py:229 | app_fit: losses_centralized [(0, 2.302417039871216), (1, 2.2418441772460938), (2, 1.9704996347427368), (3, 1.8512234687805176), (4, 1.732327938079834), (5, 1.6764194965362549), (6, 1.5929715633392334), (7, 1.6239773035049438), (8, 1.5653157234191895), (9, 1.5669541358947754), (10, 1.5670061111450195)]
INFO flwr 2024-04-06 12:53:48,067 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1395), (1, 0.2269), (2, 0.509), (3, 0.6169), (4, 0.7304), (5, 0.7895), (6, 0.8792), (7, 0.8413), (8, 0.9009), (9, 0.897), (10, 0.8976)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8976
wandb:     loss 1.56701
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_125035-cox6hd6p
wandb: Find logs at: ./wandb/offline-run-20240406_125035-cox6hd6p/logs
INFO flwr 2024-04-06 12:53:51,764 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:00:57,946 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1075076)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1075076)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 13:01:03,601	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:01:03,967	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:01:04,268	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_211244e62ff38c96.zip' (9.36MiB) to Ray cluster...
2024-04-06 13:01:04,290	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_211244e62ff38c96.zip'.
INFO flwr 2024-04-06 13:01:15,015 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 143469870490.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65772801638.0, 'CPU': 64.0}
INFO flwr 2024-04-06 13:01:15,015 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:01:15,015 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:01:15,030 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:01:15,032 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:01:15,032 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:01:15,032 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 13:01:17,049 | server.py:94 | initial parameters (loss, other metrics): 2.302820920944214, {'accuracy': 0.0755, 'data_size': 10000}
INFO flwr 2024-04-06 13:01:17,050 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:01:17,050 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1079838)[0m 2024-04-06 13:01:20.870102: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1079838)[0m 2024-04-06 13:01:20.963245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1079838)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1079839)[0m 2024-04-06 13:01:23.167290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1079829)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1079829)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1079828)[0m 2024-04-06 13:01:21.403345: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1079828)[0m 2024-04-06 13:01:21.503248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1079828)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1079828)[0m 2024-04-06 13:01:23.432030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 13:01:40,973 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:01:41,487 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:01:42,784 | server.py:125 | fit progress: (1, 2.2683920860290527, {'accuracy': 0.2275, 'data_size': 10000}, 25.73421377298655)
INFO flwr 2024-04-06 13:01:42,784 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:01:42,784 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:01:51,790 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:01:54,207 | server.py:125 | fit progress: (2, 2.016040325164795, {'accuracy': 0.5016, 'data_size': 10000}, 37.15698728899588)
INFO flwr 2024-04-06 13:01:54,207 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:01:54,207 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:02:02,280 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:02:05,718 | server.py:125 | fit progress: (3, 1.7399214506149292, {'accuracy': 0.7323, 'data_size': 10000}, 48.668265018990496)
INFO flwr 2024-04-06 13:02:05,718 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:02:05,718 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:02:14,076 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:02:18,588 | server.py:125 | fit progress: (4, 1.6584038734436035, {'accuracy': 0.8067, 'data_size': 10000}, 61.538576922990615)
INFO flwr 2024-04-06 13:02:18,589 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:02:18,589 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:02:26,584 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:02:31,914 | server.py:125 | fit progress: (5, 1.6086045503616333, {'accuracy': 0.8669, 'data_size': 10000}, 74.86477714398643)
INFO flwr 2024-04-06 13:02:31,915 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:02:31,915 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:02:40,546 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:02:46,812 | server.py:125 | fit progress: (6, 1.5982714891433716, {'accuracy': 0.8716, 'data_size': 10000}, 89.7627636109828)
INFO flwr 2024-04-06 13:02:46,813 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:02:46,813 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:02:55,375 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:03:02,963 | server.py:125 | fit progress: (7, 1.5546541213989258, {'accuracy': 0.9109, 'data_size': 10000}, 105.91299743499258)
INFO flwr 2024-04-06 13:03:02,963 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:03:02,963 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:03:11,652 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:03:20,208 | server.py:125 | fit progress: (8, 1.5495073795318604, {'accuracy': 0.9148, 'data_size': 10000}, 123.15823835399351)
INFO flwr 2024-04-06 13:03:20,208 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:03:20,208 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:03:28,550 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:03:38,671 | server.py:125 | fit progress: (9, 1.5451289415359497, {'accuracy': 0.9176, 'data_size': 10000}, 141.62089419199037)
INFO flwr 2024-04-06 13:03:38,671 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:03:38,671 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:03:46,721 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:03:57,713 | server.py:125 | fit progress: (10, 1.5384857654571533, {'accuracy': 0.9234, 'data_size': 10000}, 160.66321055198205)
INFO flwr 2024-04-06 13:03:57,713 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:03:57,713 | server.py:153 | FL finished in 160.66364019399043
INFO flwr 2024-04-06 13:03:57,714 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:03:57,714 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:03:57,714 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:03:57,714 | app.py:229 | app_fit: losses_centralized [(0, 2.302820920944214), (1, 2.2683920860290527), (2, 2.016040325164795), (3, 1.7399214506149292), (4, 1.6584038734436035), (5, 1.6086045503616333), (6, 1.5982714891433716), (7, 1.5546541213989258), (8, 1.5495073795318604), (9, 1.5451289415359497), (10, 1.5384857654571533)]
INFO flwr 2024-04-06 13:03:57,714 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0755), (1, 0.2275), (2, 0.5016), (3, 0.7323), (4, 0.8067), (5, 0.8669), (6, 0.8716), (7, 0.9109), (8, 0.9148), (9, 0.9176), (10, 0.9234)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9234
wandb:     loss 1.53849
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_130057-kbjhfduj
wandb: Find logs at: ./wandb/offline-run-20240406_130057-kbjhfduj/logs
INFO flwr 2024-04-06 13:04:01,304 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:11:07,657 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1079834)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1079834)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 13:11:13,978	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:11:14,300	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:11:14,635	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_394fd42dad13d36d.zip' (9.38MiB) to Ray cluster...
2024-04-06 13:11:14,658	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_394fd42dad13d36d.zip'.
INFO flwr 2024-04-06 13:11:25,520 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 63489210777.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 138141491815.0}
INFO flwr 2024-04-06 13:11:25,521 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:11:25,521 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:11:25,545 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:11:25,546 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:11:25,547 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:11:25,547 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 13:11:27,993 | server.py:94 | initial parameters (loss, other metrics): 2.3025619983673096, {'accuracy': 0.1093, 'data_size': 10000}
INFO flwr 2024-04-06 13:11:27,994 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:11:27,994 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1087356)[0m 2024-04-06 13:11:31.481177: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1087356)[0m 2024-04-06 13:11:31.599499: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1087356)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1087356)[0m 2024-04-06 13:11:33.726294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1087356)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1087356)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1087352)[0m 2024-04-06 13:11:31.950783: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1087352)[0m 2024-04-06 13:11:32.047297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1087352)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1087352)[0m 2024-04-06 13:11:34.329253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 13:11:47,207 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:11:47,714 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:11:48,953 | server.py:125 | fit progress: (1, 2.3025336265563965, {'accuracy': 0.1146, 'data_size': 10000}, 20.959351741999853)
INFO flwr 2024-04-06 13:11:48,954 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:11:48,954 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:11:58,349 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:12:00,726 | server.py:125 | fit progress: (2, 2.3025059700012207, {'accuracy': 0.1172, 'data_size': 10000}, 32.73210359099903)
INFO flwr 2024-04-06 13:12:00,727 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:12:00,727 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:12:09,437 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:12:12,875 | server.py:125 | fit progress: (3, 2.3024826049804688, {'accuracy': 0.1261, 'data_size': 10000}, 44.88119305600412)
INFO flwr 2024-04-06 13:12:12,875 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:12:12,876 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:12:21,304 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:12:25,601 | server.py:125 | fit progress: (4, 2.302464246749878, {'accuracy': 0.129, 'data_size': 10000}, 57.60711883899057)
INFO flwr 2024-04-06 13:12:25,601 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:12:25,602 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:12:34,050 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:12:38,926 | server.py:125 | fit progress: (5, 2.302433729171753, {'accuracy': 0.1305, 'data_size': 10000}, 70.93213011597982)
INFO flwr 2024-04-06 13:12:38,926 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:12:38,927 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:12:48,191 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:12:54,208 | server.py:125 | fit progress: (6, 2.3024089336395264, {'accuracy': 0.1388, 'data_size': 10000}, 86.21412457799306)
INFO flwr 2024-04-06 13:12:54,208 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:12:54,209 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:13:03,463 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:13:09,983 | server.py:125 | fit progress: (7, 2.302386999130249, {'accuracy': 0.1447, 'data_size': 10000}, 101.98865958099486)
INFO flwr 2024-04-06 13:13:09,983 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:13:09,983 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:13:19,606 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:13:27,062 | server.py:125 | fit progress: (8, 2.3023626804351807, {'accuracy': 0.1497, 'data_size': 10000}, 119.06796610998572)
INFO flwr 2024-04-06 13:13:27,063 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:13:27,063 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:13:35,755 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:13:44,346 | server.py:125 | fit progress: (9, 2.302328109741211, {'accuracy': 0.15, 'data_size': 10000}, 136.35195837100036)
INFO flwr 2024-04-06 13:13:44,346 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:13:44,347 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:13:53,346 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:14:03,877 | server.py:125 | fit progress: (10, 2.3022994995117188, {'accuracy': 0.1403, 'data_size': 10000}, 155.88265055199736)
INFO flwr 2024-04-06 13:14:03,877 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:14:03,877 | server.py:153 | FL finished in 155.88304476600024
INFO flwr 2024-04-06 13:14:03,877 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:14:03,877 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:14:03,877 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:14:03,878 | app.py:229 | app_fit: losses_centralized [(0, 2.3025619983673096), (1, 2.3025336265563965), (2, 2.3025059700012207), (3, 2.3024826049804688), (4, 2.302464246749878), (5, 2.302433729171753), (6, 2.3024089336395264), (7, 2.302386999130249), (8, 2.3023626804351807), (9, 2.302328109741211), (10, 2.3022994995117188)]
INFO flwr 2024-04-06 13:14:03,878 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1093), (1, 0.1146), (2, 0.1172), (3, 0.1261), (4, 0.129), (5, 0.1305), (6, 0.1388), (7, 0.1447), (8, 0.1497), (9, 0.15), (10, 0.1403)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1403
wandb:     loss 2.3023
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_131107-7ur4lolc
wandb: Find logs at: ./wandb/offline-run-20240406_131107-7ur4lolc/logs
INFO flwr 2024-04-06 13:14:07,431 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:21:14,904 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1087347)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1087347)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 13:21:19,757	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:21:20,253	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:21:20,680	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7c2ba86d41af38e4.zip' (9.40MiB) to Ray cluster...
2024-04-06 13:21:20,705	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7c2ba86d41af38e4.zip'.
INFO flwr 2024-04-06 13:21:31,698 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 138076366234.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63461299814.0}
INFO flwr 2024-04-06 13:21:31,698 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:21:31,698 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:21:31,712 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:21:31,713 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:21:31,713 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:21:31,713 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 13:21:33,963 | server.py:94 | initial parameters (loss, other metrics): 2.3024368286132812, {'accuracy': 0.093, 'data_size': 10000}
INFO flwr 2024-04-06 13:21:33,963 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:21:33,963 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1094835)[0m 2024-04-06 13:21:38.174227: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1094835)[0m 2024-04-06 13:21:38.269117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1094835)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1094835)[0m 2024-04-06 13:21:40.639856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1094835)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1094835)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1094829)[0m 2024-04-06 13:21:38.622167: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1094829)[0m 2024-04-06 13:21:38.721893: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1094829)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1094829)[0m 2024-04-06 13:21:40.841602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 13:21:54,541 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:21:55,112 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:21:56,380 | server.py:125 | fit progress: (1, 2.2975990772247314, {'accuracy': 0.2273, 'data_size': 10000}, 22.416410614008782)
INFO flwr 2024-04-06 13:21:56,380 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:21:56,380 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:22:05,661 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:22:08,143 | server.py:125 | fit progress: (2, 2.2421648502349854, {'accuracy': 0.2759, 'data_size': 10000}, 34.18017998500727)
INFO flwr 2024-04-06 13:22:08,144 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:22:08,144 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:22:17,516 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:22:20,813 | server.py:125 | fit progress: (3, 2.1536309719085693, {'accuracy': 0.3125, 'data_size': 10000}, 46.85019640001701)
INFO flwr 2024-04-06 13:22:20,814 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:22:20,814 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:22:29,454 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:22:33,862 | server.py:125 | fit progress: (4, 1.9679878950119019, {'accuracy': 0.5256, 'data_size': 10000}, 59.89825112299877)
INFO flwr 2024-04-06 13:22:33,862 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:22:33,862 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:22:42,561 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:22:48,309 | server.py:125 | fit progress: (5, 1.8338050842285156, {'accuracy': 0.6504, 'data_size': 10000}, 74.3453108209942)
INFO flwr 2024-04-06 13:22:48,309 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:22:48,309 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:22:56,741 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:23:02,755 | server.py:125 | fit progress: (6, 1.7534109354019165, {'accuracy': 0.7218, 'data_size': 10000}, 88.79206164099742)
INFO flwr 2024-04-06 13:23:02,756 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:23:02,756 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:23:11,731 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:23:19,641 | server.py:125 | fit progress: (7, 1.6894021034240723, {'accuracy': 0.7846, 'data_size': 10000}, 105.67733282101108)
INFO flwr 2024-04-06 13:23:19,641 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:23:19,641 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:23:28,359 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:23:36,869 | server.py:125 | fit progress: (8, 1.6462714672088623, {'accuracy': 0.8337, 'data_size': 10000}, 122.90586717799306)
INFO flwr 2024-04-06 13:23:36,869 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:23:36,870 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:23:45,721 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:23:55,750 | server.py:125 | fit progress: (9, 1.6229432821273804, {'accuracy': 0.8533, 'data_size': 10000}, 141.7866702050087)
INFO flwr 2024-04-06 13:23:55,750 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:23:55,751 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:24:04,309 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:24:14,762 | server.py:125 | fit progress: (10, 1.6075495481491089, {'accuracy': 0.8656, 'data_size': 10000}, 160.79906075299368)
INFO flwr 2024-04-06 13:24:14,763 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:24:14,763 | server.py:153 | FL finished in 160.79942796201794
INFO flwr 2024-04-06 13:24:14,763 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:24:14,763 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:24:14,763 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:24:14,763 | app.py:229 | app_fit: losses_centralized [(0, 2.3024368286132812), (1, 2.2975990772247314), (2, 2.2421648502349854), (3, 2.1536309719085693), (4, 1.9679878950119019), (5, 1.8338050842285156), (6, 1.7534109354019165), (7, 1.6894021034240723), (8, 1.6462714672088623), (9, 1.6229432821273804), (10, 1.6075495481491089)]
INFO flwr 2024-04-06 13:24:14,763 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.093), (1, 0.2273), (2, 0.2759), (3, 0.3125), (4, 0.5256), (5, 0.6504), (6, 0.7218), (7, 0.7846), (8, 0.8337), (9, 0.8533), (10, 0.8656)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8656
wandb:     loss 1.60755
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_132113-sftq11rr
wandb: Find logs at: ./wandb/offline-run-20240406_132113-sftq11rr/logs
INFO flwr 2024-04-06 13:24:18,359 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:31:24,782 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1094834)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1094834)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 13:31:30,332	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:31:30,693	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:31:31,039	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_894ab142388b0bd7.zip' (9.42MiB) to Ray cluster...
2024-04-06 13:31:31,062	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_894ab142388b0bd7.zip'.
INFO flwr 2024-04-06 13:31:41,770 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65691725414.0, 'memory': 143280692634.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 13:31:41,771 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:31:41,771 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:31:41,791 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:31:41,793 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:31:41,793 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:31:41,793 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 13:31:44,203 | server.py:94 | initial parameters (loss, other metrics): 2.3026111125946045, {'accuracy': 0.0967, 'data_size': 10000}
INFO flwr 2024-04-06 13:31:44,204 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:31:44,204 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1099469)[0m 2024-04-06 13:31:46.966969: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1099469)[0m 2024-04-06 13:31:47.064615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1099469)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1099469)[0m 2024-04-06 13:31:49.527102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1099473)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1099473)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1099471)[0m 2024-04-06 13:31:48.308238: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1099471)[0m 2024-04-06 13:31:48.420541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1099471)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1099471)[0m 2024-04-06 13:31:50.779036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1099479)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1099479)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 13:32:04,944 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:32:05,418 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:32:06,673 | server.py:125 | fit progress: (1, 2.2892653942108154, {'accuracy': 0.2133, 'data_size': 10000}, 22.468747284001438)
INFO flwr 2024-04-06 13:32:06,673 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:32:06,673 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:32:16,260 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:32:18,645 | server.py:125 | fit progress: (2, 2.1703696250915527, {'accuracy': 0.4667, 'data_size': 10000}, 34.441003932006424)
INFO flwr 2024-04-06 13:32:18,646 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:32:18,646 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:32:27,365 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:32:30,865 | server.py:125 | fit progress: (3, 1.8065253496170044, {'accuracy': 0.7365, 'data_size': 10000}, 46.66118605999509)
INFO flwr 2024-04-06 13:32:30,866 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:32:30,866 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:32:39,638 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:32:43,717 | server.py:125 | fit progress: (4, 1.6358323097229004, {'accuracy': 0.8436, 'data_size': 10000}, 59.51305727599538)
INFO flwr 2024-04-06 13:32:43,718 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:32:43,718 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:32:52,553 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:32:57,754 | server.py:125 | fit progress: (5, 1.6146990060806274, {'accuracy': 0.8564, 'data_size': 10000}, 73.54935799201485)
INFO flwr 2024-04-06 13:32:57,754 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:32:57,754 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:33:06,699 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:33:12,522 | server.py:125 | fit progress: (6, 1.5809009075164795, {'accuracy': 0.8891, 'data_size': 10000}, 88.3181367549987)
INFO flwr 2024-04-06 13:33:12,523 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:33:12,523 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:33:22,366 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:33:29,347 | server.py:125 | fit progress: (7, 1.5874648094177246, {'accuracy': 0.8778, 'data_size': 10000}, 105.14270404799026)
INFO flwr 2024-04-06 13:33:29,347 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:33:29,347 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:33:39,245 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:33:47,160 | server.py:125 | fit progress: (8, 1.5755246877670288, {'accuracy': 0.8923, 'data_size': 10000}, 122.95525321201421)
INFO flwr 2024-04-06 13:33:47,160 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:33:47,160 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:33:56,489 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:34:06,323 | server.py:125 | fit progress: (9, 1.5791513919830322, {'accuracy': 0.8859, 'data_size': 10000}, 142.11915336200036)
INFO flwr 2024-04-06 13:34:06,324 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:34:06,324 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:34:15,601 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:34:25,573 | server.py:125 | fit progress: (10, 1.551206111907959, {'accuracy': 0.9142, 'data_size': 10000}, 161.368620808993)
INFO flwr 2024-04-06 13:34:25,573 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:34:25,573 | server.py:153 | FL finished in 161.36910249199718
INFO flwr 2024-04-06 13:34:25,574 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:34:25,574 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:34:25,574 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:34:25,574 | app.py:229 | app_fit: losses_centralized [(0, 2.3026111125946045), (1, 2.2892653942108154), (2, 2.1703696250915527), (3, 1.8065253496170044), (4, 1.6358323097229004), (5, 1.6146990060806274), (6, 1.5809009075164795), (7, 1.5874648094177246), (8, 1.5755246877670288), (9, 1.5791513919830322), (10, 1.551206111907959)]
INFO flwr 2024-04-06 13:34:25,574 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0967), (1, 0.2133), (2, 0.4667), (3, 0.7365), (4, 0.8436), (5, 0.8564), (6, 0.8891), (7, 0.8778), (8, 0.8923), (9, 0.8859), (10, 0.9142)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9142
wandb:     loss 1.55121
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_133124-x5xmt2in
wandb: Find logs at: ./wandb/offline-run-20240406_133124-x5xmt2in/logs
INFO flwr 2024-04-06 13:34:29,166 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:41:37,971 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1099471)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1099471)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 13:41:43,957	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:41:44,389	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:41:44,716	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0f0e4f2ef0d02a9b.zip' (9.44MiB) to Ray cluster...
2024-04-06 13:41:44,738	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0f0e4f2ef0d02a9b.zip'.
INFO flwr 2024-04-06 13:41:57,378 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63373060915.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 137870475469.0}
INFO flwr 2024-04-06 13:41:57,378 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:41:57,379 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:41:57,395 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:41:57,396 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:41:57,396 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:41:57,396 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 13:42:00,772 | server.py:94 | initial parameters (loss, other metrics): 2.302750587463379, {'accuracy': 0.0967, 'data_size': 10000}
INFO flwr 2024-04-06 13:42:00,772 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:42:00,773 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1107021)[0m 2024-04-06 13:42:03.211036: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1107021)[0m 2024-04-06 13:42:03.308109: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1107021)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1107024)[0m 2024-04-06 13:42:05.288716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1107018)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1107018)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1107020)[0m 2024-04-06 13:42:03.726343: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1107020)[0m 2024-04-06 13:42:03.829783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1107020)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1107020)[0m 2024-04-06 13:42:06.013751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 13:42:19,237 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:42:19,746 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:42:21,003 | server.py:125 | fit progress: (1, 2.264946937561035, {'accuracy': 0.3247, 'data_size': 10000}, 20.229986747988733)
INFO flwr 2024-04-06 13:42:21,003 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:42:21,003 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:42:30,390 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:42:32,757 | server.py:125 | fit progress: (2, 1.9788779020309448, {'accuracy': 0.5873, 'data_size': 10000}, 31.984009256993886)
INFO flwr 2024-04-06 13:42:32,757 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:42:32,757 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:42:41,023 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:42:44,514 | server.py:125 | fit progress: (3, 1.7554185390472412, {'accuracy': 0.7262, 'data_size': 10000}, 43.7409683779988)
INFO flwr 2024-04-06 13:42:44,514 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:42:44,514 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:42:53,229 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:42:57,295 | server.py:125 | fit progress: (4, 1.6640841960906982, {'accuracy': 0.8018, 'data_size': 10000}, 56.52217884600395)
INFO flwr 2024-04-06 13:42:57,295 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:42:57,295 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:43:06,407 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:43:11,314 | server.py:125 | fit progress: (5, 1.5920454263687134, {'accuracy': 0.8781, 'data_size': 10000}, 70.54175032200874)
INFO flwr 2024-04-06 13:43:11,315 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:43:11,315 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:43:20,427 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:43:26,581 | server.py:125 | fit progress: (6, 1.5878547430038452, {'accuracy': 0.8798, 'data_size': 10000}, 85.80869142699521)
INFO flwr 2024-04-06 13:43:26,582 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:43:26,582 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:43:35,886 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:43:42,751 | server.py:125 | fit progress: (7, 1.5607409477233887, {'accuracy': 0.9054, 'data_size': 10000}, 101.97809167200467)
INFO flwr 2024-04-06 13:43:42,751 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:43:42,751 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:43:52,171 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:43:59,867 | server.py:125 | fit progress: (8, 1.5692253112792969, {'accuracy': 0.8951, 'data_size': 10000}, 119.09417144898907)
INFO flwr 2024-04-06 13:43:59,867 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:43:59,867 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:44:08,832 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:44:17,146 | server.py:125 | fit progress: (9, 1.5497596263885498, {'accuracy': 0.9148, 'data_size': 10000}, 136.37353507700027)
INFO flwr 2024-04-06 13:44:17,146 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:44:17,147 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:44:26,141 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:44:35,654 | server.py:125 | fit progress: (10, 1.5595316886901855, {'accuracy': 0.9049, 'data_size': 10000}, 154.88144602699322)
INFO flwr 2024-04-06 13:44:35,654 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:44:35,654 | server.py:153 | FL finished in 154.8817996509897
INFO flwr 2024-04-06 13:44:35,655 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:44:35,655 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:44:35,655 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:44:35,655 | app.py:229 | app_fit: losses_centralized [(0, 2.302750587463379), (1, 2.264946937561035), (2, 1.9788779020309448), (3, 1.7554185390472412), (4, 1.6640841960906982), (5, 1.5920454263687134), (6, 1.5878547430038452), (7, 1.5607409477233887), (8, 1.5692253112792969), (9, 1.5497596263885498), (10, 1.5595316886901855)]
INFO flwr 2024-04-06 13:44:35,655 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0967), (1, 0.3247), (2, 0.5873), (3, 0.7262), (4, 0.8018), (5, 0.8781), (6, 0.8798), (7, 0.9054), (8, 0.8951), (9, 0.9148), (10, 0.9049)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9049
wandb:     loss 1.55953
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_134136-d42zusm6
wandb: Find logs at: ./wandb/offline-run-20240406_134136-d42zusm6/logs
INFO flwr 2024-04-06 13:44:39,225 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:51:46,348 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1107016)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1107016)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 13:51:51,184	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:51:51,589	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:51:52,002	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_578f0c52cd201ff8.zip' (9.47MiB) to Ray cluster...
2024-04-06 13:51:52,025	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_578f0c52cd201ff8.zip'.
INFO flwr 2024-04-06 13:52:02,761 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 133976683111.0, 'object_store_memory': 61704292761.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 13:52:02,761 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:52:02,761 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:52:02,779 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:52:02,780 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:52:02,780 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:52:02,780 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 13:52:05,144 | server.py:94 | initial parameters (loss, other metrics): 2.302664041519165, {'accuracy': 0.1043, 'data_size': 10000}
INFO flwr 2024-04-06 13:52:05,144 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:52:05,145 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1114868)[0m 2024-04-06 13:52:11.111226: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1114864)[0m 2024-04-06 13:52:11.212326: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1114864)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1114859)[0m 2024-04-06 13:52:13.674002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1114872)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1114872)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1114871)[0m 2024-04-06 13:52:11.327627: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1114871)[0m 2024-04-06 13:52:11.422273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1114871)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1114862)[0m 2024-04-06 13:52:13.820488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 13:52:27,189 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:52:27,733 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:52:29,009 | server.py:125 | fit progress: (1, 2.245798349380493, {'accuracy': 0.2959, 'data_size': 10000}, 23.864231749001192)
INFO flwr 2024-04-06 13:52:29,009 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:52:29,009 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:52:38,812 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:52:41,200 | server.py:125 | fit progress: (2, 1.9770704507827759, {'accuracy': 0.5105, 'data_size': 10000}, 36.055313723976724)
INFO flwr 2024-04-06 13:52:41,200 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:52:41,200 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:52:50,452 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:52:54,191 | server.py:125 | fit progress: (3, 1.7561285495758057, {'accuracy': 0.7135, 'data_size': 10000}, 49.046545146993594)
INFO flwr 2024-04-06 13:52:54,191 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:52:54,192 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:53:03,201 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:53:07,760 | server.py:125 | fit progress: (4, 1.5954171419143677, {'accuracy': 0.8774, 'data_size': 10000}, 62.61525335398619)
INFO flwr 2024-04-06 13:53:07,760 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:53:07,760 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:53:16,778 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:53:21,907 | server.py:125 | fit progress: (5, 1.5739965438842773, {'accuracy': 0.8923, 'data_size': 10000}, 76.76226278097602)
INFO flwr 2024-04-06 13:53:21,907 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:53:21,907 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:53:31,335 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:53:38,897 | server.py:125 | fit progress: (6, 1.5638927221298218, {'accuracy': 0.9008, 'data_size': 10000}, 93.75275124100153)
INFO flwr 2024-04-06 13:53:38,898 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:53:38,898 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:53:49,679 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:53:58,368 | server.py:125 | fit progress: (7, 1.5551215410232544, {'accuracy': 0.9092, 'data_size': 10000}, 113.22309556399705)
INFO flwr 2024-04-06 13:53:58,368 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:53:58,368 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:54:08,537 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:54:16,776 | server.py:125 | fit progress: (8, 1.6088401079177856, {'accuracy': 0.8539, 'data_size': 10000}, 131.63149428900215)
INFO flwr 2024-04-06 13:54:16,776 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:54:16,776 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:54:25,803 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:54:36,486 | server.py:125 | fit progress: (9, 1.5419843196868896, {'accuracy': 0.9218, 'data_size': 10000}, 151.3410448669747)
INFO flwr 2024-04-06 13:54:36,486 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:54:36,486 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:54:45,496 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:54:56,294 | server.py:125 | fit progress: (10, 1.5419951677322388, {'accuracy': 0.921, 'data_size': 10000}, 171.14995443299995)
INFO flwr 2024-04-06 13:54:56,295 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:54:56,295 | server.py:153 | FL finished in 171.15032727600192
INFO flwr 2024-04-06 13:54:56,295 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:54:56,295 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:54:56,295 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:54:56,295 | app.py:229 | app_fit: losses_centralized [(0, 2.302664041519165), (1, 2.245798349380493), (2, 1.9770704507827759), (3, 1.7561285495758057), (4, 1.5954171419143677), (5, 1.5739965438842773), (6, 1.5638927221298218), (7, 1.5551215410232544), (8, 1.6088401079177856), (9, 1.5419843196868896), (10, 1.5419951677322388)]
INFO flwr 2024-04-06 13:54:56,295 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1043), (1, 0.2959), (2, 0.5105), (3, 0.7135), (4, 0.8774), (5, 0.8923), (6, 0.9008), (7, 0.9092), (8, 0.8539), (9, 0.9218), (10, 0.921)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.921
wandb:     loss 1.542
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_135146-enocmrq1
wandb: Find logs at: ./wandb/offline-run-20240406_135146-enocmrq1/logs
INFO flwr 2024-04-06 13:54:59,865 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:02:06,350 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1114862)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1114862)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 14:02:11,899	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:02:12,221	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:02:12,541	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c76eebd10bb11696.zip' (9.48MiB) to Ray cluster...
2024-04-06 14:02:12,573	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c76eebd10bb11696.zip'.
INFO flwr 2024-04-06 14:02:23,339 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 142803825664.0, 'object_store_memory': 65487353856.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 14:02:23,339 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:02:23,339 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:02:23,356 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:02:23,357 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:02:23,357 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:02:23,358 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 14:02:25,881 | server.py:94 | initial parameters (loss, other metrics): 2.3027548789978027, {'accuracy': 0.076, 'data_size': 10000}
INFO flwr 2024-04-06 14:02:25,881 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:02:25,881 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1119606)[0m 2024-04-06 14:02:29.179214: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1119611)[0m 2024-04-06 14:02:29.370578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1119611)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1119601)[0m 2024-04-06 14:02:31.514389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1119611)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1119611)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1119607)[0m 2024-04-06 14:02:29.658030: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1119607)[0m 2024-04-06 14:02:29.752704: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1119607)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1119607)[0m 2024-04-06 14:02:31.936187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 14:02:44,647 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:02:45,166 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:02:46,411 | server.py:125 | fit progress: (1, 2.1931698322296143, {'accuracy': 0.3306, 'data_size': 10000}, 20.52979190699989)
INFO flwr 2024-04-06 14:02:46,411 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:02:46,411 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:02:56,070 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:02:58,458 | server.py:125 | fit progress: (2, 1.9539515972137451, {'accuracy': 0.5093, 'data_size': 10000}, 32.57697119002114)
INFO flwr 2024-04-06 14:02:58,458 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:02:58,459 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:03:06,811 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:03:10,286 | server.py:125 | fit progress: (3, 1.6472320556640625, {'accuracy': 0.828, 'data_size': 10000}, 44.40535447600996)
INFO flwr 2024-04-06 14:03:10,287 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:03:10,287 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:03:19,457 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:03:23,567 | server.py:125 | fit progress: (4, 1.593308925628662, {'accuracy': 0.8745, 'data_size': 10000}, 57.68606401301804)
INFO flwr 2024-04-06 14:03:23,567 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:03:23,568 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:03:32,588 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:03:37,750 | server.py:125 | fit progress: (5, 1.5696439743041992, {'accuracy': 0.8971, 'data_size': 10000}, 71.86871776301996)
INFO flwr 2024-04-06 14:03:37,750 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:03:37,750 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:03:46,437 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:03:52,157 | server.py:125 | fit progress: (6, 1.5501168966293335, {'accuracy': 0.9134, 'data_size': 10000}, 86.27552947902586)
INFO flwr 2024-04-06 14:03:52,157 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:03:52,157 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:04:01,293 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:04:08,209 | server.py:125 | fit progress: (7, 1.5459375381469727, {'accuracy': 0.9171, 'data_size': 10000}, 102.32769173401175)
INFO flwr 2024-04-06 14:04:08,209 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:04:08,209 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:04:16,603 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:04:24,407 | server.py:125 | fit progress: (8, 1.5389080047607422, {'accuracy': 0.924, 'data_size': 10000}, 118.52551811400917)
INFO flwr 2024-04-06 14:04:24,407 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:04:24,407 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:04:33,147 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:04:41,566 | server.py:125 | fit progress: (9, 1.5355805158615112, {'accuracy': 0.9269, 'data_size': 10000}, 135.68481183302356)
INFO flwr 2024-04-06 14:04:41,566 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:04:41,566 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:04:51,068 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:05:00,587 | server.py:125 | fit progress: (10, 1.5449674129486084, {'accuracy': 0.9182, 'data_size': 10000}, 154.70629248800105)
INFO flwr 2024-04-06 14:05:00,588 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:05:00,588 | server.py:153 | FL finished in 154.70680316200014
INFO flwr 2024-04-06 14:05:00,588 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:05:00,588 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:05:00,588 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:05:00,588 | app.py:229 | app_fit: losses_centralized [(0, 2.3027548789978027), (1, 2.1931698322296143), (2, 1.9539515972137451), (3, 1.6472320556640625), (4, 1.593308925628662), (5, 1.5696439743041992), (6, 1.5501168966293335), (7, 1.5459375381469727), (8, 1.5389080047607422), (9, 1.5355805158615112), (10, 1.5449674129486084)]
INFO flwr 2024-04-06 14:05:00,589 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.076), (1, 0.3306), (2, 0.5093), (3, 0.828), (4, 0.8745), (5, 0.8971), (6, 0.9134), (7, 0.9171), (8, 0.924), (9, 0.9269), (10, 0.9182)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9182
wandb:     loss 1.54497
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_140206-zr4q3y3e
wandb: Find logs at: ./wandb/offline-run-20240406_140206-zr4q3y3e/logs
INFO flwr 2024-04-06 14:05:04,044 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:12:11,765 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1119600)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1119600)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 14:12:16,598	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:12:16,929	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:12:17,282	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dbe2e6d908bc9e35.zip' (9.50MiB) to Ray cluster...
2024-04-06 14:12:17,310	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dbe2e6d908bc9e35.zip'.
INFO flwr 2024-04-06 14:12:28,305 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58138703462.0, 'memory': 125656974746.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 14:12:28,306 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:12:28,306 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:12:28,327 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:12:28,328 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:12:28,328 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:12:28,328 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 14:12:30,776 | server.py:94 | initial parameters (loss, other metrics): 2.302499532699585, {'accuracy': 0.103, 'data_size': 10000}
INFO flwr 2024-04-06 14:12:30,777 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:12:30,777 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1127096)[0m 2024-04-06 14:12:34.510198: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1127096)[0m 2024-04-06 14:12:34.607018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1127096)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1127096)[0m 2024-04-06 14:12:36.575350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1127104)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1127104)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1127104)[0m 2024-04-06 14:12:34.829304: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1127104)[0m 2024-04-06 14:12:34.932116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1127104)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1127097)[0m 2024-04-06 14:12:37.089470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1127096)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1127096)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 14:12:54,792 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:12:55,299 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:12:56,328 | server.py:125 | fit progress: (1, 2.1793735027313232, {'accuracy': 0.3301, 'data_size': 10000}, 25.55123424300109)
INFO flwr 2024-04-06 14:12:56,328 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:12:56,329 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:13:05,505 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:13:08,243 | server.py:125 | fit progress: (2, 1.8834619522094727, {'accuracy': 0.5873, 'data_size': 10000}, 37.46640632697381)
INFO flwr 2024-04-06 14:13:08,244 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:13:08,244 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:13:16,688 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:13:20,176 | server.py:125 | fit progress: (3, 1.7111390829086304, {'accuracy': 0.7454, 'data_size': 10000}, 49.39945548499236)
INFO flwr 2024-04-06 14:13:20,177 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:13:20,177 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:13:29,354 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:13:33,830 | server.py:125 | fit progress: (4, 1.562675952911377, {'accuracy': 0.9037, 'data_size': 10000}, 63.05344391198014)
INFO flwr 2024-04-06 14:13:33,831 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:13:33,831 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:13:42,397 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:13:48,512 | server.py:125 | fit progress: (5, 1.5576730966567993, {'accuracy': 0.9058, 'data_size': 10000}, 77.7355790459842)
INFO flwr 2024-04-06 14:13:48,513 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:13:48,513 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:13:57,148 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:14:03,218 | server.py:125 | fit progress: (6, 1.5481749773025513, {'accuracy': 0.9155, 'data_size': 10000}, 92.4413764939818)
INFO flwr 2024-04-06 14:14:03,218 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:14:03,219 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:14:11,663 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:14:18,336 | server.py:125 | fit progress: (7, 1.5389211177825928, {'accuracy': 0.925, 'data_size': 10000}, 107.55941827097558)
INFO flwr 2024-04-06 14:14:18,337 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:14:18,337 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:14:28,774 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:14:36,670 | server.py:125 | fit progress: (8, 1.5475525856018066, {'accuracy': 0.9154, 'data_size': 10000}, 125.89360855999985)
INFO flwr 2024-04-06 14:14:36,671 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:14:36,671 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:14:45,800 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:14:54,276 | server.py:125 | fit progress: (9, 1.5298683643341064, {'accuracy': 0.9333, 'data_size': 10000}, 143.4990758909844)
INFO flwr 2024-04-06 14:14:54,276 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:14:54,276 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:15:02,897 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:15:12,417 | server.py:125 | fit progress: (10, 1.5231261253356934, {'accuracy': 0.939, 'data_size': 10000}, 161.640424082987)
INFO flwr 2024-04-06 14:15:12,417 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:15:12,418 | server.py:153 | FL finished in 161.64081726397853
INFO flwr 2024-04-06 14:15:12,418 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:15:12,418 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:15:12,418 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:15:12,418 | app.py:229 | app_fit: losses_centralized [(0, 2.302499532699585), (1, 2.1793735027313232), (2, 1.8834619522094727), (3, 1.7111390829086304), (4, 1.562675952911377), (5, 1.5576730966567993), (6, 1.5481749773025513), (7, 1.5389211177825928), (8, 1.5475525856018066), (9, 1.5298683643341064), (10, 1.5231261253356934)]
INFO flwr 2024-04-06 14:15:12,418 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.103), (1, 0.3301), (2, 0.5873), (3, 0.7454), (4, 0.9037), (5, 0.9058), (6, 0.9155), (7, 0.925), (8, 0.9154), (9, 0.9333), (10, 0.939)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.939
wandb:     loss 1.52313
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_141211-h4ti0v8m
wandb: Find logs at: ./wandb/offline-run-20240406_141211-h4ti0v8m/logs
INFO flwr 2024-04-06 14:15:15,972 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:22:23,629 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1127095)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1127095)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 14:22:28,406	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:22:28,716	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:22:29,071	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a0a9502e844629c9.zip' (9.52MiB) to Ray cluster...
2024-04-06 14:22:29,095	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a0a9502e844629c9.zip'.
INFO flwr 2024-04-06 14:22:40,412 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 143542470861.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65803916083.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 14:22:40,412 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:22:40,412 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:22:40,426 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:22:40,427 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:22:40,427 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:22:40,427 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 14:22:43,458 | server.py:94 | initial parameters (loss, other metrics): 2.3028297424316406, {'accuracy': 0.0971, 'data_size': 10000}
INFO flwr 2024-04-06 14:22:43,459 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:22:43,460 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1134440)[0m 2024-04-06 14:22:46.157514: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1134440)[0m 2024-04-06 14:22:46.251959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1134440)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1134440)[0m 2024-04-06 14:22:48.547257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1134439)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1134439)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1134441)[0m 2024-04-06 14:22:46.851441: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1134441)[0m 2024-04-06 14:22:46.950756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1134441)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1134441)[0m 2024-04-06 14:22:49.642629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 14:23:03,767 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:23:04,318 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:23:05,573 | server.py:125 | fit progress: (1, 2.3027899265289307, {'accuracy': 0.0974, 'data_size': 10000}, 22.112829730001977)
INFO flwr 2024-04-06 14:23:05,573 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:23:05,573 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:23:16,700 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:23:19,237 | server.py:125 | fit progress: (2, 2.302739143371582, {'accuracy': 0.0979, 'data_size': 10000}, 35.77740407499368)
INFO flwr 2024-04-06 14:23:19,238 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:23:19,238 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:23:29,678 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:23:33,719 | server.py:125 | fit progress: (3, 2.30269718170166, {'accuracy': 0.0979, 'data_size': 10000}, 50.25942435901379)
INFO flwr 2024-04-06 14:23:33,731 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:23:33,732 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:23:44,165 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:23:48,682 | server.py:125 | fit progress: (4, 2.302654504776001, {'accuracy': 0.098, 'data_size': 10000}, 65.22221171300043)
INFO flwr 2024-04-06 14:23:48,682 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:23:48,683 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:23:59,083 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:24:04,973 | server.py:125 | fit progress: (5, 2.302610158920288, {'accuracy': 0.098, 'data_size': 10000}, 81.51334861601936)
INFO flwr 2024-04-06 14:24:04,974 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:24:04,974 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:24:15,069 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:24:22,046 | server.py:125 | fit progress: (6, 2.302568197250366, {'accuracy': 0.098, 'data_size': 10000}, 98.58572484800243)
INFO flwr 2024-04-06 14:24:22,046 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:24:22,046 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:24:31,946 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:24:39,619 | server.py:125 | fit progress: (7, 2.3025155067443848, {'accuracy': 0.098, 'data_size': 10000}, 116.15907447400969)
INFO flwr 2024-04-06 14:24:39,619 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:24:39,619 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:24:49,358 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:24:57,812 | server.py:125 | fit progress: (8, 2.3024680614471436, {'accuracy': 0.098, 'data_size': 10000}, 134.35223333499744)
INFO flwr 2024-04-06 14:24:57,812 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:24:57,813 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:25:07,925 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:25:18,112 | server.py:125 | fit progress: (9, 2.3024325370788574, {'accuracy': 0.098, 'data_size': 10000}, 154.65223625401268)
INFO flwr 2024-04-06 14:25:18,112 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:25:18,113 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:25:27,960 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:25:39,102 | server.py:125 | fit progress: (10, 2.302367687225342, {'accuracy': 0.098, 'data_size': 10000}, 175.6418230110139)
INFO flwr 2024-04-06 14:25:39,102 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:25:39,102 | server.py:153 | FL finished in 175.64232685399475
INFO flwr 2024-04-06 14:25:39,102 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:25:39,103 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:25:39,103 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:25:39,103 | app.py:229 | app_fit: losses_centralized [(0, 2.3028297424316406), (1, 2.3027899265289307), (2, 2.302739143371582), (3, 2.30269718170166), (4, 2.302654504776001), (5, 2.302610158920288), (6, 2.302568197250366), (7, 2.3025155067443848), (8, 2.3024680614471436), (9, 2.3024325370788574), (10, 2.302367687225342)]
INFO flwr 2024-04-06 14:25:39,103 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0971), (1, 0.0974), (2, 0.0979), (3, 0.0979), (4, 0.098), (5, 0.098), (6, 0.098), (7, 0.098), (8, 0.098), (9, 0.098), (10, 0.098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.098
wandb:     loss 2.30237
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_142223-lug7q20f
wandb: Find logs at: ./wandb/offline-run-20240406_142223-lug7q20f/logs
INFO flwr 2024-04-06 14:25:42,720 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:32:49,062 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1134430)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1134430)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 14:32:53,963	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:32:54,392	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:32:54,767	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_925106ffd898bcf6.zip' (9.54MiB) to Ray cluster...
2024-04-06 14:32:54,790	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_925106ffd898bcf6.zip'.
INFO flwr 2024-04-06 14:33:05,536 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 65366382182.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 142521558426.0}
INFO flwr 2024-04-06 14:33:05,537 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:33:05,537 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:33:05,554 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:33:05,555 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:33:05,555 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:33:05,555 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 14:33:08,276 | server.py:94 | initial parameters (loss, other metrics): 2.302584648132324, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-06 14:33:08,277 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:33:08,277 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1139226)[0m 2024-04-06 14:33:11.526488: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1139231)[0m 2024-04-06 14:33:11.626891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1139231)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1139229)[0m 2024-04-06 14:33:13.761709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1139231)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1139231)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1139233)[0m 2024-04-06 14:33:11.887218: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1139233)[0m 2024-04-06 14:33:11.980786: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1139233)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1139233)[0m 2024-04-06 14:33:14.215478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 14:33:28,164 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:33:28,705 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:33:30,171 | server.py:125 | fit progress: (1, 2.286109209060669, {'accuracy': 0.2083, 'data_size': 10000}, 21.894344900996657)
INFO flwr 2024-04-06 14:33:30,172 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:33:30,172 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:33:40,759 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:33:43,195 | server.py:125 | fit progress: (2, 2.1331257820129395, {'accuracy': 0.2996, 'data_size': 10000}, 34.91772373599815)
INFO flwr 2024-04-06 14:33:43,195 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:33:43,195 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:33:53,176 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:33:56,623 | server.py:125 | fit progress: (3, 1.8840992450714111, {'accuracy': 0.6274, 'data_size': 10000}, 48.346219369996106)
INFO flwr 2024-04-06 14:33:56,623 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:33:56,624 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:34:06,592 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:34:10,662 | server.py:125 | fit progress: (4, 1.745097279548645, {'accuracy': 0.7218, 'data_size': 10000}, 62.38510511000641)
INFO flwr 2024-04-06 14:34:10,662 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:34:10,662 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:34:19,659 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:34:24,815 | server.py:125 | fit progress: (5, 1.6490285396575928, {'accuracy': 0.8195, 'data_size': 10000}, 76.53815822000615)
INFO flwr 2024-04-06 14:34:24,815 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:34:24,815 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:34:34,609 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:34:40,410 | server.py:125 | fit progress: (6, 1.6108770370483398, {'accuracy': 0.8607, 'data_size': 10000}, 92.1333634259936)
INFO flwr 2024-04-06 14:34:40,410 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:34:40,411 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:34:50,322 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:34:57,135 | server.py:125 | fit progress: (7, 1.5851675271987915, {'accuracy': 0.8847, 'data_size': 10000}, 108.85788267999305)
INFO flwr 2024-04-06 14:34:57,135 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:34:57,135 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:35:06,535 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:35:14,302 | server.py:125 | fit progress: (8, 1.5930430889129639, {'accuracy': 0.8744, 'data_size': 10000}, 126.02476266800659)
INFO flwr 2024-04-06 14:35:14,302 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:35:14,302 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:35:23,742 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:35:32,045 | server.py:125 | fit progress: (9, 1.5736485719680786, {'accuracy': 0.8938, 'data_size': 10000}, 143.7680133380054)
INFO flwr 2024-04-06 14:35:32,045 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:35:32,045 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:35:41,642 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:35:51,029 | server.py:125 | fit progress: (10, 1.5661554336547852, {'accuracy': 0.9015, 'data_size': 10000}, 162.75166765399626)
INFO flwr 2024-04-06 14:35:51,029 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:35:51,029 | server.py:153 | FL finished in 162.75202761599212
INFO flwr 2024-04-06 14:35:51,029 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:35:51,029 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:35:51,029 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:35:51,029 | app.py:229 | app_fit: losses_centralized [(0, 2.302584648132324), (1, 2.286109209060669), (2, 2.1331257820129395), (3, 1.8840992450714111), (4, 1.745097279548645), (5, 1.6490285396575928), (6, 1.6108770370483398), (7, 1.5851675271987915), (8, 1.5930430889129639), (9, 1.5736485719680786), (10, 1.5661554336547852)]
INFO flwr 2024-04-06 14:35:51,029 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.2083), (2, 0.2996), (3, 0.6274), (4, 0.7218), (5, 0.8195), (6, 0.8607), (7, 0.8847), (8, 0.8744), (9, 0.8938), (10, 0.9015)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9015
wandb:     loss 1.56616
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_143248-w61mfyk8
wandb: Find logs at: ./wandb/offline-run-20240406_143248-w61mfyk8/logs
INFO flwr 2024-04-06 14:35:54,601 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:43:02,306 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1139224)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1139224)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 14:43:06,951	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:43:07,357	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:43:07,797	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_35c9bd66e01693eb.zip' (9.57MiB) to Ray cluster...
2024-04-06 14:43:07,823	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_35c9bd66e01693eb.zip'.
INFO flwr 2024-04-06 14:43:18,941 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 136988332647.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62994999705.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 14:43:18,941 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:43:18,942 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:43:18,960 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:43:18,961 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:43:18,961 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:43:18,961 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 14:43:20,954 | server.py:94 | initial parameters (loss, other metrics): 2.3024637699127197, {'accuracy': 0.107, 'data_size': 10000}
INFO flwr 2024-04-06 14:43:20,955 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:43:20,955 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1147287)[0m 2024-04-06 14:43:25.491513: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1147287)[0m 2024-04-06 14:43:25.600653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1147287)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1147286)[0m 2024-04-06 14:43:27.911221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1147285)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1147285)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1147291)[0m 2024-04-06 14:43:25.777440: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1147291)[0m 2024-04-06 14:43:25.882967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1147291)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1147291)[0m 2024-04-06 14:43:28.768306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 14:43:43,767 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:43:44,351 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:43:45,963 | server.py:125 | fit progress: (1, 2.2264304161071777, {'accuracy': 0.2032, 'data_size': 10000}, 25.008632062003016)
INFO flwr 2024-04-06 14:43:45,964 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:43:45,964 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:43:58,248 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:44:00,879 | server.py:125 | fit progress: (2, 2.013462781906128, {'accuracy': 0.4564, 'data_size': 10000}, 39.92422657800489)
INFO flwr 2024-04-06 14:44:00,879 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:44:00,880 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:44:11,985 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:44:15,862 | server.py:125 | fit progress: (3, 1.8575408458709717, {'accuracy': 0.5997, 'data_size': 10000}, 54.90673176001292)
INFO flwr 2024-04-06 14:44:15,862 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:44:15,862 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:44:26,990 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:44:31,208 | server.py:125 | fit progress: (4, 1.6896699666976929, {'accuracy': 0.7803, 'data_size': 10000}, 70.25302355602616)
INFO flwr 2024-04-06 14:44:31,208 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:44:31,209 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:44:42,173 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:44:48,381 | server.py:125 | fit progress: (5, 1.661116361618042, {'accuracy': 0.8021, 'data_size': 10000}, 87.42631861701375)
INFO flwr 2024-04-06 14:44:48,382 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:44:48,382 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:44:57,974 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:45:04,423 | server.py:125 | fit progress: (6, 1.5783106088638306, {'accuracy': 0.8899, 'data_size': 10000}, 103.46857052002451)
INFO flwr 2024-04-06 14:45:04,424 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:45:04,424 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:45:14,127 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:45:21,599 | server.py:125 | fit progress: (7, 1.5811491012573242, {'accuracy': 0.8855, 'data_size': 10000}, 120.64384401700227)
INFO flwr 2024-04-06 14:45:21,599 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:45:21,599 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:45:31,367 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:45:38,933 | server.py:125 | fit progress: (8, 1.5577503442764282, {'accuracy': 0.9079, 'data_size': 10000}, 137.97791701401002)
INFO flwr 2024-04-06 14:45:38,933 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:45:38,933 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:45:48,754 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:45:57,313 | server.py:125 | fit progress: (9, 1.5526773929595947, {'accuracy': 0.9114, 'data_size': 10000}, 156.35796109502553)
INFO flwr 2024-04-06 14:45:57,313 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:45:57,313 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:46:06,657 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:46:17,645 | server.py:125 | fit progress: (10, 1.5540745258331299, {'accuracy': 0.91, 'data_size': 10000}, 176.68974126700778)
INFO flwr 2024-04-06 14:46:17,645 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:46:17,645 | server.py:153 | FL finished in 176.69043000001693
INFO flwr 2024-04-06 14:46:17,646 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:46:17,646 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:46:17,646 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:46:17,646 | app.py:229 | app_fit: losses_centralized [(0, 2.3024637699127197), (1, 2.2264304161071777), (2, 2.013462781906128), (3, 1.8575408458709717), (4, 1.6896699666976929), (5, 1.661116361618042), (6, 1.5783106088638306), (7, 1.5811491012573242), (8, 1.5577503442764282), (9, 1.5526773929595947), (10, 1.5540745258331299)]
INFO flwr 2024-04-06 14:46:17,646 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.107), (1, 0.2032), (2, 0.4564), (3, 0.5997), (4, 0.7803), (5, 0.8021), (6, 0.8899), (7, 0.8855), (8, 0.9079), (9, 0.9114), (10, 0.91)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.91
wandb:     loss 1.55407
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_144301-m110cr4w
wandb: Find logs at: ./wandb/offline-run-20240406_144301-m110cr4w/logs
INFO flwr 2024-04-06 14:46:21,210 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:53:27,977 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1147287)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1147287)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 14:53:33,050	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:53:33,385	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:53:33,828	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b507fdc59ce396dc.zip' (9.58MiB) to Ray cluster...
2024-04-06 14:53:33,852	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b507fdc59ce396dc.zip'.
INFO flwr 2024-04-06 14:53:45,349 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 141944488551.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 65119066521.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 14:53:45,349 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:53:45,350 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:53:45,364 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:53:45,365 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:53:45,365 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:53:45,365 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 14:53:48,325 | server.py:94 | initial parameters (loss, other metrics): 2.3028573989868164, {'accuracy': 0.1004, 'data_size': 10000}
INFO flwr 2024-04-06 14:53:48,326 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:53:48,327 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1151652)[0m 2024-04-06 14:53:51.337104: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1151652)[0m 2024-04-06 14:53:51.446241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1151652)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1151652)[0m 2024-04-06 14:53:53.480574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1151652)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1151652)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1151651)[0m 2024-04-06 14:53:51.611140: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1151651)[0m 2024-04-06 14:53:51.704466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1151651)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1151651)[0m 2024-04-06 14:53:53.784382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 14:54:09,093 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:54:09,610 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:54:10,838 | server.py:125 | fit progress: (1, 2.2026073932647705, {'accuracy': 0.3031, 'data_size': 10000}, 22.512167227978352)
INFO flwr 2024-04-06 14:54:10,838 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:54:10,838 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:54:21,210 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:54:23,982 | server.py:125 | fit progress: (2, 1.7830042839050293, {'accuracy': 0.6846, 'data_size': 10000}, 35.6556703929964)
INFO flwr 2024-04-06 14:54:23,982 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:54:23,982 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:54:33,619 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:54:36,904 | server.py:125 | fit progress: (3, 1.6422412395477295, {'accuracy': 0.8255, 'data_size': 10000}, 48.5782700709824)
INFO flwr 2024-04-06 14:54:36,904 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:54:36,905 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:54:46,609 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:54:51,094 | server.py:125 | fit progress: (4, 1.5798507928848267, {'accuracy': 0.8901, 'data_size': 10000}, 62.76846250399831)
INFO flwr 2024-04-06 14:54:51,095 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:54:51,095 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:55:01,210 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:55:06,147 | server.py:125 | fit progress: (5, 1.5611953735351562, {'accuracy': 0.9045, 'data_size': 10000}, 77.82070728199324)
INFO flwr 2024-04-06 14:55:06,147 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:55:06,147 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:55:16,441 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:55:22,388 | server.py:125 | fit progress: (6, 1.5564725399017334, {'accuracy': 0.9084, 'data_size': 10000}, 94.06261289000395)
INFO flwr 2024-04-06 14:55:22,389 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:55:22,389 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:55:32,819 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:55:40,425 | server.py:125 | fit progress: (7, 1.55036461353302, {'accuracy': 0.9135, 'data_size': 10000}, 112.09937718400033)
INFO flwr 2024-04-06 14:55:40,426 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:55:40,426 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:55:50,486 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:55:58,166 | server.py:125 | fit progress: (8, 1.5430529117584229, {'accuracy': 0.9201, 'data_size': 10000}, 129.84032503599883)
INFO flwr 2024-04-06 14:55:58,167 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:55:58,167 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:56:07,579 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:56:16,808 | server.py:125 | fit progress: (9, 1.540802001953125, {'accuracy': 0.922, 'data_size': 10000}, 148.48246852698503)
INFO flwr 2024-04-06 14:56:16,809 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:56:16,809 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:56:27,002 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:56:36,520 | server.py:125 | fit progress: (10, 1.5432900190353394, {'accuracy': 0.9201, 'data_size': 10000}, 168.1943537609768)
INFO flwr 2024-04-06 14:56:36,521 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:56:36,521 | server.py:153 | FL finished in 168.19483980498626
INFO flwr 2024-04-06 14:56:36,521 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:56:36,521 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:56:36,521 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:56:36,521 | app.py:229 | app_fit: losses_centralized [(0, 2.3028573989868164), (1, 2.2026073932647705), (2, 1.7830042839050293), (3, 1.6422412395477295), (4, 1.5798507928848267), (5, 1.5611953735351562), (6, 1.5564725399017334), (7, 1.55036461353302), (8, 1.5430529117584229), (9, 1.540802001953125), (10, 1.5432900190353394)]
INFO flwr 2024-04-06 14:56:36,521 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1004), (1, 0.3031), (2, 0.6846), (3, 0.8255), (4, 0.8901), (5, 0.9045), (6, 0.9084), (7, 0.9135), (8, 0.9201), (9, 0.922), (10, 0.9201)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9201
wandb:     loss 1.54329
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_145327-pg3cmzq9
wandb: Find logs at: ./wandb/offline-run-20240406_145327-pg3cmzq9/logs
INFO flwr 2024-04-06 14:56:40,137 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:03:47,333 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1151650)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1151650)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 15:03:52,922	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:03:53,362	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:03:53,792	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a262a054dafdceaf.zip' (9.61MiB) to Ray cluster...
2024-04-06 15:03:53,822	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a262a054dafdceaf.zip'.
INFO flwr 2024-04-06 15:04:04,843 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 62757083136.0, 'node:10.20.240.18': 1.0, 'memory': 136433193984.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 15:04:04,843 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:04:04,843 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:04:04,864 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:04:04,865 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:04:04,865 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:04:04,865 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 15:04:07,763 | server.py:94 | initial parameters (loss, other metrics): 2.302558422088623, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-06 15:04:07,764 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:04:07,764 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1159452)[0m 2024-04-06 15:04:10.903811: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1159452)[0m 2024-04-06 15:04:11.004459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1159452)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1159456)[0m 2024-04-06 15:04:13.043558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1159462)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1159462)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1159453)[0m 2024-04-06 15:04:11.492444: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1159453)[0m 2024-04-06 15:04:11.584044: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1159453)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1159453)[0m 2024-04-06 15:04:13.743405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 15:04:28,352 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:04:28,946 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:04:30,318 | server.py:125 | fit progress: (1, 2.151397943496704, {'accuracy': 0.324, 'data_size': 10000}, 22.55387346399948)
INFO flwr 2024-04-06 15:04:30,318 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:04:30,318 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:04:40,228 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:04:42,666 | server.py:125 | fit progress: (2, 1.793065071105957, {'accuracy': 0.6682, 'data_size': 10000}, 34.9021393499861)
INFO flwr 2024-04-06 15:04:42,666 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:04:42,667 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:04:52,878 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:04:56,398 | server.py:125 | fit progress: (3, 1.6434283256530762, {'accuracy': 0.8249, 'data_size': 10000}, 48.633657366997795)
INFO flwr 2024-04-06 15:04:56,398 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:04:56,398 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:05:06,239 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:05:10,327 | server.py:125 | fit progress: (4, 1.5819731950759888, {'accuracy': 0.8842, 'data_size': 10000}, 62.56321478498285)
INFO flwr 2024-04-06 15:05:10,327 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:05:10,328 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:05:20,122 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:05:25,268 | server.py:125 | fit progress: (5, 1.562044382095337, {'accuracy': 0.9025, 'data_size': 10000}, 77.50394666398643)
INFO flwr 2024-04-06 15:05:25,268 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:05:25,268 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:05:35,624 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:05:41,503 | server.py:125 | fit progress: (6, 1.612654447555542, {'accuracy': 0.8501, 'data_size': 10000}, 93.73935871798312)
INFO flwr 2024-04-06 15:05:41,504 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:05:41,504 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:05:52,183 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:05:58,855 | server.py:125 | fit progress: (7, 1.5447949171066284, {'accuracy': 0.9196, 'data_size': 10000}, 111.0908415299782)
INFO flwr 2024-04-06 15:05:58,855 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:05:58,855 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:06:09,343 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:06:18,249 | server.py:125 | fit progress: (8, 1.5379828214645386, {'accuracy': 0.9259, 'data_size': 10000}, 130.48555456998292)
INFO flwr 2024-04-06 15:06:18,250 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:06:18,250 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:06:28,426 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:06:36,804 | server.py:125 | fit progress: (9, 1.5359246730804443, {'accuracy': 0.9267, 'data_size': 10000}, 149.04034718198818)
INFO flwr 2024-04-06 15:06:36,805 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:06:36,805 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:06:46,293 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:06:55,698 | server.py:125 | fit progress: (10, 1.5331380367279053, {'accuracy': 0.9301, 'data_size': 10000}, 167.93418555799872)
INFO flwr 2024-04-06 15:06:55,698 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:06:55,699 | server.py:153 | FL finished in 167.93463632898056
INFO flwr 2024-04-06 15:06:55,699 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:06:55,699 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:06:55,699 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:06:55,699 | app.py:229 | app_fit: losses_centralized [(0, 2.302558422088623), (1, 2.151397943496704), (2, 1.793065071105957), (3, 1.6434283256530762), (4, 1.5819731950759888), (5, 1.562044382095337), (6, 1.612654447555542), (7, 1.5447949171066284), (8, 1.5379828214645386), (9, 1.5359246730804443), (10, 1.5331380367279053)]
INFO flwr 2024-04-06 15:06:55,699 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.324), (2, 0.6682), (3, 0.8249), (4, 0.8842), (5, 0.9025), (6, 0.8501), (7, 0.9196), (8, 0.9259), (9, 0.9267), (10, 0.9301)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9301
wandb:     loss 1.53314
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_150347-29s508i6
wandb: Find logs at: ./wandb/offline-run-20240406_150347-29s508i6/logs
INFO flwr 2024-04-06 15:06:59,283 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:14:08,184 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1159452)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1159452)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 15:14:13,108	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:14:13,428	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:14:13,750	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_25a0c1d72cae1804.zip' (9.63MiB) to Ray cluster...
2024-04-06 15:14:13,782	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_25a0c1d72cae1804.zip'.
INFO flwr 2024-04-06 15:14:25,080 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 136425306317.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62753702707.0}
INFO flwr 2024-04-06 15:14:25,081 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:14:25,081 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:14:25,098 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:14:25,099 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:14:25,099 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:14:25,099 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 15:14:27,254 | server.py:94 | initial parameters (loss, other metrics): 2.302537441253662, {'accuracy': 0.094, 'data_size': 10000}
INFO flwr 2024-04-06 15:14:27,255 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:14:27,255 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1166873)[0m 2024-04-06 15:14:31.642838: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1166873)[0m 2024-04-06 15:14:31.748036: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1166873)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1166873)[0m 2024-04-06 15:14:34.024794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1166865)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1166865)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1166869)[0m 2024-04-06 15:14:31.920217: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1166869)[0m 2024-04-06 15:14:32.018972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1166869)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1166869)[0m 2024-04-06 15:14:34.205614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 15:14:49,331 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:14:49,885 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:14:51,149 | server.py:125 | fit progress: (1, 2.106882333755493, {'accuracy': 0.4362, 'data_size': 10000}, 23.894253823003964)
INFO flwr 2024-04-06 15:14:51,150 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:14:51,150 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:15:02,038 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:15:04,422 | server.py:125 | fit progress: (2, 1.831764817237854, {'accuracy': 0.6245, 'data_size': 10000}, 37.166449067008216)
INFO flwr 2024-04-06 15:15:04,422 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:15:04,422 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:15:14,448 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:15:18,455 | server.py:125 | fit progress: (3, 1.6868467330932617, {'accuracy': 0.7725, 'data_size': 10000}, 51.19934574200306)
INFO flwr 2024-04-06 15:15:18,455 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:15:18,455 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:15:28,408 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:15:32,673 | server.py:125 | fit progress: (4, 1.5884853601455688, {'accuracy': 0.8775, 'data_size': 10000}, 65.41754027901334)
INFO flwr 2024-04-06 15:15:32,673 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:15:32,673 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:15:42,581 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:15:48,554 | server.py:125 | fit progress: (5, 1.5483365058898926, {'accuracy': 0.9159, 'data_size': 10000}, 81.29879634600366)
INFO flwr 2024-04-06 15:15:48,554 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:15:48,554 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:15:57,932 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:16:04,871 | server.py:125 | fit progress: (6, 1.5411611795425415, {'accuracy': 0.9225, 'data_size': 10000}, 97.61627207801212)
INFO flwr 2024-04-06 15:16:04,872 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:16:04,872 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:16:15,534 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:16:23,497 | server.py:125 | fit progress: (7, 1.5442419052124023, {'accuracy': 0.9183, 'data_size': 10000}, 116.24216466999496)
INFO flwr 2024-04-06 15:16:23,498 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:16:23,498 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:16:34,654 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:16:42,554 | server.py:125 | fit progress: (8, 1.5437380075454712, {'accuracy': 0.9187, 'data_size': 10000}, 135.29852878200472)
INFO flwr 2024-04-06 15:16:42,554 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:16:42,554 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:16:53,094 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:17:02,730 | server.py:125 | fit progress: (9, 1.5499589443206787, {'accuracy': 0.9127, 'data_size': 10000}, 155.47437997799716)
INFO flwr 2024-04-06 15:17:02,730 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:17:02,730 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:17:12,214 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:17:21,529 | server.py:125 | fit progress: (10, 1.5234073400497437, {'accuracy': 0.9389, 'data_size': 10000}, 174.27332663501147)
INFO flwr 2024-04-06 15:17:21,529 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:17:21,529 | server.py:153 | FL finished in 174.2737426770036
INFO flwr 2024-04-06 15:17:21,529 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:17:21,529 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:17:21,529 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:17:21,530 | app.py:229 | app_fit: losses_centralized [(0, 2.302537441253662), (1, 2.106882333755493), (2, 1.831764817237854), (3, 1.6868467330932617), (4, 1.5884853601455688), (5, 1.5483365058898926), (6, 1.5411611795425415), (7, 1.5442419052124023), (8, 1.5437380075454712), (9, 1.5499589443206787), (10, 1.5234073400497437)]
INFO flwr 2024-04-06 15:17:21,530 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.094), (1, 0.4362), (2, 0.6245), (3, 0.7725), (4, 0.8775), (5, 0.9159), (6, 0.9225), (7, 0.9183), (8, 0.9187), (9, 0.9127), (10, 0.9389)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9389
wandb:     loss 1.52341
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_151407-bwj2cxkt
wandb: Find logs at: ./wandb/offline-run-20240406_151407-bwj2cxkt/logs
INFO flwr 2024-04-06 15:17:25,094 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:24:31,408 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1166862)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1166862)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 15:24:37,148	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:24:37,530	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:24:37,864	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a2220eb5f673b5c9.zip' (9.64MiB) to Ray cluster...
2024-04-06 15:24:37,885	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a2220eb5f673b5c9.zip'.
INFO flwr 2024-04-06 15:24:48,627 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 65110495641.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 141924489831.0, 'CPU': 64.0}
INFO flwr 2024-04-06 15:24:48,628 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:24:48,628 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:24:48,651 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:24:48,653 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:24:48,653 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:24:48,653 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 15:24:52,142 | server.py:94 | initial parameters (loss, other metrics): 2.302725076675415, {'accuracy': 0.0904, 'data_size': 10000}
INFO flwr 2024-04-06 15:24:52,144 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:24:52,148 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1171271)[0m 2024-04-06 15:24:54.595125: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1171271)[0m 2024-04-06 15:24:54.697791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1171271)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1171271)[0m 2024-04-06 15:24:56.914046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1171268)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1171268)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1171265)[0m 2024-04-06 15:24:54.778220: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1171265)[0m 2024-04-06 15:24:54.873299: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1171265)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1171264)[0m 2024-04-06 15:24:57.084519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 15:25:12,695 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:25:13,211 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:25:14,465 | server.py:125 | fit progress: (1, 2.107767105102539, {'accuracy': 0.3654, 'data_size': 10000}, 22.319195321993902)
INFO flwr 2024-04-06 15:25:14,465 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:25:14,465 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:25:25,045 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:25:27,427 | server.py:125 | fit progress: (2, 1.8086130619049072, {'accuracy': 0.6633, 'data_size': 10000}, 35.281074407015694)
INFO flwr 2024-04-06 15:25:27,427 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:25:27,427 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:25:36,806 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:25:40,280 | server.py:125 | fit progress: (3, 1.5830729007720947, {'accuracy': 0.8863, 'data_size': 10000}, 48.13476814501337)
INFO flwr 2024-04-06 15:25:40,281 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:25:40,281 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:25:50,020 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:25:54,115 | server.py:125 | fit progress: (4, 1.5731008052825928, {'accuracy': 0.8902, 'data_size': 10000}, 61.96979958601878)
INFO flwr 2024-04-06 15:25:54,116 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:25:54,116 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:26:03,561 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:26:08,850 | server.py:125 | fit progress: (5, 1.5458884239196777, {'accuracy': 0.9166, 'data_size': 10000}, 76.70468753200839)
INFO flwr 2024-04-06 15:26:08,851 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:26:08,851 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:26:18,492 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:26:24,363 | server.py:125 | fit progress: (6, 1.542646050453186, {'accuracy': 0.9202, 'data_size': 10000}, 92.21690235999995)
INFO flwr 2024-04-06 15:26:24,363 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:26:24,363 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:26:34,265 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:26:41,417 | server.py:125 | fit progress: (7, 1.5335136651992798, {'accuracy': 0.9292, 'data_size': 10000}, 109.27183088799939)
INFO flwr 2024-04-06 15:26:41,418 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:26:41,418 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:26:51,715 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:26:59,608 | server.py:125 | fit progress: (8, 1.5330963134765625, {'accuracy': 0.9292, 'data_size': 10000}, 127.46220269301557)
INFO flwr 2024-04-06 15:26:59,608 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:26:59,608 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:27:09,848 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:27:18,695 | server.py:125 | fit progress: (9, 1.5405718088150024, {'accuracy': 0.9204, 'data_size': 10000}, 146.5489912860212)
INFO flwr 2024-04-06 15:27:18,695 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:27:18,707 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:27:28,672 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:27:39,578 | server.py:125 | fit progress: (10, 1.525175929069519, {'accuracy': 0.9368, 'data_size': 10000}, 167.432677549019)
INFO flwr 2024-04-06 15:27:39,579 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:27:39,579 | server.py:153 | FL finished in 167.43304810501286
INFO flwr 2024-04-06 15:27:39,579 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:27:39,579 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:27:39,579 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:27:39,579 | app.py:229 | app_fit: losses_centralized [(0, 2.302725076675415), (1, 2.107767105102539), (2, 1.8086130619049072), (3, 1.5830729007720947), (4, 1.5731008052825928), (5, 1.5458884239196777), (6, 1.542646050453186), (7, 1.5335136651992798), (8, 1.5330963134765625), (9, 1.5405718088150024), (10, 1.525175929069519)]
INFO flwr 2024-04-06 15:27:39,579 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0904), (1, 0.3654), (2, 0.6633), (3, 0.8863), (4, 0.8902), (5, 0.9166), (6, 0.9202), (7, 0.9292), (8, 0.9292), (9, 0.9204), (10, 0.9368)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9368
wandb:     loss 1.52518
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_152431-g8w7y7vs
wandb: Find logs at: ./wandb/offline-run-20240406_152431-g8w7y7vs/logs
INFO flwr 2024-04-06 15:27:43,119 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:34:49,914 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1171261)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1171261)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 15:34:55,691	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:34:56,030	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:34:56,466	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4781233431270c0e.zip' (9.66MiB) to Ray cluster...
2024-04-06 15:34:56,498	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4781233431270c0e.zip'.
INFO flwr 2024-04-06 15:35:08,211 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 128005257421.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 59145110323.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 15:35:08,212 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:35:08,212 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:35:08,229 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:35:08,231 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:35:08,231 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:35:08,232 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 15:35:10,239 | server.py:94 | initial parameters (loss, other metrics): 2.3025991916656494, {'accuracy': 0.1132, 'data_size': 10000}
INFO flwr 2024-04-06 15:35:10,239 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:35:10,240 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1179034)[0m 2024-04-06 15:35:31.847124: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1179034)[0m 2024-04-06 15:35:31.940637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1179034)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1179034)[0m 2024-04-06 15:35:34.110478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1179031)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1179031)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1179026)[0m 2024-04-06 15:35:31.890764: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1179033)[0m 2024-04-06 15:35:32.003146: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1179033)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1179026)[0m 2024-04-06 15:35:34.110472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 15:35:48,956 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:35:49,449 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:35:50,680 | server.py:125 | fit progress: (1, 2.302597761154175, {'accuracy': 0.1132, 'data_size': 10000}, 40.44094301201403)
INFO flwr 2024-04-06 15:35:50,681 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:35:50,681 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:35:59,151 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:36:01,528 | server.py:125 | fit progress: (2, 2.302596092224121, {'accuracy': 0.1137, 'data_size': 10000}, 51.28821534200688)
INFO flwr 2024-04-06 15:36:01,528 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:36:01,528 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:36:09,244 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:36:12,653 | server.py:125 | fit progress: (3, 2.3025944232940674, {'accuracy': 0.1134, 'data_size': 10000}, 62.41365450399462)
INFO flwr 2024-04-06 15:36:12,653 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:36:12,654 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:36:20,081 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:36:24,118 | server.py:125 | fit progress: (4, 2.3025925159454346, {'accuracy': 0.1133, 'data_size': 10000}, 73.87831861901213)
INFO flwr 2024-04-06 15:36:24,118 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:36:24,118 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:36:31,918 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:36:37,053 | server.py:125 | fit progress: (5, 2.3025918006896973, {'accuracy': 0.1133, 'data_size': 10000}, 86.81338766601402)
INFO flwr 2024-04-06 15:36:37,053 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:36:37,053 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:36:44,823 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:36:50,636 | server.py:125 | fit progress: (6, 2.3025894165039062, {'accuracy': 0.1136, 'data_size': 10000}, 100.3966067149886)
INFO flwr 2024-04-06 15:36:50,636 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:36:50,637 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:36:58,328 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:37:05,084 | server.py:125 | fit progress: (7, 2.30258846282959, {'accuracy': 0.1136, 'data_size': 10000}, 114.84455614199396)
INFO flwr 2024-04-06 15:37:05,084 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:37:05,084 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:37:12,984 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:37:20,669 | server.py:125 | fit progress: (8, 2.302586555480957, {'accuracy': 0.1135, 'data_size': 10000}, 130.42914471399854)
INFO flwr 2024-04-06 15:37:20,669 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:37:20,669 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:37:28,628 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:37:38,078 | server.py:125 | fit progress: (9, 2.3025851249694824, {'accuracy': 0.1136, 'data_size': 10000}, 147.83806810100214)
INFO flwr 2024-04-06 15:37:38,078 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:37:38,078 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:37:45,993 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:37:55,382 | server.py:125 | fit progress: (10, 2.302583694458008, {'accuracy': 0.1131, 'data_size': 10000}, 165.14241178901284)
INFO flwr 2024-04-06 15:37:55,382 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:37:55,382 | server.py:153 | FL finished in 165.14277115600999
INFO flwr 2024-04-06 15:37:55,382 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:37:55,383 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:37:55,383 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:37:55,383 | app.py:229 | app_fit: losses_centralized [(0, 2.3025991916656494), (1, 2.302597761154175), (2, 2.302596092224121), (3, 2.3025944232940674), (4, 2.3025925159454346), (5, 2.3025918006896973), (6, 2.3025894165039062), (7, 2.30258846282959), (8, 2.302586555480957), (9, 2.3025851249694824), (10, 2.302583694458008)]
INFO flwr 2024-04-06 15:37:55,383 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1132), (1, 0.1132), (2, 0.1137), (3, 0.1134), (4, 0.1133), (5, 0.1133), (6, 0.1136), (7, 0.1136), (8, 0.1135), (9, 0.1136), (10, 0.1131)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1131
wandb:     loss 2.30258
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_153449-za1yiyyj
wandb: Find logs at: ./wandb/offline-run-20240406_153449-za1yiyyj/logs
INFO flwr 2024-04-06 15:37:58,977 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:45:06,532 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1179026)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1179026)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 15:45:19,399	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:45:21,075	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:45:23,897	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dd6f52c86a41b020.zip' (9.69MiB) to Ray cluster...
2024-04-06 15:45:23,929	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dd6f52c86a41b020.zip'.
INFO flwr 2024-04-06 15:45:48,730 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 62782472601.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 136492436071.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 15:45:48,731 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:45:48,731 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:45:48,745 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:45:48,746 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:45:48,746 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:45:48,746 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 15:45:52,432 | server.py:94 | initial parameters (loss, other metrics): 2.3025283813476562, {'accuracy': 0.104, 'data_size': 10000}
INFO flwr 2024-04-06 15:45:52,432 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:45:52,432 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1186454)[0m 2024-04-06 15:45:54.587886: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1186457)[0m 2024-04-06 15:45:54.762435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1186457)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1186457)[0m 2024-04-06 15:45:56.787633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1186455)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1186455)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1186456)[0m 2024-04-06 15:45:55.206121: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1186456)[0m 2024-04-06 15:45:55.298405: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1186456)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1186456)[0m 2024-04-06 15:45:57.615385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 15:46:11,464 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:46:12,026 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:46:13,307 | server.py:125 | fit progress: (1, 2.302459716796875, {'accuracy': 0.0989, 'data_size': 10000}, 20.874985419010045)
INFO flwr 2024-04-06 15:46:13,307 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:46:13,308 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:46:21,917 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:46:24,644 | server.py:125 | fit progress: (2, 2.302405834197998, {'accuracy': 0.1133, 'data_size': 10000}, 32.211748477013316)
INFO flwr 2024-04-06 15:46:24,644 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:46:24,644 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:46:33,259 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:46:37,032 | server.py:125 | fit progress: (3, 2.3023481369018555, {'accuracy': 0.1028, 'data_size': 10000}, 44.59994647500571)
INFO flwr 2024-04-06 15:46:37,032 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:46:37,033 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:46:44,364 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:46:48,569 | server.py:125 | fit progress: (4, 2.30226993560791, {'accuracy': 0.1028, 'data_size': 10000}, 56.13638756101136)
INFO flwr 2024-04-06 15:46:48,569 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:46:48,569 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:46:57,041 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:47:02,496 | server.py:125 | fit progress: (5, 2.3022170066833496, {'accuracy': 0.1028, 'data_size': 10000}, 70.06398283599992)
INFO flwr 2024-04-06 15:47:02,496 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:47:02,497 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:47:10,407 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:47:17,168 | server.py:125 | fit progress: (6, 2.302136182785034, {'accuracy': 0.1029, 'data_size': 10000}, 84.73628994001774)
INFO flwr 2024-04-06 15:47:17,169 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:47:17,169 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:47:24,977 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:47:32,658 | server.py:125 | fit progress: (7, 2.3020801544189453, {'accuracy': 0.1028, 'data_size': 10000}, 100.22558074101107)
INFO flwr 2024-04-06 15:47:32,658 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:47:32,658 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:47:40,478 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:47:49,150 | server.py:125 | fit progress: (8, 2.302025079727173, {'accuracy': 0.103, 'data_size': 10000}, 116.71789013501257)
INFO flwr 2024-04-06 15:47:49,151 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:47:49,151 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:47:57,282 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:48:06,236 | server.py:125 | fit progress: (9, 2.3019495010375977, {'accuracy': 0.1119, 'data_size': 10000}, 133.80345363600645)
INFO flwr 2024-04-06 15:48:06,236 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:48:06,236 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:48:14,080 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:48:24,875 | server.py:125 | fit progress: (10, 2.3018791675567627, {'accuracy': 0.1482, 'data_size': 10000}, 152.4426382399979)
INFO flwr 2024-04-06 15:48:24,875 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:48:24,875 | server.py:153 | FL finished in 152.44311064999783
INFO flwr 2024-04-06 15:48:24,875 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:48:24,876 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:48:24,876 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:48:24,876 | app.py:229 | app_fit: losses_centralized [(0, 2.3025283813476562), (1, 2.302459716796875), (2, 2.302405834197998), (3, 2.3023481369018555), (4, 2.30226993560791), (5, 2.3022170066833496), (6, 2.302136182785034), (7, 2.3020801544189453), (8, 2.302025079727173), (9, 2.3019495010375977), (10, 2.3018791675567627)]
INFO flwr 2024-04-06 15:48:24,876 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.104), (1, 0.0989), (2, 0.1133), (3, 0.1028), (4, 0.1028), (5, 0.1028), (6, 0.1029), (7, 0.1028), (8, 0.103), (9, 0.1119), (10, 0.1482)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1482
wandb:     loss 2.30188
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_154505-63c1sp0m
wandb: Find logs at: ./wandb/offline-run-20240406_154505-63c1sp0m/logs
INFO flwr 2024-04-06 15:48:28,449 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:55:34,478 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1186458)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1186458)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 15:55:40,063	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:55:40,451	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:55:40,791	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_39a125b523704a5e.zip' (9.70MiB) to Ray cluster...
2024-04-06 15:55:40,815	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_39a125b523704a5e.zip'.
INFO flwr 2024-04-06 15:55:51,603 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 141496204698.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64926944870.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 15:55:51,603 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:55:51,603 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:55:51,621 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:55:51,622 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:55:51,622 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:55:51,623 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 15:55:53,932 | server.py:94 | initial parameters (loss, other metrics): 2.3026959896087646, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-06 15:55:53,932 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:55:53,943 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1190885)[0m 2024-04-06 15:55:57.436723: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1190885)[0m 2024-04-06 15:55:57.534444: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1190885)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1190881)[0m 2024-04-06 15:55:59.794797: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1190876)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1190876)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1190873)[0m 2024-04-06 15:55:57.900403: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1190873)[0m 2024-04-06 15:55:57.995672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1190873)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1190873)[0m 2024-04-06 15:56:00.280648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 15:56:11,618 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:56:12,115 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:56:13,346 | server.py:125 | fit progress: (1, 2.3025665283203125, {'accuracy': 0.0898, 'data_size': 10000}, 19.402571754006203)
INFO flwr 2024-04-06 15:56:13,346 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:56:13,346 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:56:21,921 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:56:24,288 | server.py:125 | fit progress: (2, 2.302464246749878, {'accuracy': 0.0973, 'data_size': 10000}, 30.344567566993646)
INFO flwr 2024-04-06 15:56:24,288 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:56:24,288 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:56:31,715 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:56:35,235 | server.py:125 | fit progress: (3, 2.302358865737915, {'accuracy': 0.0897, 'data_size': 10000}, 41.29196442299872)
INFO flwr 2024-04-06 15:56:35,235 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:56:35,236 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:56:42,928 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:56:46,999 | server.py:125 | fit progress: (4, 2.302246570587158, {'accuracy': 0.0904, 'data_size': 10000}, 53.055398544005584)
INFO flwr 2024-04-06 15:56:46,999 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:56:46,999 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:56:54,741 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:56:59,831 | server.py:125 | fit progress: (5, 2.3020994663238525, {'accuracy': 0.1009, 'data_size': 10000}, 65.88794840799528)
INFO flwr 2024-04-06 15:56:59,831 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:56:59,832 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:57:07,872 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:57:13,607 | server.py:125 | fit progress: (6, 2.3019614219665527, {'accuracy': 0.09, 'data_size': 10000}, 79.66417547600577)
INFO flwr 2024-04-06 15:57:13,608 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:57:13,608 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:57:21,436 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:57:28,322 | server.py:125 | fit progress: (7, 2.301812171936035, {'accuracy': 0.0908, 'data_size': 10000}, 94.37935808001203)
INFO flwr 2024-04-06 15:57:28,323 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:57:28,323 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:57:35,706 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:57:43,385 | server.py:125 | fit progress: (8, 2.301650285720825, {'accuracy': 0.1487, 'data_size': 10000}, 109.44181220699102)
INFO flwr 2024-04-06 15:57:43,385 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:57:43,385 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:57:51,479 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:57:59,973 | server.py:125 | fit progress: (9, 2.3014473915100098, {'accuracy': 0.1136, 'data_size': 10000}, 126.02961591098574)
INFO flwr 2024-04-06 15:57:59,973 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:57:59,973 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:58:07,852 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:58:17,554 | server.py:125 | fit progress: (10, 2.3013112545013428, {'accuracy': 0.1187, 'data_size': 10000}, 143.6105640490132)
INFO flwr 2024-04-06 15:58:17,554 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:58:17,554 | server.py:153 | FL finished in 143.61093260199414
INFO flwr 2024-04-06 15:58:17,554 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:58:17,554 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:58:17,554 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:58:17,555 | app.py:229 | app_fit: losses_centralized [(0, 2.3026959896087646), (1, 2.3025665283203125), (2, 2.302464246749878), (3, 2.302358865737915), (4, 2.302246570587158), (5, 2.3020994663238525), (6, 2.3019614219665527), (7, 2.301812171936035), (8, 2.301650285720825), (9, 2.3014473915100098), (10, 2.3013112545013428)]
INFO flwr 2024-04-06 15:58:17,555 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.0898), (2, 0.0973), (3, 0.0897), (4, 0.0904), (5, 0.1009), (6, 0.09), (7, 0.0908), (8, 0.1487), (9, 0.1136), (10, 0.1187)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1187
wandb:     loss 2.30131
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_155534-ez4u6emm
wandb: Find logs at: ./wandb/offline-run-20240406_155534-ez4u6emm/logs
INFO flwr 2024-04-06 15:58:21,067 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:05:27,560 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1190873)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1190873)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 16:05:32,394	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:05:32,832	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:05:33,372	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d04e91fa1d922203.zip' (9.72MiB) to Ray cluster...
2024-04-06 16:05:33,394	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d04e91fa1d922203.zip'.
INFO flwr 2024-04-06 16:05:44,361 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 135861695079.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62512155033.0}
INFO flwr 2024-04-06 16:05:44,361 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:05:44,361 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:05:44,379 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:05:44,380 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:05:44,380 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:05:44,380 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 16:05:47,288 | server.py:94 | initial parameters (loss, other metrics): 2.30275559425354, {'accuracy': 0.1047, 'data_size': 10000}
INFO flwr 2024-04-06 16:05:47,288 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:05:47,289 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1198663)[0m 2024-04-06 16:05:50.988968: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1198660)[0m 2024-04-06 16:05:51.082061: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1198660)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1198655)[0m 2024-04-06 16:05:53.420024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1198657)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1198657)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1198657)[0m 2024-04-06 16:05:51.131871: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1198657)[0m 2024-04-06 16:05:51.226225: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1198657)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1198660)[0m 2024-04-06 16:05:53.693352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 16:06:07,475 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:06:07,992 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:06:09,022 | server.py:125 | fit progress: (1, 2.3025405406951904, {'accuracy': 0.0987, 'data_size': 10000}, 21.73336525002378)
INFO flwr 2024-04-06 16:06:09,022 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:06:09,023 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:06:17,909 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:06:20,683 | server.py:125 | fit progress: (2, 2.3023338317871094, {'accuracy': 0.0979, 'data_size': 10000}, 33.393905014003394)
INFO flwr 2024-04-06 16:06:20,683 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:06:20,683 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:06:28,650 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:06:32,139 | server.py:125 | fit progress: (3, 2.302168846130371, {'accuracy': 0.0974, 'data_size': 10000}, 44.85054875101196)
INFO flwr 2024-04-06 16:06:32,139 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:06:32,140 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:06:39,653 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:06:45,342 | server.py:125 | fit progress: (4, 2.3019866943359375, {'accuracy': 0.0983, 'data_size': 10000}, 58.05371661300887)
INFO flwr 2024-04-06 16:06:45,343 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:06:45,343 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:06:54,720 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:07:01,316 | server.py:125 | fit progress: (5, 2.301706314086914, {'accuracy': 0.1071, 'data_size': 10000}, 74.02764330501668)
INFO flwr 2024-04-06 16:07:01,317 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:07:01,317 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:07:08,879 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:07:14,978 | server.py:125 | fit progress: (6, 2.3014655113220215, {'accuracy': 0.1655, 'data_size': 10000}, 87.68890022000414)
INFO flwr 2024-04-06 16:07:14,978 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:07:14,978 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:07:22,577 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:07:30,728 | server.py:125 | fit progress: (7, 2.3011415004730225, {'accuracy': 0.1515, 'data_size': 10000}, 103.43923567701131)
INFO flwr 2024-04-06 16:07:30,728 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:07:30,728 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:07:38,476 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:07:47,291 | server.py:125 | fit progress: (8, 2.3002395629882812, {'accuracy': 0.0974, 'data_size': 10000}, 120.00200971701997)
INFO flwr 2024-04-06 16:07:47,291 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:07:47,291 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:07:54,832 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:08:05,668 | server.py:125 | fit progress: (9, 2.299565553665161, {'accuracy': 0.0976, 'data_size': 10000}, 138.37897414001054)
INFO flwr 2024-04-06 16:08:05,668 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:08:05,668 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:08:13,789 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:08:23,717 | server.py:125 | fit progress: (10, 2.298983573913574, {'accuracy': 0.1036, 'data_size': 10000}, 156.42789507601992)
INFO flwr 2024-04-06 16:08:23,717 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:08:23,717 | server.py:153 | FL finished in 156.4283601740026
INFO flwr 2024-04-06 16:08:23,717 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:08:23,717 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:08:23,717 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:08:23,717 | app.py:229 | app_fit: losses_centralized [(0, 2.30275559425354), (1, 2.3025405406951904), (2, 2.3023338317871094), (3, 2.302168846130371), (4, 2.3019866943359375), (5, 2.301706314086914), (6, 2.3014655113220215), (7, 2.3011415004730225), (8, 2.3002395629882812), (9, 2.299565553665161), (10, 2.298983573913574)]
INFO flwr 2024-04-06 16:08:23,718 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1047), (1, 0.0987), (2, 0.0979), (3, 0.0974), (4, 0.0983), (5, 0.1071), (6, 0.1655), (7, 0.1515), (8, 0.0974), (9, 0.0976), (10, 0.1036)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1036
wandb:     loss 2.29898
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_160527-vscro6io
wandb: Find logs at: ./wandb/offline-run-20240406_160527-vscro6io/logs
INFO flwr 2024-04-06 16:08:27,369 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:15:33,681 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1198666)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1198666)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 16:15:38,420	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:15:38,719	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:15:39,172	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_130b1f61d4d628f4.zip' (9.74MiB) to Ray cluster...
2024-04-06 16:15:39,199	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_130b1f61d4d628f4.zip'.
INFO flwr 2024-04-06 16:15:50,030 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 64775528448.0, 'node:10.20.240.18': 1.0, 'memory': 141142899712.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 16:15:50,030 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:15:50,030 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:15:50,048 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:15:50,049 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:15:50,049 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:15:50,050 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 16:15:52,501 | server.py:94 | initial parameters (loss, other metrics): 2.3028926849365234, {'accuracy': 0.0994, 'data_size': 10000}
INFO flwr 2024-04-06 16:15:52,502 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:15:52,502 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1203385)[0m 2024-04-06 16:16:01.304370: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1203385)[0m 2024-04-06 16:16:01.398395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1203385)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1203385)[0m 2024-04-06 16:16:03.373389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1203385)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1203385)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1203390)[0m 2024-04-06 16:16:01.845423: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1203390)[0m 2024-04-06 16:16:01.950172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1203390)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1203390)[0m 2024-04-06 16:16:03.757207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 16:16:15,592 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:16:16,108 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:16:17,378 | server.py:125 | fit progress: (1, 2.3026845455169678, {'accuracy': 0.0974, 'data_size': 10000}, 24.876356086984742)
INFO flwr 2024-04-06 16:16:17,379 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:16:17,379 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:16:26,060 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:16:28,434 | server.py:125 | fit progress: (2, 2.3023555278778076, {'accuracy': 0.1244, 'data_size': 10000}, 35.932568268006435)
INFO flwr 2024-04-06 16:16:28,435 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:16:28,435 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:16:36,262 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:16:39,684 | server.py:125 | fit progress: (3, 2.3020217418670654, {'accuracy': 0.1108, 'data_size': 10000}, 47.18195739699877)
INFO flwr 2024-04-06 16:16:39,684 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:16:39,684 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:16:47,603 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:16:52,091 | server.py:125 | fit progress: (4, 2.3016276359558105, {'accuracy': 0.1473, 'data_size': 10000}, 59.58880928499275)
INFO flwr 2024-04-06 16:16:52,091 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:16:52,091 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:16:59,629 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:17:05,053 | server.py:125 | fit progress: (5, 2.301276922225952, {'accuracy': 0.1119, 'data_size': 10000}, 72.55096402100753)
INFO flwr 2024-04-06 16:17:05,053 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:17:05,053 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:17:13,103 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:17:19,750 | server.py:125 | fit progress: (6, 2.300790548324585, {'accuracy': 0.2382, 'data_size': 10000}, 87.24786181599484)
INFO flwr 2024-04-06 16:17:19,750 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:17:19,750 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:17:27,690 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:17:35,106 | server.py:125 | fit progress: (7, 2.3003101348876953, {'accuracy': 0.1476, 'data_size': 10000}, 102.6039729930053)
INFO flwr 2024-04-06 16:17:35,106 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:17:35,106 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:17:42,929 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:17:51,801 | server.py:125 | fit progress: (8, 2.2994296550750732, {'accuracy': 0.1009, 'data_size': 10000}, 119.29938354500337)
INFO flwr 2024-04-06 16:17:51,802 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:17:51,802 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:17:59,782 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:18:09,888 | server.py:125 | fit progress: (9, 2.298388957977295, {'accuracy': 0.1821, 'data_size': 10000}, 137.38622671898338)
INFO flwr 2024-04-06 16:18:09,888 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:18:09,889 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:18:17,884 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:18:28,907 | server.py:125 | fit progress: (10, 2.2958548069000244, {'accuracy': 0.1872, 'data_size': 10000}, 156.40560159599409)
INFO flwr 2024-04-06 16:18:28,908 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:18:28,908 | server.py:153 | FL finished in 156.40599555999506
INFO flwr 2024-04-06 16:18:28,908 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:18:28,908 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:18:28,908 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:18:28,908 | app.py:229 | app_fit: losses_centralized [(0, 2.3028926849365234), (1, 2.3026845455169678), (2, 2.3023555278778076), (3, 2.3020217418670654), (4, 2.3016276359558105), (5, 2.301276922225952), (6, 2.300790548324585), (7, 2.3003101348876953), (8, 2.2994296550750732), (9, 2.298388957977295), (10, 2.2958548069000244)]
INFO flwr 2024-04-06 16:18:28,908 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0994), (1, 0.0974), (2, 0.1244), (3, 0.1108), (4, 0.1473), (5, 0.1119), (6, 0.2382), (7, 0.1476), (8, 0.1009), (9, 0.1821), (10, 0.1872)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1872
wandb:     loss 2.29585
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_161533-gw5ieea1
wandb: Find logs at: ./wandb/offline-run-20240406_161533-gw5ieea1/logs
INFO flwr 2024-04-06 16:18:32,493 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:25:38,067 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1203381)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1203381)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 16:25:43,358	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:25:43,725	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:25:44,113	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c4477ccde4c5b2ec.zip' (9.76MiB) to Ray cluster...
2024-04-06 16:25:44,140	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c4477ccde4c5b2ec.zip'.
INFO flwr 2024-04-06 16:25:54,936 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62463956582.0, 'memory': 135749232026.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 16:25:54,937 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:25:54,937 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:25:54,955 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:25:54,957 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:25:54,958 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:25:54,958 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 16:25:57,337 | server.py:94 | initial parameters (loss, other metrics): 2.302478313446045, {'accuracy': 0.0843, 'data_size': 10000}
INFO flwr 2024-04-06 16:25:57,337 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:25:57,338 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1210740)[0m 2024-04-06 16:26:00.349565: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1210740)[0m 2024-04-06 16:26:00.447086: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1210740)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1210740)[0m 2024-04-06 16:26:02.828430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1210740)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1210740)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1210745)[0m 2024-04-06 16:26:01.621587: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1210746)[0m 2024-04-06 16:26:01.664913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1210746)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1210745)[0m 2024-04-06 16:26:03.770602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 16:26:15,551 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:26:16,052 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:26:17,312 | server.py:125 | fit progress: (1, 2.302152633666992, {'accuracy': 0.0958, 'data_size': 10000}, 19.974689498980297)
INFO flwr 2024-04-06 16:26:17,312 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:26:17,313 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:26:25,821 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:26:28,165 | server.py:125 | fit progress: (2, 2.3018319606781006, {'accuracy': 0.0958, 'data_size': 10000}, 30.827356242982205)
INFO flwr 2024-04-06 16:26:28,165 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:26:28,165 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:26:36,168 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:26:39,608 | server.py:125 | fit progress: (3, 2.301344633102417, {'accuracy': 0.0958, 'data_size': 10000}, 42.270271374000004)
INFO flwr 2024-04-06 16:26:39,608 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:26:39,608 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:26:47,228 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:26:51,614 | server.py:125 | fit progress: (4, 2.300546646118164, {'accuracy': 0.1543, 'data_size': 10000}, 54.277058633975685)
INFO flwr 2024-04-06 16:26:51,615 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:26:51,615 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:26:59,803 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:27:04,811 | server.py:125 | fit progress: (5, 2.29939866065979, {'accuracy': 0.1814, 'data_size': 10000}, 67.47376500797691)
INFO flwr 2024-04-06 16:27:04,812 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:27:04,812 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:27:12,593 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:27:18,716 | server.py:125 | fit progress: (6, 2.2975356578826904, {'accuracy': 0.0976, 'data_size': 10000}, 81.37892470197403)
INFO flwr 2024-04-06 16:27:18,717 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:27:18,717 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:27:26,646 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:27:33,389 | server.py:125 | fit progress: (7, 2.2939138412475586, {'accuracy': 0.0974, 'data_size': 10000}, 96.05133109499002)
INFO flwr 2024-04-06 16:27:33,389 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:27:33,389 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:27:41,039 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:27:48,316 | server.py:125 | fit progress: (8, 2.291532516479492, {'accuracy': 0.1535, 'data_size': 10000}, 110.97897093297797)
INFO flwr 2024-04-06 16:27:48,317 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:27:48,317 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:27:56,373 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:28:04,786 | server.py:125 | fit progress: (9, 2.290949821472168, {'accuracy': 0.1626, 'data_size': 10000}, 127.44837655499578)
INFO flwr 2024-04-06 16:28:04,786 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:28:04,786 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:28:12,786 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:28:21,821 | server.py:125 | fit progress: (10, 2.2882447242736816, {'accuracy': 0.0958, 'data_size': 10000}, 144.48328574097832)
INFO flwr 2024-04-06 16:28:21,821 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:28:21,821 | server.py:153 | FL finished in 144.48367960899486
INFO flwr 2024-04-06 16:28:21,821 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:28:21,821 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:28:21,821 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:28:21,821 | app.py:229 | app_fit: losses_centralized [(0, 2.302478313446045), (1, 2.302152633666992), (2, 2.3018319606781006), (3, 2.301344633102417), (4, 2.300546646118164), (5, 2.29939866065979), (6, 2.2975356578826904), (7, 2.2939138412475586), (8, 2.291532516479492), (9, 2.290949821472168), (10, 2.2882447242736816)]
INFO flwr 2024-04-06 16:28:21,822 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0843), (1, 0.0958), (2, 0.0958), (3, 0.0958), (4, 0.1543), (5, 0.1814), (6, 0.0976), (7, 0.0974), (8, 0.1535), (9, 0.1626), (10, 0.0958)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0958
wandb:     loss 2.28824
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_162537-k486qrz0
wandb: Find logs at: ./wandb/offline-run-20240406_162537-k486qrz0/logs
INFO flwr 2024-04-06 16:28:25,416 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:35:31,905 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1210739)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1210739)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 16:35:36,843	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:35:37,395	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:35:37,866	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ee792a588ee56fc8.zip' (9.78MiB) to Ray cluster...
2024-04-06 16:35:37,902	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ee792a588ee56fc8.zip'.
INFO flwr 2024-04-06 16:35:48,898 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 135751789568.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62465052672.0}
INFO flwr 2024-04-06 16:35:48,898 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:35:48,898 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:35:48,914 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:35:48,916 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:35:48,917 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:35:48,917 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 16:35:52,156 | server.py:94 | initial parameters (loss, other metrics): 2.302368402481079, {'accuracy': 0.0881, 'data_size': 10000}
INFO flwr 2024-04-06 16:35:52,157 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:35:52,157 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1218420)[0m 2024-04-06 16:35:55.259314: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1218419)[0m 2024-04-06 16:35:55.329207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1218419)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1218420)[0m 2024-04-06 16:35:57.423838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1218420)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1218420)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1218421)[0m 2024-04-06 16:35:55.560417: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1218421)[0m 2024-04-06 16:35:55.652142: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1218421)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1218413)[0m 2024-04-06 16:35:58.234565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 16:36:10,878 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:36:11,417 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:36:12,469 | server.py:125 | fit progress: (1, 2.3013579845428467, {'accuracy': 0.0974, 'data_size': 10000}, 20.311490118998336)
INFO flwr 2024-04-06 16:36:12,469 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:36:12,469 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:36:21,654 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:36:24,409 | server.py:125 | fit progress: (2, 2.2990052700042725, {'accuracy': 0.0974, 'data_size': 10000}, 32.251540066004964)
INFO flwr 2024-04-06 16:36:24,409 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:36:24,409 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:36:32,952 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:36:36,676 | server.py:125 | fit progress: (3, 2.2932636737823486, {'accuracy': 0.0974, 'data_size': 10000}, 44.51943077199394)
INFO flwr 2024-04-06 16:36:36,677 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:36:36,677 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:36:44,393 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:36:49,361 | server.py:125 | fit progress: (4, 2.289374589920044, {'accuracy': 0.0974, 'data_size': 10000}, 57.20388251700206)
INFO flwr 2024-04-06 16:36:49,361 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:36:49,362 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:36:57,109 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:37:02,899 | server.py:125 | fit progress: (5, 2.287363290786743, {'accuracy': 0.2171, 'data_size': 10000}, 70.74196824899991)
INFO flwr 2024-04-06 16:37:02,899 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:37:02,899 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:37:10,666 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:37:16,929 | server.py:125 | fit progress: (6, 2.2777469158172607, {'accuracy': 0.1297, 'data_size': 10000}, 84.77190543399774)
INFO flwr 2024-04-06 16:37:16,929 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:37:16,929 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:37:24,814 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:37:31,988 | server.py:125 | fit progress: (7, 2.2727108001708984, {'accuracy': 0.205, 'data_size': 10000}, 99.83120082999812)
INFO flwr 2024-04-06 16:37:31,989 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:37:31,989 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:37:40,345 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:37:49,290 | server.py:125 | fit progress: (8, 2.259281635284424, {'accuracy': 0.1819, 'data_size': 10000}, 117.13333786401199)
INFO flwr 2024-04-06 16:37:49,291 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:37:49,291 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:37:56,999 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:38:09,692 | server.py:125 | fit progress: (9, 2.203787326812744, {'accuracy': 0.2984, 'data_size': 10000}, 137.53468891599914)
INFO flwr 2024-04-06 16:38:09,692 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:38:09,692 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:38:17,169 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:38:27,058 | server.py:125 | fit progress: (10, 2.1775448322296143, {'accuracy': 0.2532, 'data_size': 10000}, 154.90077597400523)
INFO flwr 2024-04-06 16:38:27,058 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:38:27,058 | server.py:153 | FL finished in 154.90118557299138
INFO flwr 2024-04-06 16:38:27,058 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:38:27,058 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:38:27,058 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:38:27,059 | app.py:229 | app_fit: losses_centralized [(0, 2.302368402481079), (1, 2.3013579845428467), (2, 2.2990052700042725), (3, 2.2932636737823486), (4, 2.289374589920044), (5, 2.287363290786743), (6, 2.2777469158172607), (7, 2.2727108001708984), (8, 2.259281635284424), (9, 2.203787326812744), (10, 2.1775448322296143)]
INFO flwr 2024-04-06 16:38:27,059 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0881), (1, 0.0974), (2, 0.0974), (3, 0.0974), (4, 0.0974), (5, 0.2171), (6, 0.1297), (7, 0.205), (8, 0.1819), (9, 0.2984), (10, 0.2532)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2532
wandb:     loss 2.17754
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_163531-rolx0vs7
wandb: Find logs at: ./wandb/offline-run-20240406_163531-rolx0vs7/logs
INFO flwr 2024-04-06 16:38:30,677 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:45:40,810 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1218422)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1218422)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 16:45:45,427	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:45:45,876	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:45:46,300	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d4c398690fc0c35d.zip' (9.80MiB) to Ray cluster...
2024-04-06 16:45:46,326	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d4c398690fc0c35d.zip'.
INFO flwr 2024-04-06 16:45:56,992 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 140938058343.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 64687739289.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 16:45:56,993 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:45:56,993 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:45:57,016 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:45:57,017 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:45:57,017 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:45:57,017 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 16:45:59,273 | server.py:94 | initial parameters (loss, other metrics): 2.3027284145355225, {'accuracy': 0.1311, 'data_size': 10000}
INFO flwr 2024-04-06 16:45:59,276 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:45:59,277 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1222906)[0m 2024-04-06 16:46:02.957252: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1222906)[0m 2024-04-06 16:46:03.086755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1222906)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1222915)[0m 2024-04-06 16:46:05.019350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1222912)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1222912)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1222911)[0m 2024-04-06 16:46:03.488879: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1222911)[0m 2024-04-06 16:46:03.582513: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1222911)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1222911)[0m 2024-04-06 16:46:05.609636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 16:46:17,677 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:46:18,226 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:46:19,561 | server.py:125 | fit progress: (1, 2.302722215652466, {'accuracy': 0.1291, 'data_size': 10000}, 20.28495717799524)
INFO flwr 2024-04-06 16:46:19,562 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:46:19,562 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:46:28,699 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:46:31,089 | server.py:125 | fit progress: (2, 2.3027150630950928, {'accuracy': 0.132, 'data_size': 10000}, 31.81290311299381)
INFO flwr 2024-04-06 16:46:31,090 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:46:31,090 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:46:39,582 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:46:43,452 | server.py:125 | fit progress: (3, 2.3027093410491943, {'accuracy': 0.128, 'data_size': 10000}, 44.175644321978325)
INFO flwr 2024-04-06 16:46:43,452 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:46:43,452 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:46:52,068 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:46:56,119 | server.py:125 | fit progress: (4, 2.3027048110961914, {'accuracy': 0.1284, 'data_size': 10000}, 56.84225160398637)
INFO flwr 2024-04-06 16:46:56,119 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:46:56,119 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:47:04,498 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:47:09,434 | server.py:125 | fit progress: (5, 2.302699565887451, {'accuracy': 0.1264, 'data_size': 10000}, 70.15726319598616)
INFO flwr 2024-04-06 16:47:09,434 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:47:09,434 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:47:17,678 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:47:23,729 | server.py:125 | fit progress: (6, 2.3026936054229736, {'accuracy': 0.1251, 'data_size': 10000}, 84.4528946490027)
INFO flwr 2024-04-06 16:47:23,729 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:47:23,730 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:47:32,487 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:47:39,526 | server.py:125 | fit progress: (7, 2.3026862144470215, {'accuracy': 0.1252, 'data_size': 10000}, 100.24989161698613)
INFO flwr 2024-04-06 16:47:39,526 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:47:39,527 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:47:47,674 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:47:55,126 | server.py:125 | fit progress: (8, 2.3026788234710693, {'accuracy': 0.1256, 'data_size': 10000}, 115.84925164000015)
INFO flwr 2024-04-06 16:47:55,126 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:47:55,126 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:48:03,434 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:48:13,356 | server.py:125 | fit progress: (9, 2.3026716709136963, {'accuracy': 0.1236, 'data_size': 10000}, 134.07963269099128)
INFO flwr 2024-04-06 16:48:13,356 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:48:13,356 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:48:21,757 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:48:31,487 | server.py:125 | fit progress: (10, 2.302664041519165, {'accuracy': 0.1231, 'data_size': 10000}, 152.21115474199178)
INFO flwr 2024-04-06 16:48:31,488 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:48:31,488 | server.py:153 | FL finished in 152.21156773000257
INFO flwr 2024-04-06 16:48:31,488 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:48:31,488 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:48:31,488 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:48:31,488 | app.py:229 | app_fit: losses_centralized [(0, 2.3027284145355225), (1, 2.302722215652466), (2, 2.3027150630950928), (3, 2.3027093410491943), (4, 2.3027048110961914), (5, 2.302699565887451), (6, 2.3026936054229736), (7, 2.3026862144470215), (8, 2.3026788234710693), (9, 2.3026716709136963), (10, 2.302664041519165)]
INFO flwr 2024-04-06 16:48:31,489 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1311), (1, 0.1291), (2, 0.132), (3, 0.128), (4, 0.1284), (5, 0.1264), (6, 0.1251), (7, 0.1252), (8, 0.1256), (9, 0.1236), (10, 0.1231)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1231
wandb:     loss 2.30266
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_164540-dof34asu
wandb: Find logs at: ./wandb/offline-run-20240406_164540-dof34asu/logs
INFO flwr 2024-04-06 16:48:35,004 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:55:42,030 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1222906)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1222906)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 16:55:47,033	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:55:47,401	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:55:47,846	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_513382be8a36a1c0.zip' (9.82MiB) to Ray cluster...
2024-04-06 16:55:47,882	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_513382be8a36a1c0.zip'.
INFO flwr 2024-04-06 16:55:58,826 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 127802317005.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 59058135859.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 16:55:58,826 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:55:58,826 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:55:58,846 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:55:59,350 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:55:59,351 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:55:59,352 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 16:56:02,881 | server.py:94 | initial parameters (loss, other metrics): 2.3026068210601807, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-06 16:56:02,882 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:56:02,882 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1230397)[0m 2024-04-06 16:56:05.653809: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1230397)[0m 2024-04-06 16:56:05.761318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1230397)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1230392)[0m 2024-04-06 16:56:10.103074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1230397)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1230397)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1230396)[0m 2024-04-06 16:56:05.884326: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1230396)[0m 2024-04-06 16:56:05.991136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1230396)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1230400)[0m 2024-04-06 16:56:10.312364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 16:56:21,841 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:56:22,384 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:56:23,702 | server.py:125 | fit progress: (1, 2.3022732734680176, {'accuracy': 0.1026, 'data_size': 10000}, 20.819812535017263)
INFO flwr 2024-04-06 16:56:23,702 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:56:23,702 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:56:32,906 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:56:35,362 | server.py:125 | fit progress: (2, 2.3018136024475098, {'accuracy': 0.1209, 'data_size': 10000}, 32.47986420302186)
INFO flwr 2024-04-06 16:56:35,362 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:56:35,362 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:56:43,681 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:56:47,336 | server.py:125 | fit progress: (3, 2.3010385036468506, {'accuracy': 0.101, 'data_size': 10000}, 44.454209663002985)
INFO flwr 2024-04-06 16:56:47,336 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:56:47,336 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:56:55,715 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:57:00,052 | server.py:125 | fit progress: (4, 2.299759864807129, {'accuracy': 0.101, 'data_size': 10000}, 57.17017548100557)
INFO flwr 2024-04-06 16:57:00,052 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:57:00,052 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:57:08,065 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:57:13,846 | server.py:125 | fit progress: (5, 2.2979536056518555, {'accuracy': 0.1132, 'data_size': 10000}, 70.96395181899425)
INFO flwr 2024-04-06 16:57:13,846 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:57:13,846 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:57:22,114 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:57:28,488 | server.py:125 | fit progress: (6, 2.293855905532837, {'accuracy': 0.1009, 'data_size': 10000}, 85.60669150800095)
INFO flwr 2024-04-06 16:57:28,489 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:57:28,489 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:57:37,147 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:57:45,211 | server.py:125 | fit progress: (7, 2.283513069152832, {'accuracy': 0.1293, 'data_size': 10000}, 102.32956718400237)
INFO flwr 2024-04-06 16:57:45,212 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:57:45,212 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:57:53,666 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:58:02,056 | server.py:125 | fit progress: (8, 2.2701210975646973, {'accuracy': 0.1037, 'data_size': 10000}, 119.17422392201843)
INFO flwr 2024-04-06 16:58:02,056 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:58:02,057 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:58:10,607 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:58:20,370 | server.py:125 | fit progress: (9, 2.2521603107452393, {'accuracy': 0.2619, 'data_size': 10000}, 137.4882775139995)
INFO flwr 2024-04-06 16:58:20,370 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:58:20,371 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:58:28,716 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:58:39,619 | server.py:125 | fit progress: (10, 2.2105889320373535, {'accuracy': 0.419, 'data_size': 10000}, 156.73676277301274)
INFO flwr 2024-04-06 16:58:39,619 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:58:39,619 | server.py:153 | FL finished in 156.73717449599644
INFO flwr 2024-04-06 16:58:39,619 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:58:39,619 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:58:39,619 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:58:39,619 | app.py:229 | app_fit: losses_centralized [(0, 2.3026068210601807), (1, 2.3022732734680176), (2, 2.3018136024475098), (3, 2.3010385036468506), (4, 2.299759864807129), (5, 2.2979536056518555), (6, 2.293855905532837), (7, 2.283513069152832), (8, 2.2701210975646973), (9, 2.2521603107452393), (10, 2.2105889320373535)]
INFO flwr 2024-04-06 16:58:39,620 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.1026), (2, 0.1209), (3, 0.101), (4, 0.101), (5, 0.1132), (6, 0.1009), (7, 0.1293), (8, 0.1037), (9, 0.2619), (10, 0.419)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.419
wandb:     loss 2.21059
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_165541-wvn7jiym
wandb: Find logs at: ./wandb/offline-run-20240406_165541-wvn7jiym/logs
INFO flwr 2024-04-06 16:58:43,193 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:05:51,309 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1230387)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1230387)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 17:05:56,244	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:05:56,490	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:05:56,912	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_56c75339e90c0b3b.zip' (9.84MiB) to Ray cluster...
2024-04-06 17:05:56,934	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_56c75339e90c0b3b.zip'.
INFO flwr 2024-04-06 17:06:07,834 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 63126611558.0, 'node:10.20.240.18': 1.0, 'memory': 137295426970.0}
INFO flwr 2024-04-06 17:06:07,835 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:06:07,835 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:06:07,855 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:06:07,855 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:06:07,856 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:06:07,856 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 17:06:10,793 | server.py:94 | initial parameters (loss, other metrics): 2.30249285697937, {'accuracy': 0.1387, 'data_size': 10000}
INFO flwr 2024-04-06 17:06:10,793 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:06:10,794 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1238034)[0m 2024-04-06 17:06:13.756718: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1238034)[0m 2024-04-06 17:06:13.851758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1238034)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1238027)[0m 2024-04-06 17:06:15.913330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1238030)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1238030)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1238032)[0m 2024-04-06 17:06:14.482015: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1238032)[0m 2024-04-06 17:06:14.572076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1238032)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1238032)[0m 2024-04-06 17:06:16.750054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 17:06:28,615 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:06:29,129 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:06:30,444 | server.py:125 | fit progress: (1, 2.300230026245117, {'accuracy': 0.098, 'data_size': 10000}, 19.650204842007952)
INFO flwr 2024-04-06 17:06:30,444 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:06:30,444 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:06:39,365 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:06:41,774 | server.py:125 | fit progress: (2, 2.289074182510376, {'accuracy': 0.098, 'data_size': 10000}, 30.9804434000107)
INFO flwr 2024-04-06 17:06:41,774 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:06:41,774 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:06:49,935 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:06:53,537 | server.py:125 | fit progress: (3, 2.272413969039917, {'accuracy': 0.1866, 'data_size': 10000}, 42.74331350601278)
INFO flwr 2024-04-06 17:06:53,537 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:06:53,537 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:07:01,888 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:07:06,475 | server.py:125 | fit progress: (4, 2.227752208709717, {'accuracy': 0.2884, 'data_size': 10000}, 55.6811023390037)
INFO flwr 2024-04-06 17:07:06,475 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:07:06,475 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:07:14,902 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:07:20,432 | server.py:125 | fit progress: (5, 2.1270415782928467, {'accuracy': 0.3942, 'data_size': 10000}, 69.638804969989)
INFO flwr 2024-04-06 17:07:20,433 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:07:20,433 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:07:28,890 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:07:36,956 | server.py:125 | fit progress: (6, 2.0185627937316895, {'accuracy': 0.4534, 'data_size': 10000}, 86.16231377000804)
INFO flwr 2024-04-06 17:07:36,956 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:07:36,956 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:07:45,565 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:08:00,248 | server.py:125 | fit progress: (7, 1.9100890159606934, {'accuracy': 0.5873, 'data_size': 10000}, 109.45455456199124)
INFO flwr 2024-04-06 17:08:00,249 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:08:00,249 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:08:09,138 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:08:22,161 | server.py:125 | fit progress: (8, 1.857224941253662, {'accuracy': 0.6298, 'data_size': 10000}, 131.36710040501202)
INFO flwr 2024-04-06 17:08:22,161 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:08:22,161 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:08:31,274 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 17:08:50,725 | server.py:125 | fit progress: (9, 1.7969218492507935, {'accuracy': 0.6747, 'data_size': 10000}, 159.931531987997)
INFO flwr 2024-04-06 17:08:50,726 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 17:08:50,726 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:09:00,593 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 17:09:12,934 | server.py:125 | fit progress: (10, 1.7727980613708496, {'accuracy': 0.7117, 'data_size': 10000}, 182.14073436000035)
INFO flwr 2024-04-06 17:09:12,935 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 17:09:12,935 | server.py:153 | FL finished in 182.1411356930039
INFO flwr 2024-04-06 17:09:12,935 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 17:09:12,935 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 17:09:12,935 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 17:09:12,935 | app.py:229 | app_fit: losses_centralized [(0, 2.30249285697937), (1, 2.300230026245117), (2, 2.289074182510376), (3, 2.272413969039917), (4, 2.227752208709717), (5, 2.1270415782928467), (6, 2.0185627937316895), (7, 1.9100890159606934), (8, 1.857224941253662), (9, 1.7969218492507935), (10, 1.7727980613708496)]
INFO flwr 2024-04-06 17:09:12,935 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1387), (1, 0.098), (2, 0.098), (3, 0.1866), (4, 0.2884), (5, 0.3942), (6, 0.4534), (7, 0.5873), (8, 0.6298), (9, 0.6747), (10, 0.7117)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7117
wandb:     loss 1.7728
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_170551-6ofymp91
wandb: Find logs at: ./wandb/offline-run-20240406_170551-6ofymp91/logs
INFO flwr 2024-04-06 17:09:16,927 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:16:22,577 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1238021)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1238021)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 17:16:31,786	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:16:47,246	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:16:48,332	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_430b4e0712089dd2.zip' (9.86MiB) to Ray cluster...
2024-04-06 17:16:48,369	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_430b4e0712089dd2.zip'.
INFO flwr 2024-04-06 17:17:08,139 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 137688072807.0, 'CPU': 64.0, 'object_store_memory': 63294888345.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 17:17:08,140 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:17:08,140 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:17:08,164 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:17:08,165 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:17:08,165 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:17:08,165 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 17:17:10,384 | server.py:94 | initial parameters (loss, other metrics): 2.302640676498413, {'accuracy': 0.1087, 'data_size': 10000}
INFO flwr 2024-04-06 17:17:10,387 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:17:10,388 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1242801)[0m 2024-04-06 17:17:14.511982: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1242802)[0m 2024-04-06 17:17:14.651637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1242802)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1242802)[0m 2024-04-06 17:17:16.808090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1242805)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1242805)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1242804)[0m 2024-04-06 17:17:14.623065: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1242806)[0m 2024-04-06 17:17:14.688237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1242806)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1242804)[0m 2024-04-06 17:17:16.830510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 17:17:30,313 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:17:30,807 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:17:32,049 | server.py:125 | fit progress: (1, 2.301285982131958, {'accuracy': 0.2036, 'data_size': 10000}, 21.661278607003624)
INFO flwr 2024-04-06 17:17:32,050 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:17:32,050 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:17:41,057 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:17:43,654 | server.py:125 | fit progress: (2, 2.2918848991394043, {'accuracy': 0.1009, 'data_size': 10000}, 33.266370974015445)
INFO flwr 2024-04-06 17:17:43,655 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:17:43,655 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:17:52,089 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:17:55,263 | server.py:125 | fit progress: (3, 2.267956018447876, {'accuracy': 0.2106, 'data_size': 10000}, 44.8745828710089)
INFO flwr 2024-04-06 17:17:55,263 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:17:55,263 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:18:03,757 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:18:08,059 | server.py:125 | fit progress: (4, 2.1881229877471924, {'accuracy': 0.3716, 'data_size': 10000}, 57.67149129501195)
INFO flwr 2024-04-06 17:18:08,060 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:18:08,060 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:18:16,191 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:18:21,245 | server.py:125 | fit progress: (5, 2.0245614051818848, {'accuracy': 0.4365, 'data_size': 10000}, 70.85750354500487)
INFO flwr 2024-04-06 17:18:21,246 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:18:21,246 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:18:29,652 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:18:35,594 | server.py:125 | fit progress: (6, 1.8784922361373901, {'accuracy': 0.6071, 'data_size': 10000}, 85.20626766199712)
INFO flwr 2024-04-06 17:18:35,595 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:18:35,595 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:18:43,598 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:18:50,658 | server.py:125 | fit progress: (7, 1.822918176651001, {'accuracy': 0.6394, 'data_size': 10000}, 100.26989334801328)
INFO flwr 2024-04-06 17:18:50,658 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:18:50,658 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:18:58,649 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:19:06,364 | server.py:125 | fit progress: (8, 1.7118736505508423, {'accuracy': 0.7683, 'data_size': 10000}, 115.9761996140005)
INFO flwr 2024-04-06 17:19:06,364 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:19:06,365 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:19:14,572 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 17:19:23,125 | server.py:125 | fit progress: (9, 1.6727243661880493, {'accuracy': 0.8079, 'data_size': 10000}, 132.73733339400496)
INFO flwr 2024-04-06 17:19:23,126 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 17:19:23,126 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:19:31,356 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 17:19:40,517 | server.py:125 | fit progress: (10, 1.6614763736724854, {'accuracy': 0.8125, 'data_size': 10000}, 150.12918059600634)
INFO flwr 2024-04-06 17:19:40,518 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 17:19:40,518 | server.py:153 | FL finished in 150.12968814899796
INFO flwr 2024-04-06 17:19:40,518 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 17:19:40,518 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 17:19:40,518 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 17:19:40,518 | app.py:229 | app_fit: losses_centralized [(0, 2.302640676498413), (1, 2.301285982131958), (2, 2.2918848991394043), (3, 2.267956018447876), (4, 2.1881229877471924), (5, 2.0245614051818848), (6, 1.8784922361373901), (7, 1.822918176651001), (8, 1.7118736505508423), (9, 1.6727243661880493), (10, 1.6614763736724854)]
INFO flwr 2024-04-06 17:19:40,518 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1087), (1, 0.2036), (2, 0.1009), (3, 0.2106), (4, 0.3716), (5, 0.4365), (6, 0.6071), (7, 0.6394), (8, 0.7683), (9, 0.8079), (10, 0.8125)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8125
wandb:     loss 1.66148
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_171621-e81wlv3d
wandb: Find logs at: ./wandb/offline-run-20240406_171621-e81wlv3d/logs
INFO flwr 2024-04-06 17:19:44,107 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:26:51,858 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1242804)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1242804)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 17:26:57,497	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:26:57,872	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:26:58,363	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_44ac45de2dca4371.zip' (9.88MiB) to Ray cluster...
2024-04-06 17:26:58,402	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_44ac45de2dca4371.zip'.
INFO flwr 2024-04-06 17:27:09,680 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 59884168396.0, 'node:10.20.240.18': 1.0, 'memory': 129729726260.0}
INFO flwr 2024-04-06 17:27:09,681 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:27:09,681 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:27:09,698 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:27:09,699 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:27:09,699 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:27:09,699 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 17:27:12,659 | server.py:94 | initial parameters (loss, other metrics): 2.3026115894317627, {'accuracy': 0.1107, 'data_size': 10000}
INFO flwr 2024-04-06 17:27:12,659 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:27:12,659 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1250258)[0m 2024-04-06 17:27:16.106343: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1250258)[0m 2024-04-06 17:27:16.206545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1250258)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1250258)[0m 2024-04-06 17:27:17.996540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1250258)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1250258)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1250264)[0m 2024-04-06 17:27:16.523225: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1250264)[0m 2024-04-06 17:27:16.627078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1250264)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1250267)[0m 2024-04-06 17:27:19.479489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 17:27:31,426 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:27:31,937 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:27:33,202 | server.py:125 | fit progress: (1, 2.2984397411346436, {'accuracy': 0.1247, 'data_size': 10000}, 20.543075241992483)
INFO flwr 2024-04-06 17:27:33,203 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:27:33,203 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:27:42,081 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:27:44,867 | server.py:125 | fit progress: (2, 2.264542579650879, {'accuracy': 0.187, 'data_size': 10000}, 32.20813714599353)
INFO flwr 2024-04-06 17:27:44,868 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:27:44,868 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:27:53,386 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:27:56,943 | server.py:125 | fit progress: (3, 2.1634957790374756, {'accuracy': 0.3172, 'data_size': 10000}, 44.28394598400337)
INFO flwr 2024-04-06 17:27:56,943 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:27:56,944 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:28:05,267 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:28:10,286 | server.py:125 | fit progress: (4, 1.9505287408828735, {'accuracy': 0.5506, 'data_size': 10000}, 57.62663060199702)
INFO flwr 2024-04-06 17:28:10,286 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:28:10,286 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:28:18,903 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:28:27,029 | server.py:125 | fit progress: (5, 1.8258730173110962, {'accuracy': 0.6471, 'data_size': 10000}, 74.36963366501732)
INFO flwr 2024-04-06 17:28:27,029 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:28:27,029 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:28:35,440 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:28:41,396 | server.py:125 | fit progress: (6, 1.759101152420044, {'accuracy': 0.7079, 'data_size': 10000}, 88.73700852901675)
INFO flwr 2024-04-06 17:28:41,396 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:28:41,397 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:28:49,426 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:28:57,740 | server.py:125 | fit progress: (7, 1.6813162565231323, {'accuracy': 0.8084, 'data_size': 10000}, 105.08057737900526)
INFO flwr 2024-04-06 17:28:57,740 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:28:57,740 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:29:06,678 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:29:14,968 | server.py:125 | fit progress: (8, 1.634039044380188, {'accuracy': 0.8482, 'data_size': 10000}, 122.30879309700686)
INFO flwr 2024-04-06 17:29:14,968 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:29:14,969 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:29:23,251 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 17:29:33,331 | server.py:125 | fit progress: (9, 1.614438772201538, {'accuracy': 0.8586, 'data_size': 10000}, 140.6722395879915)
INFO flwr 2024-04-06 17:29:33,332 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 17:29:33,332 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:29:41,137 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 17:29:56,596 | server.py:125 | fit progress: (10, 1.6040823459625244, {'accuracy': 0.8682, 'data_size': 10000}, 163.93724981101695)
INFO flwr 2024-04-06 17:29:56,597 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 17:29:56,597 | server.py:153 | FL finished in 163.9377470239997
INFO flwr 2024-04-06 17:29:56,597 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 17:29:56,597 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 17:29:56,597 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 17:29:56,597 | app.py:229 | app_fit: losses_centralized [(0, 2.3026115894317627), (1, 2.2984397411346436), (2, 2.264542579650879), (3, 2.1634957790374756), (4, 1.9505287408828735), (5, 1.8258730173110962), (6, 1.759101152420044), (7, 1.6813162565231323), (8, 1.634039044380188), (9, 1.614438772201538), (10, 1.6040823459625244)]
INFO flwr 2024-04-06 17:29:56,598 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1107), (1, 0.1247), (2, 0.187), (3, 0.3172), (4, 0.5506), (5, 0.6471), (6, 0.7079), (7, 0.8084), (8, 0.8482), (9, 0.8586), (10, 0.8682)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8682
wandb:     loss 1.60408
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_172651-jv9v9nwi
wandb: Find logs at: ./wandb/offline-run-20240406_172651-jv9v9nwi/logs
INFO flwr 2024-04-06 17:30:00,196 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:37:08,709 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1250261)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1250261)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 17:37:13,451	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:37:13,970	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:37:14,507	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1d0cea6c42d8f3dd.zip' (9.90MiB) to Ray cluster...
2024-04-06 17:37:14,547	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1d0cea6c42d8f3dd.zip'.
INFO flwr 2024-04-06 17:37:27,132 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 137953045095.0, 'node:__internal_head__': 1.0, 'object_store_memory': 63408447897.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 17:37:27,132 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:37:27,133 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:37:27,153 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:37:27,154 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:37:27,154 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:37:27,155 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 17:37:29,408 | server.py:94 | initial parameters (loss, other metrics): 2.3025453090667725, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-06 17:37:29,409 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:37:29,409 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1254927)[0m 2024-04-06 17:37:33.569536: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1254935)[0m 2024-04-06 17:37:33.708787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1254935)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1254926)[0m 2024-04-06 17:37:36.232773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1254926)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1254926)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1254938)[0m 2024-04-06 17:37:33.714780: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1254938)[0m 2024-04-06 17:37:33.815048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1254938)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1254927)[0m 2024-04-06 17:37:36.471885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 17:37:53,670 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:37:54,182 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:37:55,444 | server.py:125 | fit progress: (1, 2.296262502670288, {'accuracy': 0.0998, 'data_size': 10000}, 26.035582898010034)
INFO flwr 2024-04-06 17:37:55,445 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:37:55,445 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:38:04,156 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:38:06,583 | server.py:125 | fit progress: (2, 2.231451988220215, {'accuracy': 0.239, 'data_size': 10000}, 37.173801966011524)
INFO flwr 2024-04-06 17:38:06,583 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:38:06,583 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:38:14,570 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:38:18,326 | server.py:125 | fit progress: (3, 2.043771743774414, {'accuracy': 0.497, 'data_size': 10000}, 48.91741717798868)
INFO flwr 2024-04-06 17:38:18,327 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:38:18,327 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:38:26,210 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:38:30,400 | server.py:125 | fit progress: (4, 1.870161533355713, {'accuracy': 0.6033, 'data_size': 10000}, 60.991300216992386)
INFO flwr 2024-04-06 17:38:30,400 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:38:30,401 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:38:39,057 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:38:44,349 | server.py:125 | fit progress: (5, 1.7551143169403076, {'accuracy': 0.7254, 'data_size': 10000}, 74.9404574619839)
INFO flwr 2024-04-06 17:38:44,350 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:38:44,350 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:38:52,692 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:38:58,882 | server.py:125 | fit progress: (6, 1.6477689743041992, {'accuracy': 0.8338, 'data_size': 10000}, 89.47287259699078)
INFO flwr 2024-04-06 17:38:58,882 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:38:58,882 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:39:07,194 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:39:14,347 | server.py:125 | fit progress: (7, 1.6092534065246582, {'accuracy': 0.8656, 'data_size': 10000}, 104.93764664899209)
INFO flwr 2024-04-06 17:39:14,347 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:39:14,347 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:39:22,418 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:39:30,592 | server.py:125 | fit progress: (8, 1.7414909601211548, {'accuracy': 0.7299, 'data_size': 10000}, 121.18329206900671)
INFO flwr 2024-04-06 17:39:30,593 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:39:30,593 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:39:39,510 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 17:39:48,956 | server.py:125 | fit progress: (9, 1.5836352109909058, {'accuracy': 0.8857, 'data_size': 10000}, 139.54694269498577)
INFO flwr 2024-04-06 17:39:48,956 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 17:39:48,956 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:39:58,691 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 17:40:08,471 | server.py:125 | fit progress: (10, 1.6228370666503906, {'accuracy': 0.844, 'data_size': 10000}, 159.06228777198703)
INFO flwr 2024-04-06 17:40:08,471 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 17:40:08,472 | server.py:153 | FL finished in 159.06270080900867
INFO flwr 2024-04-06 17:40:08,472 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 17:40:08,472 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 17:40:08,472 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 17:40:08,472 | app.py:229 | app_fit: losses_centralized [(0, 2.3025453090667725), (1, 2.296262502670288), (2, 2.231451988220215), (3, 2.043771743774414), (4, 1.870161533355713), (5, 1.7551143169403076), (6, 1.6477689743041992), (7, 1.6092534065246582), (8, 1.7414909601211548), (9, 1.5836352109909058), (10, 1.6228370666503906)]
INFO flwr 2024-04-06 17:40:08,472 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.0998), (2, 0.239), (3, 0.497), (4, 0.6033), (5, 0.7254), (6, 0.8338), (7, 0.8656), (8, 0.7299), (9, 0.8857), (10, 0.844)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.844
wandb:     loss 1.62284
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_173708-uo1ygt9v
wandb: Find logs at: ./wandb/offline-run-20240406_173708-uo1ygt9v/logs
INFO flwr 2024-04-06 17:40:12,014 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:47:20,257 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1254931)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1254931)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 17:47:25,937	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:47:26,300	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:47:26,650	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_32d035e26d8457c3.zip' (9.92MiB) to Ray cluster...
2024-04-06 17:47:26,683	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_32d035e26d8457c3.zip'.
INFO flwr 2024-04-06 17:47:37,945 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 131989621760.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 60852695040.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 17:47:37,946 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:47:37,946 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:47:37,963 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:47:37,966 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:47:37,966 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:47:37,967 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 17:47:40,510 | server.py:94 | initial parameters (loss, other metrics): 2.3026280403137207, {'accuracy': 0.1043, 'data_size': 10000}
INFO flwr 2024-04-06 17:47:40,511 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:47:40,512 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1262519)[0m 2024-04-06 17:47:44.148427: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1262520)[0m 2024-04-06 17:47:44.311356: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1262520)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1262520)[0m 2024-04-06 17:47:46.262422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1262520)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1262520)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1262522)[0m 2024-04-06 17:47:44.641157: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1262522)[0m 2024-04-06 17:47:44.742590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1262522)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1262514)[0m 2024-04-06 17:47:46.876152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 17:47:58,690 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:47:59,230 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:48:00,681 | server.py:125 | fit progress: (1, 2.292447090148926, {'accuracy': 0.1217, 'data_size': 10000}, 20.16913674600073)
INFO flwr 2024-04-06 17:48:00,681 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:48:00,681 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:48:10,413 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:48:12,885 | server.py:125 | fit progress: (2, 2.1489920616149902, {'accuracy': 0.2916, 'data_size': 10000}, 32.373303513013525)
INFO flwr 2024-04-06 17:48:12,885 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:48:12,885 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:48:21,831 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:48:25,978 | server.py:125 | fit progress: (3, 2.003685235977173, {'accuracy': 0.4631, 'data_size': 10000}, 45.46623641598853)
INFO flwr 2024-04-06 17:48:25,978 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:48:25,978 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:48:35,058 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:48:39,263 | server.py:125 | fit progress: (4, 1.882967233657837, {'accuracy': 0.5942, 'data_size': 10000}, 58.75120900501497)
INFO flwr 2024-04-06 17:48:39,263 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:48:39,263 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:48:48,432 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:48:53,460 | server.py:125 | fit progress: (5, 1.7780749797821045, {'accuracy': 0.694, 'data_size': 10000}, 72.94797985701007)
INFO flwr 2024-04-06 17:48:53,460 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:48:53,461 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:49:02,250 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:49:08,663 | server.py:125 | fit progress: (6, 1.7063109874725342, {'accuracy': 0.7667, 'data_size': 10000}, 88.15143859299133)
INFO flwr 2024-04-06 17:49:08,663 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:49:08,664 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:49:17,143 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:49:24,280 | server.py:125 | fit progress: (7, 1.704349160194397, {'accuracy': 0.7689, 'data_size': 10000}, 103.76858610199997)
INFO flwr 2024-04-06 17:49:24,280 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:49:24,281 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:49:33,117 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:49:40,836 | server.py:125 | fit progress: (8, 1.6546193361282349, {'accuracy': 0.8007, 'data_size': 10000}, 120.3240294319985)
INFO flwr 2024-04-06 17:49:40,836 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:49:40,836 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:49:49,491 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 17:49:58,430 | server.py:125 | fit progress: (9, 1.5953104496002197, {'accuracy': 0.8767, 'data_size': 10000}, 137.91864943198743)
INFO flwr 2024-04-06 17:49:58,431 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 17:49:58,431 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:50:06,916 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 17:50:16,986 | server.py:125 | fit progress: (10, 1.6023448705673218, {'accuracy': 0.8683, 'data_size': 10000}, 156.47451594501035)
INFO flwr 2024-04-06 17:50:16,986 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 17:50:16,987 | server.py:153 | FL finished in 156.4750016129983
INFO flwr 2024-04-06 17:50:16,987 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 17:50:16,987 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 17:50:16,987 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 17:50:16,987 | app.py:229 | app_fit: losses_centralized [(0, 2.3026280403137207), (1, 2.292447090148926), (2, 2.1489920616149902), (3, 2.003685235977173), (4, 1.882967233657837), (5, 1.7780749797821045), (6, 1.7063109874725342), (7, 1.704349160194397), (8, 1.6546193361282349), (9, 1.5953104496002197), (10, 1.6023448705673218)]
INFO flwr 2024-04-06 17:50:16,987 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1043), (1, 0.1217), (2, 0.2916), (3, 0.4631), (4, 0.5942), (5, 0.694), (6, 0.7667), (7, 0.7689), (8, 0.8007), (9, 0.8767), (10, 0.8683)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8683
wandb:     loss 1.60234
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_174719-2fzkc1no
wandb: Find logs at: ./wandb/offline-run-20240406_174719-2fzkc1no/logs
INFO flwr 2024-04-06 17:50:20,600 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:57:29,418 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1262513)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1262513)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 17:57:34,498	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:57:34,881	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:57:35,267	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3ee831019b13f346.zip' (9.94MiB) to Ray cluster...
2024-04-06 17:57:35,300	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3ee831019b13f346.zip'.
INFO flwr 2024-04-06 17:57:46,622 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 132431563367.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 61042098585.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 17:57:46,622 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:57:46,622 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:57:46,639 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:57:46,640 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:57:46,640 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:57:46,640 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 17:57:48,865 | server.py:94 | initial parameters (loss, other metrics): 2.3028018474578857, {'accuracy': 0.1106, 'data_size': 10000}
INFO flwr 2024-04-06 17:57:48,866 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:57:48,866 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1270808)[0m 2024-04-06 17:57:52.846708: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1270808)[0m 2024-04-06 17:57:52.956306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1270808)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1270811)[0m 2024-04-06 17:57:55.638899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1270812)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1270812)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1270814)[0m 2024-04-06 17:57:53.149115: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1270814)[0m 2024-04-06 17:57:53.225263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1270814)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1270809)[0m 2024-04-06 17:57:56.011325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 17:58:08,813 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:58:09,320 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:58:10,596 | server.py:125 | fit progress: (1, 2.302791118621826, {'accuracy': 0.1116, 'data_size': 10000}, 21.729749269987224)
INFO flwr 2024-04-06 17:58:10,596 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:58:10,596 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:58:19,752 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:58:22,328 | server.py:125 | fit progress: (2, 2.302779197692871, {'accuracy': 0.1123, 'data_size': 10000}, 33.46138407298713)
INFO flwr 2024-04-06 17:58:22,328 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:58:22,328 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:58:30,672 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:58:34,342 | server.py:125 | fit progress: (3, 2.302764415740967, {'accuracy': 0.1119, 'data_size': 10000}, 45.47565631297766)
INFO flwr 2024-04-06 17:58:34,342 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:58:34,342 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:58:43,458 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:58:47,828 | server.py:125 | fit progress: (4, 2.302753210067749, {'accuracy': 0.1132, 'data_size': 10000}, 58.96136778697837)
INFO flwr 2024-04-06 17:58:47,828 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:58:47,828 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:58:56,376 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:59:01,817 | server.py:125 | fit progress: (5, 2.3027396202087402, {'accuracy': 0.1135, 'data_size': 10000}, 72.95125520799775)
INFO flwr 2024-04-06 17:59:01,818 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:59:01,818 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:59:10,252 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:59:17,253 | server.py:125 | fit progress: (6, 2.3027257919311523, {'accuracy': 0.1142, 'data_size': 10000}, 88.38725854200311)
INFO flwr 2024-04-06 17:59:17,254 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:59:17,254 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:59:25,852 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:59:33,779 | server.py:125 | fit progress: (7, 2.302710771560669, {'accuracy': 0.1142, 'data_size': 10000}, 104.91285643499577)
INFO flwr 2024-04-06 17:59:33,779 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:59:33,780 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:59:42,637 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:59:51,195 | server.py:125 | fit progress: (8, 2.302694082260132, {'accuracy': 0.113, 'data_size': 10000}, 122.3290105490014)
INFO flwr 2024-04-06 17:59:51,196 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:59:51,196 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:59:59,402 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:00:09,892 | server.py:125 | fit progress: (9, 2.302678108215332, {'accuracy': 0.1111, 'data_size': 10000}, 141.0260950269876)
INFO flwr 2024-04-06 18:00:09,893 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:00:09,893 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:00:20,284 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:00:32,091 | server.py:125 | fit progress: (10, 2.3026647567749023, {'accuracy': 0.1115, 'data_size': 10000}, 163.22459216599236)
INFO flwr 2024-04-06 18:00:32,091 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:00:32,091 | server.py:153 | FL finished in 163.2251293399895
INFO flwr 2024-04-06 18:00:32,092 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:00:32,092 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:00:32,092 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:00:32,092 | app.py:229 | app_fit: losses_centralized [(0, 2.3028018474578857), (1, 2.302791118621826), (2, 2.302779197692871), (3, 2.302764415740967), (4, 2.302753210067749), (5, 2.3027396202087402), (6, 2.3027257919311523), (7, 2.302710771560669), (8, 2.302694082260132), (9, 2.302678108215332), (10, 2.3026647567749023)]
INFO flwr 2024-04-06 18:00:32,092 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1106), (1, 0.1116), (2, 0.1123), (3, 0.1119), (4, 0.1132), (5, 0.1135), (6, 0.1142), (7, 0.1142), (8, 0.113), (9, 0.1111), (10, 0.1115)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1115
wandb:     loss 2.30266
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_175729-zom3szea
wandb: Find logs at: ./wandb/offline-run-20240406_175729-zom3szea/logs
INFO flwr 2024-04-06 18:00:35,713 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:07:43,357 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1270808)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1270808)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 18:07:48,989	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:07:49,382	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:07:49,741	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8eae6b20f4751d40.zip' (9.96MiB) to Ray cluster...
2024-04-06 18:07:49,764	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8eae6b20f4751d40.zip'.
INFO flwr 2024-04-06 18:08:00,529 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 132300128052.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 60985769164.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 18:08:00,530 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:08:00,530 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:08:00,546 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:08:00,546 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:08:00,547 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:08:00,547 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 18:08:04,097 | server.py:94 | initial parameters (loss, other metrics): 2.30238676071167, {'accuracy': 0.124, 'data_size': 10000}
INFO flwr 2024-04-06 18:08:04,103 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:08:04,103 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1275561)[0m 2024-04-06 18:08:06.551827: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1275561)[0m 2024-04-06 18:08:06.646626: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1275561)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1275561)[0m 2024-04-06 18:08:08.723946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1275561)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1275561)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1275562)[0m 2024-04-06 18:08:06.704599: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1275562)[0m 2024-04-06 18:08:06.799809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1275562)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1275564)[0m 2024-04-06 18:08:08.903918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 18:08:21,605 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:08:22,113 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:08:23,385 | server.py:125 | fit progress: (1, 2.3004612922668457, {'accuracy': 0.1049, 'data_size': 10000}, 19.28128912000102)
INFO flwr 2024-04-06 18:08:23,385 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:08:23,385 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:08:32,502 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:08:34,891 | server.py:125 | fit progress: (2, 2.2932703495025635, {'accuracy': 0.1028, 'data_size': 10000}, 30.787832662987057)
INFO flwr 2024-04-06 18:08:34,891 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:08:34,892 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:08:43,906 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 18:08:47,405 | server.py:125 | fit progress: (3, 2.2800214290618896, {'accuracy': 0.1102, 'data_size': 10000}, 43.30173280098825)
INFO flwr 2024-04-06 18:08:47,405 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 18:08:47,406 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:08:55,550 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 18:08:59,643 | server.py:125 | fit progress: (4, 2.2648143768310547, {'accuracy': 0.2555, 'data_size': 10000}, 55.5395165119844)
INFO flwr 2024-04-06 18:08:59,643 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 18:08:59,643 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:09:08,568 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 18:09:13,623 | server.py:125 | fit progress: (5, 2.209829568862915, {'accuracy': 0.2121, 'data_size': 10000}, 69.51968812997802)
INFO flwr 2024-04-06 18:09:13,623 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 18:09:13,623 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:09:22,026 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:09:28,175 | server.py:125 | fit progress: (6, 2.1079723834991455, {'accuracy': 0.3333, 'data_size': 10000}, 84.07135424599983)
INFO flwr 2024-04-06 18:09:28,175 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:09:28,175 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:09:36,384 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:09:43,437 | server.py:125 | fit progress: (7, 1.9745427370071411, {'accuracy': 0.5199, 'data_size': 10000}, 99.33387410198338)
INFO flwr 2024-04-06 18:09:43,437 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:09:43,438 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:09:52,373 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:10:00,646 | server.py:125 | fit progress: (8, 1.8845049142837524, {'accuracy': 0.6038, 'data_size': 10000}, 116.54229730798397)
INFO flwr 2024-04-06 18:10:00,646 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:10:00,646 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:10:09,710 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:10:19,248 | server.py:125 | fit progress: (9, 1.8322163820266724, {'accuracy': 0.6457, 'data_size': 10000}, 135.14498952799477)
INFO flwr 2024-04-06 18:10:19,249 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:10:19,249 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:10:28,812 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:10:39,236 | server.py:125 | fit progress: (10, 1.7593036890029907, {'accuracy': 0.7259, 'data_size': 10000}, 155.13272457697894)
INFO flwr 2024-04-06 18:10:39,237 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:10:39,237 | server.py:153 | FL finished in 155.13347747098305
INFO flwr 2024-04-06 18:10:39,237 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:10:39,237 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:10:39,237 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:10:39,237 | app.py:229 | app_fit: losses_centralized [(0, 2.30238676071167), (1, 2.3004612922668457), (2, 2.2932703495025635), (3, 2.2800214290618896), (4, 2.2648143768310547), (5, 2.209829568862915), (6, 2.1079723834991455), (7, 1.9745427370071411), (8, 1.8845049142837524), (9, 1.8322163820266724), (10, 1.7593036890029907)]
INFO flwr 2024-04-06 18:10:39,237 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.124), (1, 0.1049), (2, 0.1028), (3, 0.1102), (4, 0.2555), (5, 0.2121), (6, 0.3333), (7, 0.5199), (8, 0.6038), (9, 0.6457), (10, 0.7259)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7259
wandb:     loss 1.7593
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_180742-wafc33dz
wandb: Find logs at: ./wandb/offline-run-20240406_180742-wafc33dz/logs
INFO flwr 2024-04-06 18:10:42,851 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:17:51,890 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1275557)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1275557)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 18:17:57,715	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:18:01,015	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:18:01,339	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_04bc3fe58228fe9f.zip' (9.98MiB) to Ray cluster...
2024-04-06 18:18:01,369	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_04bc3fe58228fe9f.zip'.
INFO flwr 2024-04-06 18:18:12,404 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 127193077146.0, 'CPU': 64.0, 'object_store_memory': 58797033062.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 18:18:12,404 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:18:12,405 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:18:12,420 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:18:12,421 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:18:12,421 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:18:12,421 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 18:18:14,963 | server.py:94 | initial parameters (loss, other metrics): 2.3025810718536377, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-06 18:18:14,964 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:18:14,964 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1282997)[0m 2024-04-06 18:18:18.413495: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1282997)[0m 2024-04-06 18:18:18.512640: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1282997)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1282997)[0m 2024-04-06 18:18:20.773074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1282996)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1282996)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1282996)[0m 2024-04-06 18:18:19.028757: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1282991)[0m 2024-04-06 18:18:19.026413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1282991)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1282991)[0m 2024-04-06 18:18:21.266746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 18:18:33,892 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:18:34,421 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:18:35,784 | server.py:125 | fit progress: (1, 2.2999284267425537, {'accuracy': 0.269, 'data_size': 10000}, 20.820361951977247)
INFO flwr 2024-04-06 18:18:35,785 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:18:35,785 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:18:44,969 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:18:47,701 | server.py:125 | fit progress: (2, 2.2677392959594727, {'accuracy': 0.1865, 'data_size': 10000}, 32.73680247698212)
INFO flwr 2024-04-06 18:18:47,701 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:18:47,701 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:18:56,445 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 18:19:00,587 | server.py:125 | fit progress: (3, 2.177382469177246, {'accuracy': 0.3308, 'data_size': 10000}, 45.623031105991686)
INFO flwr 2024-04-06 18:19:00,587 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 18:19:00,587 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:19:09,716 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 18:19:14,213 | server.py:125 | fit progress: (4, 2.0393006801605225, {'accuracy': 0.492, 'data_size': 10000}, 59.24883071897784)
INFO flwr 2024-04-06 18:19:14,213 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 18:19:14,213 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:19:22,962 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 18:19:28,329 | server.py:125 | fit progress: (5, 1.853838324546814, {'accuracy': 0.6329, 'data_size': 10000}, 73.36498847199255)
INFO flwr 2024-04-06 18:19:28,329 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 18:19:28,329 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:19:36,765 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:19:46,063 | server.py:125 | fit progress: (6, 1.8187775611877441, {'accuracy': 0.657, 'data_size': 10000}, 91.09956694499124)
INFO flwr 2024-04-06 18:19:46,064 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:19:46,064 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:19:55,146 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:20:02,948 | server.py:125 | fit progress: (7, 1.7480548620224, {'accuracy': 0.7342, 'data_size': 10000}, 107.98433456599014)
INFO flwr 2024-04-06 18:20:02,948 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:20:02,949 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:20:12,055 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:20:21,038 | server.py:125 | fit progress: (8, 1.7305045127868652, {'accuracy': 0.7472, 'data_size': 10000}, 126.07416366197867)
INFO flwr 2024-04-06 18:20:21,038 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:20:21,039 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:20:30,498 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:20:41,141 | server.py:125 | fit progress: (9, 1.7041441202163696, {'accuracy': 0.7723, 'data_size': 10000}, 146.1772052309825)
INFO flwr 2024-04-06 18:20:41,141 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:20:41,142 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:20:50,346 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:21:03,213 | server.py:125 | fit progress: (10, 1.6831085681915283, {'accuracy': 0.7878, 'data_size': 10000}, 168.24910281499615)
INFO flwr 2024-04-06 18:21:03,213 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:21:03,213 | server.py:153 | FL finished in 168.2495564359997
INFO flwr 2024-04-06 18:21:03,214 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:21:03,214 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:21:03,214 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:21:03,214 | app.py:229 | app_fit: losses_centralized [(0, 2.3025810718536377), (1, 2.2999284267425537), (2, 2.2677392959594727), (3, 2.177382469177246), (4, 2.0393006801605225), (5, 1.853838324546814), (6, 1.8187775611877441), (7, 1.7480548620224), (8, 1.7305045127868652), (9, 1.7041441202163696), (10, 1.6831085681915283)]
INFO flwr 2024-04-06 18:21:03,214 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.269), (2, 0.1865), (3, 0.3308), (4, 0.492), (5, 0.6329), (6, 0.657), (7, 0.7342), (8, 0.7472), (9, 0.7723), (10, 0.7878)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7878
wandb:     loss 1.68311
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_181751-s0w20wfn
wandb: Find logs at: ./wandb/offline-run-20240406_181751-s0w20wfn/logs
INFO flwr 2024-04-06 18:21:06,871 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:28:13,908 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1282991)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1282991)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 18:28:19,745	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:28:20,131	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:28:20,434	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5febcab3fe30bd4b.zip' (10.00MiB) to Ray cluster...
2024-04-06 18:28:20,456	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5febcab3fe30bd4b.zip'.
INFO flwr 2024-04-06 18:28:31,389 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 137241133671.0, 'node:__internal_head__': 1.0, 'object_store_memory': 63103343001.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 18:28:31,390 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:28:31,390 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:28:31,406 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:28:31,407 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:28:31,407 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:28:31,407 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 18:28:34,110 | server.py:94 | initial parameters (loss, other metrics): 2.3024919033050537, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-06 18:28:34,111 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:28:34,111 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1287540)[0m 2024-04-06 18:28:37.358023: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1287540)[0m 2024-04-06 18:28:37.464239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1287540)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1287540)[0m 2024-04-06 18:28:39.494942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1287547)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1287547)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1287527)[0m 2024-04-06 18:28:37.429052: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1287527)[0m 2024-04-06 18:28:37.522381: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1287527)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1287543)[0m 2024-04-06 18:28:39.631622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 18:28:52,345 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:28:52,866 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:28:54,120 | server.py:125 | fit progress: (1, 2.2946670055389404, {'accuracy': 0.2459, 'data_size': 10000}, 20.008781891985564)
INFO flwr 2024-04-06 18:28:54,120 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:28:54,120 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:29:03,412 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:29:05,762 | server.py:125 | fit progress: (2, 2.2270054817199707, {'accuracy': 0.2261, 'data_size': 10000}, 31.6511649589811)
INFO flwr 2024-04-06 18:29:05,763 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:29:05,763 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:29:13,935 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 18:29:17,364 | server.py:125 | fit progress: (3, 1.9856586456298828, {'accuracy': 0.5049, 'data_size': 10000}, 43.2532240569999)
INFO flwr 2024-04-06 18:29:17,365 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 18:29:17,365 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:29:25,850 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 18:29:30,083 | server.py:125 | fit progress: (4, 1.7347091436386108, {'accuracy': 0.7624, 'data_size': 10000}, 55.97219503999804)
INFO flwr 2024-04-06 18:29:30,084 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 18:29:30,084 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:29:38,902 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 18:29:44,640 | server.py:125 | fit progress: (5, 1.6463290452957153, {'accuracy': 0.8404, 'data_size': 10000}, 70.52927095600171)
INFO flwr 2024-04-06 18:29:44,641 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 18:29:44,641 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:29:52,927 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:29:59,312 | server.py:125 | fit progress: (6, 1.6191859245300293, {'accuracy': 0.8542, 'data_size': 10000}, 85.20110905499314)
INFO flwr 2024-04-06 18:29:59,313 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:29:59,313 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:30:07,676 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:30:14,871 | server.py:125 | fit progress: (7, 1.6027424335479736, {'accuracy': 0.8718, 'data_size': 10000}, 100.760018105997)
INFO flwr 2024-04-06 18:30:14,871 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:30:14,872 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:30:23,446 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:30:31,580 | server.py:125 | fit progress: (8, 1.5809262990951538, {'accuracy': 0.8889, 'data_size': 10000}, 117.4684243069787)
INFO flwr 2024-04-06 18:30:31,580 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:30:31,580 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:30:41,091 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:30:50,613 | server.py:125 | fit progress: (9, 1.5747109651565552, {'accuracy': 0.8957, 'data_size': 10000}, 136.50203317898558)
INFO flwr 2024-04-06 18:30:50,614 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:30:50,614 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:30:59,486 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:31:10,446 | server.py:125 | fit progress: (10, 1.5993361473083496, {'accuracy': 0.8668, 'data_size': 10000}, 156.3352259649837)
INFO flwr 2024-04-06 18:31:10,447 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:31:10,447 | server.py:153 | FL finished in 156.33565381899825
INFO flwr 2024-04-06 18:31:10,447 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:31:10,447 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:31:10,447 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:31:10,447 | app.py:229 | app_fit: losses_centralized [(0, 2.3024919033050537), (1, 2.2946670055389404), (2, 2.2270054817199707), (3, 1.9856586456298828), (4, 1.7347091436386108), (5, 1.6463290452957153), (6, 1.6191859245300293), (7, 1.6027424335479736), (8, 1.5809262990951538), (9, 1.5747109651565552), (10, 1.5993361473083496)]
INFO flwr 2024-04-06 18:31:10,447 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.2459), (2, 0.2261), (3, 0.5049), (4, 0.7624), (5, 0.8404), (6, 0.8542), (7, 0.8718), (8, 0.8889), (9, 0.8957), (10, 0.8668)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8668
wandb:     loss 1.59934
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_182813-i6lfvnyo
wandb: Find logs at: ./wandb/offline-run-20240406_182813-i6lfvnyo/logs
INFO flwr 2024-04-06 18:31:14,037 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:38:22,427 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1287527)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1287527)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 18:38:30,613	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:38:31,038	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:38:31,344	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a38694cdf3392ab1.zip' (10.02MiB) to Ray cluster...
2024-04-06 18:38:31,368	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a38694cdf3392ab1.zip'.
INFO flwr 2024-04-06 18:38:42,326 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 117690858087.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54724653465.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 18:38:42,327 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:38:42,327 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:38:42,341 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:38:42,342 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:38:42,342 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:38:42,342 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 18:38:45,590 | server.py:94 | initial parameters (loss, other metrics): 2.302422046661377, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-06 18:38:45,591 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:38:45,595 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1295310)[0m 2024-04-06 18:38:48.373871: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1295308)[0m 2024-04-06 18:38:48.451195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1295308)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1295310)[0m 2024-04-06 18:38:50.615090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1295305)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1295305)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1295302)[0m 2024-04-06 18:38:48.532601: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1295302)[0m 2024-04-06 18:38:48.625244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1295302)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1295305)[0m 2024-04-06 18:38:50.615259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1295302)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1295302)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(DefaultActor pid=1295307)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1295307)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
DEBUG flwr 2024-04-06 18:39:09,219 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:39:09,733 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:39:11,035 | server.py:125 | fit progress: (1, 2.28942608833313, {'accuracy': 0.266, 'data_size': 10000}, 25.443711762985913)
INFO flwr 2024-04-06 18:39:11,036 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:39:11,036 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:39:20,107 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:39:22,498 | server.py:125 | fit progress: (2, 2.12037992477417, {'accuracy': 0.3526, 'data_size': 10000}, 36.90606072600349)
INFO flwr 2024-04-06 18:39:22,498 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:39:22,498 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:39:30,526 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 18:39:33,936 | server.py:125 | fit progress: (3, 1.971299171447754, {'accuracy': 0.473, 'data_size': 10000}, 48.344241466984386)
INFO flwr 2024-04-06 18:39:33,936 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 18:39:33,936 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:39:42,137 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 18:39:47,020 | server.py:125 | fit progress: (4, 1.7529947757720947, {'accuracy': 0.7386, 'data_size': 10000}, 61.42895952000981)
INFO flwr 2024-04-06 18:39:47,021 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 18:39:47,021 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:39:55,378 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 18:40:00,948 | server.py:125 | fit progress: (5, 1.7065361738204956, {'accuracy': 0.7654, 'data_size': 10000}, 75.35637944701011)
INFO flwr 2024-04-06 18:40:00,948 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 18:40:00,948 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:40:08,929 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:40:15,438 | server.py:125 | fit progress: (6, 1.6271717548370361, {'accuracy': 0.8503, 'data_size': 10000}, 89.84651956200833)
INFO flwr 2024-04-06 18:40:15,438 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:40:15,438 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:40:24,583 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:40:32,465 | server.py:125 | fit progress: (7, 1.590964674949646, {'accuracy': 0.8803, 'data_size': 10000}, 106.87381042799097)
INFO flwr 2024-04-06 18:40:32,466 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:40:32,466 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:40:41,268 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:40:50,010 | server.py:125 | fit progress: (8, 1.5781207084655762, {'accuracy': 0.8884, 'data_size': 10000}, 124.41807253399747)
INFO flwr 2024-04-06 18:40:50,010 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:40:50,010 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:40:58,999 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:41:08,952 | server.py:125 | fit progress: (9, 1.5803667306900024, {'accuracy': 0.8874, 'data_size': 10000}, 143.3601022239891)
INFO flwr 2024-04-06 18:41:08,952 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:41:08,952 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:41:18,024 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:41:29,234 | server.py:125 | fit progress: (10, 1.5785409212112427, {'accuracy': 0.8867, 'data_size': 10000}, 163.6425946939853)
INFO flwr 2024-04-06 18:41:29,235 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:41:29,235 | server.py:153 | FL finished in 163.6438539949886
INFO flwr 2024-04-06 18:41:29,236 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:41:29,236 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:41:29,236 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:41:29,236 | app.py:229 | app_fit: losses_centralized [(0, 2.302422046661377), (1, 2.28942608833313), (2, 2.12037992477417), (3, 1.971299171447754), (4, 1.7529947757720947), (5, 1.7065361738204956), (6, 1.6271717548370361), (7, 1.590964674949646), (8, 1.5781207084655762), (9, 1.5803667306900024), (10, 1.5785409212112427)]
INFO flwr 2024-04-06 18:41:29,236 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.266), (2, 0.3526), (3, 0.473), (4, 0.7386), (5, 0.7654), (6, 0.8503), (7, 0.8803), (8, 0.8884), (9, 0.8874), (10, 0.8867)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8867
wandb:     loss 1.57854
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_183822-jr2l0xc1
wandb: Find logs at: ./wandb/offline-run-20240406_183822-jr2l0xc1/logs
INFO flwr 2024-04-06 18:41:32,846 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:48:43,575 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1295297)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1295297)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
2024-04-06 18:48:48,345	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:48:49,180	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:48:49,535	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4312322d0c071fb7.zip' (10.04MiB) to Ray cluster...
2024-04-06 18:48:49,566	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4312322d0c071fb7.zip'.
INFO flwr 2024-04-06 18:49:00,685 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 133111430964.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 61333470412.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 18:49:00,686 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:49:00,686 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:49:00,705 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:49:00,706 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:49:00,707 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:49:00,707 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 18:49:02,872 | server.py:94 | initial parameters (loss, other metrics): 2.3023722171783447, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-06 18:49:02,873 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:49:02,873 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1302693)[0m 2024-04-06 18:49:06.346710: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1302693)[0m 2024-04-06 18:49:06.455080: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1302693)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1302693)[0m 2024-04-06 18:49:08.683994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1302691)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1302691)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1302692)[0m 2024-04-06 18:49:07.446057: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1302692)[0m 2024-04-06 18:49:07.539516: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1302692)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1302692)[0m 2024-04-06 18:49:09.812142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 18:49:22,040 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:49:22,609 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:49:24,081 | server.py:125 | fit progress: (1, 2.2640740871429443, {'accuracy': 0.1902, 'data_size': 10000}, 21.207956391008338)
INFO flwr 2024-04-06 18:49:24,081 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:49:24,082 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:49:34,452 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:49:37,127 | server.py:125 | fit progress: (2, 1.9533771276474, {'accuracy': 0.5706, 'data_size': 10000}, 34.25359749500058)
INFO flwr 2024-04-06 18:49:37,127 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:49:37,127 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:49:46,476 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 18:49:50,136 | server.py:125 | fit progress: (3, 1.7442045211791992, {'accuracy': 0.7549, 'data_size': 10000}, 47.26263482199283)
INFO flwr 2024-04-06 18:49:50,136 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 18:49:50,136 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:49:59,342 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 18:50:03,870 | server.py:125 | fit progress: (4, 1.6603871583938599, {'accuracy': 0.822, 'data_size': 10000}, 60.99731249798788)
INFO flwr 2024-04-06 18:50:03,871 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 18:50:03,871 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:50:13,853 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 18:50:19,240 | server.py:125 | fit progress: (5, 1.5989187955856323, {'accuracy': 0.8709, 'data_size': 10000}, 76.3666013579932)
INFO flwr 2024-04-06 18:50:19,240 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 18:50:19,240 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:50:28,657 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:50:35,511 | server.py:125 | fit progress: (6, 1.5865405797958374, {'accuracy': 0.8826, 'data_size': 10000}, 92.63831117600785)
INFO flwr 2024-04-06 18:50:35,512 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:50:35,512 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:50:45,144 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:50:52,938 | server.py:125 | fit progress: (7, 1.5861289501190186, {'accuracy': 0.8822, 'data_size': 10000}, 110.06544545799261)
INFO flwr 2024-04-06 18:50:52,939 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:50:52,939 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:51:02,206 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:51:10,608 | server.py:125 | fit progress: (8, 1.5688729286193848, {'accuracy': 0.8949, 'data_size': 10000}, 127.73530059101176)
INFO flwr 2024-04-06 18:51:10,609 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:51:10,609 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:51:19,927 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:51:30,795 | server.py:125 | fit progress: (9, 1.5637887716293335, {'accuracy': 0.8997, 'data_size': 10000}, 147.92153207201045)
INFO flwr 2024-04-06 18:51:30,795 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:51:30,795 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:51:40,079 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:51:51,113 | server.py:125 | fit progress: (10, 1.5565831661224365, {'accuracy': 0.9096, 'data_size': 10000}, 168.2399308239983)
INFO flwr 2024-04-06 18:51:51,114 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:51:51,114 | server.py:153 | FL finished in 168.2408751460025
INFO flwr 2024-04-06 18:51:51,114 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:51:51,114 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:51:51,114 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:51:51,114 | app.py:229 | app_fit: losses_centralized [(0, 2.3023722171783447), (1, 2.2640740871429443), (2, 1.9533771276474), (3, 1.7442045211791992), (4, 1.6603871583938599), (5, 1.5989187955856323), (6, 1.5865405797958374), (7, 1.5861289501190186), (8, 1.5688729286193848), (9, 1.5637887716293335), (10, 1.5565831661224365)]
INFO flwr 2024-04-06 18:51:51,115 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.1902), (2, 0.5706), (3, 0.7549), (4, 0.822), (5, 0.8709), (6, 0.8826), (7, 0.8822), (8, 0.8949), (9, 0.8997), (10, 0.9096)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9096
wandb:     loss 1.55658
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_184843-d9npie0m
wandb: Find logs at: ./wandb/offline-run-20240406_184843-d9npie0m/logs
INFO flwr 2024-04-06 18:51:54,759 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:59:03,359 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1302684)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1302684)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 18:59:09,039	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:59:09,470	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:59:09,786	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4ccfb12b8ca66847.zip' (10.06MiB) to Ray cluster...
2024-04-06 18:59:09,811	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4ccfb12b8ca66847.zip'.
INFO flwr 2024-04-06 18:59:20,744 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 136168935629.0, 'object_store_memory': 62643829555.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 18:59:20,744 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:59:20,745 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:59:20,767 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:59:20,768 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:59:20,769 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:59:20,770 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 18:59:22,937 | server.py:94 | initial parameters (loss, other metrics): 2.302631139755249, {'accuracy': 0.1017, 'data_size': 10000}
INFO flwr 2024-04-06 18:59:22,938 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:59:22,938 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1307543)[0m 2024-04-06 18:59:26.838181: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1307539)[0m 2024-04-06 18:59:26.977337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1307539)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1307539)[0m 2024-04-06 18:59:29.144920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1307534)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1307534)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1307542)[0m 2024-04-06 18:59:27.041506: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1307542)[0m 2024-04-06 18:59:27.132671: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1307542)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1307533)[0m 2024-04-06 18:59:29.141786: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 18:59:42,141 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:59:42,669 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:59:44,049 | server.py:125 | fit progress: (1, 2.2529633045196533, {'accuracy': 0.1933, 'data_size': 10000}, 21.11125615297351)
INFO flwr 2024-04-06 18:59:44,050 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:59:44,050 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:59:53,545 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:59:55,979 | server.py:125 | fit progress: (2, 1.952471137046814, {'accuracy': 0.5246, 'data_size': 10000}, 33.04106538198539)
INFO flwr 2024-04-06 18:59:55,980 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:59:55,980 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:00:05,056 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:00:08,716 | server.py:125 | fit progress: (3, 1.8007574081420898, {'accuracy': 0.6601, 'data_size': 10000}, 45.77750543298316)
INFO flwr 2024-04-06 19:00:08,716 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:00:08,716 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:00:17,212 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:00:21,737 | server.py:125 | fit progress: (4, 1.6648967266082764, {'accuracy': 0.7993, 'data_size': 10000}, 58.79861982399598)
INFO flwr 2024-04-06 19:00:21,737 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:00:21,737 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:00:31,546 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:00:37,332 | server.py:125 | fit progress: (5, 1.611831545829773, {'accuracy': 0.8561, 'data_size': 10000}, 74.39351208199514)
INFO flwr 2024-04-06 19:00:37,332 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:00:37,332 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:00:46,661 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:00:53,077 | server.py:125 | fit progress: (6, 1.6115671396255493, {'accuracy': 0.8546, 'data_size': 10000}, 90.13918528598151)
INFO flwr 2024-04-06 19:00:53,078 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:00:53,079 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:01:02,285 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:01:09,548 | server.py:125 | fit progress: (7, 1.616715669631958, {'accuracy': 0.847, 'data_size': 10000}, 106.60996724298457)
INFO flwr 2024-04-06 19:01:09,548 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:01:09,549 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:01:19,125 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:01:28,487 | server.py:125 | fit progress: (8, 1.5798801183700562, {'accuracy': 0.8859, 'data_size': 10000}, 125.54868741697283)
INFO flwr 2024-04-06 19:01:28,487 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:01:28,488 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:01:38,110 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:01:47,480 | server.py:125 | fit progress: (9, 1.5573149919509888, {'accuracy': 0.9069, 'data_size': 10000}, 144.54188117699232)
INFO flwr 2024-04-06 19:01:47,480 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:01:47,481 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:01:57,046 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:02:07,181 | server.py:125 | fit progress: (10, 1.5499296188354492, {'accuracy': 0.9137, 'data_size': 10000}, 164.2424047109962)
INFO flwr 2024-04-06 19:02:07,181 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:02:07,181 | server.py:153 | FL finished in 164.24295266298577
INFO flwr 2024-04-06 19:02:07,181 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:02:07,181 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:02:07,182 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:02:07,182 | app.py:229 | app_fit: losses_centralized [(0, 2.302631139755249), (1, 2.2529633045196533), (2, 1.952471137046814), (3, 1.8007574081420898), (4, 1.6648967266082764), (5, 1.611831545829773), (6, 1.6115671396255493), (7, 1.616715669631958), (8, 1.5798801183700562), (9, 1.5573149919509888), (10, 1.5499296188354492)]
INFO flwr 2024-04-06 19:02:07,182 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1017), (1, 0.1933), (2, 0.5246), (3, 0.6601), (4, 0.7993), (5, 0.8561), (6, 0.8546), (7, 0.847), (8, 0.8859), (9, 0.9069), (10, 0.9137)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9137
wandb:     loss 1.54993
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_185903-ylzy96kn
wandb: Find logs at: ./wandb/offline-run-20240406_185903-ylzy96kn/logs
INFO flwr 2024-04-06 19:02:10,468 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:09:22,011 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1307533)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1307533)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 19:09:26,757	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:09:27,186	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:09:27,664	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_20dd42c95c0b0a42.zip' (10.08MiB) to Ray cluster...
2024-04-06 19:09:27,690	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_20dd42c95c0b0a42.zip'.
INFO flwr 2024-04-06 19:09:38,764 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 133977161933.0, 'node:__internal_head__': 1.0, 'object_store_memory': 61704497971.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 19:09:38,764 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:09:38,765 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:09:38,790 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:09:38,791 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:09:38,791 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:09:38,791 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 19:09:42,546 | server.py:94 | initial parameters (loss, other metrics): 2.30244779586792, {'accuracy': 0.0995, 'data_size': 10000}
INFO flwr 2024-04-06 19:09:42,547 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:09:42,547 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1315013)[0m 2024-04-06 19:09:44.615727: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1315013)[0m 2024-04-06 19:09:44.712250: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1315013)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1315012)[0m 2024-04-06 19:09:46.896735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1315009)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1315009)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1315003)[0m 2024-04-06 19:09:45.028163: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1315003)[0m 2024-04-06 19:09:45.126096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1315003)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1315011)[0m 2024-04-06 19:09:47.131172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 19:10:01,223 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:10:01,764 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:10:03,064 | server.py:125 | fit progress: (1, 2.3024163246154785, {'accuracy': 0.0998, 'data_size': 10000}, 20.516713933000574)
INFO flwr 2024-04-06 19:10:03,064 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:10:03,064 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:10:12,425 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:10:14,944 | server.py:125 | fit progress: (2, 2.302384376525879, {'accuracy': 0.0992, 'data_size': 10000}, 32.3966459420044)
INFO flwr 2024-04-06 19:10:14,944 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:10:14,944 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:10:23,717 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:10:27,431 | server.py:125 | fit progress: (3, 2.302358388900757, {'accuracy': 0.1076, 'data_size': 10000}, 44.883982586994534)
INFO flwr 2024-04-06 19:10:27,431 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:10:27,432 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:10:36,537 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:10:40,632 | server.py:125 | fit progress: (4, 2.3023364543914795, {'accuracy': 0.1064, 'data_size': 10000}, 58.084707716014236)
INFO flwr 2024-04-06 19:10:40,632 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:10:40,632 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:10:50,325 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:10:55,908 | server.py:125 | fit progress: (5, 2.3022985458374023, {'accuracy': 0.1028, 'data_size': 10000}, 73.36124763698899)
INFO flwr 2024-04-06 19:10:55,909 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:10:55,909 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:11:05,482 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:11:12,455 | server.py:125 | fit progress: (6, 2.3022751808166504, {'accuracy': 0.1273, 'data_size': 10000}, 89.90823641599854)
INFO flwr 2024-04-06 19:11:12,456 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:11:12,456 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:11:23,332 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:11:30,440 | server.py:125 | fit progress: (7, 2.30224609375, {'accuracy': 0.1298, 'data_size': 10000}, 107.89327038600459)
INFO flwr 2024-04-06 19:11:30,441 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:11:30,441 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:11:39,270 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:11:46,712 | server.py:125 | fit progress: (8, 2.3022141456604004, {'accuracy': 0.1289, 'data_size': 10000}, 124.16537367599085)
INFO flwr 2024-04-06 19:11:46,713 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:11:46,713 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:11:56,121 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:12:04,842 | server.py:125 | fit progress: (9, 2.3021767139434814, {'accuracy': 0.1256, 'data_size': 10000}, 142.2949677889992)
INFO flwr 2024-04-06 19:12:04,842 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:12:04,843 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:12:13,629 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:12:23,046 | server.py:125 | fit progress: (10, 2.30214524269104, {'accuracy': 0.1199, 'data_size': 10000}, 160.4984248999972)
INFO flwr 2024-04-06 19:12:23,046 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:12:23,046 | server.py:153 | FL finished in 160.49890181899536
INFO flwr 2024-04-06 19:12:23,046 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:12:23,046 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:12:23,046 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:12:23,046 | app.py:229 | app_fit: losses_centralized [(0, 2.30244779586792), (1, 2.3024163246154785), (2, 2.302384376525879), (3, 2.302358388900757), (4, 2.3023364543914795), (5, 2.3022985458374023), (6, 2.3022751808166504), (7, 2.30224609375), (8, 2.3022141456604004), (9, 2.3021767139434814), (10, 2.30214524269104)]
INFO flwr 2024-04-06 19:12:23,047 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0995), (1, 0.0998), (2, 0.0992), (3, 0.1076), (4, 0.1064), (5, 0.1028), (6, 0.1273), (7, 0.1298), (8, 0.1289), (9, 0.1256), (10, 0.1199)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1199
wandb:     loss 2.30215
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_190921-sefd8gka
wandb: Find logs at: ./wandb/offline-run-20240406_190921-sefd8gka/logs
INFO flwr 2024-04-06 19:12:26,683 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:19:33,287 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1315003)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1315003)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 19:19:39,101	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:19:39,400	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:19:39,706	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_02787adeedf182e7.zip' (10.10MiB) to Ray cluster...
2024-04-06 19:19:39,729	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_02787adeedf182e7.zip'.
INFO flwr 2024-04-06 19:19:50,460 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 133264545178.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 61399090790.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 19:19:50,461 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:19:50,461 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:19:50,485 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:19:50,487 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:19:50,487 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:19:50,487 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 19:19:53,326 | server.py:94 | initial parameters (loss, other metrics): 2.3024680614471436, {'accuracy': 0.0967, 'data_size': 10000}
INFO flwr 2024-04-06 19:19:53,326 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:19:53,327 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1319593)[0m 2024-04-06 19:19:56.246313: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1319593)[0m 2024-04-06 19:19:56.349698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1319593)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1319594)[0m 2024-04-06 19:19:58.580115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1319593)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1319593)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1319592)[0m 2024-04-06 19:19:56.786339: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1319592)[0m 2024-04-06 19:19:56.881651: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1319592)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1319597)[0m 2024-04-06 19:19:59.069532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 19:20:11,805 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:20:12,342 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:20:13,656 | server.py:125 | fit progress: (1, 2.295659065246582, {'accuracy': 0.1135, 'data_size': 10000}, 20.32980425399728)
INFO flwr 2024-04-06 19:20:13,657 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:20:13,657 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:20:23,524 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:20:25,939 | server.py:125 | fit progress: (2, 2.2043297290802, {'accuracy': 0.299, 'data_size': 10000}, 32.612564056995325)
INFO flwr 2024-04-06 19:20:25,939 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:20:25,939 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:20:34,972 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:20:38,358 | server.py:125 | fit progress: (3, 2.102449893951416, {'accuracy': 0.3388, 'data_size': 10000}, 45.031395705998875)
INFO flwr 2024-04-06 19:20:38,358 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:20:38,358 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:20:47,402 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:20:51,697 | server.py:125 | fit progress: (4, 1.8949207067489624, {'accuracy': 0.5873, 'data_size': 10000}, 58.37084272300126)
INFO flwr 2024-04-06 19:20:51,698 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:20:51,698 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:21:01,273 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:21:06,402 | server.py:125 | fit progress: (5, 1.7531744241714478, {'accuracy': 0.7414, 'data_size': 10000}, 73.07521398199606)
INFO flwr 2024-04-06 19:21:06,402 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:21:06,402 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:21:15,941 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:21:22,677 | server.py:125 | fit progress: (6, 1.6964783668518066, {'accuracy': 0.7833, 'data_size': 10000}, 89.35075350498664)
INFO flwr 2024-04-06 19:21:22,678 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:21:22,678 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:21:33,685 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:21:40,982 | server.py:125 | fit progress: (7, 1.6721103191375732, {'accuracy': 0.7997, 'data_size': 10000}, 107.65556203998858)
INFO flwr 2024-04-06 19:21:40,982 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:21:40,982 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:21:49,720 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:21:57,477 | server.py:125 | fit progress: (8, 1.669281005859375, {'accuracy': 0.8012, 'data_size': 10000}, 124.15044528298313)
INFO flwr 2024-04-06 19:21:57,477 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:21:57,478 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:22:07,202 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:22:15,895 | server.py:125 | fit progress: (9, 1.6592280864715576, {'accuracy': 0.8072, 'data_size': 10000}, 142.56828273300198)
INFO flwr 2024-04-06 19:22:15,895 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:22:15,895 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:22:25,096 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:22:34,215 | server.py:125 | fit progress: (10, 1.654665231704712, {'accuracy': 0.8113, 'data_size': 10000}, 160.88883478398202)
INFO flwr 2024-04-06 19:22:34,216 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:22:34,216 | server.py:153 | FL finished in 160.88920161497663
INFO flwr 2024-04-06 19:22:34,216 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:22:34,216 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:22:34,216 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:22:34,216 | app.py:229 | app_fit: losses_centralized [(0, 2.3024680614471436), (1, 2.295659065246582), (2, 2.2043297290802), (3, 2.102449893951416), (4, 1.8949207067489624), (5, 1.7531744241714478), (6, 1.6964783668518066), (7, 1.6721103191375732), (8, 1.669281005859375), (9, 1.6592280864715576), (10, 1.654665231704712)]
INFO flwr 2024-04-06 19:22:34,216 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0967), (1, 0.1135), (2, 0.299), (3, 0.3388), (4, 0.5873), (5, 0.7414), (6, 0.7833), (7, 0.7997), (8, 0.8012), (9, 0.8072), (10, 0.8113)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8113
wandb:     loss 1.65467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_191932-r7zq7bh5
wandb: Find logs at: ./wandb/offline-run-20240406_191932-r7zq7bh5/logs
INFO flwr 2024-04-06 19:22:37,820 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:29:45,372 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1319585)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1319585)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 19:29:51,387	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:29:51,744	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:29:52,154	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b1d0f13376298091.zip' (10.12MiB) to Ray cluster...
2024-04-06 19:29:52,176	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b1d0f13376298091.zip'.
INFO flwr 2024-04-06 19:30:03,153 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 127134666548.0, 'CPU': 64.0, 'object_store_memory': 58771999948.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 19:30:03,154 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:30:03,154 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:30:03,178 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:30:03,179 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:30:03,179 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:30:03,179 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 19:30:05,189 | server.py:94 | initial parameters (loss, other metrics): 2.302729606628418, {'accuracy': 0.1182, 'data_size': 10000}
INFO flwr 2024-04-06 19:30:05,190 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:30:05,190 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1327495)[0m 2024-04-06 19:30:08.799791: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1327495)[0m 2024-04-06 19:30:08.921851: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1327495)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1327495)[0m 2024-04-06 19:30:11.496779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1327495)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1327495)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1327493)[0m 2024-04-06 19:30:09.669885: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1327493)[0m 2024-04-06 19:30:09.772866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1327493)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1327501)[0m 2024-04-06 19:30:12.058350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 19:30:25,454 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:30:25,987 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:30:27,253 | server.py:125 | fit progress: (1, 2.290621280670166, {'accuracy': 0.1032, 'data_size': 10000}, 22.063051439006813)
INFO flwr 2024-04-06 19:30:27,253 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:30:27,254 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:30:37,386 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:30:39,812 | server.py:125 | fit progress: (2, 2.16304874420166, {'accuracy': 0.2516, 'data_size': 10000}, 34.62211880000541)
INFO flwr 2024-04-06 19:30:39,813 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:30:39,813 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:30:48,881 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:30:52,389 | server.py:125 | fit progress: (3, 1.9541525840759277, {'accuracy': 0.5193, 'data_size': 10000}, 47.198553393012844)
INFO flwr 2024-04-06 19:30:52,389 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:30:52,389 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:31:01,888 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:31:06,974 | server.py:125 | fit progress: (4, 1.8452037572860718, {'accuracy': 0.6125, 'data_size': 10000}, 61.78432760600117)
INFO flwr 2024-04-06 19:31:06,975 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:31:06,975 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:31:16,883 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:31:21,856 | server.py:125 | fit progress: (5, 1.6349927186965942, {'accuracy': 0.8492, 'data_size': 10000}, 76.66572458602604)
INFO flwr 2024-04-06 19:31:21,856 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:31:21,856 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:31:31,582 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:31:37,663 | server.py:125 | fit progress: (6, 1.5979660749435425, {'accuracy': 0.8737, 'data_size': 10000}, 92.47273839800619)
INFO flwr 2024-04-06 19:31:37,663 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:31:37,663 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:31:46,720 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:31:53,367 | server.py:125 | fit progress: (7, 1.5808221101760864, {'accuracy': 0.8866, 'data_size': 10000}, 108.17706661502598)
INFO flwr 2024-04-06 19:31:53,367 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:31:53,368 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:32:02,412 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:32:10,234 | server.py:125 | fit progress: (8, 1.5808950662612915, {'accuracy': 0.8856, 'data_size': 10000}, 125.0436124560074)
INFO flwr 2024-04-06 19:32:10,234 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:32:10,234 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:32:19,698 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:32:28,031 | server.py:125 | fit progress: (9, 1.566467046737671, {'accuracy': 0.8989, 'data_size': 10000}, 142.84137793802074)
INFO flwr 2024-04-06 19:32:28,032 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:32:28,032 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:32:37,223 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:32:46,693 | server.py:125 | fit progress: (10, 1.5727068185806274, {'accuracy': 0.8924, 'data_size': 10000}, 161.50278260101913)
INFO flwr 2024-04-06 19:32:46,693 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:32:46,693 | server.py:153 | FL finished in 161.50317113602068
INFO flwr 2024-04-06 19:32:46,693 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:32:46,694 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:32:46,694 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:32:46,694 | app.py:229 | app_fit: losses_centralized [(0, 2.302729606628418), (1, 2.290621280670166), (2, 2.16304874420166), (3, 1.9541525840759277), (4, 1.8452037572860718), (5, 1.6349927186965942), (6, 1.5979660749435425), (7, 1.5808221101760864), (8, 1.5808950662612915), (9, 1.566467046737671), (10, 1.5727068185806274)]
INFO flwr 2024-04-06 19:32:46,694 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1182), (1, 0.1032), (2, 0.2516), (3, 0.5193), (4, 0.6125), (5, 0.8492), (6, 0.8737), (7, 0.8866), (8, 0.8856), (9, 0.8989), (10, 0.8924)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8924
wandb:     loss 1.57271
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_192944-n0qpcwne
wandb: Find logs at: ./wandb/offline-run-20240406_192944-n0qpcwne/logs
INFO flwr 2024-04-06 19:32:50,268 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:39:57,168 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1327493)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1327493)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 19:40:01,716	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:40:02,137	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:40:02,566	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_97084865b292c1c5.zip' (10.13MiB) to Ray cluster...
2024-04-06 19:40:02,591	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_97084865b292c1c5.zip'.
INFO flwr 2024-04-06 19:40:13,283 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 137878615450.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63376549478.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 19:40:13,283 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:40:13,283 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:40:13,301 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:40:13,302 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:40:13,302 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:40:13,303 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 19:40:15,856 | server.py:94 | initial parameters (loss, other metrics): 2.302356719970703, {'accuracy': 0.1118, 'data_size': 10000}
INFO flwr 2024-04-06 19:40:15,857 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:40:15,858 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1331869)[0m 2024-04-06 19:40:18.929707: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1331869)[0m 2024-04-06 19:40:19.028002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1331869)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1331858)[0m 2024-04-06 19:40:21.201906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1331866)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1331866)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1331867)[0m 2024-04-06 19:40:19.914109: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1331867)[0m 2024-04-06 19:40:20.000923: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1331867)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1331867)[0m 2024-04-06 19:40:22.146055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 19:40:34,375 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:40:34,866 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:40:36,113 | server.py:125 | fit progress: (1, 2.271505355834961, {'accuracy': 0.218, 'data_size': 10000}, 20.255863680009497)
INFO flwr 2024-04-06 19:40:36,114 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:40:36,114 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:40:45,545 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:40:47,977 | server.py:125 | fit progress: (2, 2.1156036853790283, {'accuracy': 0.366, 'data_size': 10000}, 32.1191187229997)
INFO flwr 2024-04-06 19:40:47,977 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:40:47,977 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:40:56,822 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:41:00,611 | server.py:125 | fit progress: (3, 1.8745179176330566, {'accuracy': 0.5777, 'data_size': 10000}, 44.75350729198544)
INFO flwr 2024-04-06 19:41:00,611 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:41:00,611 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:41:09,416 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:41:13,505 | server.py:125 | fit progress: (4, 1.6118913888931274, {'accuracy': 0.8678, 'data_size': 10000}, 57.64731849299278)
INFO flwr 2024-04-06 19:41:13,505 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:41:13,505 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:41:23,158 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:41:28,319 | server.py:125 | fit progress: (5, 1.5962705612182617, {'accuracy': 0.8753, 'data_size': 10000}, 72.46175387798576)
INFO flwr 2024-04-06 19:41:28,319 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:41:28,320 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:41:37,781 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:41:44,879 | server.py:125 | fit progress: (6, 1.5839699506759644, {'accuracy': 0.8854, 'data_size': 10000}, 89.02158850501291)
INFO flwr 2024-04-06 19:41:44,879 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:41:44,879 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:41:53,572 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:42:00,420 | server.py:125 | fit progress: (7, 1.5784645080566406, {'accuracy': 0.8879, 'data_size': 10000}, 104.56292395800119)
INFO flwr 2024-04-06 19:42:00,421 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:42:00,421 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:42:09,342 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:42:16,946 | server.py:125 | fit progress: (8, 1.5593405961990356, {'accuracy': 0.9049, 'data_size': 10000}, 121.08827731799101)
INFO flwr 2024-04-06 19:42:16,946 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:42:16,946 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:42:25,769 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:42:34,737 | server.py:125 | fit progress: (9, 1.5534553527832031, {'accuracy': 0.9118, 'data_size': 10000}, 138.8795264159853)
INFO flwr 2024-04-06 19:42:34,737 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:42:34,737 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:42:44,045 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:42:53,503 | server.py:125 | fit progress: (10, 1.5597656965255737, {'accuracy': 0.9038, 'data_size': 10000}, 157.6452026150073)
INFO flwr 2024-04-06 19:42:53,503 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:42:53,503 | server.py:153 | FL finished in 157.6456143569958
INFO flwr 2024-04-06 19:42:53,503 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:42:53,503 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:42:53,503 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:42:53,504 | app.py:229 | app_fit: losses_centralized [(0, 2.302356719970703), (1, 2.271505355834961), (2, 2.1156036853790283), (3, 1.8745179176330566), (4, 1.6118913888931274), (5, 1.5962705612182617), (6, 1.5839699506759644), (7, 1.5784645080566406), (8, 1.5593405961990356), (9, 1.5534553527832031), (10, 1.5597656965255737)]
INFO flwr 2024-04-06 19:42:53,504 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1118), (1, 0.218), (2, 0.366), (3, 0.5777), (4, 0.8678), (5, 0.8753), (6, 0.8854), (7, 0.8879), (8, 0.9049), (9, 0.9118), (10, 0.9038)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9038
wandb:     loss 1.55977
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_193956-3ojxfdf7
wandb: Find logs at: ./wandb/offline-run-20240406_193956-3ojxfdf7/logs
INFO flwr 2024-04-06 19:42:57,073 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:50:05,289 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1331855)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1331855)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 19:50:10,634	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:50:11,186	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:50:11,672	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_513c1a0c749f3930.zip' (10.16MiB) to Ray cluster...
2024-04-06 19:50:11,704	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_513c1a0c749f3930.zip'.
INFO flwr 2024-04-06 19:50:22,681 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 124804845773.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 57773505331.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 19:50:22,681 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:50:22,681 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:50:22,702 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:50:22,703 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:50:22,703 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:50:22,703 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 19:50:25,864 | server.py:94 | initial parameters (loss, other metrics): 2.302572011947632, {'accuracy': 0.1388, 'data_size': 10000}
INFO flwr 2024-04-06 19:50:25,864 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:50:25,865 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1339567)[0m 2024-04-06 19:50:28.045684: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1339567)[0m 2024-04-06 19:50:28.140886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1339567)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1339567)[0m 2024-04-06 19:50:30.906023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1339567)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1339567)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1339562)[0m 2024-04-06 19:50:29.438114: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1339562)[0m 2024-04-06 19:50:29.514418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1339562)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1339562)[0m 2024-04-06 19:50:31.808958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 19:50:46,335 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:50:46,869 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:50:48,142 | server.py:125 | fit progress: (1, 2.2394790649414062, {'accuracy': 0.2344, 'data_size': 10000}, 22.27714172200649)
INFO flwr 2024-04-06 19:50:48,142 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:50:48,142 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:50:58,070 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:51:00,482 | server.py:125 | fit progress: (2, 1.975833773612976, {'accuracy': 0.4679, 'data_size': 10000}, 34.61697740398813)
INFO flwr 2024-04-06 19:51:00,482 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:51:00,482 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:51:09,704 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:51:13,212 | server.py:125 | fit progress: (3, 1.7608646154403687, {'accuracy': 0.7045, 'data_size': 10000}, 47.3471580450132)
INFO flwr 2024-04-06 19:51:13,212 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:51:13,212 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:51:22,068 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:51:26,226 | server.py:125 | fit progress: (4, 1.6612114906311035, {'accuracy': 0.8, 'data_size': 10000}, 60.36128057099995)
INFO flwr 2024-04-06 19:51:26,226 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:51:26,226 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:51:36,378 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:51:41,635 | server.py:125 | fit progress: (5, 1.5757759809494019, {'accuracy': 0.8969, 'data_size': 10000}, 75.7700618589879)
INFO flwr 2024-04-06 19:51:41,635 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:51:41,635 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:51:50,716 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:51:57,077 | server.py:125 | fit progress: (6, 1.585537314414978, {'accuracy': 0.8802, 'data_size': 10000}, 91.21195705898572)
INFO flwr 2024-04-06 19:51:57,077 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:51:57,077 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:52:06,209 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:52:13,146 | server.py:125 | fit progress: (7, 1.5535130500793457, {'accuracy': 0.9126, 'data_size': 10000}, 107.281876086985)
INFO flwr 2024-04-06 19:52:13,147 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:52:13,147 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:52:21,926 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:52:29,350 | server.py:125 | fit progress: (8, 1.5546926259994507, {'accuracy': 0.9095, 'data_size': 10000}, 123.48588252300397)
INFO flwr 2024-04-06 19:52:29,351 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:52:29,351 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:52:38,175 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:52:46,791 | server.py:125 | fit progress: (9, 1.5443536043167114, {'accuracy': 0.9212, 'data_size': 10000}, 140.92668243701337)
INFO flwr 2024-04-06 19:52:46,792 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:52:46,792 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:52:55,891 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:53:05,069 | server.py:125 | fit progress: (10, 1.5495274066925049, {'accuracy': 0.9147, 'data_size': 10000}, 159.20392884500325)
INFO flwr 2024-04-06 19:53:05,069 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:53:05,069 | server.py:153 | FL finished in 159.20437514901278
INFO flwr 2024-04-06 19:53:05,069 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:53:05,069 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:53:05,069 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:53:05,069 | app.py:229 | app_fit: losses_centralized [(0, 2.302572011947632), (1, 2.2394790649414062), (2, 1.975833773612976), (3, 1.7608646154403687), (4, 1.6612114906311035), (5, 1.5757759809494019), (6, 1.585537314414978), (7, 1.5535130500793457), (8, 1.5546926259994507), (9, 1.5443536043167114), (10, 1.5495274066925049)]
INFO flwr 2024-04-06 19:53:05,070 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1388), (1, 0.2344), (2, 0.4679), (3, 0.7045), (4, 0.8), (5, 0.8969), (6, 0.8802), (7, 0.9126), (8, 0.9095), (9, 0.9212), (10, 0.9147)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9147
wandb:     loss 1.54953
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_195004-yzr53xbc
wandb: Find logs at: ./wandb/offline-run-20240406_195004-yzr53xbc/logs
INFO flwr 2024-04-06 19:53:08,752 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:00:15,783 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1339561)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1339561)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 20:00:22,878	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:00:23,295	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:00:23,711	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0ff9678c5e30b26b.zip' (10.18MiB) to Ray cluster...
2024-04-06 20:00:23,742	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0ff9678c5e30b26b.zip'.
INFO flwr 2024-04-06 20:00:34,694 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'object_store_memory': 60636669542.0, 'node:10.20.240.18': 1.0, 'memory': 131485562266.0}
INFO flwr 2024-04-06 20:00:34,694 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:00:34,694 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:00:34,712 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:00:34,713 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:00:34,713 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:00:34,713 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 20:00:37,461 | server.py:94 | initial parameters (loss, other metrics): 2.302633762359619, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-06 20:00:37,461 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:00:37,461 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1347307)[0m 2024-04-06 20:00:40.404278: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1347307)[0m 2024-04-06 20:00:40.503025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1347307)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1347310)[0m 2024-04-06 20:00:43.684993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1347310)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1347310)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1347305)[0m 2024-04-06 20:00:41.375841: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1347305)[0m 2024-04-06 20:00:41.452944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1347305)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1347306)[0m 2024-04-06 20:00:44.000533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 20:00:57,065 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:00:57,529 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:00:58,839 | server.py:125 | fit progress: (1, 2.2168471813201904, {'accuracy': 0.2102, 'data_size': 10000}, 21.377446183993015)
INFO flwr 2024-04-06 20:00:58,839 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:00:58,839 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:01:09,448 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:01:12,049 | server.py:125 | fit progress: (2, 1.992409348487854, {'accuracy': 0.5024, 'data_size': 10000}, 34.587684275989886)
INFO flwr 2024-04-06 20:01:12,049 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:01:12,049 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:01:20,862 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:01:24,311 | server.py:125 | fit progress: (3, 1.7018706798553467, {'accuracy': 0.7833, 'data_size': 10000}, 46.849768257001415)
INFO flwr 2024-04-06 20:01:24,311 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:01:24,312 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:01:33,550 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:01:37,652 | server.py:125 | fit progress: (4, 1.5764390230178833, {'accuracy': 0.8927, 'data_size': 10000}, 60.1904202309961)
INFO flwr 2024-04-06 20:01:37,652 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:01:37,652 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:01:46,742 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:01:51,665 | server.py:125 | fit progress: (5, 1.5919513702392578, {'accuracy': 0.8763, 'data_size': 10000}, 74.20335632498609)
INFO flwr 2024-04-06 20:01:51,665 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:01:51,665 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:02:01,238 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:02:07,310 | server.py:125 | fit progress: (6, 1.5608680248260498, {'accuracy': 0.9055, 'data_size': 10000}, 89.84857004898367)
INFO flwr 2024-04-06 20:02:07,310 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:02:07,310 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:02:16,611 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:02:23,455 | server.py:125 | fit progress: (7, 1.5510499477386475, {'accuracy': 0.9128, 'data_size': 10000}, 105.99357905198121)
INFO flwr 2024-04-06 20:02:23,455 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:02:23,455 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:02:32,550 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:02:39,987 | server.py:125 | fit progress: (8, 1.5505294799804688, {'accuracy': 0.9136, 'data_size': 10000}, 122.52589066300425)
INFO flwr 2024-04-06 20:02:39,987 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:02:39,988 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:02:48,734 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:02:57,390 | server.py:125 | fit progress: (9, 1.5480432510375977, {'accuracy': 0.916, 'data_size': 10000}, 139.92922956700204)
INFO flwr 2024-04-06 20:02:57,391 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:02:57,391 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:03:06,428 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:03:15,805 | server.py:125 | fit progress: (10, 1.551071286201477, {'accuracy': 0.9126, 'data_size': 10000}, 158.3433315169823)
INFO flwr 2024-04-06 20:03:15,805 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:03:15,805 | server.py:153 | FL finished in 158.3437396469817
INFO flwr 2024-04-06 20:03:15,805 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:03:15,805 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:03:15,805 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:03:15,805 | app.py:229 | app_fit: losses_centralized [(0, 2.302633762359619), (1, 2.2168471813201904), (2, 1.992409348487854), (3, 1.7018706798553467), (4, 1.5764390230178833), (5, 1.5919513702392578), (6, 1.5608680248260498), (7, 1.5510499477386475), (8, 1.5505294799804688), (9, 1.5480432510375977), (10, 1.551071286201477)]
INFO flwr 2024-04-06 20:03:15,805 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.2102), (2, 0.5024), (3, 0.7833), (4, 0.8927), (5, 0.8763), (6, 0.9055), (7, 0.9128), (8, 0.9136), (9, 0.916), (10, 0.9126)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9126
wandb:     loss 1.55107
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_200015-wd21wbsf
wandb: Find logs at: ./wandb/offline-run-20240406_200015-wd21wbsf/logs
INFO flwr 2024-04-06 20:03:19,399 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: 2NN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=200, bias=True)
			    (1): ReLU()
			    (2): Linear(in_features=200, out_features=200, bias=True)
			    (3): ReLU()
			    (4): Linear(in_features=200, out_features=200, bias=True)
			    (5): ReLU()
			    (6): Linear(in_features=200, out_features=10, bias=True)
			    (7): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:10:26,093 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1347305)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1347305)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 20:10:30,579	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:10:30,937	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:10:31,233	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_24b7d0c06408aaaf.zip' (10.20MiB) to Ray cluster...
2024-04-06 20:10:31,256	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_24b7d0c06408aaaf.zip'.
INFO flwr 2024-04-06 20:10:42,183 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 138008336180.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63432144076.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 20:10:42,183 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:10:42,184 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:10:42,197 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:10:42,198 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:10:42,198 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:10:42,198 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 20:10:45,252 | server.py:94 | initial parameters (loss, other metrics): 2.3025975227355957, {'accuracy': 0.096, 'data_size': 10000}
INFO flwr 2024-04-06 20:10:45,253 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:10:45,254 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1351893)[0m 2024-04-06 20:10:48.010818: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1351893)[0m 2024-04-06 20:10:48.107101: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1351893)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1351884)[0m 2024-04-06 20:10:50.146271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1351890)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1351890)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1351885)[0m 2024-04-06 20:10:48.320753: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1351885)[0m 2024-04-06 20:10:48.422748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1351885)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1351885)[0m 2024-04-06 20:10:50.442543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 20:11:03,319 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:11:03,846 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:11:05,118 | server.py:125 | fit progress: (1, 2.181483507156372, {'accuracy': 0.3237, 'data_size': 10000}, 19.86408487299923)
INFO flwr 2024-04-06 20:11:05,118 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:11:05,118 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:11:14,120 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:11:16,519 | server.py:125 | fit progress: (2, 1.830032229423523, {'accuracy': 0.6531, 'data_size': 10000}, 31.265268555027433)
INFO flwr 2024-04-06 20:11:16,519 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:11:16,519 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:11:25,626 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:11:29,587 | server.py:125 | fit progress: (3, 1.6905525922775269, {'accuracy': 0.7767, 'data_size': 10000}, 44.33367017400451)
INFO flwr 2024-04-06 20:11:29,588 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:11:29,588 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:11:40,080 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:11:44,555 | server.py:125 | fit progress: (4, 1.668562889099121, {'accuracy': 0.7986, 'data_size': 10000}, 59.30197168001905)
INFO flwr 2024-04-06 20:11:44,556 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:11:44,556 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:11:54,207 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:11:59,203 | server.py:125 | fit progress: (5, 1.6641616821289062, {'accuracy': 0.8015, 'data_size': 10000}, 73.94982628701837)
INFO flwr 2024-04-06 20:11:59,204 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:11:59,204 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:12:08,619 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:12:14,756 | server.py:125 | fit progress: (6, 1.628922700881958, {'accuracy': 0.8319, 'data_size': 10000}, 89.5027604700008)
INFO flwr 2024-04-06 20:12:14,756 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:12:14,757 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:12:23,692 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:12:30,446 | server.py:125 | fit progress: (7, 1.5652143955230713, {'accuracy': 0.8992, 'data_size': 10000}, 105.19255564000923)
INFO flwr 2024-04-06 20:12:30,446 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:12:30,446 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:12:39,346 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:12:47,020 | server.py:125 | fit progress: (8, 1.5668684244155884, {'accuracy': 0.8966, 'data_size': 10000}, 121.7666312950023)
INFO flwr 2024-04-06 20:12:47,020 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:12:47,021 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:12:55,605 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:13:03,967 | server.py:125 | fit progress: (9, 1.5431745052337646, {'accuracy': 0.9207, 'data_size': 10000}, 138.71346116502536)
INFO flwr 2024-04-06 20:13:03,967 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:13:03,967 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:13:12,934 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:13:22,673 | server.py:125 | fit progress: (10, 1.5472412109375, {'accuracy': 0.9167, 'data_size': 10000}, 157.4197971530084)
INFO flwr 2024-04-06 20:13:22,674 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:13:22,674 | server.py:153 | FL finished in 157.4202037510113
INFO flwr 2024-04-06 20:13:22,674 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:13:22,674 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:13:22,674 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:13:22,674 | app.py:229 | app_fit: losses_centralized [(0, 2.3025975227355957), (1, 2.181483507156372), (2, 1.830032229423523), (3, 1.6905525922775269), (4, 1.668562889099121), (5, 1.6641616821289062), (6, 1.628922700881958), (7, 1.5652143955230713), (8, 1.5668684244155884), (9, 1.5431745052337646), (10, 1.5472412109375)]
INFO flwr 2024-04-06 20:13:22,674 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.096), (1, 0.3237), (2, 0.6531), (3, 0.7767), (4, 0.7986), (5, 0.8015), (6, 0.8319), (7, 0.8992), (8, 0.8966), (9, 0.9207), (10, 0.9167)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9167
wandb:     loss 1.54724
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_201025-igt5rxjx
wandb: Find logs at: ./wandb/offline-run-20240406_201025-igt5rxjx/logs
[2m[36m(DefaultActor pid=1351885)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1351885)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
