2024-04-02 13:54:58.185670: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2024-04-02 13:54:58.185734: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-02 13:55:21.032811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-02 13:55:21.032806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 11:37:01 2024).
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 11:37:01 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-02 14:03:13,007 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-02 14:03:20,883	INFO worker.py:1621 -- Started a local Ray instance.
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-02 14:03:22,746 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-02 14:03:25,891	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-02 14:03:26,066	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/Logistic Regression/SGD/2024-04-02_12-10.npz is very large (59.89MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/Logistic Regression/SGD/2024-04-02_12-10.npz']})`
2024-04-02 14:03:26,269	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e690b036af5ecac9.zip' (64.17MiB) to Ray cluster...
2024-04-02 14:03:26,442	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-02 14:03:26,519	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e690b036af5ecac9.zip'.
2024-04-02 14:03:26,787	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-02 14:03:27,422	WARNING packaging.py:393 -- File /home/s2240084/conFEDential/.captured/MNIST/Logistic Regression/SGD/2024-04-02_12-10.npz is very large (59.89MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/home/s2240084/conFEDential/.captured/MNIST/Logistic Regression/SGD/2024-04-02_12-10.npz']})`
2024-04-02 14:03:27,732	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_951d87ef86bc03ec.zip' (64.17MiB) to Ray cluster...
2024-04-02 14:03:28,008	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_951d87ef86bc03ec.zip'.
INFO flwr 2024-04-02 14:03:37,788 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 64.0, 'memory': 176780268954.0, 'accelerator_type:TITAN': 1.0, 'object_store_memory': 80048686694.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0}
INFO flwr 2024-04-02 14:03:37,791 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-02 14:03:37,791 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.125}
INFO flwr 2024-04-02 14:03:37,847 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-02 14:03:37,849 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-02 14:03:37,850 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-02 14:03:37,850 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-02 14:03:39,326 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 79887506227.0, 'CPU': 64.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 176404181197.0}
INFO flwr 2024-04-02 14:03:39,326 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-02 14:03:39,327 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.125}
INFO flwr 2024-04-02 14:03:39,356 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-02 14:03:39,357 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-02 14:03:39,358 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-02 14:03:39,358 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-02 14:03:42,743 | server.py:94 | initial parameters (loss, other metrics): 2.302588701248169, {'accuracy': 0.0679, 'data_size': 10000}
INFO flwr 2024-04-02 14:03:42,744 | server.py:104 | FL starting
DEBUG flwr 2024-04-02 14:03:42,744 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1760205)[0m 2024-04-02 14:03:43.604623: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1760205)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1761642)[0m 2024-04-02 14:03:43.935292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1761642)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1760205)[0m 2024-04-02 14:03:46.102130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1761644)[0m 2024-04-02 14:03:46.303541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-02 14:03:49,043 | server.py:94 | initial parameters (loss, other metrics): 2.3025906085968018, {'accuracy': 0.146, 'data_size': 10000}
INFO flwr 2024-04-02 14:03:49,043 | server.py:104 | FL starting
DEBUG flwr 2024-04-02 14:03:49,044 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1760210)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1760210)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1760210)[0m 2024-04-02 14:03:43.756744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1760210)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1760210)[0m 2024-04-02 14:03:46.268584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1761642)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1761642)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1761643)[0m 2024-04-02 14:03:44.101669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1761643)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1761643)[0m 2024-04-02 14:03:46.572948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
[2m[36m(DefaultActor pid=1761642)[0m Could not load library libcudnn_cnn_train.so.8. Error: /deepstore/software/nvidia/cuda-11.x/cudnn-8.8.1.3/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN10cask_cudnn2kr8isAmpereEPNS0_5spa_tERKi, version libcudnn_cnn_infer.so.8
ERROR flwr 2024-04-02 14:04:08,444 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761642, ip=10.20.240.12, actor_id=2f3b6f16d47df3fa22e1770a01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f039690d780>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761642, ip=10.20.240.12, actor_id=2f3b6f16d47df3fa22e1770a01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f039690d780>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 95 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,454 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761642, ip=10.20.240.12, actor_id=2f3b6f16d47df3fa22e1770a01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f039690d780>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761642, ip=10.20.240.12, actor_id=2f3b6f16d47df3fa22e1770a01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f039690d780>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 95 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
DEBUG flwr 2024-04-02 14:04:08,890 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-02 14:04:08,931 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
ERROR flwr 2024-04-02 14:04:08,973 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761644, ip=10.20.240.12, actor_id=a4c578660456d4a2fa3830a001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f949dc09450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761644, ip=10.20.240.12, actor_id=a4c578660456d4a2fa3830a001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f949dc09450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 64 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,974 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761644, ip=10.20.240.12, actor_id=a4c578660456d4a2fa3830a001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f949dc09450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761644, ip=10.20.240.12, actor_id=a4c578660456d4a2fa3830a001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f949dc09450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 64 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
ERROR flwr 2024-04-02 14:04:08,976 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761647, ip=10.20.240.12, actor_id=c715fa41090b3aea335db36001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f164e7716c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761647, ip=10.20.240.12, actor_id=c715fa41090b3aea335db36001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f164e7716c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 21 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,977 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761646, ip=10.20.240.12, actor_id=0c62722abf39728559caae9f01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f1319449630>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761646, ip=10.20.240.12, actor_id=0c62722abf39728559caae9f01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f1319449630>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 80 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,977 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761647, ip=10.20.240.12, actor_id=c715fa41090b3aea335db36001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f164e7716c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761647, ip=10.20.240.12, actor_id=c715fa41090b3aea335db36001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f164e7716c0>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 21 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
ERROR flwr 2024-04-02 14:04:08,978 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761640, ip=10.20.240.12, actor_id=9b472c48284c35dcba32ed7701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9f886bd840>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761640, ip=10.20.240.12, actor_id=9b472c48284c35dcba32ed7701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9f886bd840>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 89 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,978 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761646, ip=10.20.240.12, actor_id=0c62722abf39728559caae9f01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f1319449630>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761646, ip=10.20.240.12, actor_id=0c62722abf39728559caae9f01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f1319449630>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 80 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
ERROR flwr 2024-04-02 14:04:08,980 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761640, ip=10.20.240.12, actor_id=9b472c48284c35dcba32ed7701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9f886bd840>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761640, ip=10.20.240.12, actor_id=9b472c48284c35dcba32ed7701000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f9f886bd840>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 89 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
ERROR flwr 2024-04-02 14:04:08,981 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761643, ip=10.20.240.12, actor_id=266d3ed170501f9819c0744601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f071a789450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761643, ip=10.20.240.12, actor_id=266d3ed170501f9819c0744601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f071a789450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 85 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,982 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761637, ip=10.20.240.12, actor_id=4b0462c7288ca9dbcddb302d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ee987e0d660>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761637, ip=10.20.240.12, actor_id=4b0462c7288ca9dbcddb302d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ee987e0d660>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 92 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,983 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761642, ip=10.20.240.12, actor_id=2f3b6f16d47df3fa22e1770a01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f039690d780>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761642, ip=10.20.240.12, actor_id=2f3b6f16d47df3fa22e1770a01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f039690d780>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,984 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761643, ip=10.20.240.12, actor_id=266d3ed170501f9819c0744601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f071a789450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761643, ip=10.20.240.12, actor_id=266d3ed170501f9819c0744601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f071a789450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 85 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
ERROR flwr 2024-04-02 14:04:08,984 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761637, ip=10.20.240.12, actor_id=4b0462c7288ca9dbcddb302d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ee987e0d660>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761637, ip=10.20.240.12, actor_id=4b0462c7288ca9dbcddb302d01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ee987e0d660>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 92 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
ERROR flwr 2024-04-02 14:04:08,985 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761639, ip=10.20.240.12, actor_id=3136cfc40277d52dde5a22df01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fe627305600>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761639, ip=10.20.240.12, actor_id=3136cfc40277d52dde5a22df01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fe627305600>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 49 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:08,985 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761642, ip=10.20.240.12, actor_id=2f3b6f16d47df3fa22e1770a01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f039690d780>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761642, ip=10.20.240.12, actor_id=2f3b6f16d47df3fa22e1770a01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f039690d780>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
ERROR flwr 2024-04-02 14:04:08,986 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761639, ip=10.20.240.12, actor_id=3136cfc40277d52dde5a22df01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fe627305600>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761639, ip=10.20.240.12, actor_id=3136cfc40277d52dde5a22df01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7fe627305600>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 49 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
ERROR flwr 2024-04-02 14:04:09,579 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=1761644, ip=10.20.240.12, actor_id=a4c578660456d4a2fa3830a001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f949dc09450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761644, ip=10.20.240.12, actor_id=a4c578660456d4a2fa3830a001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f949dc09450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)

ERROR flwr 2024-04-02 14:04:09,580 | ray_client_proxy.py:162 | [36mray::DefaultActor.run()[39m (pid=1761644, ip=10.20.240.12, actor_id=a4c578660456d4a2fa3830a001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f949dc09450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit
    new_parameters, data_size = helper.train(
  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train
    loss.backward()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: GET was unable to find an engine to execute this computation

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=1761644, ip=10.20.240.12, actor_id=a4c578660456d4a2fa3830a001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f949dc09450>)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/s2240084/conFEDential/src/run_simulation.py", line 87, in fit\n    new_parameters, data_size = helper.train(\n  File "/tmp/ray/session_2024-04-02_14-03-22_820531_1755979/runtime_resources/working_dir_files/_ray_pkg_951d87ef86bc03ec/src/training/helper.py", line 60, in train\n    loss.backward()\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward\n    torch.autograd.backward(\n  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: GET was unable to find an engine to execute this computation\n',)
DEBUG flwr 2024-04-02 14:04:09,581 | server.py:236 | fit_round 1 received 0 results and 10 failures
ERROR flwr 2024-04-02 14:04:09,582 | app.py:313 | list index out of range
ERROR flwr 2024-04-02 14:04:09,660 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/server_aggregation_strategies/capture_generator.py", line 81, in aggregate_fit
    self._capture_parameters(captured_parameters)
  File "/home/s2240084/conFEDential/src/server_aggregation_strategies/capture_generator.py", line 35, in _capture_parameters
    captures = self._load_npz_file(captured_parameters)
  File "/home/s2240084/conFEDential/src/server_aggregation_strategies/capture_generator.py", line 61, in _load_npz_file
    list(captured_parameters.values())[0]]
IndexError: list index out of range

ERROR flwr 2024-04-02 14:04:09,660 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
INFO flwr 2024-04-02 14:04:10,494 | server.py:125 | fit progress: (1, 2.302588701248169, {'accuracy': 0.0679, 'data_size': 10000}, 27.75024837598903)
INFO flwr 2024-04-02 14:04:10,495 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-02 14:04:10,495 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
wandb: 
wandb: Run summary:
wandb: accuracy 0.146
wandb:     loss 2.30259
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240402_140322-o2ksmfq7
wandb: Find logs at: ./wandb/offline-run-20240402_140322-o2ksmfq7/logs
DEBUG flwr 2024-04-02 14:04:18,996 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-02 14:04:20,373 | server.py:125 | fit progress: (2, 2.302588701248169, {'accuracy': 0.0679, 'data_size': 10000}, 37.6294118789956)
INFO flwr 2024-04-02 14:04:20,374 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-02 14:04:20,374 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-02 14:04:28,481 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-02 14:04:29,881 | server.py:125 | fit progress: (3, 2.302588701248169, {'accuracy': 0.0679, 'data_size': 10000}, 47.13654854800552)
INFO flwr 2024-04-02 14:04:29,881 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-02 14:04:29,881 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-02 14:04:37,514 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-02 14:04:39,243 | server.py:125 | fit progress: (4, 2.302588701248169, {'accuracy': 0.0679, 'data_size': 10000}, 56.49849176200223)
INFO flwr 2024-04-02 14:04:39,243 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-02 14:04:39,243 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-02 14:04:47,172 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-02 14:04:48,647 | server.py:125 | fit progress: (5, 2.302588701248169, {'accuracy': 0.0679, 'data_size': 10000}, 65.90296000000671)
INFO flwr 2024-04-02 14:04:48,647 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-02 14:04:48,648 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-02 14:04:56,313 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-02 14:04:58,155 | server.py:125 | fit progress: (6, 2.302588701248169, {'accuracy': 0.0679, 'data_size': 10000}, 75.41118279099464)
INFO flwr 2024-04-02 14:04:58,155 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-02 14:04:58,156 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-02 14:05:05,910 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-02 14:05:07,674 | server.py:125 | fit progress: (7, 2.302588701248169, {'accuracy': 0.0679, 'data_size': 10000}, 84.93024298601085)
INFO flwr 2024-04-02 14:05:07,675 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-02 14:05:07,675 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-ctit082: error: *** STEP 278583.1 ON ctit082 CANCELLED AT 2024-04-02T14:05:12 ***
*** SIGTERM received at time=1712059512 on cpu 38 ***
slurmstepd-ctit082: error: *** STEP 278583.0 ON ctit082 CANCELLED AT 2024-04-02T14:05:12 ***
*** SIGTERM received at time=1712059512 on cpu 16 ***
slurmstepd-ctit082: error: *** JOB 278583 ON ctit082 CANCELLED AT 2024-04-02T14:05:12 ***
PC: @     0x7f1652b7399f  (unknown)  poll
    @     0x7f1652aa4090  (unknown)  (unknown)
    @ ... and at least 7 more frames
[2024-04-02 14:05:12,650 E 1755979 1755979] logging.cc:361: *** SIGTERM received at time=1712059512 on cpu 38 ***
[2024-04-02 14:05:12,650 E 1755979 1755979] logging.cc:361: PC: @     0x7f1652b7399f  (unknown)  poll
[2024-04-02 14:05:12,651 E 1755979 1755979] logging.cc:361:     @     0x7f1652aa4090  (unknown)  (unknown)
[2024-04-02 14:05:12,651 E 1755979 1755979] logging.cc:361:     @ ... and at least 7 more frames
Loaded 168 configs, running...
PC: @     0x7f71bf4cd454  (unknown)  do_futex_wait.constprop.0
    @     0x7f71bf930090  (unknown)  (unknown)
    @     0x563c00000000  (unknown)  (unknown)
[2024-04-02 14:05:12,661 E 1755977 1755977] logging.cc:361: *** SIGTERM received at time=1712059512 on cpu 16 ***
[2024-04-02 14:05:12,661 E 1755977 1755977] logging.cc:361: PC: @     0x7f71bf4cd454  (unknown)  do_futex_wait.constprop.0
[2024-04-02 14:05:12,665 E 1755977 1755977] logging.cc:361:     @     0x7f71bf930090  (unknown)  (unknown)
[2024-04-02 14:05:12,670 E 1755977 1755977] logging.cc:361:     @     0x563c00000000  (unknown)  (unknown)
